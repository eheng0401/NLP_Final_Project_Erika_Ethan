text	label
A '''linear transformation''' is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] from one [[Definition:Module|module]] to another.	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\left({V, f}\right)$ be a [[Definition:Bilinear Space|bilinear space]] over $\mathbb K$. Let $v \in V$ be [[Definition:Anisotropic Vector (Bilinear Form)|anisotropic]]. Let $\left\langle{v}\right\rangle$ be its [[Definition:Linear Span|span]]. Let $v^\perp$ be its [[Definition:Orthogonal Complement (Bilinear Form)|orthogonal complement]]. Then $\left({V, f}\right)$ is the [[Definition:Internal Orthogonal Sum (Bilinear Space)|internal orthogonal sum]] of $\left\langle{v}\right\rangle$ and $v^\perp$: :$V = \left\langle{v}\right\rangle \oplus v^\perp$	1
The [[Definition:P-adic Integers|$p$-adic integers]] $\Z_p$ is the [[Definition:Valuation Ring Induced by Non-Archimedean Norm|valuation ring induced]] by $\norm {\,\cdot\,}_p$ by definition. By [[Valuation Ring of Non-Archimedean Division Ring is Clopen|Valuation Ring of Non-Archimedean Division Ring is Clopen]] then the [[Definition:P-adic Integers|$p$-adic integers]] $\Z_p$ is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:P-adic Metric on P-adic Numbers|$p$-adic metric]]. {{qed}} [[Category:Valuation Ring of Non-Archimedean Division Ring is Clopen]] qhjehmabhkaqu8whhsdtkl33zp63q8w	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V_1, V_2, \ldots, V_n$ be [[Definition:Vector Space|$K$-vector spaces]]. Let $\struct {V, + , \circ}_K$ be their [[Definition:Direct Product of Vector Spaces|direct product]]. Then $\struct {V, + , \circ}_K$ is a [[Definition:Vector Space|$K$-vector space]].	1
Let $T = \struct {S, \tau}$ be a [[Definition:Separable Space|separable space]]. Then there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] $H \subseteq S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. That is, $H^- = S$ where $H^-$ is the [[Definition:Closure (Topology)|closure]] of $H$ in $S$. So in $T_{\bar p}^*$, $S\subseteq H^-=S^-\subseteq S^*_p$. Hence $H^- = S$ or $H^- = S \cup \set p = S^*_p$. From [[Topological Closure is Closed]] we deduce that $H^-$ is [[Definition:Closed Set (Topology)|closed]] in $T_{\bar p}^*$. From the definition of [[Definition:Open Extension Topology|open extension topology]], the [[Definition:Closed Set (Topology)|closed sets]] of $T_{\bar p}^*$ are $S^*_p \setminus U$ where $U \in \tau$. Because $\set p \notin \tau$, $S$ is not [[Definition:Closed Set (Topology)|closed]]. Thus it must be that $H^- = S^*_p$. Hence $H$ is a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] $H \subseteq S^*_p$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T^*_{\bar p}$. {{qed|lemma}} {{AimForCont}} $T^*_{\bar p} = \struct {S^*_p, \tau}$ is [[Definition:Separable Space|separable]], but that $T = \struct {S, \tau}$ is not [[Definition:Separable Space|separable]]. Let $H \subseteq S^*_p$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] which is [[Definition:Everywhere Dense|dense]] in $T^*_{\bar p}$. If $H \subseteq S$, then $H^- = S$ in $T$, but this cannot be since $T$ is not [[Definition:Separable Space|separable]]. Then $p \in H$. Consider the subset $V = H \setminus \set p$. From [[Topological Closure is Closed]] we have that $V^-$ is [[Definition:Closed Set (Topology)|closed]] in $T_{\bar p}^*$. From the definition of [[Definition:Open Extension Space|open extension space]], all [[Definition:Non-Empty Set|non-empty]] [[Definition:Closed Set (Topology)|closed sets]] contain the point $p$. So $V \cup \set p = H \subseteq V^-$. From [[Set Closure is Smallest Closed Set in Topological Space]], $S^*_p = H^- \subseteq V^- \implies V$ is dense in $T_{\bar p}^*$. But because $V \subseteq S$, $V$ is [[Definition:Everywhere Dense|dense]] in $S$ also. From [[Subset of Countably Infinite Set is Countable]], $V$ is [[Definition:Countable Set|countable]] as it is a [[Definition:Subset|subset]] of a [[Definition:Countable Set|countable set]]. Thus $T$ is [[Definition:Separable Space|separable]], which is a [[Definition:Contradiction|contradiction]]. Finally, our assumption is false: $T$ is indeed [[Definition:Separable Space|separable]]. {{qed}}	1
Let $\struct {D, +, \times}$ be a [[Definition:Euclidean Domain|Euclidean domain]] whose [[Definition:Ring Zero|zero]] is $0$ and whose [[Definition:Euclidean Valuation|Euclidean valuation]] is $\nu$. We need to show that every [[Definition:Ideal of Ring|ideal]] of $\struct {D, +, \times}$ is a [[Definition:Principal Ideal of Ring|principal ideal]]. Let $U$ be an [[Definition:Ideal of Ring|ideal]] of $\struct {D, +, \times}$ such that $U \ne \set 0$. Let $d \in U$ such that $d \ne 0$ and $\map \nu d$ is as small as possible for elements of $U$. By definition, $\nu$ is defined as $\nu : D \setminus \set 0 \to \N$, so the [[Definition:Codomain of Mapping|codomain]] of $\nu$ is a subset of the [[Definition:Natural Numbers|natural numbers]]. By the [[Well-Ordering Principle]], such an element $d$ exists as an element of the [[Definition:Preimage of Element under Mapping|preimage]] of the [[Definition:Smallest Element|least member]] of the [[Definition:Image of Subset under Mapping|image]] of $U$. <!--{{explain|Need to ensure that the [[Well-Ordering Principle]] can be applied here.}}--> Let $a \in U$. Let us write $a = d q + r$ where either $r = 0$ or $\map \nu r < \map \nu d$. Then $r = a - d q$ and so $r \in U$. Suppose $r \ne 0$. That would mean $\map \nu r < \map \nu d$ contradicting $d$ as the element of $U$ with the smallest $\nu$. So $r = 0$, which means $a = q d$. That is, every element of $U$ is a [[Definition:Divisor of Ring Element|multiple]] of $d$. So $U$ is the [[Definition:Principal Ideal of Ring|principal ideal]] generated by $d$. This deduction holds for all [[Definition:Ideal of Ring|ideals]] of $D$. Hence the result. {{qed}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct{G, +_G, \circ}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Let $\circ’ : G \times R \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: x \circ’ \lambda = \lambda \circ x$ Then $\struct{G, +_G, \circ, \circ’}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +_R, \times_R}$.	1
Let $\Omega \subset \R^{n+k}$ be [[Definition:Open Set of Real Euclidean Space|open]]. Let $f : \Omega \to \R^k$ be [[Definition:Differentiable Vector-Valued Function|differentiable]]. Let the $i$th [[Definition:Partial Derivative of Real-Valued Function|partial derivatives]] of $f$ be [[Definition:Continuous Function|continuous]] in $\Omega$ for $n+1 \leq i \leq n+k$. Let $(a,b) \in \Omega$, with $a\in \R^n$ and $b\in \R^k$. Let $f(a,b) = 0$. For $(x_0,y_0)\in\Omega$, let $D_2 f(x_0,y_0)$ denote the [[Definition:Differential of Vector-Valued Function|differential]] of the function $y\mapsto f(x_0, y)$ at $y_0$. Let the [[Definition:Linear Mapping|linear map]] $D_2 f(a,b)$ be [[Definition:Invertible Linear Mapping|invertible]]. Then there exist [[Definition:Neighborhood|neighborhoods]] $U\subset\Omega$ of $a$ and $V\subset\R^k$ of $b$ such that there exists a unique [[Definition:Function|function]] $g : U \to V$ such that $f(x, g(x)) = 0$ for all $x\in U$. Moreover, $g$ is [[Definition:Differentiable Vector-Valued Function|differentiable]], and its [[Definition:Differential of Vector-Valued Function|differential]] satisfies: :$dg (x) = - \left( (D_2f)(x, g(x)) \right)^{-1} \circ (D_1 f)(x, g(x))$ for all $x\in U$.	1
:$\map { {B_r}^-} x \cap \map { {B_s}^-} y \ne \O \iff \map { {B_r}^-} x \subseteq \map { {B_s}^-} y$ or $\map { {B_s}^-} y \subseteq \map { {B_r}^-} x$	1
Recall that [[Real Numbers form Field]]. The result follows directly from [[Ring of Square Matrices over Field is Ring with Unity]]. {{qed}}	1
By [[P-Norm is Norm]], $\norm {\, \cdot \,}_p$ is a [[Definition:Norm on Vector Space|norm]]. By definition, the [[Definition:Taxicab Norm|taxicab norm]] is $\norm {\, \cdot \,}_1$. Therefore, the [[Definition:Taxicab Norm|taxicab norm]] is a [[Definition:Norm on Vector Space|norm]]. {{qed}}	1
That $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Absolutely Convergent Series|absolutely convergent]] means that $\displaystyle \sum_{n \mathop = 1}^\infty \norm {a_n}$ [[Definition:Convergent Series|converges]] in $\R$. Hence the sequence of [[Definition:Partial Sum|partial sums]] is a [[Definition:Cauchy Sequence|Cauchy sequence]] by [[Convergent Sequence in Normed Vector Space is Cauchy Sequence]]. Now let $\epsilon > 0$. Let $N \in \N$ such that for all $m, n \in \N$, $m \ge n \ge N$ implies that: :$\displaystyle \sum_{k \mathop = n + 1}^m \norm {a_k} = \size {\sum_{k \mathop = 1}^m \norm {a_k} - \sum_{k \mathop = 1}^n \norm {a_k} } < \epsilon$ This $N$ exists because the sequence is [[Definition:Cauchy Sequence|Cauchy]]. Now observe that, for $m \ge n \ge N$, one also has: {{begin-eqn}} {{eqn | l = \norm {\sum_{k \mathop = 1}^m a_k - \sum_{k \mathop = 1}^n a_k} | r = \norm {\sum_{k \mathop = n + 1}^m a_k} }} {{eqn | o = \le | r = \sum_{k \mathop = n + 1}^m \norm {a_k} | c = [[Definition:Norm on Vector Space|Triangle inequality]] for $\norm {\, \cdot \,}$ }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} It follows that the [[Definition:Series|sequence of partial sums]] of $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Cauchy Sequence|Cauchy]]. As $X$ is a [[Definition:Banach Space|Banach space]], this implies that $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ [[Definition:Convergent Series|converges]].	1
{{begin-eqn}} {{eqn | l = \cmod {e^{i z} } | r = \cmod {\map \exp {i R \, \map \exp {i \theta} } } | c = }} {{eqn | r = \cmod {\map \exp {i R \paren {\cos \theta + i \sin \theta} } } | c = }} {{eqn | r = \cmod {\map \exp {R \paren {-\sin \theta + i \cos \theta} } } | c = }} {{eqn | r = \cmod {\map \exp {- R \sin \theta} \, \map \exp {i \cos \theta} } | c = }} {{eqn | r = \map \exp {- R \sin \theta} | c = [[Modulus and Argument of Complex Exponential]] }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Finite Set|finite]] [[Definition:Ring (Abstract Algebra)|ring]] with a [[Definition:Multiplicative Norm on Ring|multiplicative norm]]. Then $R$ is a [[Definition:Field (Abstract Algebra)|field]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $T \in \map {B_0} H$ be a [[Definition:Compact Linear Operator|compact linear operator]], and let $T$ be [[Definition:Idempotent Operator|idempotent]]. Then: :$T \in \map {B_{00} } H$ that is, $T$ is a [[Definition:Bounded Linear Transformation|bounded]] [[Definition:Finite Rank Operator|finite rank operator]].	1
{{begin-eqn}} {{eqn | l = y | r = u \circ x | c = }} {{eqn | ll= \leadstoandfrom | l = x | r = u^{-1} \circ y | c = {{Defof|Unit of Ring}} }} {{end-eqn}} By the definition of [[Definition:Divisor of Ring Element|divisor]]: :$x \divides y$ and $y \divides x$ {{qed|lemma}} Let $x \divides y$ and $y \divides x$. Then $\exists s, t \in D$ such that: :$(1): \quad y = t \circ x$ and: :$(2): \quad x = s \circ y$ If either $x = 0_D$ or $y = 0_D$, then so must be the other (as an [[Definition:Integral Domain|integral domain]] has no [[Definition:Zero Divisor of Ring|zero divisors]] by definition). So $x = 1_D \circ y$ and $y = 1_D \circ x$, and the result holds. Otherwise: {{begin-eqn}} {{eqn | l = 1_D \circ x | r = x | c = {{Defof|Unity of Ring}} }} {{eqn | r = s \circ y | c = from $(2)$ }} {{eqn | r = s \circ \paren {t \circ x} | c = from $(1)$ }} {{eqn | r = \paren {s \circ t} \circ x | c = {{Defof|Associative Operation}} }} {{end-eqn}} So: :$s \circ t = 1_D$ and both $s \in U_D$ and $t \in U_D$. The result follows. {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$. Let $N \in \N$ Let $\sequence {y_n}$ be the [[Definition:Sequence|sequence]] defined by: :$\forall n, y_n = x_{N + n}$ Let $\sequence {y_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $R$. Then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $R$.	1
Let $\mathbf Q$ be an [[Definition:Orthogonal Matrix|orthogonal matrix]]. Then: :$\det \mathbf Q = \pm 1$ where $\det \mathbf Q$ is the [[Definition:Determinant of Matrix|determinant]] of $\mathbf Q$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Then $H$ is a [[Definition:Separable Space|separable space]] iff it has [[Definition:Countable|countable]] [[Definition:Dimension (Hilbert Space)|dimension]].	1
Let: : $z_1 = r_1 \left({\cos \theta_1 + i \sin \theta_1}\right)$ : $z_2 = r_2 \left({\cos \theta_2 + i \sin \theta_2}\right)$ Then: {{begin-eqn}} {{eqn | l = \left\vert{z_1 z_2}\right\vert | r = \left\vert{r_1 \left({\cos \theta_1 + i \sin \theta_1}\right) r_2 \left({\cos \theta_2 + i \sin \theta_2}\right)}\right\vert | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \left\vert{r_1 r_2 \left({\cos \left({\theta_1 + \theta_2}\right) + i \sin \left({\theta_1 + \theta_2}\right)}\right)}\right\vert | c = [[Product of Complex Numbers in Polar Form]] }} {{eqn | r = r_1 r_2 | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \left\vert{z_1}\right\vert \left\vert{z_2}\right\vert | c = {{Defof|Polar Form of Complex Number}} }} {{end-eqn}} {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]] with [[Definition:Ring Zero|zero]] $0_R$. Let $\mathcal O$ be [[Definition:Valuation Ring Induced by Non-Archimedean Norm|valuation ring induced]] by $\norm{\,\cdot\,}$. Then $\mathcal O$ is a both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\norm{\,\cdot\,}$.	1
Let $p$ be [[Definition:Irreducible Element of Ring|irreducible]] in $D$. Let $U_D$ be the [[Definition:Group of Units of Ring|group of units]] of $D$. By definition, an [[Definition:Irreducible Element of Ring|irreducible element]] is not a [[Definition:Unit of Ring|unit]]. So from [[Principal Ideals in Integral Domain]]: :$\ideal p \subset D$ Suppose the [[Definition:Principal Ideal of Ring|principal ideal]] $\ideal p$ is not [[Definition:Maximal Ideal of Ring|maximal]]. Then there exists an [[Definition:Ideal of Ring|ideal]] $K$ of $D$ such that: :$\ideal p \subset K \subset R$ Because $D$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]: :$\exists x \in R: K = \ideal x$ Thus: :$\ideal p \subset \ideal x \subset D$ Because $\ideal p \subset \ideal x$: :$x \divides p$ by [[Principal Ideals in Integral Domain]]. That is: :$\exists t \in D: p = t \circ x$ But $p$ is [[Definition:Irreducible Element of Ring|irreducible]] in $D$, so $x \in U_D$ or $t \in U_D$. That is, either $x$ is a [[Definition:Unit of Ring|unit]] or $x$ is an [[Definition:Associate in Integral Domain|associate]] of $p$. But since $K \subset D$: :$\ideal x \ne D$ so $x \notin U_D$ by [[Principal Ideals in Integral Domain]]. Also, since $\ideal p \subset \ideal x$: :$\ideal p \ne \ideal x$ so $x$ is not an [[Definition:Associate in Integral Domain|associate]] of $p$, by [[Principal Ideals in Integral Domain]]. This contradiction shows that $\ideal p$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$. {{qed}}	1
Let $\mathbf u$ and $\mathbf v$ be [[Definition:Vector (Euclidean Space)|vectors]] of non-zero [[Definition:Vector Length|length]]. Let $\left\Vert{\mathbf u}\right\Vert$ and $\left\Vert{\mathbf v}\right\Vert$ be their respective [[Definition:Vector Length|lengths]]. Then $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ is the [[Definition:Angle Bisector|angle bisector]] of $\mathbf u$ and $\mathbf v$.	1
The supposition that every [[Definition:Vector Space|vector space]] has a [[Definition:Basis of Vector Space|basis]], along with the [[Definition:Zermelo-Fraenkel Axioms|Zermelo-Fraenkel axioms]], implies that the [[Axiom:Axiom of Choice|axiom of choice]] holds.	1
The $n \times n$ [[Definition:Determinant of Matrix|determinant]]: :$D_n = \begin{vmatrix} 1 & -1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 1 & 1 & -1 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 1 & -1 & \cdots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & 1 & 1 & -1 \\ 0 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 \\ \end{vmatrix}$ evaluates to $F_{n + 1}$.	1
For all $n \in \N_{\gt 0}$, let $n \cdot 1_R$ denote the sum of $1_R$ with itself $n$-times. That is: :$n \cdot 1_R = \underbrace {1_R + 1_R + \dots + 1_R}_{n \, times}$ Then: :$\norm {n \cdot 1_R} \le n$.	1
By the definition of a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] then: :$(1): \quad$ there exists a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]] $\phi: R \to R'$. :$(2): \quad \struct {R', \norm {\, \cdot \,}' }$ is a [[Definition:Complete Metric Space|complete metric space]]. :$(3): \quad \phi \sqbrk R$ is a [[Definition:Everywhere Dense|dense]] [[Definition:Topological Subspace|subspace]] in $\struct {R', \norm {\, \cdot \,}' }$. By [[Ring Homomorphism Preserves Subrings/Corollary|image of a ring homomorphism is a subring]] then $\phi \sqbrk R$ is a [[Definition:Subring|subring]] of $R'$ and $\phi: R \to \phi \sqbrk R$ is an [[Definition:Ring Isomorphism|isomorphism]]. By [[Epimorphism from Division Ring to Ring]] then $\phi \sqbrk R$ is a [[Definition:Division Subring|division subring]] of $R'$. By [[Division Subring of Normed Division Ring|Division Subring of Normed Division Ring]] then $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ is a [[Definition:Normed Division Subring|normed division subring]] of $\struct {R', \norm {\, \cdot \,}' }$. By [[Distance-Preserving Surjection is Isometry of Metric Spaces]] then $\phi: R \to \phi \sqbrk R$ is an [[Definition:Isometry (Metric Spaces)|isometry]]. The result follows. {{qed}} [[Category:Normed Division Rings]] [[Category:Complete Metric Spaces]] [[Category:Completion of Normed Division Ring]] jig8wbrux01r8hlv2ykm1x9upahaez2	1
Let $\displaystyle \lim_{n \mathop \to \infty} x_n = 0$ and $\displaystyle \lim_{n \mathop \to \infty} y_n = 0$. The [[Definition:Sequence|sequence]] $\sequence {x_n} + \paren {-\sequence {y_n} } = \sequence {x_n - y_n}$. By [[Combination Theorem for Sequences/Difference Rule|Difference Rule for Sequences]], $\displaystyle \lim_{n \mathop \to \infty} x_n - y_n = 0 - 0 = 0.$ The result follows. {{qed}}	1
=== Necessary Condition === Let $\sequence {a_n}$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Then every element of $G$ is a [[Definition:Linear Combination|linear combination]] of $\set {a_1, \ldots, a_n}$, which is a [[Definition:Generator of Module|generator]] of $G$, by [[Generated Submodule is Linear Combinations]]. Thus there exists at least one such sequence of [[Definition:Scalar (Module)|scalar]]. Now suppose there were two such sequences of [[Definition:Scalar (Module)|scalars]]: $\sequence {\lambda_n}$ and $\sequence {\mu_n}$. That is, suppose $\displaystyle \sum_{k \mathop = 1}^n \lambda_k a_k = \sum_{k \mathop = 1}^n \mu_k a_k$. Then: {{begin-eqn}} {{eqn | l =\ sum_{k \mathop = 1}^n \paren {\lambda_k - \mu_k} a_k | r = \sum_{k \mathop = 1}^n \paren {\lambda_k a_k - \mu_k a_k} | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \lambda_k a_k - \sum_{k \mathop = 1}^n \mu_k a_k | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} So $\lambda_k = \mu_k$ for all $k \in \closedint 1 n$ as $\sequence {a_n}$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. {{qed|lemma}} === Sufficient Condition === Now suppose there is [[Definition:Exactly One|one and only one]] [[Definition:Sequence|sequence]] $\sequence {\lambda_n}$ such that the condition holds. It is clear that $\set {a_1, \ldots, a_n}$ [[Definition:Generator of Module|generates]] $G$. Suppose $\displaystyle \sum_{k \mathop = 1}^n \lambda_k a_k = 0$. Then, since also $\displaystyle \sum_{k \mathop = 1}^n 0 a_k = 0$, we have, by hypothesis: :$\forall k \in \closedint 1 n: \lambda_k = 0$ Therefore $\sequence {a_n}$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $A, B \in B \left({H, K}\right)$ be [[Definition:Bounded Linear Transformation|bounded linear transformations]]. Then the operation of [[Definition:Adjoint Linear Transformation|adjoining]] $^*$ satisfies, for all $\lambda \in \Bbb F$: :$(1): \qquad \left({\lambda A}\right)^* = \overline \lambda A^*$ :$(2): \qquad \left({A + B}\right)^* = A^* + B^*$ That is, $^*: B \left({H, K}\right) \to B \left({K, H}\right)$ is a [[Definition:Linear Transformation|linear transformation]].	1
Let $I := \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $\map \CC I$ be the [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|space of real-valued functions, continuous]] on $I$. Let $\map {\CC^1} I$ be the [[Definition:Space of Continuous Functions of Differentiability Class k|space of real-valued functions, continuously differentiable]] on $I$. Let $\norm {\, \cdot \,}_{1, \infty}$ be the [[Definition:C^k Norm|$\CC^1$ norm]]. $\struct {\map {\CC^1} I, \norm {\, \cdot \,}_{1, \infty} }$ be the [[Space of Continuously Differentiable on Closed Interval Real-Valued Functions with C^1 Norm forms Normed Vector Space|normed space of real-valued functions, continuously differentiable on]] $I$. Then $\struct {\map {\CC^1} I, \norm {\, \cdot \,}_{1, \infty} }$ is a [[Definition:Banach Space|Banach space]].	1
From [[Test for Left Ideal]], the following need to be proved: :$(1): \quad G \ne \O$ :$(2): \quad \forall \mathop {\mathbf X}, \mathop {\mathbf Y} \in G: \mathbf X + \paren {-\mathbf Y} \in G$ :$(3): \quad \forall \mathop{\mathbf J} \in G, \mathop {\mathbf R} \in \map {\MM_S} 2: \mathbf R \times \mathbf J \in G$ === Condition $(1): \quad G \ne \O$ === By definition of $G$: :$\quad \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \in G$ {{qed|lemma}} === Condition $(2): \quad \forall \mathop {\mathbf X}, \mathop{\mathbf Y} \in G: \mathbf X + \paren {-\mathbf Y} \in G$ === Let: :$\quad \mathbf X = \begin{bmatrix} x_1 & 0 \\ x_2 & 0 \end{bmatrix}, \quad \mathbf Y = \begin{bmatrix} y_1 & 0 \\ y_2 & 0 \end{bmatrix} \in G$ Then: :$\quad \mathbf X - \mathbf Y = \begin{bmatrix} x_1 - y_1 & 0 \\ x_2 - y_2 & 0 \end{bmatrix} \in G$ {{qed|lemma}} === Condition $(3): \quad \forall \mathop{\mathbf J} \in G, \mathop{\mathbf R} \in \map {\MM_S} 2: \mathbf R \times \mathbf J \in G$ === Let: :$\quad \mathbf J = \begin{bmatrix} j_1 & 0 \\ j_2 & 0 \end{bmatrix} \in G, \quad \mathbf R = \begin{bmatrix} r_{1 1} & r_{2 1} \\ r_{1 2} & r_{2 2} \end{bmatrix} \in \map {\MM_S} 2$ Then: :$\quad \mathbf R \times \mathbf J = \begin{bmatrix} r_{1 1} \times j_1 + r_{2 1} \times j_2 & 0 \\ r_{1 2} \times j_1 + r_{2 2} \times j_2 & 0 \end{bmatrix} \in G$ {{qed}} [[Category:Left Module Does Not Necessarily Induce Right Module over Ring]] sm252o3vjhqgi1ob3en3cqc03cxnxv6	1
Let $x \in H$ be arbitrary. Then: {{begin-eqn}} {{eqn | l = x | o = \in | r = \map \ker A }} {{eqn | ll= \leadstoandfrom | l = A x | r = \mathbf 0_K | c = {{Defof|Kernel of Linear Transformation}} }} {{eqn | ll= \leadstoandfrom | l = A x | o = \in | r = K^\perp | c = [[Linear Subspace Dense iff Zero Orthocomplement]] ($K$ is trivially dense) }} {{eqn | ll= \leadstoandfrom | l = \innerprod {A x} K_K | r = \set 0 | c = {{Defof|Orthocomplement}} }} {{eqn | ll= \leadstoandfrom | l = \innerprod x {A^* K}_H | r = \set 0 | c = {{Defof|Adjoint Linear Transformation}} }} {{eqn | ll= \leadstoandfrom | l = \innerprod x {\Img {A^*} }_H | r = \set 0 | c = {{Defof|Image of Mapping}} }} {{eqn | ll= \leadstoandfrom | l = x | o = \in | r = \paren {\Img {A^*} }^\perp | c = {{Defof|Orthocomplement}} }} {{end-eqn}} Hence by definition of [[Definition:Set Equality/Definition 1|set equality]]: :$\ker A = \paren {\Img {A^*} }^\perp$ {{qed}}	1
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]]. Let $S \subseteq A$ be a [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subset]] of $A$. Let $x, y \in A$. Then: :$\map d {x, S} \le \map d {x, y} + \map d {y, S}$ where $\map d {x, S}$ is the [[Definition:Distance between Element and Subset of Metric Space|distance from $x$ to $S$]].	1
Let $\mathbf A$ be an $m \times n$ [[Definition:Matrix|matrix]]. Let $i, j \in \closedint 1 m: i \ne j$ Let $\kappa_k$ denote the $k$th [[Definition:Column of Matrix|column]] of $\mathbf A$ for $1 \le k \le n$: :$\kappa_k = \begin {pmatrix} a_{1 k} \\ a_{2 k} \\ \vdots \\ a_{m k} \end {pmatrix}$ Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] acting on $\mathbf A$ as: {{begin-axiom}} {{axiom | n = \text {ERO} 3 | t = Interchange [[Definition:Column of Matrix|columns]] $i$ and $j$ | m = \kappa_i \leftrightarrow \kappa_j }} {{end-axiom}} Then $e$ can be expressed as a [[Definition:Finite Sequence|finite sequence]] of exactly $4$ instances of the other two [[Definition:Elementary Column Operation|elementary column operations]]. {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Column of Matrix|column]] $i$ by $\lambda$ | m = \kappa_i \to \lambda \kappa_i }} {{axiom | n = \text {ERO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Column of Matrix|column]] $j$ to [[Definition:Column of Matrix|column]] $i$ | m = \kappa_i \to \kappa_i + \lambda \kappa_j }} {{end-axiom}}	1
Let $\R^n$ be an [[Definition:Dimension of Vector Space|n-dimensional]] [[Definition:Real Vector Space|real vector space]]. Let $\norm {\, \cdot \,}_2$ be the [[Definition:Euclidean Norm|Euclidean norm]]. Then $\struct {\R^n, \norm {\, \cdot \,}_2}$ is a [[Definition:Normed Vector Space|normed vector space]].	1
Let $\struct {G, +_G, \circ}_R$ and $\struct {H, +_H, \circ}_R$ be [[Definition:Module|$R$-modules]]. Let $\phi: G \to H$ be a [[Definition:Module Homomorphism|module homomorphism]]. Then $\phi$ is a '''module isomorphism''' {{iff}} $\phi$ is a [[Definition:Bijection|bijection]].	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$. Let $b$ be a [[Definition:Bilinear Form|bilinear form]] on $V$. Let $b$ be [[Definition:Alternating Bilinear Form|alternating]]. Then $b$ is [[Definition:Reflexive Bilinear Form|reflexive]].	1
{{ProofWanted|Whitelaw leaves this unresolved at the end of $\S 62$ as an exercise for the student. I haven't read ahead that far, but it may be proved in the exercises. Will return to this later.}}#	1
Let $\sequence {x_n}$ be the constant [[Definition:Sequence|sequence]]: :$\forall n, x_n = a$ Given $\epsilon > 0$: :$\forall n, m \ge 1: \norm {x_n - x_m} = \norm {a - a} = \norm {0} = 0 < \epsilon$ The result follows. {{qed}}	1
If two [[Definition:Square Matrix|square matrices of order $n > 1$]] over a [[Definition:Ring with Unity|ring with unity]] $R$ are [[Definition:Matrix Equivalence|equivalent]], they are not necessarily [[Definition:Matrix Similarity|similar]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|sequence in $R$]]. {{TFAE|def = Convergent Sequence in Normed Division Ring}} === [[Definition:Convergent Sequence/Normed Division Ring/Definition 1|Definition 1]] === {{:Definition:Convergent Sequence/Normed Division Ring/Definition 1}} === [[Definition:Convergent Sequence/Normed Division Ring/Definition 2|Definition 2]] === {{:Definition:Convergent Sequence/Normed Division Ring/Definition 2}} === [[Definition:Convergent Sequence/Normed Division Ring/Definition 3|Definition 3]] === {{:Definition:Convergent Sequence/Normed Division Ring/Definition 3}}	1
We have that an [[Definition:Elementary Row Operation|elementary row operation]] $e$ is used to transform $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ to $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$. Now, whatever $e$ is, $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$ is the [[Definition:Augmented Matrix of Simultaneous Linear Equations|augmented matrix]] of a system of [[Definition:Simultaneous Linear Equations|simultaneous linear equations]] $S'$. We investigate each type of [[Definition:Elementary Row Operation|elementary row operation]] in turn. In the below, let: :$r_k$ denote [[Definition:Row of Matrix|row]] $k$ of $\mathbf A$ :$r'_k$ denote [[Definition:Row of Matrix|row]] $k$ of $\mathbf A'$ for arbitrary $k$ such that $1 \le k \le m$. By definition of [[Definition:Elementary Row Operation|elementary row operation]], only the [[Definition:Row of Matrix|row]] or [[Definition:Row of Matrix|rows]] directly operated on by $e$ is or are different between $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ and $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$. Hence it is understood that in the following, only those [[Definition:Equation|equations]] corresponding to those [[Definition:Row of Matrix|rows]] directly affected will be under consideration. === $\text {ERO} 1$: Scalar Product of Row === Let $e \begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \to \lambda r_k$ where $\lambda \ne 0$. Then the [[Definition:Equation|equation]] in $S$: :$(1 \text a): \displaystyle \sum_{i \mathop = 1}^n \alpha_{k i} x_i = \beta_k$ is replaced in $S'$ by: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \lambda \alpha_{k i} x_i | r = \lambda \beta_k | c = }} {{eqn | n = 2a | ll= \leadsto | l = \lambda \sum_{i \mathop = 1}^n \alpha_{k i} x_i | r = \lambda \beta_k | c = }} {{end-eqn}} It is seen that $\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(1 \text a)$ {{iff}} $\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(2 \text a)$. {{qed|lemma}} === $\text {ERO} 2$: Add Scalar Product of Row to Another === Let $e \begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \to r_k + \lambda r_l$ Then the [[Definition:Equation|equation]] in $S$: :$(1 \text b): \displaystyle \sum_{i \mathop = 1}^n \alpha_{k i} x_i = \beta_k$ is replaced in $S'$ by: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \paren {\alpha_{k i} + \lambda \alpha_{l i} } x_i | r = \beta_k + \lambda \beta_l | c = }} {{eqn | n = 2b | ll= \leadsto | l = \sum_{i \mathop = 1}^n \alpha_{k i} x_i + \lambda \sum_{i \mathop = 1}^n \alpha_{l i} x_i | r = \lambda \beta_k + \lambda \beta_l | c = }} {{end-eqn}} It is seen that $\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(1 \text b)$ {{iff}} $\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(2 \text b)$. {{qed|lemma}} === $\text {ERO} 3$: Exchange Rows === Let $e \begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \leftrightarrow r_l$ Then the [[Definition:Equation|equations]] in $S$: {{begin-eqn}} {{eqn | n = 1c | l = \sum_{i \mathop = 1}^n \alpha_{k i} x_i | r = \beta_k | c = }} {{eqn | n = 2c | l = \sum_{i \mathop = 1}^n \alpha_{l i} x_i | r = \beta_l | c = }} {{end-eqn}} exist unchanged in $S'$, but are in different positions. It follows trivially that: :$\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(1 \text c)$ in $S$ {{iff}}: :$\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(1 \text c)$ in $S'$ and: :$\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(2 \text c)$ in $S$ {{iff}}: :$\tuple {x_1, x_2, \ldots x_n}$ is a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $(2 \text c)$ in $S'$. {{qed|lemma}} Thus in all cases, for each [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ to $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$, $S$ is [[Definition:Equivalent Systems of Simultaneous Linear Equations|equivalent]] to $S'$. Finally we note that from [[Existence of Inverse Elementary Row Operation]], there exists an [[Definition:Elementary Row Operation|elementary row operation]] $e'$ which transforms $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$ to $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$. Hence, [[Definition:Mutatis Mutandis|mutatis mutandis]], the above argument can be used to demonstrate that $S'$ is [[Definition:Equivalent Systems of Simultaneous Linear Equations|equivalent]] to $S$. Hence the result. {{qed}}	1
:$\norm {-x} = \norm {x}$	1
[[Definition:Column Equivalence|Column equivalence]] is an [[Definition:Equivalence Relation|equivalence relation]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]]. Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence in Normed Division Ring|null sequences]]. Then the [[Definition:Quotient Ring|quotient ring]] $\CC / \NN$ is a [[Definition:Division Ring|division ring]].	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\Bbb S := \set {x \in X : \norm {x} = 1}$ be a [[Definition:Unit Sphere/Normed Vector Space|unit sphere]] in $M$. Then $\Bbb S$ is [[Definition:Closed Set in Normed Vector Space|closed]] in $M$.	1
To simplify the notation, let the elements of $\Z_2$ be identified as $0$ for $\eqclass 0 2$ and $1$ for $\eqclass 1 2$. Let $f$ be the [[Definition:Mapping|mapping]] from the [[Definition:Parity Ring|parity ring]] $R := \struct {\set {\text{even}, \text{odd} }, +, \times}$ and the [[Definition:Ring of Integers Modulo m|ring of integers modulo $2$]] $\struct {\Z_2, +_2, \times_2}$: :$f: \struct {\set {\text{even}, \text{odd} }, +, \times} \to \struct {\Z_2, +_2, \times_2}$: ::$\forall x \in R: \map f x = \begin{cases} 0 & : x = \text{even} \\ 1 & : x = \text{odd} \end{cases}$ The [[Definition:Bijection|bijective]] nature of $f$ is apparent: :$f^{-1}: \struct {\Z_2, +_2, \times_2} \to \struct {\set {\text{even}, \text{odd} }, +, \times}$: ::$\forall x \in \Z_2: \map {f^{-1} } x = \begin{cases} \text{even} & : x = 0 \\ \text{odd} & : x = 1 \end{cases}$ Thus the following equations can be checked: {{begin-eqn}} {{eqn | ll= 0 +_2 0 = | l = \map f {\text{even} } +_2 \map f {\text{even} } | r = \map f {\text{even} + \text{even} } | rr= = 0 }} {{eqn | ll= 0 +_2 1 = | l = \map f {\text{even} } +_2 \map f {\text{odd} } | r = \map f {\text{even} + \text{odd} } | rr= = 1 }} {{eqn | ll= 1 +_2 0 = | l = \map f {\text{odd} } +_2 \map f {\text{even} } | r = \map f {\text{odd} + \text{even} } | rr= = 1 }} {{eqn | ll= 1 +_2 1 = | l = \map f {\text{odd} } +_2 \map f {\text{odd} } | r = \map f {\text{odd} + \text{odd} } | rr= = 0 }} {{end-eqn}} and: {{begin-eqn}} {{eqn | ll= 0 \times_2 0 = | l = \map f {\text{even} } \times_2 \map f {\text{even} } | r = \map f {\text{even} \times \text{even} } | rr= = 0 }} {{eqn | ll= 0 \times_2 1 = | l = \map f {\text{even} } \times_2 \map f {\text{odd} } | r = \map f {\text{even} \times \text{odd} } | rr= = 0 }} {{eqn | ll= 1 \times_2 0 = | l = \map f {\text{odd} } \times_2 \map f {\text{even} } | r = \map f {\text{odd} \times \text{even} } | rr= = 0 }} {{eqn | ll= 1 \times_2 1 = | l = \map f {\text{odd} } \times_2 \map f {\text{odd} } | r = \map f {\text{odd} \times \text{odd} } | rr= = 1 }} {{end-eqn}} {{qed}} These results can be determined from their [[Definition:Cayley Table|Cayley tables]]: === [[Parity Ring/Cayley Tables|Cayley Tables for Parity Ring]] === {{:Parity Ring/Cayley Tables}} === [[Ring of Integers Modulo 2/Cayley Tables|Cayley Tables for $\Z_2$]] === {{:Ring of Integers Modulo 2/Cayley Tables}}	1
Let $\struct {R_1, \norm {\, \cdot \,}_1 }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {R_2, \norm {\, \cdot \,}_2 }$ be a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] of $\struct {R_1, \norm {\, \cdot \,}_1 }$ with [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]] $\phi: R_1 \to R_2$. Then for all $x \in R_2$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $R_1$: :$x = \displaystyle \lim_{n \mathop \to \infty} \map \phi {x_n}$ and :$\norm x_2 = \displaystyle \lim_{n \mathop \to \infty} \norm {x_n}_1$	1
Let $\mathbb V$, $\mathbb W$ be [[Definition:Basis (Linear Algebra)|bases]] for $V$, $W$ respectively. [[Definition:By Hypothesis|By hypothesis]] $\dim_K V = \dim_K W$. Thus by the definition of [[Definition:Dimension of Vector Space|dimension]]: :$\mathbb V \sim \mathbb W$ Therefore we can choose a [[Definition:Bijection|bijection]] $\phi: \mathbb V \leftrightarrow \mathbb W$. Define the [[Definition:Mapping|mapping]] $\lambda: V \to W$ by: :$\displaystyle \lambda \left({\sum \limits_{\mathbf v \mathop \in \mathbb V} a_{\mathbf v} \mathbf v}\right) = \sum \limits_{\mathbf v \mathop \in \mathbb V} a_\mathbf v \phi \left({\mathbf v}\right)$ For $\mathbf v \in \mathbb V$ let $l_\mathbf v \in V^\star$ be the [[Unique Linear Transformation Between Vector Spaces|unique linear transformation defined on the basis $\mathbb V$]] by: :$\forall \mathbf v' \in \mathbb V : l_\mathbf v \left({\mathbf v'}\right) = \delta_{\mathbf v, \mathbf v'}$ where $\delta : V \times V \to K$ is the [[Definition:Kronecker Delta|Kronecker delta]] and $V^*$ is the [[Definition:Algebraic Dual|dual]] of $V$. Now: ::$\displaystyle l_{\mathbf v} \left({\sum \limits_{\mathbf u \mathop \in \mathbb V} a_\mathbf u \mathbf u}\right) = l_\mathbf v \left({ \sum_{\mathbf u \in \mathbb V \backslash \{ \mathbf v \}} a_\mathbf u \mathbf u + a_{\mathbf v}\mathbf v }\right) = \sum_{\mathbf u \in \mathbb V \backslash \{ \mathbf v \}} a_\mathbf u l_\mathbf v \left({ \mathbf u }\right) + a_{\mathbf v} l_\mathbf v \left({ \mathbf v }\right)$ By the definition of $l_{\mathbf v}$ and by [[Vector Scaled by Zero is Zero Vector]], all the [[Definition:Term (Algebra)|terms]] but the last [[Definition:Vanish|vanish]], and so: :$\displaystyle \forall \mathbf v \in \mathbb V : l_\mathbf v \left({\sum \limits_{\mathbf u \mathop \in \mathbb V} a_\mathbf u \mathbf u}\right) = a_\mathbf v$ For all $\mathbf v, \mathbf v' \in V, c \in K$: {{begin-eqn}} {{eqn | l = \lambda \left({c \mathbf v + \mathbf v'}\right) | r = \lambda \left({c \sum \limits_{\mathbf u \mathop \in \mathbb V} l_\mathbf u \left({\mathbf v}\right) \mathbf u + \sum \limits_{\mathbf u \mathop \in \mathbb V} l_\mathbf u \left({\mathbf v'}\right) \mathbf u}\right) }} {{eqn | r = \lambda \left({\sum \limits_{\mathbf u \mathop \in \mathbb V} \left({c l_\mathbf u \left({\mathbf v}\right) + l_\mathbf u \left({\mathbf v'}\right)}\right) \mathbf u}\right) }} {{eqn | r = \sum \limits_{\mathbf u \mathop \in \mathbb V} \left({c l_\mathbf u \left({\mathbf v}\right) + l_\mathbf u \left({\mathbf v'}\right)}\right) \phi \left({\mathbf u}\right) }} {{eqn| r = c \sum \limits_{\mathbf u \mathop \in \mathbb V} l_\mathbf u \left({\mathbf v}\right) \phi \left({\mathbf u}\right) + \sum \limits_{\mathbf u \mathop \in \mathbb V} l_\mathbf u \left({\mathbf v}\right) \phi \left({\mathbf u}\right) }} {{eqn | r = c \lambda \left({\mathbf v}\right) + \lambda \left({\mathbf v'}\right) }} {{end-eqn}} Thus $\lambda$ is [[Definition:Linear Transformation|linear]]. Let $\mathbf x \in \ker \lambda$ where $\ker \lambda$ denotes the [[Definition:Kernel of Linear Transformation/Vector Space|kernel]] of $\lambda$. Then: :$\displaystyle \mathbf 0 = \lambda \left({\mathbf x}\right) = \sum \limits_{\mathbf v \mathop \in \mathbb V} l_\mathbf v \left({\mathbf x}\right) \mathbf v$ Therefore: :$\forall \mathbf v \in \mathbb V : l_\mathbf v \left({\mathbf x}\right) = 0$ because $\mathbb V$ is [[Definition:Linearly Independent Set|linearly independent]]. By [[Vector Scaled by Zero is Zero Vector]], $\mathbf x = \mathbf 0$. That is: : $\ker \lambda = \left\{{\mathbf 0}\right\}$ By [[Linear Transformation is Injective iff Kernel Contains Only Zero]], it follows that $\lambda$ is [[Definition:Injection|injective]]. Recall that $\phi$ is a [[Definition:Bijection|bijection]]. From [[Inverse of Bijection is Bijection]], $\phi$ is [[Definition:Invertible Mapping|invertible]]. Suppose $\mathbf y \in W$. Then: {{begin-eqn}} {{eqn | l = \mathbf y | r = \sum \limits_{\mathbf w \mathop \in \mathbb W} l_\mathbf w \left({\mathbf y}\right) \mathbf w }} {{eqn | r = \sum \limits_{\mathbf v \mathop \in \mathbb V} l_{\phi^{-1} \left({\mathbf v}\right)} \left({\mathbf y}\right) \phi^{-1} \left({\mathbf v}\right) }} {{end-eqn}} where this last vector belongs to $\lambda \left({V}\right)$. Thus $\lambda$ is [[Definition:Surjection|surjective]]. $\lambda$ has been shown to be [[Definition:Injection|injective]] and [[Definition:Surjection|surjective]], and so is a [[Definition:Bijection|bijection]]. $\lambda$ has also been shown to be [[Definition:Linear Transformation|linear transformation]]. Thus, by definition, $\lambda$ is an [[Definition:Vector Space Isomorphism|isomorphism]]. {{explain|The definition for isomorphism as given by that link is more explicit than using the term "linear" - it boils down to the same thing of course, but this needs to be made explicit. '''I don't see the difference''' --[[User:Linus44|Linus44]] ([[User talk:Linus44|talk]]) 14:22, 21 May 2013 (UTC)}} {{qed}} [[Category:Linear Algebra]] mvq7dnwbbihlhiav0zro3khtti05ym8	1
Let $I := \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $\map \CC I$ be a [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|space of continuous on closed interval real-valued functions]]. Let $\struct {\R, +_\R, \times_\R}$ be the [[Definition:Field of Real Numbers|field of real numbers]]. Let $\paren +$ be the [[Definition:Pointwise Addition of Real-Valued Functions|pointwise addition of real-valued functions]]. Let $\paren {\, \cdot \,}$ be the [[Definition:Pointwise Scalar Multiplication of Real-Valued Functions|pointwise scalar multiplication of real-valued functions]]. Then $\struct {\map \CC I, +, \, \cdot \,}_\R$ is a [[Definition:Vector Space|vector space]].	1
:$\paren {c \mathbf u} \cdot \mathbf v = c \paren {\mathbf u \cdot \mathbf v}$	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \sum_{i \mathop = 1}^n u_i^2 | c = Definition of [[Definition:Dot Product/Definition 1|Dot Product]] }} {{eqn | o = \ge | r = 0 | c = as $u_i \in \R$ it follows that $u_i^2 \ge 0$ }} {{end-eqn}} {{qed}}	1
In the following: : $\mathrm A$ and $\mathrm B$ are constant [[Definition:Square Matrix|square matrices]] : $P$ is a [[Definition:Singular Matrix|nonsingular matrix]] : $t, s \in \R$ The [[Definition:Matrix Exponential|matrix exponential]] $e^{\mathrm A t}$ has the following properties:	1
Let $G$ be an [[Definition:Dimension of Vector Space|$n$-dimensional]] [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]]. Let $J: G \to G^{**}$ be the [[Definition:Evaluation Isomorphism|evaluation isomorphism]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. Let $M$ be an [[Definition:Dimension of Vector Space|$m$-dimensional]] [[Definition:Vector Subspace|subspace]] of $G$. Let $N$ be a [[Definition:Dimension of Vector Space|$p$-dimensional]] [[Definition:Vector Subspace|subspace]] of $G^*$. Let $M^\circ$ be the [[Definition:Annihilator on Algebraic Dual|annihilator]] of $M$. Then: :$(1): \quad M^\circ$ is an $\paren {n - m}$-dimensional subspace of $G^*$, and $M^{\circ \circ} = \map J M$ :$(2): \map {\quad J^{-1} } {N^\circ}$ is an $\paren {n - p}$-dimensional subspace of $G$ :$(3): \quad$ The [[Definition:Mapping|mapping]] $M \to M^\circ$ is a [[Definition:Bijection|bijection]] from the set of all $m$-dimensional subspaces of $G$ onto the set of all $\paren {n - m}$-dimensional subspaces of $G^*$ :$(4): \quad$ Its inverse is the bijection $N \to \map {J^{-1} } {N^\circ}$.	1
Let $G$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\beta \in K$. Then the [[Definition:Mapping|mapping]]: :$s_\beta: G \to G$ defined by $\map {s_\beta} {\mathbf x} = \beta \mathbf x$ is a [[Definition:Linear Operator|linear operator]] on $G$. If $\beta \ne 0$ then $s_\beta$ is an [[Definition:Vector Space Automorphism|automorphism]] of $G$, and $\paren {s_\beta}^{-1} = s_{\beta^{-1} }$ The linear operators $s_\beta$, where $\beta \ne 0$, are called '''similarities of $G$'''.	1
Let $\struct {\mathbf V, +, \circ}_{\mathbb F}$ be a [[Definition:Vector Space|vector space]] over $\mathbb F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then every $\mathbf v \in \struct {\mathbf V, +}$ is [[Definition:Right Cancellable Element|right cancellable]]: :$\forall \mathbf a, \mathbf b, \mathbf c \in \mathbf V: \mathbf a + \mathbf c = \mathbf b + \mathbf c \implies \mathbf a = \mathbf b$	1
Consider a [[Definition:Particle|particle]] $p$ moving in the [[Definition:Plane|plane]]. Let the [[Definition:Position|position]] of $p$ be given in [[Definition:Polar Coordinates|polar coordinates]] as $\left\langle{r, \theta}\right\rangle$. Let: :$\mathbf u_r$ be the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Radial Coordinate|radial coordinate]] of $p$ :$\mathbf u_\theta$ be the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Angular Coordinate|angular coordinate]] of $p$ Then the [[Definition:Derivative|derivative]] of $\mathbf u_r$ and $\mathbf u_\theta$ {{WRT}} $\theta$ can be expressed as: {{begin-eqn}} {{eqn | l = \dfrac {\mathrm d \mathbf u_r} {\mathrm d \theta} | r = \mathbf u_\theta }} {{eqn | l = \dfrac {\mathrm d \mathbf u_\theta} {\mathrm d \theta} | r = -\mathbf u_r }} {{end-eqn}}	1
Let $A / \mathfrak a$ be the [[Definition:Quotient Ring|quotient ring]]. By [[Proper Ideal iff Quotient Ring is Nontrivial]], $A / \mathfrak a$ is [[Definition:Non-Trivial Ring|non-trivial]]. By [[Krull's Theorem]], $A / \mathfrak a$ has a [[Definition:Maximal Ideal of Ring|maximal ideal]]. By [[Correspondence Theorem for Quotient Rings]], $A$ has a [[Definition:Maximal Ideal of Ring|maximal ideal]] containing $\mathfrak a$. {{qed}} [[Category:Commutative Algebra]] [[Category:Maximal Ideals of Rings]] ido4yhyw70by29yomtpi83yvszusm01	1
:$\norm{-1_R} = 1$	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n$]]. We proceed by induction on $n$, the number of [[Definition:Row of Matrix|rows]] of $\mathbf A$. === Basis for the Induction === For $n = 1$, we have a matrix of just one [[Definition:Element of Matrix|element]], which is trivially [[Definition:Diagonal Matrix|diagonal]], hence both upper and lower triangular. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Fix $n \in \N$, and assume all $n \times n$-matrices can be upper triangularised by [[Definition:Elementary Row Operation|elementary row operations]]. If $R$ is a [[Definition:Field (Abstract Algebra)|field]], assume all $n \times n$-matrices can be upper triangularised by elementary row operations of type 2. This forms our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]. === Induction Step === Let $\mathbf A = \left[{a}\right]_{n+1}$ be a [[Definition:Square Matrix|square matrix]] of order $n + 1$. When the first [[Definition:Column of Matrix|column]] of $\mathbf A$ contains only zeroes, it is upper triangularisable [[Definition:Iff|iff]] the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ is. Each [[Definition:Elementary Row Operation|elementary row operation]] used in triangularisation process of the submatrix $\mathbf A \left({1; 1}\right)$ will not change the zeros of the first column of $\mathbf A$. So when $\mathbf A \left({1; 1}\right)$ is upper triangularised, then $A$ will also be upper triangularised. From the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]], we conclude that $\mathbf A$ can be upper triangularised by elementary row operations. Now suppose that its first column contains a non-zero value. Suppose that $a_{11} = 0$. Let $j$ be the smallest row index such that $a_{j1} \ne 0$, and note that $j$ exists by assumption. Now apply the following operation of type 2: :$r_1 \to r_1 + r_j$ As $a_{j1} \ne 0$, this enforces $a_{11} \ne 0$, and we continue as in the case below. Suppose $a_{11} \ne 0$. We use the following operations for all $j \in \left\{{2, \ldots, n + 1}\right\}$: :$(1): \quad$ Put $c = a_{j1}$. :$(2): \quad r_j \to a_{11} r_j$ :$(3): \quad r_j \to r_j - c r_1$ This will put the first column to zero (except for the first element, $a_{11}$). It follows that $\mathbf A$ can be upper triangularised precisely when the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ can. Again, the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]] renders $\mathbf A$ upper triangularisable by [[Definition:Elementary Row Operation|elementary row operations]]. This completes the case distinction, and hence the result follows by [[Principle of Mathematical Induction|induction]]. To put the matrix $\mathbf A$ into [[Definition:Lower Triangular Matrix|lower triangular form]], just do the same thing, but start with the last column and the last diagonal element $a_{n + 1 \; n + 1}$. {{qed}}	1
By [[Quotient Ring of Cauchy Sequences is Division Ring]] then $\CC \,\big / \NN$ is a [[Definition:Division Ring| division ring]]. It remains to be proved that: :$\norm {\, \cdot \,}_1$ is [[Definition:Well-Defined Mapping|well-defined]] :$\norm {\, \cdot \,}_1$ satisfies the [[Definition:Norm Axioms|norm axioms]]. === [[Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 1|Lemma 1]] === {{:Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 1}}{{qed|lemma}} === [[Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 2|Lemma 2]] === {{:Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 2}}{{qed|lemma}} === [[Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 3|Lemma 3]] === {{:Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 3}}{{qed|lemma}} === [[Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 4|Lemma 4]] === {{:Quotient Ring of Cauchy Sequences is Normed Division Ring/Lemma 4}}{{qed|lemma}} The result follows. {{qed}}	1
By [[Quotient of Cauchy Sequences is Metric Completion/Lemma 1|Lemma 1 of Quotient of Cauchy Sequences is Metric Completion]] we have that: :$\mathcal C \,\big / \mathcal N = \tilde {\mathcal C}$ Let $\eqclass {x_n} {}$ and $\eqclass {x_n} {}$ be [[Definition:Equivalence Class|equivalence classes]] in $\mathcal C \,\big / \mathcal N = \tilde {\mathcal C}$. Then: {{begin-eqn}} {{eqn | l = \map {d'} {\eqclass {x_n}{}, \eqclass {x_n}{} } | r = \norm {\eqclass {x_n}{} - \eqclass {x_n}{} } | c = Definition of $d'$ }} {{eqn | r = \norm {\eqclass {x_n - y_n}{} } | c = {{Defof|Quotient Ring}} Addition }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n - y_n} | c = {{Defof|Norm on Division Ring}}: $\norm {\,\cdot\,}$ on $\mathcal C \,\big / \mathcal N$ }} {{eqn | r = \lim_{n \mathop \to \infty} \map d {x_n, y_n} | c = {{Defof|Metric Induced by Norm on Division Ring}}: by the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$ on $R$ }} {{eqn | r = \map {\tilde d} {\eqclass {x_n}{}, \eqclass {x_n}{} } | c = Definition of $\tilde d$ }} {{end-eqn}} The result follows. {{qed}}	1
Let $n$ be an [[Definition:Integer|integer]] such that $n > 1$. Then the expression for $n$ as the [[Definition:Integer Multiplication|product]] of one or more [[Definition:Prime Number|primes]] is [[Definition:Unique|unique]] up to the order in which they appear.	1
Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $A = \sqbrk {a_{i j} }$ be a [[Definition:Complex Number|complex]] [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $A$. Then there exists $i \in \set {1, 2, \ldots, n}$ such that: :$\lambda \in \map {\mathbb D} {a_{i i}, R_i}$ where: :$\displaystyle R_i = \sum_{j \mathop \ne i} \cmod {a_{ i j} }$ :$\map {\mathbb D} {a, R}$ denotes the [[Definition:Complex Disk|complex disk]] of [[Definition:Center of Disk|center]] $a$ and [[Definition:Radius of Disk|radius]] $R$.	1
When $A$ is So suppose that: :$\forall a, b \in A: \paren {a \oplus a} \oplus b = a \oplus \paren {a \oplus b}$ :$\forall a, b \in A: \paren {b \oplus a} \oplus a = b \oplus \paren {a \oplus a}$ Then: :$\sqbrk {a, a, b} = 0$ :$\sqbrk {b, a, a} = 0$ where $\sqbrk {a, a, b}$ denotes the [[Definition:Associator|associator]] of $a, b \in A_R$. Now let us compute, using the linearity of $\oplus$ and the two suppositions: {{begin-eqn}} {{eqn | l = \paren {a - b} \oplus \paren {\paren {a - b} \oplus a} | r = a \oplus \paren {\paren {a - b} \oplus a} - b \oplus \paren {\paren {a - b} \oplus a} }} {{eqn | r = a \oplus \paren {a \oplus a} - a \oplus \paren {b \oplus a} - b \oplus \paren {a \oplus a} + b \oplus \paren {b \oplus a} }} {{eqn | l = \paren {\paren {a - b} \oplus \paren {a - b} } \oplus a | r = \paren {a \oplus a} \oplus a - \paren {a \oplus b} \oplus a - \paren {b \oplus a} \oplus a + \paren {b \oplus b} \oplus a }} {{eqn | ll= \leadsto | l = 0 | r = \sqbrk {a - b, a - b, a} }} {{eqn | r = \sqbrk {a, a, a} - \sqbrk {a, b, a} - \sqbrk {b, a, a} + \sqbrk {b, b, a} | c = Subtract the two expressions }} {{eqn | r = \sqbrk {a, b, a} }} {{end-eqn}} {{explain|This suffices, but why it does could do with explanation (although trivial). Also, where do I use that $A$ isn't Boolean? Maybe better use talk page}} {{qed}} {{Namedfor|Emil Artin|cat = Artin}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] with [[Definition:Basis (Linear Algebra)|basis]] $B$. Let $S$ be a [[Definition:Generator of Module|generating set]] for $M$. Then: :$\size B \le \size S$. That is, there exists an [[Definition:Injection|injection]] from $B$ to $S$.	1
Let $V$ be a [[Definition:Vector Space|vector space]]. Then $V = \left\{{\mathbf 0}\right\}$ [[Definition:Iff|iff]] $\dim \left({V}\right) = 0$, where $\dim$ signifies [[Definition:Dimension (Linear Algebra)|dimension]].	1
:$\forall n \in N: \norm n \ge n^\alpha$	1
Let: :$\map {\mathrm N} {\mathbf A} = \set {\mathbf x \in \R^n : \mathbf {A x} = \mathbf 0}$ be the [[Definition:Null Space|null space]] of $\mathbf A$, where: :$ \mathbf A_{m \times n} = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end {bmatrix}$, $\mathbf x_{n \times 1} = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$, $\mathbf 0_{m \times 1} = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$ are [[Definition:Matrix|matrices]] where each [[Definition:Column of Matrix|column]] is an [[Definition:Element|element]] of a [[Definition:Real Vector Space|real vector space]]. Then $\map {\mathrm N} {\mathbf A}$ is [[Definition:Closed for Scalar Product|closed]] under [[Definition:Scalar Multiplication on Vector Space|scalar multiplication]]: :$\forall \mathbf v \in \map {\mathrm N} {\mathbf A} ,\forall \lambda \in \R: \lambda \mathbf v \in \map {\mathrm N} {\mathbf A}$	1
Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]] and $\mathbf B$ be an [[Definition:Matrix|$n \times p$ matrix]] such that the [[Definition:Column of Matrix|columns]] of $\mathbf A$ and $\mathbf B$ are members of $\R^m$ and $\R^n$, respectively. Let $\lambda \in \mathbb F \in \set {\R, \C}$ be a [[Definition:Scalar (Matrix Theory)|scalar]]. Then: :$\mathbf A \paren {\lambda \mathbf B} = \lambda \paren {\mathbf A \mathbf B}$	1
:$\displaystyle \sum {r_i^2} \sum {s_i^2} \ge \left({\sum {r_i s_i}}\right)^2$	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be a [[Definition:Right Module|right $R$-module]]. Let $N$ be a [[Definition:Left Module|left $R$-module]]. Let $M \otimes_R N$ denote their [[Definition:Tensor Product of Modules|tensor product]]. Then: :$0\otimes_R n = m \otimes_R 0 = 0 \otimes_R 0$ is the [[Definition:Zero of Tensor Product|zero]] in $M \otimes_R N$.	1
We have that [[Polynomial Forms over Field is Euclidean Domain]]. We also have that [[Euclidean Domain is Principal Ideal Domain]]. Hence the result. {{qed}}	1
Let $\struct {F, +, \times}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Unity of Field|unity]] is $1_F$. Let $F_n \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] over $F$ whose [[Definition:Degree of Polynomial|degree]] is less than $n$. Then the [[Definition:Dimension of Vector Space|dimension]] of the [[Definition:Vector Space|vector space]] $F_n \sqbrk X$ is $n$.	1
Let: : $\mathbf r: x \mapsto \left\langle{r_1 \left({x}\right), r_2 \left({x}\right), \ldots, r_n \left({x}\right)}\right\rangle$ : $\mathbf q: x \mapsto \left\langle{q_1 \left({x}\right), q_2 \left({x}\right), \ldots, q_n \left({x}\right)}\right\rangle$ be [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued functions]]. The [[Definition:Derivative of Vector-Valued Function|derivative]] of their [[Definition:Dot Product|dot product]] is given by: :$\dfrac \d {\d x} \left({\mathbf r \left({x}\right) \cdot \mathbf q \left({x}\right)}\right) = \mathbf r' \left({x}\right) \cdot \mathbf q \left({x}\right) + \mathbf r \left({x}\right) \cdot \mathbf q' \left({x}\right)$	1
:$\psi$ is an [[Definition:Isometry|isometry]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ and $H$ be [[Definition:Module|$R$-modules]]. Let $G^*$ and $H^*$ be the [[Definition:Algebraic Dual|algebraic duals]] of $G$ and $H$ respectively. Let $\map {\LL_R} {G, H}$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Let $u \in \map {\LL_R} {G, H}$. Let $u^t: H^* \to G^*$ be the [[Definition:Transpose of Linear Transformation|transpose]] of $u$. Then $u^t: H^* \to G^*$ is itself a [[Definition:Linear Transformation|linear transformation]].	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $f, g \in A$. {{TFAE}} :$(1): \quad$ There exists an $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]] $h : A_f \to A_g$ between [[Definition:Localization of Ring at Element|localizations]], the '''[[Definition:Induced Homomorphism between Localizations of Ring|induced homomorphism]]'''. :$(2): \quad f$ [[Definition:Divisor of Ring Element|divides]] some [[Definition:Power of Ring Element|power]] of $g$. :$(3): \quad$ There is an [[Definition:Set Inclusion|inclusion]] of [[Definition:Vanishing Set of Subset of Ring|vanishing sets]]: $\map V f \subseteq \map V g$. That is, every [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]] [[Definition:Element of Set|containing]] $f$ also contains $g$. :$(4): \quad$ There is an [[Definition:Set Inclusion|inclusion]] of [[Definition:Principal Open Subset of Spectrum|principal open subsets]]: $\map D f \supseteq \map D g$	1
A [[Definition:Hilbert Matrix|Hilbert matrix]] is a special case of a [[Definition:Cauchy Matrix|Cauchy matrix]].	1
=== Necessary Condition === {{begin-eqn}} {{eqn | l = z | r = 0 | c = }} {{eqn | r = 0 + 0 i | c = }} {{eqn | ll= \leadsto | l = \cmod z | r = \sqrt {0^2 + 0^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === {{begin-eqn}} {{eqn | l = \cmod z | r = 0 | c = }} {{eqn | ll= \leadsto | l = \cmod {a + b i} | r = 0 | c = Definition of $z$ }} {{eqn | ll= \leadsto | l = \sqrt {a^2 + b^2} | r = 0 | c = {{Defof|Complex Modulus}} }} {{eqn | ll= \leadsto | l = a^2 + b^2 | r = 0 | c = [[Definition:Square (Algebra)|squaring]] both sides }} {{eqn | ll= \leadsto | l = a | r = 0 | c = [[Square of Real Number is Non-Negative]] }} {{eqn | l = b | r = 0 | c = }} {{eqn | ll= \leadsto | l = z | r = 0 | c = Definition of $z$ }} {{end-eqn}} {{qed}}	1
Let $\struct {G, \cdot}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\map {\MM_G} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\struct {G, \cdot}$. Then $\struct {\map {\MM_G} {m, n}, \circ}$, where $\circ$ is [[Definition:Hadamard Product|Hadamard product]], is also a [[Definition:Group|group]].	1
Let $\struct {R, +, *, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. Let $p \in \R_{\ge 1} \cup \set \infty$. Let $d_p$ be the [[Definition:P-Product Metric|$p$-product metric]] on $R \times R$. Let $R^* = R \setminus \set 0$ Let $d^*$ be the [[Definition:Restriction|restriction]] of $d$ to $R^*$. Then the following results hold:	1
=== Lemma 1 === Given values $z_1, \ldots, z_{p + 1}$ and $1 \le m \le p$, then: {{begin-eqn}} {{eqn | n = 2 | l = \displaystyle \map {e_m} {z_1, \ldots, z_p, z_{p + 1} } | r = z_{p+1} \map {e_{m - 1} } {z_1, \ldots, z_p} + \map {e_m} {z_1, \ldots, z_p} | c = [[Elementary Symmetric Function/Examples/Recursion]] }} {{end-eqn}} {{qed|lemma}} === Lemma 2 === Let $X = \set {x_1, \ldots, x_n}$ and $\mathbf u = x_j$ for some $j = 1, \ldots, n$. Then: {{begin-eqn}} {{eqn | n = 3 | l = \displaystyle \sum_{k \mathop = 0}^{n - i} \paren {-1}^k \, \map {e_{n - i - k} } X \, \mathbf u^k | r = \map {e_{n - i} } {X \setminus \set {\mathbf u} } | c = Eisinberg (1981) }} {{end-eqn}} '''Proof of Lemma 2''': Let $S$ denote the {{LHS}} of $(3)$. Let $U = X \setminus \set {\mathbf u}$. Then: {{begin-eqn}} {{eqn | l = S | r = \paren {-1}^{n - i} \mathbf u^{n - i} + \sum_{k \mathop = 0}^{n - i - 1} \paren {-1}^k \map {e_{n - i - k} } X {\mathbf u}^k | c = splitting off the term for $k = n - i$ }} {{eqn | r = \paren {-1}^{n - i} \, {\mathbf u}^{n - i} + \sum_{k \mathop = 0}^{n - i - 1} \paren {-1}^k \, \paren {\mathbf u \, \map {e_{n - i - k - 1} } U + \map {e_{n - i - k} } U} \, \mathbf u^k | c = by $(2)$ in Lemma 1 with $p = n - 1$ and $m = n - i - k$ }} {{eqn | r = \sum_{k \mathop = 0}^{n - i - 1} \paren {-1}^k \, \map {e_{n - i - k - 1} } U \mathbf u^{k + 1} + \sum_{k \mathop = 0}^{n - i} \paren {-1}^k \map {e_{n - i - k} } U \, \mathbf u^k | c = reassembling summations }} {{eqn | r = \sum_{k \mathop = 1}^{n - i} \paren {-1}^{k - 1} \, \map {e_{n - i - k} } U \mathbf u^k + \sum_{k \mathop = 0}^{n - i} \paren {-1}^k \map {e_{n - i - k} } U \, \mathbf u^k | c = reindexing the first sum }} {{eqn | r = \sum_{k \mathop = 1}^{n - i} \paren {-1}^k \paren {-\map {e_{n - i - k} } U + \map {e_{n - i - k} } U} \mathbf u^k + \paren {-1}^0 \map {e_{n - i} } U \mathbf u^0 | c = splitting off the term for $k = 0$ and collecting under one summation }} {{eqn | r = \map {e_{n-i} } U | c = }} {{end-eqn}} {{qed|lemma}} '''Proof of the Theorem''' {{begin-eqn}} {{eqn | l = d_{ij} | r = \dfrac{\displaystyle \paren {-1}^{n - i} \map { e_{n - i} } {X \setminus \set {x_j} } } {\displaystyle \prod_{m \mathop = 1, m \mathop \ne j }^n \paren {x_j - x_m} } | c = transposing $W_n$, then applying [[Inverse of Vandermonde Matrix/Corollary|corollary to Inverse of Vandermonde Matrix]] }} {{eqn | r = \dfrac {\displaystyle \sum_{k \mathop = 0}^{n - i} \paren {-1}^k \, \map {e_{n - i - k} } X \, x_j^k} {\displaystyle \prod_{m \mathop = 1, m \mathop \ne j }^n \paren {x_j - x_m} } | c = $(3)$ in Lemma 2 }} {{eqn | r = \dfrac {\displaystyle \sum_{k \mathop = 0}^{n - i} a_{i + k} \, x_j^k} {\displaystyle \prod_{m \mathop = 1, m \mathop \ne j}^n \paren {x_j - x_m} } | c = [[Viete's Formulas]] }} {{end-eqn}} {{qed}}	1
Let: :$\mathbf r: t \mapsto \begin {bmatrix} x \\ y \\ z \end{bmatrix}$ :$\mathbf q: t \mapsto \begin {bmatrix} \chi \\ \gamma \\ \zeta \end{bmatrix}$ be [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued functions]], where: :$x, y, z, \chi, \gamma, \zeta$ are ([[Definition:Image of Mapping|images]] of) [[Definition:Differentiable Real Function|differentiable]] [[Definition:Real Function|real functions]]. The [[Definition:Derivative of Vector-Valued Function|derivative]] of the [[Definition:Vector Cross Product|vector cross product]] of $\mathbf r$ and $\mathbf q$ is given by: :$D_t \left({\mathbf r \left({t}\right) \times \mathbf q \left({t}\right)}\right) = \mathbf r' \left({x}\right) \times \mathbf q \left({x}\right) + \mathbf r \left({x}\right) \times \mathbf q'\left({x}\right)$	1
Follows from [[Composite of Homomorphisms is Homomorphism/R-Algebraic Structure|Composite of R-Algebraic Structure Homomorphisms is Homomorphism]], as it is a [[Definition:Subring|subring]] of the ring of all endomorphisms of the [[Definition:Abelian Group|abelian group]] $\struct {G, +}$. {{ProofWanted}} [[Category:Linear Operators]] eda5k9ojbrvu9i00azjlutffjtyrjib	1
{{begin-eqn}} {{eqn | l = \left[{a, b}\right] | r = a \oplus b - b \oplus a | c = }} {{eqn | r = - b \oplus a + a \oplus b | c = }} {{eqn | r = - b \oplus a - \left({-\left({a \oplus b}\right)}\right) | c = }} {{eqn | r = - \left({b \oplus a - a \oplus b}\right) | c = }} {{eqn | r = -\left[{b, a}\right] | c = }} {{end-eqn}} {{qed}}	1
We have that: :$\displaystyle \sum_{i \mathop = 1}^n a_{ij} = \sum_{i \mathop = 1}^n b_{ij} = 1$ Then: {{begin-eqn}} {{eqn | l=\sum_{i \mathop = 1}^n \left({\mathbf A \mathbf B}\right)_{ij} | r=\sum_{i \mathop = 1}^n \left({\sum_{k \mathop = 1}^n a_{ik}b_{kj} }\right) | c= }} {{eqn | r=\sum_{i,k \mathop = 1}^n a_{ik} b_{kj} | c= }} {{eqn | r=\sum_{k \mathop = 1}^n \left({b_{kj} \sum_{i \mathop = 1}^n a_{ik} }\right) | c= }} {{eqn | r=\sum_{k \mathop = 1}^n \left({b_{kj} \cdot 1}\right) | c= }} {{eqn | r=1 | c= }} {{end-eqn}} {{qed}} [[Category:Matrix Algebra]] 8ak0ay2ys94grfg0lw0sz5z33mpibuv	1
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $M$ be a [[Definition:Free Module|free]] $R$-[[Definition:Module|module]]. Let $B$ and $C$ be [[Definition:Basis of Module|bases]] of $M$. Then $B$ and $C$ are [[Definition:Equinumerous|equinumerous]].	1
{{ProofWanted|tedious}}	1
By definition of the [[Definition:P-adic Norm|$p$-adic norm]]: :$\norm p_p = \frac 1 p < 1$ By definition of the [[Definition:Absolute Value|absolute value]]: :$\size p = p > 1$ By definition of [[Definition:Equivalent Division Ring Norms/Open Unit Ball Equivalent|open unit ball equivalence]], $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. By [[Equivalence of Definitions of Equivalent Division Ring Norms]] and the definition of [[Definition:Equivalent Division Ring Norms/Topologically Equivalent|topologically equivalent norms]] then the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_p$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\size{\,\cdot\,}$. {{qed}}	1
Let $x_0 \in R^*$. Let $\epsilon \gt 0$ be given. Let $\delta = \min \set {\dfrac {\norm {x_0} } 2, \dfrac {\norm {x_0}^2 \epsilon} 2 }$ Let $x \in R^*$ such that: :$\map {d^*} {x, x_0} < \delta$ By the definition of the [[Definition:Metric Subspace|subspace metric]] on $R^*$ and the definition of the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] on $R$: :$\map {d^*} {x, x_0} = \map d {x, x_0} = \norm {x - x_0} < \delta$ Then: {{begin-eqn}} {{eqn | l = \norm {x_0} | o = \le | r = \norm {x - x_0} + \norm x | c = {{NormAxiom|3}} }} {{eqn | o = }} {{eqn | o = < | r = \delta + \norm x | c = by assumption: $\norm {x - x_0} < \delta$ }} {{eqn | o = }} {{eqn | o = \le | r = \dfrac {\norm {x_0} } 2 + \norm x | c = by assumption: $\delta \le \dfrac {\norm {x_0} } 2$ }} {{eqn | o = }} {{eqn | ll= \leadsto | l = \dfrac {\norm {x_0} } 2 | o = < | r = \norm x | c = subtracting $\dfrac {\norm {x_0} } 2$ from both sides }} {{eqn | o = }} {{eqn | ll= \leadsto | l = \dfrac 2 {\norm {x_0} } | o = > | r = \dfrac 1 {\norm x} | c = inverting both sides of the equation }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \map d {x^{-1}, x_0^{-1} } | r = \norm {x^{-1} - x_0^{-1} } | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | o = }} {{eqn | r = \dfrac 1 {\norm x} \paren {\norm x \norm {x^{-1} - x_0^{-1} } \norm {x_0} } \dfrac 1 {\norm {x_0} } | c = }} {{eqn | o = }} {{eqn | r = \dfrac 1 {\norm x \norm {x_0} } \paren {\norm {x \paren {x^{-1} - x_0^{-1} } x_0} } | c = {{NormAxiom|2}} }} {{eqn | o = }} {{eqn | r = \dfrac 1 {\norm x \norm {x_0} } \paren {\norm {x x^{-1} x_0 - x x_0^{-1} x_0} } | c = [[Definition:Ring (Abstract Algebra)|Ring Axiom $(\text D)$: Product is Distributive over Addition]] }} {{eqn | o = }} {{eqn | r = \dfrac 1 {\norm x \norm {x_0} } \paren {\norm {x_0 - x} } | c = {{Defof|Division Ring}} }} {{eqn | o = }} {{eqn | o = < | r = \dfrac 2 {\norm {x_0}^2} \paren {\norm {x_0 - x} } | c = from $\dfrac 1 {\norm x} < \dfrac 2 {\norm {x_0} }$ above }} {{eqn | o = }} {{eqn | o = < | r = \dfrac {2 \delta} {\norm {x_0}^2} | c = by assumption: $\norm {x - x_0} < \delta$ }} {{eqn | o = }} {{eqn | o = \le | r = \dfrac 2 {\norm {x_0}^2} \paren {\dfrac {\norm {x_0}^2 \epsilon} 2} | c = by assumption: $\delta \le \dfrac {\norm {x_0}^2 \epsilon} 2$ }} {{eqn | o = }} {{eqn | r = \epsilon | c = cancelling terms }} {{end-eqn}} Since $x_0$ and $\epsilon$ were arbitrary, by the definition of [[Definition:Continuous Mapping (Metric Space)|continuity]] then the [[Definition:Mapping|mapping]]: :$\iota: \struct {R^*, d^*} \to \struct {R, d} : \map \iota x = x^{-1}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]]. {{qed}}	1
Define a mapping: :$\phi: \Z / N \Z \to \Z / n_1 \Z \times \cdots \times \Z / n_r \Z$ by: :$\phi \left({d \pmod N}\right) = \left({d \pmod {n_1}, \ldots, d \pmod {n_r} }\right)$ Then, by [[Mappings Between Residue Classes]], $\phi$ is [[Definition:Well-Defined Mapping|well-defined]]. By the definition of multiplication and addition in $\Z / k \Z$, $k \in \Z$ we have: :$\left({a \pmod k}\right) + \left({b \pmod k}\right) = \left({a + b}\right) \pmod k$ and :$\left({a \pmod k}\right) \cdot \left({b \pmod k}\right) = \left({a \cdot b}\right) \pmod k$ Thus taking $k = n_1, \ldots, n_r$ separately we see that $\phi$ is a [[Definition:Ring Homomorphism|ring homomorphism]]. Let: :$\left({a_1 \pmod {n_1}, \ldots, a_r \pmod {n_r} }\right) \in \Z / n_1 \Z \times \cdots \times \Z / n_r \Z$ By the [[Chinese Remainder Theorem/General Result|Chinese Remainder Theorem]] there exists a unique $x \in \Z / N \Z$ such that: :$\phi \left({x}\right) = \left({a_1 \pmod {n_1}, \ldots, a_r \pmod {n_r} }\right)$ Since such an $x$ exists, $\phi$ is [[Definition:Surjection|surjective]]. Since this $x$ is unique modulo $N$, it follows that $\phi$ is [[Definition:Injection|injective]]. {{Qed}}	1
First consider the classical form of the [[Definition:Vandermonde Matrix|Vandermonde matrix]]: :$W_n = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 1} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 1} \\ \end{bmatrix}$ By [[Vandermonde Determinant]], the [[Definition:Determinant of Matrix|determinant]] of $W_n$ is: :$\displaystyle \map \det {W_n} = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {x_i - x_j} \ne 0$ Since this is non-zero, by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], the [[Definition:Inverse Matrix|inverse matrix]], denoted $B = \sqbrk {b_{i j} }$, is guaranteed to exist. Using the definition of the [[Definition:Matrix Product (Conventional)|matrix product]] and the [[Definition:Inverse Matrix|inverse]]: :$\displaystyle \sum_{k \mathop = 1}^n b_{k j} x_i^{k - 1} = \delta_{i j}$ That is, if $\map {P_j} x$ is the [[Definition:Polynomial (Analysis)|polynomial]]: :$\displaystyle \map {P_j} x := \sum_{k \mathop = 1}^n b_{k j}x^{k - 1}$ then: :$\map {P_j} {x_1} = 0, \ldots, \map {P_j} {x_{j - 1} } = 0, \map {P_j} {x_j} = 1, \map {P_j} {x_{j + 1} } = 0, \ldots, \map {P_j} {x_n} = 0$ By the [[Lagrange Interpolation Formula]], the $j$th [[Definition:Row of Matrix|row]] of $B$ is composed of the [[Definition:Polynomial Coefficient|coefficients]] of the $j$th [[Definition:Lagrange Basis Polynomial|Lagrange basis polynomial]]: :$\displaystyle \map {P_j} x = \sum_{k \mathop = 1}^n b_{k j} x^{k - 1} = \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \frac {x - x_m} {x_j - x_m}$ Identifying the $k$th order [[Definition:Polynomial Coefficient|coefficient]] in these two [[Definition:Polynomial (Analysis)|polynomial]]s yields: :$b_{k j} = \begin{cases} \paren {-1}^{n - k} \paren {\dfrac {\displaystyle \sum_{\substack {1 \mathop \le m_1 \mathop < \ldots \mathop < m_{n - k} \mathop \le n \\ m_1, \ldots, m_{n - k} \mathop \ne j} } x_{m_1} \cdots x_{m_{n - k} } } {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m} } } & : 1 \le k < n \\ \qquad \qquad \qquad \dfrac 1 {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m}} & : k = n \end{cases}$ which gives: :$b_{k j} = \begin{cases} \paren {-1}^{k - 1} \paren {\dfrac {\displaystyle \sum_{\substack {1 \mathop \le m_1 \mathop < \ldots \mathop < m_{n - k} \mathop \le n \\ m_1, \ldots, m_{n - k} \mathop \ne j} } x_{m_1} \cdots x_{m_{n - k} } } {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_m - x_j} } } & : 1 \le k < n \\ \qquad \qquad \qquad \dfrac 1 {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m} } & : k = n \end{cases}$ For the general case, we observe that by simple multiplication: :$\displaystyle V_n = \begin {pmatrix} \begin {bmatrix} x_1 & 0 & \cdots & 0 \\ 0 & x_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & x_n \end {bmatrix} \cdot W_n \end {pmatrix}^\intercal$ So by [[Inverse of Matrix Product]] and [[Inverse of Diagonal Matrix]]: :$\displaystyle V_n^{-1} = \begin {bmatrix} x_1^{-1} & 0 & \cdots & 0 \\ 0 & x_2^{-1} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & x_n^{-1} \end {bmatrix} \cdot \paren {W_n^{-1} }^\intercal$ Let $c_{k j}$ denote the $\tuple {k, j}$th coefficient of $V_n^{-1}$. Since the first matrix in the product expression for $V_n^{-1}$ above is [[Definition:Diagonal Matrix|diagonal]]: :$c_{kj} = \dfrac 1 {x_k} b_{j k}$ which establishes the result. {{Qed}}	1
Let: : $\mathbf f \left({x}\right) = \displaystyle \sum_{k \mathop = 1}^n f_k \left({x}\right) \mathbf e_k$ be a [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued function]]. The [[Definition:Dot Product|dot product]] of $\mathbf f$ with its [[Definition:Derivative of Vector-Valued Function|derivative]] is given by: :$\mathbf f \left({x}\right) \cdot \dfrac {\d \mathbf f \left({x}\right)} {\d x} = \left\lvert{\mathbf f \left({x}\right)}\right\rvert \dfrac {\d \left\lvert{\mathbf f \left({x}\right)}\right\rvert} {\d x}$ where $\left\lvert{\mathbf f \left({x}\right)}\right\rvert \ne 0$.	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $S = \set {a_1, a_2, \dotsc, a_n}$ be a [[Definition:Set|set]] of non-[[Definition:Ring Zero|zero]] [[Definition:Element|elements]] of $D$. Let $y_1$ and $y_2$ be [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisors]] of $S$. Then $y_1$ and $y_2$ are [[Definition:Associate in Integral Domain|associates]].	1
Let $I := \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $\map \CC I$ be a [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|space of continuous on closed interval real-valued functions]]. Let $\map {\CC^1} I$ be a [[Definition:Space of Continuous Functions of Differentiability Class k|space of continuously differentiable functions]] on [[Definition:Closed Real Interval|closed interval]] $I$. Let $\struct {\R, +_\R, \times_\R}$ be the [[Definition:Field of Real Numbers|field of real numbers]]. Let $\paren +$ be the [[Definition:Pointwise Addition of Real-Valued Functions|pointwise addition of real-valued functions]]. Let $\paren {\, \cdot \,}$ be the [[Definition:Pointwise Scalar Multiplication of Real-Valued Functions|pointwise scalar multiplication of real-valued functions]]. Then $\struct {\map {\CC^1} I, +, \, \cdot \,}_\R$ is a [[Definition:Vector Space|vector space]].	1
We have that [[Integers are Euclidean Domain]]. Then we have that [[Euclidean Domain is Principal Ideal Domain]]. Hence the result. {{qed}}	1
=== Sufficient condition === Let $U$ be [[Definition:Cyclic Group|cyclic]]. Let $n \ge 0$ be an integer. Let $n = p_1^{e_1} \cdots p_r^{e_r}$, be the [[Definition:Prime Decomposition|decomposition]] of $n$ into [[Definition:Distinct|distinct]] [[Definition:Prime Number|prime]] [[Definition:Power (Algebra)|powers]] given by the [[Fundamental Theorem of Arithmetic]]. Then by the [[Chinese Remainder Theorem/Corollary|corollary to the Chinese remainder theorem]] we have an [[Definition:Ring Isomorphism|isomorphism]]: :$\Z / n \Z \simeq \Z / p_1 \Z \times \cdots \times \Z / p_r \Z$ By [[Units of Direct Product are Direct Product of Units]] we have: :$\paren {\Z / n \Z}^\times \simeq \paren {\Z / p_1 \Z}^\times \times \cdots \times \paren {\Z / p_r \Z}^\times$ Suppose that $r \ge 2$, and choose $i, j \in \set {1, \ldots, r}$ such that $i \ne j$. If $\paren {\Z / p_i \Z}^\times$ or $\paren {\Z / p_j \Z}^\times$ is not [[Definition:Cyclic Group|cyclic]], then $\paren {\Z / n \Z}^\times$ cannot be [[Definition:Cyclic Group|cyclic]]. {{explain|Link to a result establishing the above fact.}} Therefore suppose that $\paren {\Z / p_i \Z}^\times$ and $\paren {\Z / p_j \Z}^\times$ are [[Definition:Cyclic Group|cyclic]]. By [[Order of Group of Units of Integers Modulo m]] these [[Definition:Group|groups]] have [[Definition:Order of Structure|orders]]: :$\map \phi {p_i^{e_i} }$ and: :$\map \phi { p_j^{e_j} }$ respectively, where $\phi$ is the [[Definition:Euler Phi Function|Euler $\phi$ function]]. By [[Euler Phi Function of Integer]] we have: :$\map \phi {p_i^{e_i} } = p_i^{e_i - 1} \paren {p_i - 1}$ and :$\map \phi {p_j^{e_j} } = p_j^{e_j - 1} \paren {p_j - 1}$ If $p_i, p_j$ are [[Definition:Odd Integer|odd]], $2$ divides $p_i - 1$ and $p_j - 1$. Therefore $2$ divides $\map \phi {p_i^{e_i} }$ and $\map \phi {p_j^{e_j} }$. In particular, $\map \phi {p_i^{e_i} }$ and $\map \phi {p_j^{e_j} }$ are not [[Definition:Coprime Integers|coprime]]. Now by [[Group Direct Product of Cyclic Groups]], $\left({\Z / n \Z}\right)^\times$ is not [[Definition:Cyclic Group|cyclic]]. Let $p_i$ or $p_j$ be [[Definition:Even Integer|even]]. {{WLOG}}, we can assume $p_i = 2$. Then: :$\map \phi {p_i^{e_i} } = \map \phi {2^{e_i} } = p_i^{e_i - 1} \paren {p_i - 1}$ So if $e_i \ge 2$, then $2$ divides $\map \phi {p_i^{e_i} }$ and $\map \phi {p_j^{e_j} }$. In particular $\map \phi {p_i^{e_i} }$ and $\map \phi {p_j^{e_j} }$ are not [[Definition:Coprime Integers|coprime]]. Again by [[Group Direct Product of Cyclic Groups]], $\paren {\Z / n \Z}^\times$ is not [[Definition:Cyclic Group|cyclic]]. Thus if $\paren {\Z / n \Z}^\times$ is [[Definition:Cyclic Group|cyclic]], then $n = 2^e \times p^\alpha$ with $e = 0$ or $e = 1$, $\alpha \ge 0$ and $p \ge 3$ [[Definition:Prime Number|prime]]. {{Qed|lemma}} === Necessary Condition === {{ProofWanted}} [[Category:Ring of Integers Modulo m]] kzf5rgrqazm8xul4km4k01d3vtkttqk	1
Let $v \in V$ and $s \in R$. Then: {{begin-eqn}} {{eqn|l = m_r \left({s \circ v}\right) |r = r \circ \left({s \circ v}\right) |c = Definition of [[Definition:Rescaling|rescaling]] }} {{eqn|r = \left({r \cdot s}\right) \circ v |c = $V$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = \left({s \cdot r}\right) \circ v |c = $R$ is a [[Definition:Commutative Ring|commutative ring]] }} {{eqn|r = s \circ \left({r \circ v}\right) |c = $V$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = s \circ m_r \left({v}\right) |c = Definition of [[Definition:Rescaling|rescaling]] }} {{end-eqn}} Next, for $v,w \in V$: {{begin-eqn}} {{eqn|l = m_r \left({v + w}\right) |r = r \circ \left({v + w}\right) |c = Definition of [[Definition:Rescaling|rescaling]] }} {{eqn|r = r \circ v + r \circ w |c = $V$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = m_r \left({v}\right) + m_r \left({w}\right) |c = Definition of [[Definition:Rescaling|rescaling]] }} {{end-eqn}} It follows that $m_r$ is a [[Definition:Linear Transformation|linear transformation]]. {{qed}} [[Category:Linear Transformations]] f21xhuuisueybdb7ar42npjhkjtfq0t	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\GL {n, K}$ be the [[Definition:General Linear Group|general linear group]] of [[Definition:Order of Square Matrix|order $n$]] over $K$. Then $\GL {n, K}$ is a [[Definition:Group|group]].	1
Let us take each type of [[Definition:Elementary Row Operation|elementary row operation]] in turn. For each $\map e {\mathbf A}$, we will construct $\map {e'} {\mathbf A'}$ which will transform $\mathbf A'$ into a new [[Definition:Matrix|matrix]] $\mathbf A'' \in \map \MM {m, n}$, which will then be demonstrated to equal $\mathbf A$. In the below, let: :$r_k$ denote [[Definition:Row of Matrix|row]] $k$ of $\mathbf A$ :$r'_k$ denote [[Definition:Row of Matrix|row]] $k$ of $\mathbf A'$ :$r''_k$ denote [[Definition:Row of Matrix|row]] $k$ of $\mathbf A''$ for arbitrary $k$ such that $1 \le k \le m$. By definition of [[Definition:Elementary Row Operation|elementary row operation]]: :only the [[Definition:Row of Matrix|row]] or [[Definition:Row of Matrix|rows]] directly operated on by $e$ is or are different between $\mathbf A$ and $\mathbf A'$ and similarly: :only the [[Definition:Row of Matrix|row]] or [[Definition:Row of Matrix|rows]] directly operated on by $e'$ is or are different between $\mathbf A'$ and $\mathbf A''$. Hence it is understood that in the following, only those [[Definition:Row of Matrix|rows]] directly affected will be under consideration when showing that $\mathbf A = \mathbf A''$. === [[Existence of Inverse Elementary Row Operation/Scalar Product of Row|$\text {ERO} 1$: Scalar Product of Row]] === {{:Existence of Inverse Elementary Row Operation/Scalar Product of Row}}{{qed|lemma}} === [[Existence of Inverse Elementary Row Operation/Add Scalar Product of Row to Another|$\text {ERO} 2$: Add Scalar Product of Row to Another]] === {{:Existence of Inverse Elementary Row Operation/Add Scalar Product of Row to Another}}{{qed|lemma}} === [[Existence of Inverse Elementary Row Operation/Exchange Rows|$\text {ERO} 3$: Exchange Rows]] === {{:Existence of Inverse Elementary Row Operation/Exchange Rows}}{{qed|lemma}} Thus in all cases, for each [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A$ to $\mathbf A'$, we have constructed the only possible [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A'$ to $\mathbf A$. Hence the result. {{qed}}	1
Let $\struct {R_1, \norm {\, \cdot \,}_1 }, \struct {R_2, \norm {\, \cdot \,}_2 }$ be [[Definition:Normed Division Ring|normed division rings]]. Let $\struct {R_2, \norm {\, \cdot \,}_2 }$ be a [[Definition:Completion (Normed Division Ring)|completion]] of $\struct {R_1, \norm {\, \cdot \,}_1 }$ with [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]] $\phi: R_1 \to R_2$. Then for all $x \in R_2$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $R_1$: :$x = \displaystyle \lim_{n \mathop \to \infty} \map \phi {x_n}$	1
Let $A = \left({A_F, \oplus}\right)$ be an [[Definition:Algebra over Field|algebra over a field]] $F$. Then $A$ is a [[Definition:Division Algebra|division algebra]] {{iff}} it has no [[Definition:Zero Divisor of Algebra|zero divisors]]. That is: :$\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$ If the product of two elements of $A$ is zero, then at least one of those elements must itself be zero. Some sources use this as the definition of a [[Definition:Division Algebra|division algebra]] and from it deduce: :$\forall a, b \in A_F, b \ne \mathbf 0_A: \exists_1 x \in A_F, y \in A_F: a = b \oplus x, a = y \oplus b$	1
Let $G$ be an [[Definition:Dimension of Vector Space|$n$-dimensional]] [[Definition:Vector Space|vector space]]. Let $H$ be a [[Definition:Vector Space|vector space]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Let $\map \rho \phi$ and $\map \nu \phi$ be the [[Definition:Rank of Linear Transformation|rank]] and [[Definition:Nullity of Linear Transformation|nullity]] respectively of $\phi$. Then the [[Definition:Image of Mapping|image]] of $\phi$ is [[Definition:Finite Dimensional Vector Space|finite-dimensional]], and: :$\map \rho \phi + \map \nu \phi = n$ By definition of [[Definition:Rank of Linear Transformation|rank]] and [[Definition:Nullity of Linear Transformation|nullity]], it can be seen that this is equivalent to the alternative way of stating this result: :$\map \dim {\Img \phi} + \map \dim {\map \ker \phi} = \map \dim G$ {{wtd|and the theorem is applicable to matrices}}	1
We show that the [[Definition:Set Complement|complement]] $X \setminus \map {B_\epsilon^-} x$ is [[Definition:Open Set in Normed Vector Space|open]] in $M$. Let $y \in X \setminus \map {B_\epsilon^-} x$. Then by definition of [[Definition:Closed Ball in Normed Vector Space|closed ball]]: :$\norm {x - y} > \epsilon$ Put: :$\delta := \norm {x - y} - \epsilon > 0$ Then: :$\norm {x - y} - \delta = \epsilon$ Let $z \in \map {B_\delta} y$. Then: {{begin-eqn}} {{eqn | l = \norm {x - z} | o = \ge | r = \norm {x - y} - \norm {y - z} | c = [[Reverse Triangle Inequality]] }} {{eqn | o = > | r = \norm {x - y} - \delta }} {{eqn | r = \epsilon }} {{end-eqn}} and so: :$z \notin \map {B_\epsilon^-} x$ Then: :$\map {B_\delta} y \subseteq X \setminus \map {B_\epsilon^-} x$ so $X \setminus \map {B_\epsilon^-} x$ is [[Definition:Open Set in Normed Vector Space|open]] in $M$. Hence, by definition of [[Definition:Closed Set in Normed Vector Space|closed set]]: :$\map {B_\epsilon^-} x$ is [[Definition:Closed Set in Normed Vector Space|closed]] in $M$. {{qed}}	1
Let $d_\infty$ be the [[Definition:Chebyshev Distance|Chebyshev distance metric]] on $R \times R$. Let $\tau^\times$ be the [[Definition:Product Topology|product topology]] on $R \times R$. By [[P-Product Metric Induces Product Topology|$p$-Product Metric Induces Product Topology]], $\tau^\times$ is the [[Definition:Topology Induced by Metric|topology induced by the metric]] $d_\infty$. Let $R^* = R \setminus \set 0$. Let $d^*$ be the [[Definition:Restriction|restriction]] of $d$ to $R^*$. Let $\tau^*$ be the [[Definition:Subspace Topology|subspace topology]] on $R^*$. By [[Metric Subspace Induces Subspace Topology]] then $\tau^*$ is the [[Definition:Topology Induced by Metric|topology induced by the metric]] $d^*$ By [[Normed Division Ring Operations are Continuous]] and [[Continuous Mapping is Continuous on Induced Topological Spaces]], the [[Definition:Mapping|mappings]]: ::$\phi : \struct {R \times R, \tau^\times} \to \struct {R, \tau} : \map \phi {x, y} = x + y$ ::$\theta : \struct {R ,\tau} \to \struct {R, \tau} : \map \theta x = -x$ ::$\psi : \struct {R \times R, \tau^\times} \to \struct {R, \tau} : \map \psi {x, y} = x y$ ::$\xi : \struct {R^* ,\tau^*} \to \struct {R, \tau} : \map \xi x = x^{-1}$ are [[Definition:Continuous Mapping (Topology)|continuous]]. By the definition of a [[Definition:Topological Division Ring|topological division ring]] then the result follows. {{qed}}	1
If two [[Definition:Column of Matrix|columns]] of a [[Definition:Square Matrix|square matrix]] over a [[Definition:Commutative Ring|commutative ring]] $\struct {R, +, \circ}$ are identical, then its [[Definition:Determinant of Matrix|determinant]] is zero.	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\Gamma_1$ be a [[Definition:Row Operation|row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$. Let $\Gamma_2$ be a [[Definition:Row Operation|row operation]] which transforms $\mathbf B$ to another new [[Definition:Matrix|matrix]] $\mathbf C \in \map \MM {m, n}$. Then there exists another [[Definition:Row Operation|row operation]] $\Gamma$ which transforms $\mathbf A$ back to $\mathbf C$ such that $\Gamma$ consists of $\Gamma_1$ followed by $\Gamma_2$.	1
A [[Definition:Totally Bounded Metric Space|totally bounded metric space]] is [[Definition:Separable Space|separable]].	1
Let $V$ be an [[Definition:Inner Product Space|inner product space]] over a [[Definition:Field (Abstract Algebra)|subfield]] $\Bbb F$ of $\C$. Let $\left\langle{\cdot, \cdot}\right\rangle_V$ be the [[Definition:Inner Product|inner product]] on $V$. Let $d: V \times V \to \R_{\ge 0}$ be the [[Definition:Metric Induced by Norm|metric induced]] by the [[Definition:Inner Product Norm|inner product norm]]. Let $H$ be the [[Definition:Completion (Metric Space)|completion]] of $V$ with respect to $d$. Then $\left\langle{\cdot, \cdot}\right\rangle_V$ can be extended to an [[Definition:Inner Product|inner product]] on $H$. By definition, $H$ will be a [[Definition:Hilbert Space|Hilbert space]]. Therefore, the theorem can alternatively be stated as: :Any [[Definition:Inner Product Space|inner product space]] may be [[Definition:Completion (Metric Space)|completed]] to a [[Definition:Hilbert Space|Hilbert space]].	1
This is demonstrated by straightforward application of [[Definition:Matrix Product (Conventional)|conventional matrix multiplication]]: {{begin-eqn}} {{eqn | l = \mathbf i \mathbf j | r = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} i \cdot 0 + 0 \cdot -1 & i \cdot 1 + 0 \cdot 0 \\ 0 \cdot 0 + -i \cdot -1 & 0 \cdot 1 + -i \cdot 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = \mathbf k | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = -\mathbf j \mathbf i | r = -\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 0 \cdot i + 1 \cdot 0 & 0 \cdot 0 + 1 \cdot -i \\ -1 \cdot i + 0 \cdot 0 & -1 \cdot 0 + 0 \cdot -i \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 0 & -i \\ -i & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = \mathbf k | c = }} {{end-eqn}} ---- {{begin-eqn}} {{eqn | l = \mathbf j \mathbf k | r = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 \cdot 0 + 1 \cdot i & 0 \cdot i + 1 \cdot 0 \\ -1 \cdot 0 + 0 \cdot i & -1 \cdot i + 0 \cdot 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} | c = }} {{eqn | r = \mathbf i | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = -\mathbf k \mathbf j | r = -\begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 0 \cdot 0 + i \cdot -1 & 0 \cdot 1 + i \cdot 0 \\ i \cdot 0 + 0 \cdot -1 & i \cdot 1 + 0 \cdot 0 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} -i & 0 \\ 0 & i \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} | c = }} {{eqn | r = \mathbf i | c = }} {{end-eqn}} ---- {{begin-eqn}} {{eqn | l = \mathbf k \mathbf i | r = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 \cdot i + i \cdot 0 & 0 \cdot 0 + i \cdot -i \\ i \cdot i + 0 \cdot 0 & i \cdot 0 + 0 \cdot -i \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} | c = }} {{eqn | r = \mathbf j | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = -\mathbf i \mathbf k | r = -\begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} i \cdot 0 + 0 \cdot i & i \cdot i + 0 \cdot 0 \\ 0 \cdot 0 + -i \cdot i & 0 \cdot i + -i \cdot 0 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} | c = }} {{eqn | r = \mathbf j | c = }} {{end-eqn}} ---- {{begin-eqn}} {{eqn | l = \mathbf i^2 | r = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} i \cdot i + 0 \cdot 0 & i \cdot 0 + 0 \cdot -i \\ 0 \cdot i + -i \cdot 0 & -i \cdot 0 + -i \cdot -i \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} | c = }} {{eqn | r = -\mathbf 1 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \mathbf j^2 | r = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 \cdot 0 + 1 \cdot -1 & 0 \cdot 1 + 1 \cdot 0 \\ -1 \cdot 0 + 0 \cdot -1 & -1 \cdot 1 + 0 \cdot 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} | c = }} {{eqn | r = -\mathbf 1 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \mathbf k^2 | r = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 0 \cdot 0 + i \cdot i & 0 \cdot i + i \cdot 0 \\ i \cdot 0 + 0 \cdot i & i \cdot i + 0 \cdot 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} | c = }} {{eqn | r = -\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} | c = }} {{eqn | r = -\mathbf 1 | c = }} {{end-eqn}} ---- {{begin-eqn}} {{eqn | l = \mathbf i \mathbf j \mathbf k | r = \paren {\mathbf i \mathbf j} \mathbf k | c = [[Matrix Multiplication is Associative]] }} {{eqn | r = \mathbf k \mathbf k | c = from above: $\mathbf i \mathbf j = \mathbf k$ }} {{eqn | r = -\mathbf 1 | c = from above: $\mathbf k^2 = - \mathbf 1$ }} {{end-eqn}} {{qed}}	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $A$ and $B$ be $k$-[[Definition:Unital Associative Commutative Algebra|algebras]]. Let $f : A \to B$ be a $k$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]]. Let $B$ be [[Definition:Finitely Generated Algebra|finitely generated]] over $k$. Let $\mathfrak m$ be a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $B$. Then its [[Definition:Preimage of Subset under Mapping|preimage]] $f^{-1}(\mathfrak m)$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $A$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A = \displaystyle \sum_{i \mathop \in I} \alpha_i P_i$ be a [[Definition:Diagonalizable Operator|diagonalizable operator]] on $H$. Let $B \in \map B H$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Then the following are equivalent: :$(1): \quad A B = B A$ :$(2): \quad$ For all $i \in I$, $\Rng {P_i}$ is a [[Definition:Reducing Subspace|reducing subspace]] for $B$ where $\Rng {P_i}$ denotes [[Definition:Range of Relation|range]].	1
Let $\struct {X, \norm {\, \cdot \,}}$ is a [[Definition:Normed Vector Space|normed vector space]]. Let $D \subseteq X$ be a [[Definition:Subset|subset]] of $X$. Let $D^-$ be the [[Definition:Closure/Normed Vector Space|closure]] of $D$. Then $D$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] iff $D^- = X$.	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. The [[Definition:Determinant of Matrix|determinant]] of the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]] over $R$ is equal to $1_R$.	1
Follows directly from: :[[Separable Metric Space is Second-Countable]] :[[Second-Countable Space is Separable]] {{qed}} {{ACC|Second-Countable Space is Separable}}	1
{{begin-eqn}} {{eqn | l = \cmod {t^z} | r = \cmod {t^{\map \Re z + i \map \Im z} } }} {{eqn | r = \cmod {t^{\map \Re z} t^{i \map \Im z} } | c = [[Sum of Complex Indices of Real Number]] }} {{eqn | r = \cmod {t^{\map \Re z} } \cmod {t^{i \map \Im z} } | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | r = \cmod {t^{\map \Re z} } | c = [[Modulus of Exponential of Imaginary Number is One#Corollary|Modulus of Exponential of Imaginary Number is One:Corollary]] }} {{eqn | r = t^{\map \Re z} | c = [[Power of Positive Real Number is Positive/Real Number|Power of Positive Real Number is Positive]] }} {{end-eqn}} {{qed}} [[Category:Complex Modulus]] [[Category:Complex Analysis]] ix6871u9khqhoqtc8e7ei0pqor9yd6y	1
{{AimForCont}} the supposition false. That is, suppose there is at least one [[Definition:Positive Integer|positive integer]] that can be expressed in more than one way as a product of [[Definition:Prime Number|primes]]. Let the smallest of these be $m$. Thus: :$m = p_1 p_2 \cdots p_r = q_1 q_2 \cdots q_s$ where all of $p_1, \ldots p_r, q_1, \ldots q_s$ are [[Definition:Prime Number|prime]]. By definition, $m$ is not itself [[Definition:Prime Number|prime]]. Therefore: :$r, s \ge 2$ Let us arrange that the [[Definition:Prime Number|primes]] which compose $m$ are in order of size: :$p_1 \le p_2 \le \dots \le p_r$ and: :$q_1 \le q_2 \le \dots \le q_s$ Let us arrange that $p_1 \le q_1$. Suppose $p_1 = q_1$. Then: :$\dfrac m {p_1} = p_2 p_3 \cdots p_r = q_2 q_3 \cdots q_s = \dfrac m {q_1}$ But then we have the [[Definition:Positive Integer|positive integer]] $\dfrac m {p_1}$ being expressible in two different ways. This contradicts the fact that $m$ is the smallest [[Definition:Positive Integer|positive integer]] that can be so expressed. Therefore: :$p_1 \ne q_1 \implies p_1 < q_1 \implies p_1 < q_2, q_3, \ldots, q_s$ as we arranged them in order. From [[Prime not Divisor implies Coprime]]: :$1 < p_1 < q_j: 1 < j < s \implies p_1 \nmid q_j$ But: :$p_1 \divides m \implies p_1 \divides q_1 q_2 \ldots q_s$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Thus from [[Euclid's Lemma for Prime Divisors]]: :$\exists j: 1 \le j \le s: p_1 \divides q_j$ But $q_j$ was supposed to be a [[Definition:Prime Number|prime]]. This is a [[Definition:Contradiction|contradiction]]. Hence, by [[Proof by Contradiction]], the supposition was false. {{Qed}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Module|module]] over $R$ whose [[Definition:Dimension of Module|dimension]] is [[Definition:Finite|finite]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. Let $N$ be a [[Definition:Submodule|submodule]] of $G^*$. Let $J$ be the [[Definition:Evaluation Isomorphism|evaluation isomorphism]] from $G$ onto $G^{**}$. Let $N^\circ$ be the [[Definition:Annihilator on Algebraic Dual|annihilator]] of $N$. Then: :$J^{-1} \left({N^\circ}\right) = \left\{{x \in G: \forall t' \in N: t' \left({x}\right) = 0}\right\}$	1
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $M$ be a [[Definition:Square Matrix|square matrix]] over $R$ of [[Definition:Order of Square Matrix|order]] $n > 0$. Let $I_n$ be the $n\times n$ [[Definition:Identity Matrix|identity matrix]]. Let $R[x]$ be the [[Definition:Polynomial Ring in One Variable|polynomial ring in one variable]] over $R$. The '''characteristic polynomial''' of $M$ is the [[Definition:Determinant of Matrix|determinant]] of a matrix over $R[x]$: :$p_M (x) = \operatorname{det}(xI - M)$.	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $A_{r s}$ denote the [[Definition:Cofactor of Element|cofactor]] of the [[Definition:Element of Matrix|element]] whose [[Definition:Index of Matrix Element|indices]] are $\tuple {r, s}$. The '''cofactor matrix''' of $\mathbf A$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]: :$\mathbf C = \begin {bmatrix} A_{1 1} & A_{1 2} & \cdots & A_{1 n} \\ A_{2 1} & A_{2 2} & \cdots & A_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n 1} & A_{n 2} & \cdots & A_{n n} \\ \end {bmatrix}$	1
Let $s, t \in \closedint 1 m$ such that $s \ne t$. === Case $1$ === Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\kappa_s \to \lambda \kappa_s$: :$E_{k i} = \begin{cases} \delta_{k i} & : i \ne s \\ \lambda \delta_{k i} & : i = s \end{cases}$ where $\delta$ denotes the [[Definition:Kronecker Delta|Kronecker delta]]. Then: {{begin-eqn}} {{eqn | l = \sqbrk {A E}_{i j} | r = \sum_{k \mathop = 1}^m A_{j k} E_{k i} | c = }} {{eqn | r = \begin {cases} A_{j i} & : i \ne r \\ \lambda A_{j i} & : i = r \end{cases} | c = }} {{eqn | ll= \leadsto | l = \mathbf {A E} | r = e \paren {\mathbf A} | c = }} {{end-eqn}} {{qed|lemma}} === Case $2$ === Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\kappa_s \to \kappa_s + \lambda \kappa_t$: :$E_{k i} = \begin {cases} \delta_{k i} & : i \ne s \\ \delta_{k s} + \lambda \delta_{k t} & : i = s \end {cases}$ where $\delta$ denotes the [[Definition:Kronecker Delta|Kronecker delta]]. Then: {{begin-eqn}} {{eqn | l = \sqbrk {A E}_{j i} | r = \sum_{k \mathop = 1}^m A_{j k} E_{k i} | c = }} {{eqn | r = \begin {cases} A_{j i} & : j \ne s \\ A_{i j} + \lambda A_{j t} & : i = s \end {cases} | c = }} {{eqn | ll= \leadsto | l = \mathbf {A E} | r = e \paren {\mathbf A} | c = }} {{end-eqn}} {{qed|lemma}} === Case $3$ === Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\kappa_s \leftrightarrow \kappa_t$: By [[Exchange of Columns as Sequence of Other Elementary Column Operations]], this [[Definition:Elementary Column Operation|elementary column operation]] can be expressed as: :$\paren {e_1 e_2 e_3 e_4 \mathbf A} = e \paren {\mathbf A}$ where the $e_i$ are [[Definition:Elementary Column Operation|elementary column operation]] of the other two types. For each $e_i$, let $\mathbf E_i = e_i \paren {\mathbf I}$. Then: {{begin-eqn}} {{eqn | l = e \paren {\mathbf A} | r = e_1 e_2 e_3 e_4 \paren {\mathbf A} | c = Definition of $e$ }} {{eqn | r = \mathbf A \mathbf E_4 \mathbf E_3 \mathbf E_2 \mathbf E_1 | c = Cases $1$ and $2$ }} {{eqn | r = \mathbf A e_4 \paren {\mathbf I} \mathbf E_3 \mathbf E_2 \mathbf E_1 }} {{eqn | r = \mathbf A e_3 e_4 \paren {\mathbf I} \mathbf E_2 \mathbf E_1 }} {{eqn | r = \mathbf A e_2 e_3 e_4 \paren {\mathbf I} \mathbf E_1 }} {{eqn | r = \mathbf A e_1 e_2 e_3 e_4 \paren {\mathbf I} }} {{eqn | r = \mathbf A e \paren {\mathbf I} }} {{eqn | r = \mathbf A \mathbf E }} {{end-eqn}} {{qed}}	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :$\exists \alpha \in \R_{\gt 0}: \forall n \in \N: \norm n_1 = \norm n_2^\alpha$ By [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] then: :$\forall n \in \N: \norm {-n}_1 = \norm n_1 = \norm n_2^\alpha = \norm {-n}_2^\alpha$ Hence: :$\forall k \in \Z: \norm k_1 = \norm k_2^\alpha$ By [[Properties of Norm on Division Ring/Norm of Quotient|Norm of Quotient]] then: :$\forall \dfrac a b \in \Q: \norm {\dfrac a b}_1 = \dfrac {\norm a_1} {\norm b_1} = \dfrac {\norm a_2^\alpha} {\norm b_2^\alpha} = \norm {\dfrac a b}_2^\alpha$ By [[Definition:Equivalent Division Ring Norms/Norm is Power of Other Norm|Norm is Power of Other Norm]] then $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ are [[Definition:Equivalent Division Ring Norms|equivalent]].	1
:$G$ is a [[Definition:Right Ideal|right ideal]] of $\struct {\map {\MM_S} 2, +, \times}$.	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $A \in B \struct {H, K}$ be a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. Then $\ker A = \paren {\Img {A^*} }^\perp$, where: :$A^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$ :$\ker A$ is the [[Definition:Kernel of Linear Transformation|kernel]] of $A$ :$\Img {A^*}$ is the [[Definition:Image of Mapping|image]] of $A^*$ :$\perp$ signifies [[Definition:Orthocomplement|orthocomplementation]]	1
Let the [[Definition:Quaternion|set of quaternions]] be denoted $\struct {\H, +, \times}$. From [[Quaternions form Skew Field]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\H, +, \times}$ is a [[Definition:Skew Field|skew field]]. By definition, a [[Definition:Skew Field|skew field]] is a [[Definition:Division Ring|division ring]]. {{ProofWanted}}	1
Follows directly from the fact that a [[Group is Subgroup of Itself|group is a subgroup of itself]]. {{qed}}	1
From [[Special Linear Group is Subgroup of General Linear Group]] we have that the [[Definition:Special Linear Group|special linear group]] $\SL {n, K}$ is a [[Definition:Subgroup|subgroup]] of $\GL {n, K}$. From [[Special Linear Group is not Abelian]], $\SL {n, K}$ is not [[Definition:Abelian Group|abelian]]. From [[Subgroup of Abelian Group is Abelian]] it follows by the [[Rule of Transposition]] that $\GL {n, K}$ is not [[Definition:Abelian Group|abelian]]. {{qed}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}$ be a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. Let $\circ' : R \times G \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: \lambda \circ' x = x \circ \lambda $ Then $\struct {G, +_G, \circ'}$ is a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$ {{iff}}: :$\forall \lambda, \mu \in R: \forall x \in G: x \circ \paren{ \lambda \times_R \mu} = x \circ \paren {\mu \times_R \lambda}$	1
=== [[Definition:Orthogonal Basis of Vector Space|Orthogonal Basis of Vector Space]] === {{:Definition:Orthogonal Basis of Vector Space}} === [[Definition:Orthogonal Basis of Inner Product Space|Orthogonal Basis of Inner Product Space]] === {{:Definition:Orthogonal Basis of Inner Product Space}} === [[Definition:Orthogonal Basis of Bilinear Space|Orthogonal Basis of Bilinear Space]] === {{:Definition:Orthogonal Basis of Bilinear Space}} [[Category:Definitions/Linear Algebra]] 5ccrihgddx0hdkbw28ybw4mk0efqf71	1
Let $e_3$ be the [[Definition:Elementary Row Operation|elementary row operation]] $\text {ERO} 3$: {{begin-axiom}} {{axiom | n = \text {ERO} 3 | t = Exchange [[Definition:Row of Matrix|rows]] $i$ and $j$ | m = r_i \leftrightarrow r_j }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_3$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e_3$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_3$ is: :$\map \det {\mathbf E_3} = -1$	1
Let: :$ \mathbf A_{m \times n} = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}$ :$\mathbf x_{n \times 1} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$ :$\mathbf y_{n \times 1} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$ be [[Definition:Matrix|matrices]] where each [[Definition:Column of Matrix|column]] is an [[Definition:Element|element]] of a [[Definition:Real Vector Space|real vector space]]. Let $T$ be the [[Definition:Mapping|mapping]]: :$T: \R^m \to \R^n, \mathbf x \mapsto \mathbf A \mathbf x$ Then $T$ is a [[Definition:Linear Transformation on Vector Space|linear transformation]].	1
Let $g, h \in H$. Then by the definition of [[Definition:Adjoint Linear Transformation|adjoint]]: :$\left\langle{Ag, Ah}\right\rangle_K = \left\langle{A^*Ag, h}\right\rangle_H$ From the [[Definition:Adjoint Linear Transformation|uniqueness of the adjoint]], it follows that: :$\left\langle{Ag, Ah}\right\rangle_K = \left\langle{g, h}\right\rangle_H$ holds [[Definition:Iff|iff]] $A^*A = I_H$. Hence the result by definition of [[Definition:Isometry (Hilbert Spaces)|isometry]]. {{qed}}	1
Follows directly from [[Cardinality of Set of Bijections]]. {{qed}}	1
From [[Ring of Integers Modulo m is Ring]], $\left({\Z_m, +_m, \times_m}\right)$ is a [[Definition:Commutative and Unitary Ring|commutative ring with unity $\left[\!\left[{1}\right]\!\right]_m$]]. Thus by definition $\left({\Z_m, \times_m}\right)$ is a [[Definition:Commutative Monoid|commutative monoid]]. The result follows from [[Multiplicative Inverse in Monoid of Integers Modulo m]]. {{qed}}	1
:$\map f {x_1} \equiv 0 \pmod p$	1
Let $\epsilon > 0$ be given. By assumption $\exists N \in \N$ such that: :$(1) \quad \forall n > N: \norm {x_{n + 1} - x_n} < 0$ Suppose $n, m > N$, and $m = n + r > n$. Then: {{begin-eqn}} {{eqn | l = \norm {x_m - x_n} | r = \norm {x_{n + r} - x_{n + r - 1} + x_{n + r - 1} - x_{n + r - 2} + \dotsb + x_{n + 1} - x_n} }} {{eqn | r = \max \set {\norm {x_{n + r} - x_{n + r - 1} }, \norm {x_{n + r - 1} - x_{n + r - 2} }, \dotsc, \norm {x_{n + 1} - x_n} } | c = as $\norm {\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] }} {{eqn | r = \norm {x_{n + s} - x_{n + s - 1} } | c = for some $s$: $0 < s \le r$ }} {{eqn | o = < | r = \epsilon | c = by $(1)$ }} {{end-eqn}} It follows that: ::$\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]].	1
By [[Matrices over Field form Vector Space]]: :$K^{n \times n}$ forms a [[Definition:Vector Space|vector space]] under usual [[Definition:Matrix Addition|matrix addition]] and scalar multiplication. By [[Dimension of Vector Space of Matrices]]: :$K^{n \times n}$ has [[Definition:Dimension (Linear Algebra)|dimension]] $n^2$. Consider the collection of vectors: :$I, A, A^2, \ldots, A^{n^2}$ Since this is a collection of $n^2 + 1$ vectors, and $K^{n \times n}$ has [[Definition:Dimension (Linear Algebra)|dimension]] $n^2$, we have by [[Size of Linearly Independent Subset is at Most Size of Finite Generator]]: :$I, A, A^2, \ldots, A^{n^2}$ are [[Definition:Linearly Dependent|linearly dependent]]. That is, there exists $\alpha_0, \alpha_1, \ldots, \alpha_{n^2} \in K$ not all zero such that: :$\displaystyle \sum_{i \mathop = 0}^{n^2} \alpha_i A^i = 0$ That is, the [[Definition:Polynomial|polynomial]]: :$\displaystyle \sum_{i \mathop = 0}^{n^2} \alpha_i X^i \in K \sqbrk X$ has $\map P A = 0$, and [[Definition:Degree of Polynomial|degree]] at most $n^2$. Let: :$\displaystyle S = \set {P \in K \sqbrk X \setminus \set 0 \mid \map P A = 0}$ $S$ is certainly [[Definition:Non-Empty Set|non-empty]] since we have found such an element in the computation above. Now consider the set: :$\displaystyle D = \set {\deg P \mid P \in S}$ Since $D$ is a [[Definition:Subset|subset]] of the [[Definition:Natural Number|natural numbers]], it contains a [[Definition:Smallest/Ordered Set|least element]] $N$ by the [[Well-Ordering Principle]]. Since the polynomial we constructed has [[Definition:Degree of Polynomial|degree]] at most $n^2$, we have $N \le n^2$. Let $Q \in S$ be of [[Definition:Degree of Polynomial|degree]] $N$. Let $a_N$ be the coefficient of the $X^N$ term in $Q$. Then $\mu = \dfrac 1 {a_N} Q$ is a [[Definition:Monic Polynomial|monic polynomial]] of minimum [[Definition:Degree of Polynomial|degree]] with $\map \mu A = 0$. So $\mu$ is a [[Definition:Minimal Polynomial|minimal polynomial]] for $A$. {{qed}} [[Category:Linear Algebra]] [[Category:Minimal Polynomials]] tb8lxiwzhe3qho6xa228up6rkyunipb	1
Let $V$ and $W$ be [[Definition:Vector Space|$K$-vector spaces]]. Then $\phi: V \to W$ is a '''vector space monomorphism''' {{iff}}: : $(1): \quad \phi$ is an [[Definition:Injection|injection]] : $(2): \quad \forall \mathbf x, \mathbf y \in V: \phi \left({\mathbf x + \mathbf y}\right) = \phi \left({\mathbf x}\right) + \phi \left({\mathbf y}\right)$ : $(3): \quad \forall \mathbf x \in V: \forall \lambda \in K: \phi \left({\lambda \mathbf x}\right) = \lambda \phi \left({\mathbf x}\right)$	1
{{qed}} [[Category:Completion of Normed Division Ring]] 9vfxeb93vrn3k8326fc1qvfwizzrxnp	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Division Ring|division ring]] $R$. Let $B$ be a [[Definition:Basis (Linear Algebra)|basis]] for $V$. Let $x \in V$. Then there is a unique [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $C$ of $R \times B$ such that: :$\displaystyle x = \sum_{\left({r, v}\right) \mathop \in C} r \cdot v$ :$\forall \left({r, v}\right) \in C: r \ne 0_R$	1
Let $\R$ be the [[Definition:Field of Real Numbers|field of real numbers]]. Let $\F$ be a [[Definition:Subfield|subfield]] of $\R$. Let $V$ be a [[Definition:Vector Space|vector space]] over $\F$ Let $\innerprod \cdot \cdot: V \times V \to \mathbb F$ be a [[Definition:Mapping|mapping]]. Then $\innerprod \cdot \cdot: V \times V \to \mathbb F$ is '''symmetric''' {{iff}}: :$\forall x, y \in V: \innerprod x y = \innerprod y x$	1
Let $\struct {F, +, \circ}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Let $F \sqbrk X$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms]] in an [[Definition:Indeterminate (Polynomial Theory)|indeterminate]] $X$ over $F$. Then the [[Definition:Unit of Ring|units]] of $F \sqbrk X$ are all the elements of $F \sqbrk X$ whose [[Definition:Degree (Polynomial)|degree]] is $0$.	1
Let $V$ be a [[Definition:Banach Space|Banach space]], and let $\left\Vert{\cdot}\right\Vert$ denote its [[Definition:Norm on Vector Space|norm]]. Let $\left({v_i}\right)_{i\in I}$ be an [[Definition:Indexed Set|indexed]] [[Definition:Subset|subset]] of $V$. Let the [[Definition:Generalized Sum|generalized sum]] $\displaystyle \sum \left\{{v_i: i \in I}\right\}$ [[Definition:Absolutely Convergent Generalized Sum|converge absolutely]]. Then $\displaystyle \left\Vert{\sum \left\{{v_i: i \in I}\right\}}\right\Vert \le \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\}$.	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]] $0$. Let $\sequence {x_n}$, $\sequence {y_n} $ be [[Definition:Sequence|sequences in $R$]]. Let $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0$. Let $\sequence {y_n}$ be a [[Definition:Cauchy Sequence|Cauchy sequence]]. Then: :$\sequence {x_n y_n}$ and $\sequence {y_n x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0$.	1
From [[Dot Product Operator is Bilinear]]: :$\paren {c \mathbf u + \mathbf v} \cdot \mathbf w = c \paren {\mathbf u \cdot \mathbf w} + \paren {\mathbf v \cdot \mathbf w}$ Setting $c = 1$ yields the result. {{qed}}	1
Let $\struct {G, \cdot}$ be a [[Definition:Group|group]]. Let $f: \struct {V, \phi} \to \struct {V', \mu}$ be a [[Definition:G-Module Homomorphism |homomorphism of $G$-modules]]. Then its [[Definition:Kernel of Linear Transformation|kernel]] $\map \ker f$ is a [[Definition:G-Submodule|$G$-submodule]] of $V$.	1
=== 1 implies 2 === Follows from [[Reflexive Bilinear Form is Symmetric or Alternating]] {{qed}} === 2 implies 1 === Follows from: :[[Symmetric Bilinear Form is Reflexive]] :[[Alternating Bilinear Form is Reflexive]] {{qed}} [[Category:Bilinear Forms]] gcjjc8czzhhl5k20imdbg71du2p651d	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\struct {U_R, \circ}$ be the [[Definition:Group of Units of Ring|group of units]] of $\struct {R, +, \circ}$. Let $a, b \in R, c, d \in U_R$. Then: :$\dfrac a c = \dfrac b d \iff a \circ d = b \circ c$ where $\dfrac x z$ is defined as $x \circ \paren {z^{-1} }$, that is, $x$ [[Definition:Division Product|divided by]] $z$.	1
Let $\HH$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in \map B \HH$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Let $M$ be a [[Definition:Closed Linear Subspace|closed linear subspace]] of $\HH$. Let $P$ denote the [[Definition:Orthogonal Projection|orthogonal projection]] on $M$. Let $\begin{pmatrix} W & X \\ Y & Z \end{pmatrix}$ be the [[Definition:Matrix Notation (Bounded Linear Operator)|matrix notation]] for $A$ with respect to $M$. Then the following four statements are equivalent: :$(1): \quad M$ is a [[Definition:Reducing Subspace|reducing subspace]] for $A$ :$(2): \quad P A = A P$ :$(3): \quad X = Y = 0$ :$(4): \quad M$ is an [[Definition:Invariant Subspace|invariant subspace]] for both $A$ and its [[Definition:Adjoint Linear Transformation|adjoint]] $A^*$	1
Let $\mathbf P$ and $\mathbf Q$ be [[Definition:Proper Orthogonal Matrix|proper orthogonal matrices]]. Let $\mathbf P \mathbf Q$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf P$ and $\mathbf Q$. Then $\mathbf P \mathbf Q$ is a [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]].	1
Consider $p_1/p_2 \in \Q$. With $\norm {\,\cdot\,}_{p_1}$: {{begin-eqn}} {{eqn| l = \norm{p_1/p_2}_{p_1} | r = \norm{p_1}_{p_1} \norm{1/p_2}_{p_1} | c = [[Definition:Norm/Division Ring|Norm axiom (M2) (Multiplicativity)]] }} {{eqn| r = \norm{p_1}_{p_1} \times 1 | c = $p_1$ does not divide $p_2$ }} {{eqn| r = 1/{p_1} }} {{eqn| r = 1 | o = \lt }} {{end-eqn}} On the other hand, with $\norm {\,\cdot\,}_{p_2}$: {{begin-eqn}} {{eqn| l = \norm{p_1/p_2}_{p_2} | r = \norm{p_1}_{p_2} \norm{1/p_2}_{p_2} | c = [[Definition:Norm/Division Ring|Norm axiom (M2) (Multiplicativity)]] }} {{eqn| r = 1 \times \norm{1/p_2}_{p_2} | c = $p_2$ does not divide $p_1$ }} {{eqn| r = p_2 }} {{eqn| r = 1 | o = \gt }} {{end-eqn}} By [[Definition:Equivalent Division Ring Norms/Open Unit Ball Equivalent|open unit ball equivalence]], $\norm {\,\cdot\,}_{p_1}$ and $\norm {\,\cdot\,}_{p_2}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. By [[Equivalence of Definitions of Equivalent Division Ring Norms]] and [[Definition:Equivalent Division Ring Norms/Topologically Equivalent|topological equivalence]] then the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_{p_1}$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_{p_2}$. {{qed}}	1
Let $M = \struct{X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Then the [[Definition:Set|set]] $X$ is an [[Definition:Open Set in Normed Vector Space|open set]] of $M$.	1
=== [[Triangle Inequality/Geometry|Geometry]] === {{:Triangle Inequality/Geometry}} === [[Triangle Inequality/Real Numbers|Real Numbers]] === {{:Triangle Inequality/Real Numbers}} === [[Triangle Inequality/Complex Numbers|Complex Numbers]] === {{:Triangle Inequality/Complex Numbers}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Module|$R$-module]]. Let $S$ be a [[Definition:Set|set]]. Let $\struct {G^S, +_G', \circ}_R$ be the [[Definition:Module of All Mappings|module of all mappings]] from $S$ to $G$. Then $\struct {G^S, +_G', \circ}_R$ is an [[Definition:Module|$R$-module]].	1
Let $A \subseteq B \subseteq C$ be [[Definition:Ring (Abstract Algebra)|rings]]. Suppose $B$ is a [[Definition:Finitely Generated Module|finitely generated]] $A$-[[Definition:Module|module]], and $C$ is a finitely generated $B$-module. Then $C$ is a finitely generated $A$-module.	1
Note that by definition, $\map {\CC^{\paren m} } {\mathbb J} \subseteq \R^{\mathbb J}$. Let $f, g \in \map {\CC^{\paren m} } {\mathbb J}$. Let $\lambda \in \R$. Applying [[Linear Combination of Derivatives]] $m$ times we have: :$f + \lambda g$ is $m$-times [[Definition:Differentiable Function|differentiable]] on $\mathbb J$ with [[Definition:Nth Derivative|$m$th derivative]] $f^{\paren m} + \lambda g^{\paren m}$. Since both $f$ and $g$ are of [[Definition:Differentiability Class|differentiability class]] $m$: :$f^{\paren m}$ and $g^{\paren m}$ are [[Definition:Continuous Function|continuous]] on $\mathbb J$. From [[Combination Theorem for Continuous Functions/Combined Sum Rule|Combination Theorem for Continuous Functions: Combined Sum Rule]]: :$f^{\paren m} + \lambda g^{\paren m} = \paren {f + \lambda g}^{\paren m}$ is [[Definition:Continuous Function|continuous]] on $\mathbb J$. So: :$f + \lambda g \in \map {\CC^{\paren m} } {\mathbb J}$ Therefore, by [[One-Step Vector Subspace Test]]: :$\struct {\map {\CC^{\paren m} } {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of $\struct {\R^{\mathbb J}, +, \times}_\R$. {{qed}}	1
By [[Unit Matrix is its own Inverse]] the [[Definition:Inverse Matrix|inverse]] $I_n^{-1}$ of $I_n$ is $I_n$. By definition a [[Definition:Unit Matrix|unit matrix]] is a [[Definition:Diagonal Matrix|diagonal matrix]]. Hence by [[Diagonal Matrix is Symmetric]]: :$I_n = I_n^\intercal$ where $I_n^\intercal$ is the [[Definition:Transpose of Matrix|transpose]] of $I_n$. Thus: :$I_n^{-1} = I_n^\intercal$ and the result follows by definition of [[Definition:Orthogonal Matrix|orthogonal]]. {{qed}} [[Category:Unit Matrices]] [[Category:Orthogonal Matrices]] thfuv0wq4j5hsyxq6htlcafz1z4aotx	1
Let $A = \left({A_F, \oplus}\right)$ be a [[Definition:Star-Algebra|$*$-algebra]]. Let $A' = \left({A'_F, \oplus'}\right) = \left({A, \oplus}\right)^2$ be the [[Definition:Algebra over Field|algebra]] formed from $A$ by the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]]. Then $A'$ is also a [[Definition:Star-Algebra|$*$-algebra]].	1
Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l \in R$. Let $\epsilon > 0$. Then also $\dfrac \epsilon 2 > 0$. Because $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$, we have: :$\exists N: \forall n > N: \norm {x_n - l} < \dfrac \epsilon 2$ So if $m > N$ and $n > N$, then: {{begin-eqn}} {{eqn | l = \norm {x_n - x_m} | r = \norm {x_n - l + l - x_m} }} {{eqn | o = \le | r = \norm {x_n - l} + \norm {l - x_m} | c = [[Definition:Norm Axioms|Triangle Inequality]] }} {{eqn | o = < | r = \frac \epsilon 2 + \frac \epsilon 2 | c = by choice of $N$ }} {{eqn | r = \epsilon | c = }} {{end-eqn}} Thus $\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]]. {{qed}}	1
Because of [[Determinant of Transpose]], it is necessary to prove only one of these identities. Let: :$D = \begin {vmatrix} a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r 1} & \cdots & a_{r k} & \cdots & a_{r n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix}$ First, note that from [[Determinant with Row Multiplied by Constant]], we have: :$\begin{vmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \end {vmatrix} = a_{r 1} \begin {vmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \end {vmatrix}$ and similarly: :$\begin {vmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & a_{r 2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \end {vmatrix} = a_{r 2} \begin {vmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \end{vmatrix}$ and so on for the whole of [[Definition:Row of Matrix|row]] $r$. From [[Determinant as Sum of Determinants]]: :$\displaystyle \begin {vmatrix} a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r 1} & \cdots & a_{r k} & \cdots & a_{r n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix} = \sum_{k \mathop = 1}^n \paren {a_{r k} \begin {vmatrix} a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ 0 & \cdots & 1 & \cdots & 0 \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix} }$ Consider the [[Definition:Determinant of Matrix|determinant]]: :$\begin{vmatrix} a_{1 1} & \cdots & a_{1 \paren {k - 1} } & a_{1 k} & a_{1 \paren {k + 1} } & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} \paren {k - 1} } & a_{\paren {r - 1} k} & a_{\paren {r - 1} \paren {k + 1} } & \cdots & a_{\paren {r - 1} n} \\ 0 & \cdots & 0 & 1 & 0 & \cdots & 0 \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} \paren {k - 1} } & a_{\paren {r + 1} k} & a_{\paren {r + 1} \paren {k + 1} } & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n \paren {k - 1} } & a_{n k} & a_{n \paren {k + 1} } & \cdots & a_{n n} \end {vmatrix}$ Exchange [[Definition:Row of Matrix|row]]s $r$ and $r - 1$, then (the new) [[Definition:Row of Matrix|row]] $r - 1$ with [[Definition:Row of Matrix|row]] $r - 2$, until finally [[Definition:Row of Matrix|row]] $r$ is at the top. [[Definition:Row of Matrix|Row]] 1 will be in [[Definition:Row of Matrix|row]] 2, [[Definition:Row of Matrix|row]] 2 in [[Definition:Row of Matrix|row]] 3, and so on. This is permuting the [[Definition:Row of Matrix|row]]s by a [[Definition:Cyclic Permutation|$k$-cycle]] of length $r$. Call that [[Definition:Cyclic Permutation|$k$-cycle]] $\rho$. Then from [[Parity of K-Cycle]]: :$\map \sgn \rho = \paren {-1}^{r - 1}$ Thus: :$\begin {vmatrix} 0 & \cdots & 1 & \cdots & 0 \\ a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} k} & \cdots & a_{\paren {r - 1} n} \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} k} & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix} = \paren {-1}^{r - 1} \begin {vmatrix} a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} k} & \cdots & a_{\paren {r - 1} n} \\ 0 & \cdots & 1 & \cdots & 0 \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} k} & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix}$ The same argument can be applied to [[Definition:Column of Matrix|column]]s. Thus: :$\begin {vmatrix} 1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\ a_{1 k} & a_{1 1} & \cdots & a_{1 \paren {k - 1} } & a_{1 \paren {k + 1} } & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} k} & a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} \paren {k - 1} } & a_{\paren {r - 1} \paren {k + 1} } & \cdots & a_{\paren {r - 1} n} \\ a_{\paren {r + 1} k} & a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} \paren {k - 1} } & a_{\paren {r + 1} \paren {k + 1} } & \cdots & a_{\paren {r + 1} n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{n k} & a_{n 1} & \cdots & a_{n \paren {k - 1} } & a_{n \paren {k + 1} } & \cdots & a_{n n} \end {vmatrix} = \paren {-1}^{k-1}\begin {vmatrix} 0 & \cdots & 1 & \cdots & 0 \\ a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} k} & \cdots & a_{\paren {r - 1} n} \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} k} & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix}$ and so: :$\begin{vmatrix} 1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\ a_{1 k} & a_{1 1} & \cdots & a_{1 \paren {k - 1}} & a_{1 \paren {k + 1} } & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} k} & a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} \paren {k - 1} } & a_{\paren {r - 1} \paren {k + 1} } & \cdots & a_{\paren {r - 1} n} \\ a_{\paren {r + 1} k} & a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} \paren {k - 1} } & a_{\paren {r + 1} \paren {k + 1} } & \cdots & a_{\paren {r + 1} n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{n k} & a_{n 1} & \cdots & a_{n \paren {k - 1} } & a_{n \paren {k + 1} } & \cdots & a_{n n} \end {vmatrix} = \paren {-1}^{r + k} \begin {vmatrix} a_{1 1} & \cdots & a_{1 k} & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} k} & \cdots & a_{\paren {r - 1} n} \\ 0 & \cdots & 1 & \cdots & 0 \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} k} & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n k} & \cdots & a_{n n} \end {vmatrix}$ Then: :$\paren {-1}^{r + k} \begin {vmatrix} a_{1 1} & \cdots & a_{1 \paren {k - 1} } & a_{1 \paren {k + 1} } & \cdots & a_{1 n} \\ \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{\paren {r - 1} 1} & \cdots & a_{\paren {r - 1} \paren {k - 1} } & a_{\paren {r - 1} \paren {k + 1} } & \cdots & a_{\paren {r - 1} n} \\ a_{\paren {r + 1} 1} & \cdots & a_{\paren {r + 1} \paren {k - 1} } & a_{\paren {r + 1} \paren {k + 1} } & \cdots & a_{\paren {r + 1} n} \\ \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n \paren {k - 1} } & a_{n \paren {k + 1} } & \cdots & a_{n n} \end{vmatrix}$ is $A_{r k}$, the [[Definition:Cofactor of Element|cofactor]] of $a_{r k}$ in $D$. But from [[Determinant with Unit Element in Otherwise Zero Row]], we have: :$\begin {vmatrix} 1 & 0 & \cdots & 0 \\ b_{2 1} & b_{2 2} & \cdots & b_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n 1} & b_{n 2} & \cdots & b_{n n} \end {vmatrix} = \begin {vmatrix} b_{2 2} & \cdots & b_{2 n} \\ \vdots & \ddots & \vdots \\ b_{n 2} & \cdots & b_{n n} \end {vmatrix}$ Assembling all the pieces derived above, the result follows. {{qed}}	1
If two [[Definition:Column of Matrix|columns]] of a [[Definition:Matrix|matrix]] with [[Definition:Determinant of Matrix|determinant]] $D$ are [[Definition:Transposition|transposed]], its determinant becomes $-D$.	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Complete Normed Division Ring|complete normed division ring]]. Let $\struct {S, \norm {\, \cdot \,}}$ be a [[Definition:Everywhere Dense|dense]] [[Definition:Normed Division Subring|normed division subring]] of $\struct {R, \norm {\, \cdot \,}}$. Then for all $x \in R$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $S$: :$x = \displaystyle \lim_{n \mathop \to \infty} x_n$	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]] $0$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\, \cdot \,}$ to the following [[Definition:Limit of Sequence (Metric Space)|limit]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = l \ne 0$ Then: :$\exists N: \forall n > N: \norm {x_n} > \dfrac {\norm l} 2$	1
Consider the [[Definition:Set|set]] $H$ of all points of $M$ which have [[Definition:Finite Set|finitely many]] [[Definition:Rational Number|rational]] [[Definition:Coordinate of Ordered Tuple|coordinates]] and all the rest [[Definition:Zero (Number)|zero]]. Then $H$ forms a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $A$ which is [[Definition:Everywhere Dense|(everywhere) dense]]. {{finish|Demonstrate that it is [[Definition:Everywhere Dense|(everywhere) dense]].}} The result follows by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Every [[Definition:Principal Ideal Domain|principal ideal domain]] is a [[Definition:Unique Factorization Domain|unique factorization domain]].	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $\mathfrak a \subseteq A$ be a [[Definition:Proper Ideal of Ring|proper ideal]]. Then there exists a [[Definition:Maximal Ideal of Ring|maximal ideal]] $\mathfrak m$ with $\mathfrak a \subseteq \mathfrak m$.	1
Let $f: M \to \R^\omega$ be the [[Definition:Mapping|mapping]] defined as: :$\forall x \in M: \map f x = \sequence {\map d {x, x_i} }$ where $\set {x_i}$ is a [[Definition:Countable Set|countable]] [[Definition:Everywhere Dense|dense]] [[Definition:Subset|subset]] of $A$. It remains to be shown that $f$ is a [[Definition:Homeomorphism (Topological Spaces)|homeomorphism]]. {{ProofWanted}}	1
Let $\mathbf A$ be a [[Definition:Complex Number|complex-valued]] [[Definition:Matrix|matrix]]. Let $\mathbf A^*$ denote the [[Definition:Hermitian Conjugate|Hermitian conjugate]] of $\mathbf A$. Then the [[Definition:Unary Operation|operation]] of [[Definition:Hermitian Conjugate|Hermitian conjugate]] is an [[Definition:Involution (Mapping)|involution]]: :$\paren {\mathbf A^*}^* = \mathbf A$	1
By definition of an [[Definition:Ideal of Ring|ideal]], $J$ is both a [[Definition:Left Ideal|left ideal]] and a [[Definition:Right Ideal|right ideal]]. From [[Left Ideal is Left Module over Ring]] then $\struct {J, +, \circ_l}$ is a [[Definition:Left Module|left module]]. From [[Right Ideal is Right Module over Ring]] then $\struct {J, +, \circ_r}$ is a [[Definition:Right Module|right module]]. Then: {{begin-eqn}} {{eqn | lo= \forall x, y \in R \land \forall j \in J: | l = \paren {x \circ_l j} \circ_r y | r = \paren {x \times j} \times y | c = Definition of $\circ_l$ and $\circ_r$ }} {{eqn | r = x \times \paren c{j \times y} | c = [[Definition:Ring Axioms|Ring Axiom $(\text M 2)$: Associativity of Product]] }} {{eqn | r = x \circ_l \paren {j \circ_r y} | c = Definition of $\circ_l$ and $\circ_r$ }} {{end-eqn}} Hence $\struct {J, +, \circ_l, \circ_r}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +, \times}$ by definition. {{qed}}	1
Let $V$ be an [[Definition:Inner Product Space|inner product space]] over a [[Definition:Field (Abstract Algebra)|subfield]] $\Bbb F$ of $\C$. Let $\norm {\, \cdot \,}$ denote the [[Definition:Inner Product Norm|inner product norm]] on $V$. Then $\norm {\, \cdot \,}$ is a [[Definition:Norm on Vector Space|norm]] on $V$.	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector Quantity|vector quantities]]. Let $\mathbf a$ and $\mathbf b$ be [[Definition:Perpendicular|perpendicular]]. Then: :$\mathbf a \cdot \mathbf b = 0$ where $\cdot$ denotes [[Definition:Dot Product|dot product]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\MM_{m \times n}$ be the [[Definition:Set|set]] of all $m \times n$ [[Definition:Matrix|matrices]] over $R$. Then under [[Definition:Matrix Product (Conventional)|conventional matrix multiplication]]: :$\mathbf {A B} = \mathbf {B A}$ for all $\mathbf A \in \MM_{m \times n}, \; \mathbf B \in \MM_{n \times p}$ {{iff}} $p = m = n = 1$.	1
Let $\mathbf r$ be a [[Definition:Position Vector|position vector]] embedded in a [[Definition:Cartesian Plane|Cartesian plane]] $\CC$ with [[Definition:Origin|origin]] $O$. Let $\CC$ be [[Definition:Plane Rotation|rotated]] [[Definition:Anticlockwise|anticlockwise]] through an [[Definition:Angle|angle]] $\varphi$ about the [[Definition:Axis of Rotation|axis of rotation]] $O$. Let $\CC'$ denote the [[Definition:Cartesian Plane|Cartesian plane]] in its new position. Let $\mathbf r$ be kept fixed during this [[Definition:Plane Rotation|rotation]]. Let $\tuple {x, y}$ denote the [[Definition:Component of Vector|components]] of $\mathbf r$ with respect to $\CC$. Let $\tuple {x', y'}$ denote the [[Definition:Component of Vector|components]] of $\mathbf r$ with respect to $\CC'$. Then: {{begin-eqn}} {{eqn | l = x' | r = x \cos \varphi + y \sin \varphi }} {{eqn | l = y' | r = -x \sin \varphi + y \cos \varphi }} {{end-eqn}}	1
{{ProofWanted|Need to consider which definition you start from}}	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $A$ be a [[Definition:Non-Trivial Ring|non-trivial]] [[Definition:Finitely Generated R-Algebra|finitely generated]] [[Definition:K-Algebra|$k$-algebra]]. {{explain|the above link is for [[Definition:Non-Trivial Ring]] -- we need to define a [[Definition:Non-Trivial Algebra]]}} Then there exists $n \in \N$ and a [[Definition:Finite Ring Homomorphism|finite]] [[Definition:Monomorphism (Abstract Algebra)|injective morphism]] of [[Definition:K-Algebra|$k$-algebra]]: :$k \sqbrk {x_1, \dotsc, x_n} \to A$ {{DefinitionWanted|Instead of using [[Definition:Finite Ring Homomorphism]], another page is to be generated defining the module-theory version.}}	1
There exists at least one example of a [[Definition:Topological Space|topological space]] which satisfies the [[Definition:Countable Chain Condition|countable chain condition]] which is not also a [[Definition:Separable Space|separable space]].	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\struct {G, +_G}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\struct {G, +_G, \circ}_K$ be a [[Definition:Vector Space|$K$-vector space]] whose [[Definition:Zero Scalar|zero]] is $0_K$. Let $x \in G: x \ne e$. Then $\set x$ is a [[Definition:Linearly Independent Set|linearly independent subset]] of $G$.	1
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]]. Let $U_D$ be the [[Definition:Group of Units of Ring|group of units]] of $D$. Let $\ideal x$ be the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $x$]]. Let $x, y \in \struct {D, +, \circ}$. Then:	1
In the below: :$r_i$ denotes the initial state of [[Definition:Row of Matrix|row]] $i$ :$r_j$ denotes the initial state of [[Definition:Row of Matrix|row]] $j$ :$r_i'$ denotes the state of [[Definition:Row of Matrix|row]] $i$ after having had the latest [[Definition:Elementary Row Operation|elementary row operation]] applied :$r_j'$ denotes the state of [[Definition:Row of Matrix|row]] $j$ after having had the latest [[Definition:Elementary Row Operation|elementary row operation]] applied. $(1)$: Apply [[Definition:Elementary Row Operation|$\text {ERO} 2$]] to [[Definition:Row of Matrix|row]] $j$ for $\lambda = 1$: :$r_j \to r_j + r_i$ After this operation: {{begin-eqn}} {{eqn | l = r_i' | r = r_i }} {{eqn | l = r_j' | r = r_i + r_j }} {{end-eqn}} {{qed|lemma}} $(2)$: Apply [[Definition:Elementary Row Operation|$\text {ERO} 2$]] to [[Definition:Row of Matrix|row]] $i$ for $\lambda = -1$: :$r_i \to r_i + \paren {-r_j}$ After this operation: {{begin-eqn}} {{eqn | l = r_i' | r = r_i - \paren {r_i + r_j} }} {{eqn | r = -r_j }} {{eqn | l = r_j' | r = r_i + r_j }} {{end-eqn}} {{qed|lemma}} $(3)$: Apply [[Definition:Elementary Row Operation|$\text {ERO} 2$]] to [[Definition:Row of Matrix|row]] $j$ for $\lambda = 1$: :$r_j \to r_j + r_i$ After this operation: {{begin-eqn}} {{eqn | l = r_i' | r = -r_j }} {{eqn | l = r_j' | r = r_i + r_j - r_j }} {{eqn | r = r_i }} {{end-eqn}} {{qed|lemma}} $(4)$: Apply [[Definition:Elementary Row Operation|$\text {ERO} 1$]] to [[Definition:Row of Matrix|row]] $i$ for $\lambda = -1$: :$r_i \to -r_i$ After this operation: {{begin-eqn}} {{eqn | l = r_i' | r = -\paren {-r_j} }} {{eqn | r = r_j }} {{eqn | l = r_j' | r = r_i }} {{end-eqn}} {{qed|lemma}} Thus, after all the $4$ [[Definition:Elementary Row Operation|elementary row operations]] have been applied, we have: {{begin-eqn}} {{eqn | l = r_i' | r = r_j }} {{eqn | l = r_j' | r = r_i }} {{end-eqn}} Hence the result. {{qed}}	1
Let $\mathcal L$ be a [[Definition:Horizontal Line|horizontal line]] embedded in the [[Definition:Cartesian Plane|Cartesian plane]] $\mathcal C$. Then the [[Definition:Equation of Geometric Figure|equation]] of $\mathcal L$ can be given by: :$y = b$ where $\tuple {0, b}$ is the [[Definition:Point|point]] at which $\mathcal L$ [[Definition:Intersection (Geometry)|intersects]] the [[Definition:Y-Axis|$y$-axis]]. :[[File:Graph-of-horizontal-line.png|520px]]	1
For $\struct {S, +, \times}$ to be a [[Definition:Ring (Abstract Algebra)|ring]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is a [[Definition:Semigroup|semigroup]]. For $\struct {S, \times}$ to be a [[Definition:Semigroup|semigroup]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is [[Definition:Closed Algebraic Structure|closed]]. That is: :$\forall x, y \in S: x \times y \in S$ Let $\mathbf A$ and $\mathbf B$ be [[Definition:Element|elements]] of $S$. The [[Definition:Matrix Product (Conventional)|matrix multiplication operation]] is defined on $\mathbf A$ and $\mathbf B$ {{iff}} $\mathbf A$ is of [[Definition:Order of Matrix|order]] $m \times p$ and $\mathbf A$ is of [[Definition:Order of Matrix|order]] $p \times n$, for some $m, n, p \in \N_{>0}$. That is, the second [[Definition:Dimensions of Matrix|dimension]] of $\mathbf A$ must be equal to the first [[Definition:Dimensions of Matrix|dimension]] of $\mathbf B$. But in this case, the second [[Definition:Dimensions of Matrix|dimension]] of $\mathbf A$ is $n$, and the first [[Definition:Dimensions of Matrix|dimension]] of $\mathbf B$ is $m$. As we have been given that $m \ne n$, it follows that [[Definition:Matrix Product (Conventional)|matrix multiplication operation]] is not defined on $S$. Hence the result. {{qed}}	1
Let $\ell^p$ be a [[Definition:P-Sequence Space|p-sequence space]]. Let $\norm {\, \cdot \,}_p$ be a [[Definition:P-Norm|p-norm]]. Then $\struct {\ell^p, \norm {\, \cdot \,}_p}$ is a [[Definition:Banach Space|Banach space]].	1
Every [[Definition:Real Matrix|real]] [[Definition:Symmetric Matrix|symmetric matrix]] is [[Definition:Hermitian Matrix|Hermitian]].	1
{{AimForCont}} $x^2 + 1$ has a [[Definition:Non-Trivial Factorization|non-trivial factorization]] in $\R \sqbrk X$. Then: :$\exists \alpha, \beta \in \R: \paren {X - \alpha} \paren {X - \beta}$ and from the [[Polynomial Factor Theorem]]: :$\alpha^2 + 1 = 0$ But that means: :$\alpha^2 = -1$ and such an $\alpha$ does not exist in $\R$. Hence the result by [[Proof by Contradiction]]. {{qed}}	1
The [[Cauchy-Binet Formula]] gives: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m} }\right) \det \left({\mathbf B_{j_1 j_2 \ldots j_m}}\right)$ where: :$\mathbf A$ is an [[Definition:Matrix|$m \times n$ matrix]] :$\mathbf B$ is an [[Definition:Matrix|$n \times m$ matrix]]. :For $1 \le j_1, j_2, \ldots, j_m \le n$: ::$\mathbf A_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. ::$\mathbf B_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. When $m = 1$, the relation: :$1 \le j_1 < j_2 < \cdots < j_m \le n$ degenerates to: :$1 \le j \le n$ By definition of [[Definition:Determinant of Order 1|order $1$ determinant]]: :$\det \left({\left[{a_j}\right]}\right) = a_j$ :$\det \left({\left[{b_j}\right]}\right) = b_j$ Hence the result. {{qed}}	1
:[[File:Triangle Inequality.png|350 px]] Let $ABC$ be a [[Definition:Triangle (Geometry)|triangle]] [[Axiom:Euclid's Second Postulate|We can extend]] $BA$ past $A$ into a [[Definition:Straight Line|straight line]]. [[Construction of Equal Straight Lines from Unequal|There exists a point $D$]] such that $DA = CA$. Therefore, from [[Isosceles Triangle has Two Equal Angles]]: :$\angle ADC = \angle ACD$ Thus by [[Axiom:Euclid's Common Notions|Euclid's fifth common notion]]: :$\angle BCD > \angle BDC$ Since $\triangle DCB$ is a triangle having $\angle BCD$ greater than $\angle BDC$, [[Greater Angle of Triangle Subtended by Greater Side|this means that $BD > BC$]]. But: :$BD = BA + AD$ and: :$AD = AC$ Thus: :$BA + AC > BC$ A similar argument shows that $AC + BC > BA$ and $BA + BC > AC$. {{qed}} {{Euclid Note|20|I|It is a [[Definition:Euclidean Geometry|geometric]] interpretation of the [[Triangle Inequality|triangle inequality]].}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $\left\{ {M_i}\right\}_{i \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Unitary Module|unitary]] $R$-[[Definition:Module|modules]]. Let $\left({M, +, \circ}\right)$ be their [[Definition:Module Direct Product|direct product]]. Then $\left({M, +, \circ}\right)$ is a [[Definition:Unitary Module|unitary]] $R$-[[Definition:Module|module]].	1
Let $T = \struct {S, \tau}$ be a [[Definition:Countable Complement Topology|countable complement topology]] on an [[Definition:Uncountable Set|uncountable set]] $S$. Then $T$ is not a [[Definition:Separable Space|separable space]].	1
=== Necessary Condition === Let $\struct {G, +_G, \circ'}$ be a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. Then: {{begin-eqn}} {{eqn | l = \paren {\lambda \times_R \mu} \circ x | r = x \circ' \paren {\lambda \times_R \mu} | c = Definition of $\circ'$ }} {{eqn | r = \paren {x \circ' \lambda} \circ' \mu | c = [[Definition:Right Module Axioms|Right module axiom $(\text {RM} 3)$]]: [[Definition:Associative Operation|Associativity]] of [[Definition:Scalar Multiplication on Module|Scalar Multiplication]] }} {{eqn | r = \mu \circ \paren {\lambda \circ x} | c = Definition of $\circ'$ }} {{eqn | r = \paren {\mu \times_R \lambda} \circ x | c = [[Definition:Left Module Axioms|Left module axiom $(\text M 3)$]]: [[Definition:Associative Operation|Associativity]] of [[Definition:Scalar Multiplication on Module|Scalar Multiplication]] }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === Let the [[Definition:Scalar Multiplication on Module|scalar multiplication]] $\circ$ satisfy: :$\forall \lambda, \mu \in R: \forall x \in G: \paren {\lambda \times_R \mu} \circ x = \paren {\mu \times_R \lambda} \circ x$ It needs to be shown that $\struct {G, +_G, \circ'}$ satisfies the [[Definition:Right Module Axioms|right module axioms]]. ==== $(\text {RM} 1)$ : Scalar Multiplication (Right) Distributes over Module Addition ==== Let $\lambda, \mu \in R, x \in G$. Then: {{begin-eqn}} {{eqn | l = \paren {x +_G y} \circ' \lambda | r = \lambda \circ \paren {x +_G y} | c = Definition of $\circ'$ }} {{eqn | r = \lambda \circ x +_G \lambda \circ y | c = [[Definition:Left Module Axioms|Left module axiom $(\text M 1)$]] on $\circ$ }} {{eqn | r = x \circ' \lambda +_G y \circ' \lambda | c = Definition of $\circ'$ }} {{end-eqn}} {{qed|lemma}} ==== $(\text {RM} 2)$ : Scalar Multiplication (Left) Distributes over Scalar Addition ==== Let $\lambda \in R, x, y \in G$. Then: {{begin-eqn}} {{eqn | l = x \circ' \paren{\lambda +_R \mu} | r = \paren {\lambda +_R \mu} \circ x | c = Definition of $\circ'$ }} {{eqn | r = \lambda \circ x +_G \mu \circ y | c = [[Definition:Left Module Axioms|Left module axiom $(\text M 2)$]] on $\circ$ }} {{eqn | r = x \circ' \lambda +_G x \circ' \mu | c = Definition of $\circ'$ }} {{end-eqn}} {{qed|lemma}} ==== $(\text {RM} 3)$ : Associativity of Scalar Multiplication ==== Let $\lambda, \mu \in R, x \in G$. Then: {{begin-eqn}} {{eqn | l = x \circ' \paren {\lambda \times_R \mu} | r = \paren {\lambda \times_R \mu} \circ x | c = Definition of $\circ'$ }} {{eqn | r = \paren {\mu \times_R \lambda} \circ x | c = Assumption }} {{eqn | r = \mu \circ \paren {\lambda \circ x} | c = [[Definition:Left Module Axioms|Left module axiom $(\text M 3)$]] on $\circ$ }} {{eqn | r = \paren {x \circ' \lambda} \circ' \mu | c = Definition of $\circ'$ }} {{end-eqn}} {{qed}}	1
Let $T$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\norm {T - I} < 1$ in the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]], where $I$ the [[Definition:Identity Matrix|identity matrix]]. Then there is a [[Definition:Square Matrix|square matrix]] $S$ such that: :$e^S = T$ where $e^S$ is the [[Definition:Matrix Exponential|matrix exponential]].	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Unity of Ring|unity]] $1_R$. Then: :$\norm {\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] $\implies \forall n \in \N_{>0}: \norm {n \cdot 1_R} \le 1$. where: $n \cdot 1_R = \underbrace {1_R + 1_R + \dots + 1_R}_{\text {$n$ times} }$	1
Let $H, K$ be [[Definition: Hilbert Space|Hilbert spaces]]. Let $T \in \map {B_{00} } {H, K}$ be a [[Definition:Bounded Linear Transformation|bounded]] [[Definition:Finite Rank Operator|finite rank operator]]. Then $T \in \map {B_0} {H, K}$, that is, $T$ is [[Definition:Compact Linear Transformation|compact]].	1
Let $\left({G, \cdot}\right)$ be a [[Definition:Finite Group|finite group]]. Let $\rho: G \to \operatorname{GL} \left({V}\right)$ be a [[Definition:Linear Representation|linear representation]] of $G$ on $V$ of [[Definition:Dimension (Representation Theory)|degree]] $1$. Then $\rho$ is an [[Definition:Irreducible Linear Representation|irreducible linear representation]].	1
By the [[Second Isomorphism Theorem/Vector Spaces|second isomorphism theorem]]: :$\dfrac {M + N} M \equiv \dfrac N {M \cap N}$ The result follows. {{qed}}	1
Let $y \ne 0_R$ where $0_R$ is the [[Definition:Ring Zero|zero]] of $R$. Then: :$\norm {x + y} \le \max \set {\norm x, \norm y} \iff \norm {x y^{-1} + 1_R} \le \max \set {\norm {x y^{-1} }, 1}$	1
Checking in turn each of the critera for [[Definition:Equivalence Relation|equivalence]]: === Reflexive === $\mathbf A = \mathbf{I_m}^{-1} \mathbf A \mathbf{I_n}$ trivially, for all [[Definition:Matrix|$m \times n$ matrices]] $\mathbf A$. Thus [[Definition:Reflexive Relation|reflexivity]] holds. {{qed|lemma}} === Symmetric === Let $\mathbf B = \mathbf Q^{-1} \mathbf A \mathbf P$. As $\mathbf P$ and $\mathbf Q$ are both [[Definition:Invertible Matrix|invertible]], we have: {{begin-eqn}} {{eqn | l=\mathbf Q \mathbf B \mathbf P^{-1} | r=\mathbf Q \mathbf Q^{-1} \mathbf A \mathbf P \mathbf P^{-1} | c= }} {{eqn | r=\mathbf{I_m} \mathbf A \mathbf{I_n} | c= }} {{eqn | r=\mathbf A | c= }} {{end-eqn}} Thus [[Definition:Symmetric Relation|symmetry]] holds. {{qed|lemma}} === Transitive === Let $\mathbf B = \mathbf Q_1^{-1} \mathbf A \mathbf P_1$ and $\mathbf C = \mathbf Q_2^{-1} \mathbf B \mathbf P_2$. Then $\mathbf C = \mathbf Q_2^{-1} \mathbf Q_1^{-1} \mathbf A \mathbf P_1 \mathbf P_2$. [[Definition:Transitive Relation|Transitivity]] follows from the definition of [[Definition:Invertible Matrix|invertible matrix]], that the product of two invertible matrices is itself invertible. {{qed|lemma}} Hence the result by definition of [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
Let $\mathbf A$ and $\mathbf B$ be [[Definition:Matrix|$m \times n$ matrices]] over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\map \phi {\mathbf A}$ denote the [[Definition:Rank of Matrix|rank]] of $\mathbf A$. Let $\mathbf A \equiv \mathbf B$ denote that $\mathbf A$ and $\mathbf B$ are [[Definition:Matrix Equivalence|matrix equivalent]]. Then: :$\mathbf A \equiv \mathbf B$ {{iff}}: :$\map \phi {\mathbf A} = \map \phi {\mathbf B}$	1
Let: :$\map {\mathbf f} x = \displaystyle \sum_{k \mathop = 1}^n \map {f_k} x \mathbf e_k$ be a [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued function]]. Let $\map {\mathbf f} x$ be such that its [[Definition:Magnitude|magnitude]] is [[Definition:Constant|constant]]: :$\size {\map {\mathbf f} x} = c$ for some $c \in \R$. Then the [[Definition:Dot Product|dot product]] of $\mathbf f$ with its [[Definition:Derivative of Vector-Valued Function|derivative]] is zero: :$\map {\mathbf f} x \cdot \dfrac {\d \map {\mathbf f} x} {\d x} = 0$	1
By [[Null Sequences form Maximal Left and Right Ideal/Lemma 1|Lemma 1 of Null Sequences form Maximal Left and Right Ideal]] then $\NN$ is an [[Definition:Ideal of Ring|ideal]] of $\CC$. Hence $\NN$ is a [[Definition:Right Ideal of Ring|right ideal]] of $\CC$. It remains to show that $\NN$ is [[Definition:Maximal Right Ideal of Ring|maximal]]. By [[Null Sequences form Maximal Left and Right Ideal/Lemma 2/Lemma 2.1|Lemma 2.1 of Null Sequences form Maximal Left and Right Ideal]] then $\NN \subsetneq \CC$. By [[Definition:Maximal Right Ideal of Ring|maximal right ideal]] it needs to be shown that: :There is no [[Definition:Right Ideal of Ring|right ideal]] $\JJ$ of $\CC$ such that $\NN \subsetneq \JJ \subsetneq \CC$ Let $\JJ$ be a [[Definition:Right Ideal of Ring|Right ideal]] of $\CC$ such that $\NN \subsetneq \JJ \subseteq \CC$. It will be shown that $\JJ$ = $\CC$, from which the result will follow. Let $\sequence {x_n} \in \JJ \setminus \NN$ By [[Combination Theorem for Cauchy Sequences/Inverse Rule|Inverse Rule for Cauchy sequences]] then :$\exists K \in \N: \sequence {\paren {x_{K + n} }^{-1} }_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. Let $\sequence {y_n}$ be the sequence defined by: :$y_n = \begin{cases} 0 & : n \le K \\ \paren {x_n}^{-1} & : n > K \end{cases}$ By [[Cauchy Sequence with Finite Elements Prepended is Cauchy Sequence]] then $\sequence {y_n} \in \CC$ By the definition of a [[Definition:Right Ideal of Ring|right ideal]] the product $\sequence {x_n} \sequence {y_n} = \sequence {x_n y_n} \in \JJ$ By the definition of $\sequence {y_n}$ then: :$x_n y_n = \begin{cases} 0 & : n \le K \\ 1 & : n > K \end{cases}$ Let $\mathcal 1 = \tuple {1, 1, 1, \dots}$ be the [[Definition:Unity of Ring|unity]] of $\CC$ Then $\mathcal 1 - \sequence {x_n} \sequence {y_n}$ is the sequence $\sequence {w_n}$ defined by: :$w_n = \begin {cases} 1 & : n \le K \\ 0 & : n > K \end {cases}$ By [[Convergent Sequence with Finite Elements Prepended is Convergent Sequence]] then $\sequence {w_n}$ is convergent to 0. So $\sequence {w_n} \in \NN \subsetneq \JJ$ Since \sequence {x_n} $\sequence {y_n}, \sequence {w_n} \in \JJ$ by the definition of a [[Definition:Ideal of Ring|ring ideal]] then: :$\sequence {w_n} + \sequence {x_n} \sequence {y_n} = \mathcal 1 \in \JJ$ By the definition of a [[Definition:Right Ideal of Ring|right ideal]] then: :$\forall \sequence {a_n} \in \CC, \mathcal 1 \circ \sequence {a_n} = \sequence {a_n} \in \JJ$ Hence $\JJ = \CC$ {{qed}}	1
Follows directly from [[Hölder's Inequality]] with $p = q = 2$. {{qed}}	1
{{proof wanted}} {{Namedfor|Jørgen Pedersen Gram|name2 = Erhard Schmidt|cat = Gram|cat2 = Schmidt}}	1
Let $f: \R \to \R$ be an [[Definition:Additive Function (Conventional)|additive function]]. Then $f$ is an [[Definition:Odd Function|odd function]].	1
=== Necessary Condition === Let the [[Definition:Binary Operation|operation]] $\cdot$ be [[Definition:Associative Operation|associative]] on $\struct {S, \cdot}$. Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{m n}$ and $\mathbf C = \sqbrk c_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A \circ \mathbf B} \circ \mathbf C | r = \paren {\sqbrk a_{m n} \circ \sqbrk b_{m n} } \circ \sqbrk c_{m n} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{eqn | r = \sqbrk {a \cdot b}_{m n} \circ \sqbrk c_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \sqbrk {\paren {a \cdot b} \cdot c}_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \sqbrk {a \cdot \paren {b \cdot c} }_{m n} | c = as $\cdot$ is [[Definition:Associative Operation|associative]] }} {{eqn | r = \sqbrk a_{m n} \circ \sqbrk {b \cdot c}_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \sqbrk a_{m n} \circ \paren {\sqbrk b_{m n} \circ \sqbrk c_{m n} } | c = {{Defof|Hadamard Product}} }} {{eqn | r = \mathbf A \circ \paren {\mathbf B \circ \mathbf C} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{end-eqn}} That is, $\circ$ is [[Definition:Associative Operation|associative]] on $\map {\MM_S} {m, n}$. {{qed|lemma}} === Sufficient Condition === Suppose $\struct {S, \cdot}$ is such that $\cdot$ is not [[Definition:Associative Operation|associative]]. Then there exists $a$, $b$ and $c$ in $S$ such that: :$a \cdot \paren {b \cdot c} \ne \paren {a \cdot b} \cdot c$ Let $\mathbf A$, $\mathbf B$ and $\mathbf C$ be [[Definition:Element|elements]] of $\map {\MM_S} {m, n}$ such that: :$a_{i j} = a$, $b_{i j} = b$, $c_{i j} = c$ where: :$a_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf A$ :$b_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf B$ :$c_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf C$ Then: :$a_{i j} \cdot \paren {b_{i j} \cdot c_{i j} } \ne \paren {a_{i j} \cdot b_{i j} } \cdot c_{i j}$ That is: :$\paren {\mathbf A \circ \mathbf B} \circ \mathbf C \ne \mathbf A \circ \paren {\mathbf B \circ \mathbf C}$ because they differ (at least) at [[Definition:Index of Matrix Element|indices]] $\tuple {i, j}$. That is, $\circ$ is not [[Definition:Associative Operation|associative]] on $\map {\MM_S} {m, n}$. {{qed}}	1
The [[Definition:Angle Between Vectors|angle between]] two non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Euclidean Space)|vectors in $\R^n$]] can be calculated by: {{begin-eqn}} {{eqn | l = \theta | r = \arccos \frac {\mathbf v \cdot \mathbf w} {\norm {\mathbf v} \norm {\mathbf w} } }} {{end-eqn}} where: :$\mathbf v \cdot \mathbf w$ represents the [[Definition:Dot Product|dot product]] of $\mathbf v$ and $\mathbf w$ :$\norm {\, \cdot \,}$ represents [[Definition:Vector Length|vector length]]. :$\arccos$ represents [[Definition:Arccosine|arccosine]]	1
Let $\mathbf A$ be a [[Definition:Matrix|matrix]] over a [[Definition:Field (Abstract Algebra)|field]]. Let $\mathbf A^\intercal$ denote the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Let $\mathbf A$ be an [[Definition:Invertible Matrix|invertible matrix]]. Then $\mathbf A^\intercal$ is also [[Definition:Invertible Matrix|invertible]] and: :$\paren {\mathbf A^\intercal}^{-1} = \paren {\mathbf A^{-1} }^\intercal$ where $\mathbf A^{-1}$ denotes the [[Definition:Inverse Matrix|inverse]] of $\mathbf A$.	1
By definition of the [[Definition:Unit Matrix|unit matrix]]: :$I_{a b} = \delta_{a b}$ where: :$I_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf I$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$. By definition, $\mathbf E$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $m$]] formed by applying $e$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$. That is, all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $i$ of $\mathbf I$ are to be exchanged with the corresponding [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $j$. By definition of [[Definition:Unit Matrix|unit matrix]]: :all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $i$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{i i}$, which is $1$. :all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $j$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{j j}$, which is $1$. Thus in $\mathbf E$: :where $a \ne i$ and $a \ne j$, $E_{a b} = \delta_{a b}$ (all [[Definition:Row of Matrix|rows]] except $i$ and $j$ are unchanged) :where $a = i$, $E_{a b} = \delta_{j b}$ (the contents of row $j$) :where $a = j$, $E_{a b} = \delta_{i b}$ (the contents of row $i$) That is: :$E_{a b} = \begin {cases} \delta_{a b} & : \text {if $a \ne i$ and $a \ne j$} \\ \delta_{j b} & : \text {if $a = i$} \\ \delta_{i b} & : \text {if $a = j$} \end {cases}$ Hence the result. {{qed}} [[Category:Elementary Matrix corresponding to Elementary Row Operation]] swum7srrfe0166pzo0sd3i1yzcw3b9d	1
=== Closedness === Let $\sequence {x_n}_{n \mathop \in \N}$ be a [[Definition:Sequence|sequence in $K$]]. Suppose, $\sequence {x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $L \in K$. Then there is a [[Definition:Subsequence|subsequence]] $\sequence {x_{n_k}}_{k \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|convergent]] to $L' \in K$. But $\sequence {x_{n_k}}_{k \mathop \in \N}$ is a [[Definition:Subsequence|subsequence]] $\sequence {x_n}_{n \mathop \in \N}$. Hence, $\sequence {x_{n_k}}_{k \mathop \in \N}$ [[Limit of Subsequence equals Limit of Sequence/Normed Vector Space|converges]] to $L$. By [[Convergent Sequence in Normed Vector Space has Unique Limit|uniqueness of limits]], $L = L'$. By definition, $K$ is [[Definition:Closed Set of Normed Vector Space|closed]]. === Boundedness === Suppose, $K$ is not [[Definition:Bounded Normed Vector Space|bounded]]. Then: :$\forall n \in \N : \exists x_n \in K: \norm {x_n} > n$ Therefore, no [[Definition:Subsequence|subsequence]] of $\sequence {x_n}_{n \mathop \in \N}$ is [[Definition:Convergent Sequence in Normed Vector Space|convergent]]. Hence, $K$ is not [[Definition:Compact Space/Normed Vector Space/Subspace|compact]]. This is in [[Definition:Contradiction|contradiction]] with the [[Definition:Assumption|assumption]]. Thus, $K$ is [[Definition:Bounded Normed Vector Space|bounded]].	1
:[[File:AreaOfParallelogramComplex.png|400px]] From [[Area of Parallelogram]]: :$\AA = \text{base} \times \text{height}$ In this context: :$\text {base} = \cmod {z_2}$ and: :$\text {height} = \cmod {z_1} \sin \theta$ The result follows by definition of [[Definition:Vector Cross Product/Complex/Definition 2|complex cross product]]. {{qed}}	1
Let $\struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ be an [[Definition:Absolutely Convergent Series|absolutely convergent series]] in $X$. Then $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Convergent Series|convergent]] {{iff}} $X$ is a [[Definition:Banach Space|Banach space]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {R', \norm {\, \cdot \,}' }$ be a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] of $\struct {R, \norm {\, \cdot \,} }$ Then: :$\struct {R, \norm {\, \cdot \,} }$ is [[Definition:Isometric Isomorphism|isometrically isomorphic]] to a [[Definition:Everywhere Dense|dense]] [[Definition:Normed Division Subring|normed division subring]] of $\struct {R', \norm {\, \cdot \,}' }$.	1
Let $\mathbf A$ be an [[Definition:Orthogonal Matrix|orthogonal matrix]]. Then its [[Definition:Inverse Matrix|inverse]] $\mathbf A^{-1}$ is also [[Definition:Orthogonal Matrix|orthogonal]].	1
Let $y \in \map {B_r} x$. Let $a \in \map {B_r} y$. By the definition of an [[Definition:Open Ball of Normed Division Ring|open ball]], then: :$\norm {a - y} < r$ :$\norm {y - x} < r$ Hence: {{begin-eqn}} {{eqn | l = \norm {a - x} | r = \norm {a - y + y - x} }} {{eqn | o = \le | r = \max \set {\norm {a - y}, \norm{y - x} } | c = {{Defof|Non-Archimedean Division Ring Norm}} }} {{eqn | o = < | r = r | c = }} {{end-eqn}} By the definition of an [[Definition:Open Ball of Normed Division Ring|open ball]]: :$a \in \map {B_r} x$ Hence: :$\map {B_r} y \subseteq \map {B_r} x$ By [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]]: :$\norm {x - y} < r$ By the definition of an [[Definition:Open Ball of Normed Division Ring|open ball]]: :$x \in \map {B_r} y$ Similarly it follows that: :$\map {B_r} x \subseteq \map {B_r} y$ By [[Definition:Set Equality/Definition 2|set equality]]: :$\map {B_r} y = \map {B_r} x$ {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$ be a [[Definition:Matrix|matrix]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $F$. Then $A$ is [[Definition:Row Equivalence|row equivalent]] to a [[Definition:Reduced Echelon Matrix|reduced echelon matrix]] of [[Definition:Order of Matrix|order]] $m \times n$.	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Real Euclidean Space|Euclidean space]] $\R^3$. Let $\times$ denote the [[Definition:Vector Cross Product|vector cross product]]. Then: :$(1): \quad$ $\left\Vert{ \mathbf a \times \mathbf b }\right\Vert^2 = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 - \left({\mathbf a \cdot \mathbf b}\right)^2$ :$(2): \quad$ $\left\Vert{ \mathbf a \times \mathbf b }\right\Vert = \left\Vert{\mathbf a}\right\Vert \left\Vert{\mathbf b}\right\Vert \left\vert{\sin \theta}\right\vert$ where $\theta$ is the [[Definition:Angle Between Vectors|angle between $\mathbf a$ and $\mathbf b$]], or an arbitrary [[Definition:Real Number|number]] if $\mathbf a$ or $\mathbf b$ is the [[Definition:Zero Vector|zero vector]].	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $S, T \subseteq A$ be [[Definition:Multiplicatively Closed Subset of Ring|multiplicatively closed subsets]]. {{TFAE}} :$(1): \quad$ There exists an $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]] $h : A_S \to A_T$ between [[Definition:Localization of Ring|localizations]], the '''[[Definition:Induced Homomorphism between Localizations of Ring|induced homomorphism]]'''. :$(2): \quad S$ is a [[Definition:Subset|subset]] of the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $T$. :$(3): \quad$ The [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $S$ is a subset of the saturation of $T$. :$(4): \quad$ Every [[Definition:Prime Ideal of Ring|prime ideal]] [[Definition:Set Meeting Set|meeting]] $S$ also meets $T$.	1
This is proved in [[Expansion Theorem for Determinants]]. {{qed}} [[Category:Determinants]] ejpg48vysa61ntozt8m9l4q41r80xl6	1
Let $\left({\mathbf V, +, \circ}\right)_{\mathbb F}$ be a [[Definition:Vector Space|vector space]] over $\mathbb F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then every $\mathbf v \in \left({\mathbf V, +}\right)$ is [[Definition:Left Cancellable Element|left cancellable]]: :$\forall \mathbf a, \mathbf b, \mathbf c \in \mathbf V: \mathbf c + \mathbf a = \mathbf c + \mathbf b \implies \mathbf a = \mathbf b$	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\map {\MM_R} 2$ denote the [[Definition:Matrix Space|$2 \times 2$ matrix space]] over $R$. The operation of [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]] is not [[Definition:Commutative Operation|commutative]] over $\map {\MM_R} 2$.	1
Consider this [[Simultaneous Linear Equations/Examples/Arbitrary System 2|system of simultaneous linear equations]]: {{begin-eqn}} {{eqn | n = 1 | l = x_1 + x_2 | r = 2 }} {{eqn | n = 2 | l = 2 x_1 + 2 x_2 | r = 3 }} {{end-eqn}} From its [[Simultaneous Linear Equations/Examples/Arbitrary System 2|evaluation]] it is seen to have no [[Definition:Solution to System of Simultaneous Equations|solutions]]. Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Division Ring|division ring]]. Let $\norm {\,\cdot\,}_1$ and $\norm {\,\cdot\,}_2$ be [[Definition:Equivalent Division Ring Norms|equivalent norms]] on $R$. Then: :$\struct {R,\norm {\,\cdot\,}_1}$ is [[Definition:Complete Normed Division Ring|complete]] {{iff}} $\struct {R,\norm {\,\cdot\,}_2}$ is [[Definition:Complete Normed Division Ring|complete]].	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\map {\MM_R} n$ be a [[Definition:Matrix Space|$n \times n$ matrix space]] over $R$. Then [[Definition:Matrix Product (Conventional)|matrix multiplication (conventional)]] over $\map {\MM_R} n$ is [[Definition:Closure (Abstract Algebra)|closed]].	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be a $R$-[[Definition:Right Module|right module]]. Let $N$ be a $R$-[[Definition:Left Module|left module]]. Then: :$T = \displaystyle \bigoplus_{s \mathop \in M \mathop \times N} R s$ is a [[Definition:Left Module|left module]].	1
Let $\displaystyle \map f {e_i} = \sum_{j \mathop = 1}^n c_{ij} e_j$ Let $A$ be the [[Definition:Matrix|matrix]] [[Definition:Relative Matrix|relative]] to the basis $\tuple {e_1, \ldots, e_n}$. Then by the above assumption, $A_{ij} = c_{ij}$. Then: {{begin-eqn}} {{eqn | l = \map \tr f | r = \map \tr A | c = {{Defof|Trace of Linear Operator}} }} {{eqn | r = \sum_{i \mathop = 1}^n A_{ii} | c = {{Defof|Trace of Matrix}} }} {{eqn | r = \sum_{i \mathop = 1}^n c_{ii} | c = From above }} {{end-eqn}} Now it remains to show that $c_{ii} = \innerprod {\map f {e_i} } {e_i}$: {{begin-eqn}} {{eqn | l = \innerprod {\map f {e_i} } {e_i} | r = \innerprod {\sum_{j \mathop = 1}^n c_{ij} e_j} {e_i} | c = From above assumption }} {{eqn | r = \sum_{j \mathop = 1}^n c_{ij} \innerprod {e_j} {e_i} | c = [[Definition:Inner Product|Axiom $(2)$ of Inner Product: Bilinearity]] }} {{eqn | r = c_{ii} (1) | c = {{Defof|Orthonormal Subset}}: other terms vanish }} {{eqn | r = c_{ii} | c = }} {{end-eqn}} {{qed}}	1
Let $h \in H, l \in L$. Then: {{begin-eqn}} {{eqn | l = \innerprod {\paren {AB} h} l_L | r = \innerprod {B h} {A^* l}_K | c = {{Defof|Adjoint Linear Transformation}} of $A$ }} {{eqn | r = \innerprod h {B^* A^* l}_H | c = {{Defof|Adjoint Linear Transformation}} of $B$ }} {{end-eqn}} Thus, by [[Existence and Uniqueness of Adjoint]]: :$\paren {A B}^* = B^* A^*$ {{qed}}	1
Let $\mathbf x = \sequence {x_n}_{n \mathop \in \N} \in \ell^2$. By [[Definition:P-Sequence Space|definition]] of $\ell^2$: :$\displaystyle \sum_{i \mathop = 0}^\infty \size {x_i}^2 < \infty$ Let $\displaystyle s_n := \sum_{i \mathop = 0}^n \size {x_i}^2$ be a [[Definition:Sequence|sequence]] of [[Definition:Partial Sum|partial sums]] of $\displaystyle s = \sum_{i \mathop = 0}^\infty \size {x_i}^2$. We have that $s$ is a [[Definition:Convergent Sequence in Normed Vector Space|convergent sequence]]: :$\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall n \in \N: n > N \implies \size {s_n - s} < \epsilon$ Note that: {{begin-eqn}} {{eqn | l = \size {s_n - s} | r = \size {\sum_{i \mathop = 0}^n \size {x_i}^2 - \sum_{i \mathop = 0}^\infty \size {x_i}^2} }} {{eqn | r = \size {\sum_{i \mathop = n \mathop + 1}^\infty \size {x_i}^2} }} {{end-eqn}} Let $\displaystyle N \in \N : \sum_{n \mathop = N + 1}^\infty \size {x_n}^2 < \epsilon^2$ Let $\mathbf y := \tuple {x_1 \ldots, x_N, 0, \ldots}$. By [[Definition:Space of Almost-Zero Sequences|definition]], $\mathbf y \in c_{00}$. We have that: {{begin-eqn}} {{eqn | l = \norm {\mathbf x - \mathbf y}_2^2 | r = \sum_{i \mathop = 0}^\infty \size {x_i - y_i}^2 | c = {{defof|Euclidean Norm}} }} {{eqn | r = \sum_{i \mathop = N + 1}^\infty \size {x_i}^2 }} {{eqn | o = < | r = \epsilon^2 }} {{eqn | ll = \leadsto | l = \norm {\mathbf x - \mathbf y}_2 | o = < | r = \epsilon }} {{end-eqn}} By [[Definition:Definition|definition]], $c_{00}$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] in $\ell^2$. {{qed}}	1
By the definition of [[Definition:Localization of Ring|localization]], there exist unique [[Definition:Ring Homomorphism|homomorphisms]]: :$g : A_S \to \tilde A_S$ :$h : \tilde A_S \to A_S$ such that: :$h \circ \iota = \tilde \iota$ and: :$g \circ \tilde \iota = \iota$ Therefore: :$\tilde \iota = h \circ \iota = h \circ \left({g \circ \tilde \iota}\right) = \left({h \circ g}\right) \circ \tilde \iota$ The [[Definition:Identity Mapping|identity mapping]] also satisfies this equality (that is, $\tilde \iota = \left({h \circ g}\right) \circ \tilde \iota$). Therefore by the uniqueness of $h$ and $g$ we have: :$h \circ g = I_{\tilde A_S}$ where $I_{\tilde A_S}$ denotes the [[Definition:Identity Mapping|identity mapping]] on $\tilde A_S$. Similarly we have that: :$g \circ h = I_{A_S}$ where $I_{A_S}$ denotes the [[Definition:Identity Mapping|identity mapping]] on $A_S$. Therefore by [[Bijection iff Left and Right Inverse]] $g = \phi$ is a [[Definition:Bijection|bijective]] [[Definition:Ring Homomorphism|homomorphisms]], that is, an [[Definition:Ring Isomorphism|isomorphism]]. {{qed}} [[Category:Commutative Algebra]] q003p2eryv948l1wioz91exvyyozyl6	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Every [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]] is [[Definition:Bounded Sequence in Normed Division Ring|bounded]].	1
Let $\struct {\C^n, +, \cdot}_\C$ be a [[Definition:Complex Vector Space|complex vector space]]. Let $S \subseteq \C^n$. Then $S$ is a '''linearly dependent set''' if there exists a [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ which is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]]. That is, such that: :$\displaystyle \exists \set {\lambda_k: 1 \le k \le n} \subseteq \C: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0$ where $\set {\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n} \subseteq S$, and such that at least one of $\lambda_k$ is not equal to $0$.	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. The $n$th [[Definition:Orthogonal Group|orthogonal group]] on $k$ is a [[Definition:Group|group]].	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $n \in \N_{>0}$. Let $\struct {R^n, +, \times}_R$ be the '''[[Definition:Module on Cartesian Product|$R$-module $R^n$]]'''. Then $\struct {R^n, +, \times}_R$ is an [[Definition:Module|$R$-module]].	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $F$. Let $L$ be a [[Definition:Linearly Independent Set|linearly independent]] [[Definition:Subset|subset]] of $V$. Let $S$ be a [[Definition:Spanning Set|set that spans $V$]]. Suppose that $L \subseteq S$. Then $V$ has a [[Definition:Basis of Vector Space|basis]] $B$ such that $L \subseteq B \subseteq S$.	1
Let $f: \R \to \R$ be an [[Definition:Additive Function (Conventional)|additive function]] which is not linear. Then the [[Definition:Graph of Mapping|graph]] of $f$ is [[Definition:Everywhere Dense|dense]] in the [[Definition:Real Number Plane|real number plane]].	1
It is clear that $(1) \implies (2) \implies (3)$ and for $(4) \implies (2)$: For any $\epsilon > 0$, there exists $\delta = \dfrac \epsilon c$, such that when $\norm {\mathbf 0_H - h}_H < \delta$: :$\norm {\map A h - \map A {\mathbf 0_H} }_K \le c \norm h_H < c\delta = \epsilon$ Now we prove $(3) \implies (1)$: Let $A$ be continuous at some point $h_0$. For any sequence $h_n \to h$ in $H$: :$h_n - h + h_0 \to h_0$ Hence: :$\displaystyle \lim_{n \mathop \to \infty} \map A {h_n - h + h_0} = \lim_{n \mathop \to \infty} \map A {h_n} - \map A h + \map A {h_0} = \map A {h_0}$ We see that: :$\displaystyle \lim_{n \mathop \to \infty} \map A {h_n} = \map A h$ Thus $A$ is continuous. Now for the proof $(2) \implies (4)$. We have that $A$ is continuous at $\mathbf 0_H$. Hence there exists an open ball of positive real radius $a$, centred at $\mathbf 0_H$, such that its image under $A$ is included in the open ball of radius $1$, centred at $\mathbf 0_K$. This is $\norm h_H < a$. Then: :$\norm {\map A h}_K < 1$ Let $h$ be an arbitrary element in $H$. Let $\epsilon > 0$. We have: :$\norm a \dfrac h {\norm h_H + \epsilon}_H < a$ Hence: :$\norm {\map A {a \dfrac h {\norm h_H + \epsilon} }_K} = a \dfrac {\norm {\map A h}_K} {\norm h_H + \epsilon} < 1$ Therefore: :$\norm {\map A h}_K < \dfrac 1 a \norm h_H + \dfrac \epsilon a$ Let $\epsilon \to 0$. Then: :$c = \dfrac 1 a$ {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $T \in \map {B_0} {H, K}$ be a [[Definition:Compact Linear Transformation|compact linear transformation]]. Then $\map \cl {\Rng T}$ is [[Definition:Separable Space|separable]].	1
Let $H$ be a [[Definition:Vector Subspace|subspace]] of $G$. Every [[Definition:Linearly Independent Set|linearly independent subset]] of the [[Definition:Vector Space|vector space]] $H$ is a [[Definition:Linearly Independent Set|linearly independent subset]] of the [[Definition:Vector Space|vector space]] $G$. Therefore, it has no more than $n$ elements by [[Size of Linearly Independent Subset is at Most Size of Finite Generator]]. So the set of all [[Definition:Natural Numbers|natural numbers]] $k$ such that $H$ has a [[Definition:Linearly Independent Set|linearly independent subset]] of $k$ [[Definition:Vector (Linear Algebra)|vectors]] has a largest element $m$, and $m \le n$. Now, let $B$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $H$ having $m$ [[Definition:Vector (Linear Algebra)|vectors]]. If the subspace [[Definition:Generator of Vector Space|generated]] by $B$ were not $H$, then $H$ would contain a [[Definition:Linearly Independent Set|linearly independent subset]] of $m + 1$ [[Definition:Vector (Linear Algebra)|vectors]]. This follows by [[Linearly Independent Subset also Independent in Generated Subspace]]. This would contradict the definition of $m$. Hence $B$ is a [[Definition:Generator of Vector Space|generator]] for $H$ and is thus a [[Definition:Basis of Vector Space|basis]] for $H$. Thus $H$ is [[Definition:Finite Dimensional Vector Space|finite dimensional]] and $\map \dim H \le \map \dim G$. Now, if $\map \dim H = \map \dim G$, then a basis of $H$ is a [[Definition:Basis of Vector Space|basis]] of $G$ by [[Sufficient Conditions for Basis of Finite Dimensional Vector Space]], and therefore $H = G$. {{Qed}}	1
Let $\left({G, \cdot}\right)$ be a [[Definition:Finite Group|finite]] [[Definition:Abelian Group|abelian group]]. Let $V$ be a non-[[Definition:Null Module|null]] [[Definition:Vector Space|vector space]] over an [[Definition:Algebraically Closed Field|algebraically closed field]] $k$. Let $\rho: G \to \operatorname{GL} \left({V}\right)$ be a [[Definition:Linear Representation|linear representation]]. Then $\rho$ is [[Definition:Irreducible Linear Representation|irreducible]] [[Definition:Iff|iff]] $\dim \left({V}\right) = 1$, where, $\dim$ denotes [[Definition:Dimension (Linear Algebra)|dimension]].	1
For every $t\in N$, the [[Definition:Mapping|mapping]]: :$f_t : M \to M : x \mapsto f(x,t)$ is a [[Definition:Contraction Mapping|contraction]]. By the [[Banach Fixed-Point Theorem]], there exists a [[Definition:unique|unique]] $g(t) \in M$ such that $f_t(g(t)) = g(t)$. We show that $g$ is [[Definition:Lipschitz Continuous|Lipschitz continuous]]. Let $K<1$ be a [[Definition:Uniform Lipschitz Constant|uniform Lipschitz constant]] for $f$. Let $L$ be a [[Definition:Lipschitz Constant|Lipschitz constant]] for $f$. Let $s,t\in N$. Then {{begin-eqn}} {{eqn | l = d(g(s), g(t)) | r = d(f(g(s), s), f(g(t), t)) | c = Definition of $g$ }} {{eqn | o = \leq | r = d(f(g(s), s), f(g(t), s)) + d(f(g(t), s), f(g(t), t)) | c = {{defof|Metric}} }} {{eqn | o = \leq | r = K \cdot d(g(s), g(t)) + d(f(g(t), s), f(g(t), t)) | c = $f$ is a [[Definition:Uniform Contraction Mapping|uniform contraction]] }} {{end-eqn}} and thus: {{begin-eqn}} {{eqn | l = d(g(s), g(t)) | o = \leq | r = \dfrac1{1-K}d(f(g(t), s), f(g(t), t)) }} {{eqn | o = \leq | r = \dfrac L{1-K} d(s,t) | c = $f$ is [[Definition:Lipschitz Continuous|lipschitz continuous]] }} {{end-eqn}} Thus $g$ is [[Definition:Lipschitz Continuous|lipschitz continuous]]. {{qed}} [[Category:Implicit Functions]] 66rdm27e18ivrvk98jlfgzalfq9xt22	1
Let $f$ and $g$ be [[Definition:Real Function|real functions]] which are [[Definition:Continuous on Interval|continuous]] on the [[Definition:Closed Real Interval|closed interval]] $\closedint a b$. Then: :$\displaystyle \paren {\int_a^b \map f t \, \map g t \rd t}^2 \le \int_a^b \paren {\map f t}^2 \rd t \int_a^b \paren {\map g t}^2 \rd t$	1
Let $H = \set {\xi_1, \xi_2, \ldots, \xi r}$. Consider the [[Definition:Basis of Vector Space|basis]] $B = \set {\alpha_1, \alpha_2, \ldots, \alpha_n}$ of $E$. Consider the [[Definition:Set|set]] $G = H \cup B = \set {\xi_1, \xi_2, \ldots, \xi r, \alpha_1, \alpha_2, \ldots, \alpha_n}$. We have that $G$ is a [[Definition:Generator of Vector Space|generator]] of $E$. As $B$ is a [[Definition:Basis of Vector Space|basis]], it follows that each of $H$ is a [[Definition:Linear Combination|linear combination]] of $B$. Thus $G = H \cup B$ is [[Definition:Linearly Dependent Set|linearly dependent]]. Thus one of the [[Definition:Element|elements]] $\alpha_i$ of $B$ is a [[Definition:Linear Combination|linear combination]] of the preceding [[Definition:Element|elements]] of $G$. Eliminate this one, and do the same thing with with $G \setminus \set {\alpha_i}$. Eventually there will exist a [[Definition:Set|set]] which is a [[Definition:Basis of Vector Space|basis]] of $E$ containing all the [[Definition:Element|elements]] of $H$. {{Qed}}	1
Let $\left({v, w}\right) \in V \times V$ with $b \left({v, w}\right) = 0$. We have: {{begin-eqn}} {{eqn | l = 0 | r = b \left({v + w, v + w}\right) | c = $b$ is [[Definition:Alternating Bilinear Form|alternating]] }} {{eqn | r = b \left({v, v}\right) + b \left({v, w}\right) + b \left({w, v}\right) + b \left({w, w}\right) | c = {{Defof|Bilinear Form}} }} {{eqn | r = b \left({v, w}\right) + b \left({w, v}\right) | c = $b$ is [[Definition:Alternating Bilinear Form|alternating]] }} {{eqn | r = b \left({w, v}\right) | c = $b \left({v, w}\right) = 0$ }} {{end-eqn}} Because $\left({v, w}\right)$ was arbitrary, $b$ is [[Definition:Reflexive Bilinear Form|reflexive]]. {{qed}}	1
=== Proof of $(1)$ === For $x, y \in V$, compute: {{begin-eqn}} {{eqn | l = \left\Vert{x + y}\right\Vert^2 | r = \left\langle{x + y, x + y}\right\rangle | c = Definition of $\left\Vert{\cdot}\right\Vert$ }} {{eqn | r = \left\langle{x, x}\right\rangle + \left\langle{x, y}\right\rangle + \left\langle{y, x}\right\rangle + \left\langle{y, y}\right\rangle | c = [[Definition:Semi-Inner Product|Linearity]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn | o = \le | r = \left\Vert{x}\right\Vert^2 + \sqrt{\left\langle{x, x}\right\rangle \left\langle{y, y}\right\rangle} + \sqrt{\left\langle{y, y}\right\rangle \left\langle{x, x}\right\rangle} + \left\Vert{y}\right\Vert^2 | c = [[Cauchy-Bunyakovsky-Schwarz Inequality/Inner Product Spaces|Cauchy-Bunyakovsky-Schwarz Inequality]] }} {{eqn | r = \left\Vert{x}\right\Vert^2 + 2 \left\Vert{x}\right\Vert \left\Vert{y}\right\Vert + \left\Vert{y}\right\Vert^2 | c = Definition of $\left\Vert{\cdot}\right\Vert$ }} {{eqn | r = \left({\left\Vert{x}\right\Vert + \left\Vert{y}\right\Vert}\right)^2 }} {{end-eqn}} Taking square roots on either side gives the result. {{qed|lemma}} === Proof of $(2)$ === For $x \in V$, $a \in \Bbb F$, compute: {{begin-eqn}} {{eqn | l = \left\Vert{a x}\right\Vert^2 | r = \left\langle{a x, a x}\right\rangle | c = Definition of $\left\Vert{\cdot}\right\Vert$ }} {{eqn | r = a \left\langle{x, a x}\right\rangle | c = [[Definition:Semi-Inner Product|Linearity]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn | r = a \overline{\left\langle{a x, x}\right\rangle} | c = [[Definition:Semi-Inner Product|Conjugate symmetry]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn | r = a \overline a \overline{\left\langle{x, x}\right\rangle} | c = [[Definition:Semi-Inner Product|Linearity]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn | r = \left\vert{a}\right\vert^2 \left\Vert{x}\right\Vert^2 | c = [[Modulus in Terms of Conjugate]] }} {{end-eqn}} Taking square roots on either side gives the result. {{qed}}	1
Let $T = \struct {S, \tau}$ be a [[Definition:Separable Space|separable space]]. Then by [[Definition:Separable Space|definition]] there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $X$ which is [[Definition:Everywhere Dense|everywhere dense]]. We need to show that every [[Definition:Disjoint Sets|disjoint set]] of [[Definition:Open Set (Topology)|open sets]] of $T$ is [[Definition:Countable Set|countable]]. {{ProofWanted}}	1
{{begin-eqn}} {{eqn | l = \left({\mathbf u + \mathbf v}\right) \cdot \mathbf w | r = \sum_{i \mathop = 1}^n \left({u_i + v_i}\right) w_i | c = {{Defof|Vector Sum}} and {{Defof|Dot Product}} }} {{eqn | r = \sum_{i \mathop = 1}^n \left({u_i w_i + v_i w_i}\right) | c = [[Real Multiplication Distributes over Real Addition]] }} {{eqn | r = \sum_{i \mathop = 1}^n u_i w_i + \sum_{i \mathop = 1}^n v_i w_i | c = }} {{eqn | r = \mathbf u \cdot \mathbf w + \mathbf v \cdot \mathbf w | c = {{Defof|Dot Product}} }} {{end-eqn}} {{qed}}	1
Let $U$ be an arbitrary [[Definition:Ideal of Ring|ideal]] of $\Z$. Let $c$ be a non-[[Definition:Ring Zero|zero]] [[Definition:Element|element]] of $U$. Then both $c$ and $-c$ belong to $\ideal a$ and one of them is [[Definition:Positive Integer|positive]]. Thus $U$ contains [[Definition:Strictly Positive Integer|strictly positive]] elements. Let $b$ be the [[Definition:Smallest Element|smallest]] [[Definition:Strictly Positive Integer|strictly positive]] element of $U$. By the [[Set of Integers Bounded Below by Integer has Smallest Element]], $b$ is guaranteed to exist. If $\ideal b$ denotes the [[Definition:Generator of Ideal|ideal generated by $b$]], then $\ideal b \subseteq U$ because $b\in U$ and $U$ is an ideal. Let $a \in U$. By the [[Division Theorem]]: :$\exists q, r \in \Z, 0 \le r < b: a = b q + r$ As $a, b \in U$ it follows that so does $r = a - b q$. By definition of $b$ it follows that $r = 0$. Thus: :$a = b q \in \ideal b$ and so: :$U \subseteq \ideal b$ From the above: :$U = \ideal b$ It follows by definition that $U$ is a [[Definition:Principal Ideal of Ring|principal ideal]] of $\Z$. Recall that $U$ was an arbitrary [[Definition:Ideal of Ring|ideal]] of $\Z$. Hence by definition $\Z$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathbf A$ be a [[Definition:Matrix|matrix]] in the [[Definition:Matrix Space|matrix space]] $\mathcal M_{m, n}\left({R}\right)$. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose of $\mathbf A$]]. The '''left null space''' '''$\mathbf A$''' is defined as the [[Definition:Null Space|null space]] of $\mathbf A^\intercal$.	1
Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] such that $e \paren {\mathbf X} = \mathbf Y$. Then this result follows immediately from [[Elementary Row Operations as Matrix Multiplications]]: :$e \paren {\mathbf X} = \mathbf {E X} = \mathbf Y$ where $\mathbf E = e \paren {\mathbf I}$. {{qed}}	1
By the [[Second Isomorphism Theorem/Vector Spaces|second isomorphism theorem]]: :$\dfrac {M + N} M \equiv \dfrac N {M \cap N}$ The result follows. {{qed}}	1
=== Definition 1 equivalent to Definition 3 === {{begin-eqn}} {{eqn | o = | r = \map \Im {\overline {z_1} z_2} | c = {{Defof|Vector Cross Product|subdef = Complex|index = 3}} }} {{eqn | r = \map \Im {\paren {x_1 - i y_1} \paren {x_2 + i y_2} } | c = {{Defof|Complex Conjugate}} }} {{eqn | r = \map \Im {\paren {x_1 x_2 + y_1 y_2} + i \paren {x_1 y_2 - x_2 y_1} } | c = {{Defof|Complex Multiplication}} }} {{eqn | r = x_1 y_2 - x_2 y_1 | c = {{Defof|Imaginary Part}} }} {{end-eqn}} {{qed|lemma}} === Definition 2 equivalent to Definition 3 === {{begin-eqn}} {{eqn | o = | r = \map \Im {\overline {z_1} z_2} | c = {{Defof|Vector Cross Product|subdef = Complex|index = 3}} }} {{eqn | r = r_1 r_2 \sin \paren {\theta_2 - \theta_1} | c = [[Complex Cross Product in Exponential Form]] }} {{eqn | r = \cmod {z_1} \, \cmod {z_2} \map \sin {\theta_2 - \theta_1} | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \cmod {z_1} \, \cmod {z_2} \sin \theta | c = where $\theta = \theta_2 - \theta_1$ is the angle between $z_1$ and $z_2$ }} {{end-eqn}} {{qed|lemma}} === Definition 1 equivalent to Definition 4 === {{begin-eqn}} {{eqn | o = | r = \frac {\overline {z_1} z_2 - z_1 \overline {z_2} } {2 i} | c = Definition of [[Definition:Dot Product/Complex/Definition 4|Complex Dot Product: Definition 4]] }} {{eqn | r = \frac {\paren {x_1 - i y_1} \paren {x_2 + i y_2} - \paren {x_1 + i y_1} \paren {x_2 - i y_2} } {2 i} | c = {{Defof|Complex Conjugate}} }} {{eqn | r = \frac {\paren {\paren {x_1 x_2 + y_1 y_2} + i \paren {x_1 y_2 - x_2 y_1} } - \paren {\paren {x_1 x_2 + y_1 y_2} + i \paren {-x_1 y_2 + x_2 y_1} } } {2 i} | c = {{Defof|Complex Multiplication}} }} {{eqn | r = x_1 y_2 - x_2 y_1 | c = after algebra }} {{end-eqn}} {{qed}}	1
Let $k$ be a [[Definition:Division Ring|division ring]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $k$. Let $X$ and $Y$ be [[Definition:Basis of Vector Space|bases]] of $V$. Then $X$ and $Y$ are [[Definition:Equinumerous|equinumerous]].	1
Let $\left({R, +, \cdot}\right)$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\left({V, +, \circ}\right)_R$ be an [[Definition:Module|$R$-module]]. Then for any $r \in R$, the [[Definition:Rescaling|rescaling]]: :$m_r: V \to V, v \mapsto r \circ v$ is a [[Definition:Linear Transformation|linear transformation]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $E$ be an [[Definition:Orthonormal Subset|orthonormal subset]] of $H$. Then the following six statements are equivalent: :$(1): \qquad E$ is a [[Definition:Basis (Hilbert Space)|basis]] for $H$ :$(2): \qquad h \in H, h \perp E \implies h = \mathbf 0$, where $\perp$ denotes [[Definition:Orthogonal (Hilbert Space)|orthogonality]] :$(3): \qquad \vee E = H$, where $\vee E$ denotes the [[Definition:Closed Linear Span|closed linear span]] of $E$ :$(4): \qquad \forall h \in H: h = \displaystyle \sum \left\{{\left\langle{h, e}\right\rangle e: e \in E}\right\}$ :$(5): \qquad \forall g, h \in H: \left\langle{g, h}\right\rangle = \displaystyle \sum \left\{{\left\langle{g, e}\right\rangle\left\langle{e, h}\right\rangle: e \in E}\right\}$ :$(6): \qquad \forall h \in H: \left\Vert{h}\right\Vert^2 = \displaystyle \sum \left\{{\left\vert{\left\langle{h, e}\right\rangle}\right\vert^2: e \in E}\right\}$ In the last three statements, $\displaystyle \sum$ denotes a [[Definition:Generalized Sum|generalized sum]]. Statement $(6)$ is commonly known as '''Parseval's identity'''.	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $n \ge 0$ be a [[Definition:Natural Number|natural number]]. Let $E$ be an [[Definition:Dimension of Vector Space|$n$-dimensional]] [[Definition:Vector Space|vector space]] over $K$. Let $B \subseteq E$ be a [[Definition:Subset|subset]] such that $\card B = n$. {{TFAE}} : $(1): \quad$ $B$ is a [[Definition:Basis of Vector Space|basis]] of $E$. : $(2): \quad$ $B$ is [[Definition:Linearly Independent Set|linearly independent]]. : $(3): \quad$ $B$ is a [[Definition:Generator of Module|generator]] for $E$.	1
Let $e_3$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\text {ECO} 3$: {{begin-axiom}} {{axiom | n = \text {ECO} 3 | t = Exchange [[Definition:Column of Matrix|columns]] $i$ and $j$ | m = \kappa_i \leftrightarrow \kappa_j }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_3$ be the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to $e_3$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_3$ is: :$\map \det {\mathbf E_3} = -1$	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\theta_1$ and $\theta_2$ be [[Definition:Argument of Complex Number|arguments]] of $z_1$ and $z_2$, respectively. Then: :$\cmod {z_1 + z_2}^2 = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \cmod {z_1} \cmod {z_2} \, \map \cos {\theta_1 - \theta_2}$	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $\ideal p$ be the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $p$]]. Then $p$ is [[Definition:Irreducible Element of Ring|irreducible]] {{iff}} $\ideal p$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$.	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A3}} that the [[Definition:Identity Element|identity element]] of [[Definition:Ring Addition|ring addition]] is the [[Definition:Ring Zero|ring zero]] $0_R$. The result then follows directly from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
Let $\mathbf a$ be a [[Definition:Vector Quantity|vector quantity]]. Let $m$ be a [[Definition:Scalar Quantity|scalar quantity]]. Then: :$m \mathbf a = m \paren {\size {\mathbf a} \hat {\mathbf a} } = \paren {m \size {\mathbf a} } \hat {\mathbf a}$ where: :$\size {\mathbf a}$ denotes the [[Definition:Magnitude|magnitude]] of $\mathbf a$ :$\hat {\mathbf a}$ denotes the [[Definition:Unit Vector|unit vector]] in the [[Definition:Direction|direction]] $\mathbf a$.	1
Let $V$ be a [[Definition:Vector Space|vector space]]. Let $\BB = \tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ be a [[Definition:Basis of Vector Space|basis of $V$]]. Then $\BB$ is an '''orthonormal basis of $V$''' {{iff}}: :$(1): \quad \tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ is an [[Definition:Orthogonal Basis of Vector Space|orthogonal basis of $V$]] :$(2): \quad \norm {\mathbf e_1} = \norm {\mathbf e_2} = \cdots = \norm {\mathbf e_1} = 1$	1
Let $\mathbf v$ be a [[Definition:Vector Quantity|vector quantity]] in [[Definition:Ordinary Space|ordinary $3$-space]]. Let $\mathbf i, \mathbf j, \mathbf k$ be [[Definition:Orthonormal Base Vector|orthonormal base vectors]]. Then: :$\mathbf v = \paren {\mathbf v \cdot \mathbf i} \mathbf i + \paren {\mathbf v \cdot \mathbf j} \mathbf j + \paren {\mathbf v \cdot \mathbf k} \mathbf k$	1
:The [[Definition:Closed Ball of Normed Division Ring|closed $r$-ball of $x$]], $\map { {B_r}^-} x$, is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\norm {\,\cdot\,}$.	1
By [[Isomorphism from R^n via n-Term Sequence]], the mapping $\phi: G \to H$ defined as: : $\displaystyle \phi \left({\sum_{k \mathop = 1}^n \lambda_k a_k}\right) = \sum_{k \mathop = 1}^n \lambda_k b_k$ is well-defined. Thus: : $\forall k \in \left[{1 \,.\,.\, n}\right]: \phi \left({a_k}\right) = b_k$ {{wtd|Verification that $\phi$ is linear needs to be done.}} By [[Linear Transformation of Generated Module]], $\phi$ is the only [[Definition:Linear Transformation|linear transformation]] whose value at $a_k$ is $b_k$ for all $k \in \left[{1 \,.\,.\, n}\right]$. {{Qed}}	1
Let $\left({\mathbf V, +, \circ}\right)_{\mathbb F}$ be a [[Definition:Vector Space|vector space]] over $\mathbb F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Let $\mathbb F$ be [[Definition:Infinite Set|infinite]]. Then: :$\forall \mathbf v, -\mathbf v \in \mathbf V: \mathbf v = - \mathbf v \iff \mathbf v = \mathbf 0$	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $I$ be a [[Definition:Set|set]]. Let $R^{(I)}$ be the [[Definition:Free Module on Set|free $R$-module on $I$]]. Let $B$ be its [[Definition:Canonical Basis of Free Module on Set|canonical basis]]. Then $B$ is a [[Definition:Basis of Module|basis]] of $R^{(I)}$.	1
A [[Definition:Zero Vector Quantity|zero vector]] has no [[Definition:Direction|direction]].	1
{{proof wanted}} [[Category:Abelian Groups]] [[Category:Module Theory]] b55k22ba0vpqn8b9dm2luz8dfifd531	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\left\vert{z}\right\vert$ be the [[Definition:Complex Modulus|modulus]] of $z$. Then: : $\left\vert{z^2}\right\vert = \left\vert{z}\right\vert^2$	1
If $h = \mathbf 0$ there is nothing to prove. Let $h \ne \mathbf 0$. By the definition of the [[Definition:Supremum of Set|supremum]]: : $\dfrac{\left|{Lh}\right|} {\left\Vert{h}\right\Vert} \le \left\Vert{L}\right\Vert_3 = \left\Vert{L}\right\Vert$ whence: : $\left|{Lh}\right| \le \left\|{L}\right\| \left\|{h}\right\|$ {{qed}}	1
We have that [[Unit Matrix is Proper Orthogonal]], so $\map {\operatorname {SO} } {n, k}$ is not [[Definition:Empty Set|empty]]. Let $\mathbf A, \mathbf B \in \map {\operatorname {SO} } {n, k}$. Then, by definition, $\mathbf A$ and $\mathbf B$ are [[Definition:Proper Orthogonal Matrix|proper orthogonal]]. Then by [[Inverse of Proper Orthogonal Matrix is Proper Orthogonal]]: :$\mathbf B^{-1}$ is a [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]]. By [[Product of Proper Orthogonal Matrices is Proper Orthogonal Matrix]]: :$\mathbf A \mathbf B^{-1}$ is a [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]]. Thus by definition of [[Definition:Special Orthogonal Group|special orthogonal group]]: :$\mathbf A \mathbf B^{-1} \in \map {\operatorname {SO} } {n, k}$ Hence the result by [[One-Step Subgroup Test]]. {{qed}} [[Category:Orthogonal Groups]] c84yl527lbmgzjz3uiuiri8mcwzpl8t	1
Let $n$ be the [[Definition:Cardinality of Finite Set|cardinality]] of $S$. Let $\sigma: \N_{< n} \to S$ be a [[Definition:Bijection|bijection]], where $\N_{< n}$ is an [[Definition:Initial Segment of Natural Numbers|initial segment of the natural numbers]]. By definition of [[Definition:Summation|summation]], we have to prove the following [[Definition:Inequality|inequality]] of [[Definition:Indexed Summation|indexed summations]]: :$\displaystyle \left\vert{\sum_{i \mathop = 0}^{n - 1} f \left({\sigma \left({i}\right)}\right)}\right\vert \le \sum_{i \mathop = 0}^{n-1} \left({\left\vert{f}\right\vert \circ \sigma}\right) \left({i}\right) $ By [[Absolute Value of Mapping Composed with Mapping]]: : $\left\vert{f}\right\vert \circ \sigma = \left\vert{f \circ \sigma}\right\vert$ The above equality now follows from [[Triangle Inequality for Indexed Summations]]. {{qed}}	1
Let $\Bbb F$ denote one of the [[Definition:Standard Number System|standard number systems]]. Let $\map \MM {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\Bbb F$. Let $\mathbf A$ be an [[Definition:Element|element]] of $\map \MM {m, n}$. Let $-\mathbf A$ be the [[Definition:Negative Matrix|negative]] of $\mathbf A$. Then $-\mathbf A$ is the [[Definition:Inverse Element|inverse]] for the operation $+$, where $+$ is [[Definition:Matrix Entrywise Addition|matrix entrywise addition]].	1
Let $e_1$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\text {ECO} 1$: {{begin-axiom}} {{axiom | n = \text {ECO} 1 | t = For some $\lambda \ne 0$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Column of Matrix|column]] $k$ by $\lambda$ | m = \kappa_k \to \lambda \kappa_k }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_1$ be the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to $e_1$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_1$ is: :$\map \det {\mathbf E_1} = \lambda$	1
Let $\struct {X, \norm {\, \cdot \,} }$ be a [[Definition:Normed Vector Space|normed vector space]]. Then: :$\forall x, y \in X: \norm {x - y} \ge \size {\norm x - \norm y}$	1
We have that [[Polynomial Forms over Field is Euclidean Domain]]. We also have that [[Euclidean Domain is Principal Ideal Domain]]. Hence the result. {{qed}}	1
From [[Additive Function of Zero is Zero]]: :$\map f 0 = 0$ Thus, for all $x \in \R$, we have: {{begin-eqn}} {{eqn | l = 0 | r = \map f 0 | c = }} {{eqn | r = \map f {x + \paren {-x} } | c = }} {{eqn | r = \map f x + \map f {-x} | c = }} {{end-eqn}} It follows that the function $f$ is [[Definition:Odd Function|odd]]: :$\forall x \in \R: \map f {-x} = -\map f x$. {{qed}} [[Category:Additive Functions]] [[Category:Odd Functions]] kktj3q1hz1r50dop4kgcdw1dz9fa7hv	1
We have: :$\displaystyle\left\{{ h \in H : \left\Vert{h}\right\Vert = 1 }\right\} \subseteq \left\{{ h \in H : \left\Vert{h}\right\Vert \le 1 }\right\} \subseteq H$ So it follows from the definition of the [[Definition:Supremum of Set|supremum]] that :$\left\Vert{L}\right\Vert_2 \le \left\Vert{L}\right\Vert_1 \le \left\Vert{L}\right\Vert_3$ Next we show that $\left\Vert{L}\right\Vert_2 = \left\Vert{L}\right\Vert_3$. {{begin-eqn}} {{eqn|l= \left\Vert{L}\right\Vert_2 |o= = |r= \sup \left\{ {\left\Vert{Lh}\right\Vert: \left\Vert{h}\right\Vert = 1}\right\} }} {{eqn|l= |o= = |r= \sup \left\{ {\left\Vert{Lh'}\right\Vert: h \in H\setminus\left\{\mathbf 0\right\},\ h' = h/\left\Vert{h}\right\Vert} \right\} |c= as $\left\Vert h/\left\Vert{h}\right\Vert \right\Vert = \left\Vert{h}\right\Vert/\left\Vert{h}\right\Vert = 1$ for all $h \in H\setminus\left\{\mathbf 0 \right\}$ }} {{eqn|l= |o= = |r= \sup \left\{ {\left\Vert{\frac1{\left\Vert{h}\right\Vert}Lh}\right\Vert: h \in H\setminus \left\{ \mathbf 0 \right\} }\right\} |c= as $L$ is [[Definition:Linear Functional|linear]] }} {{eqn|l= |o= = |r= \sup \left\{ {\frac{\left\Vert{Lh}\right\Vert}{\left\Vert{h}\right\Vert}: h \in H\setminus \left\{ \mathbf 0 \right\} }\right\} |c= by property $(N2)$ of a [[Definition:Norm on Vector Space|norm]] }} {{eqn|l= |o= = |r= \left\Vert{L}\right\Vert_3 }} {{end-eqn}} Therefore :$\left\Vert{L}\right\Vert_1 = \left\Vert{L}\right\Vert_2 = \left\Vert{L}\right\Vert_3$ Moreover, if $\left|{Lh}\right| \le c \left\|{h}\right\|$ for all $h \in H\setminus\left\{\mathbf 0 \right\}$, then we have: :$\displaystyle\left\|{L}\right\|_3 = \sup \left\{{\dfrac {\left|{Lh}\right|} {\left\|{h}\right\|}: h \in H \setminus \left\{\mathbf 0 \right\}}\right\} \le c$ Taking the [[Definition:Infimum of Set|infimum]] over all such $c$ this reads: :$\left\Vert{L}\right\Vert_3 \le \left\Vert{L}\right\Vert_4$ Suppose that $c_0 := \left\Vert{L}\right\Vert_4 > \left\Vert{L}\right\Vert_3$. Then by the definitions of these two norms, this means that there exists $\epsilon > 0$ such that for every $h \in H\setminus\left\{\mathbf 0 \right\}$: :$\displaystyle\frac{|Lh|}{\Vert h\Vert} + \epsilon \le c_0$ But this in turn implies that for every $h \in H\setminus\left\{\mathbf 0 \right\}$: :$|Lh| \leq c_0 \Vert h\Vert - \epsilon \Vert h\Vert = \left( c_0 - \epsilon \right) \Vert h\Vert$ This contradicts the fact that $c_0$ is the least such number satisfying this inequality. Therefore: : $\left\Vert{L}\right\Vert_3 = \left\Vert{L}\right\Vert_4$ and the proof is complete. {{qed}}	1
The first equivalence follows from [[Correspondence Between Group Actions and Permutation Representations]]. === 1 implies 2 === Let $\rho : G \to \operatorname{GL} \left({V}\right)$ be a [[Definition:Linear Representation|linear representation]] of $G$ on $V$. Then for all $g \in G$, $v_1, v_2 \in V$: {{begin-eqn}} {{eqn | l = \phi \left({g, v_1 + v_2}\right) | r = \rho \left({g}\right) \left({v_1 + v_2}\right) | c = {{Defof|Group Action Associated to Permutation Representation}} }} {{eqn | r = \rho \left({g}\right) \left({v_1}\right) + \rho \left({g}\right) \left({v_2}\right) | c = $\rho \left({g}\right)$ is linear }} {{eqn | r =\phi \left({g, v_1}\right) + \phi \left({g, v_2}\right) | c = {{Defof|Group Action Associated to Permutation Representation}} }} {{end-eqn}} and for all $g \in G$, $v \in V$, $\lambda \in k$: {{begin-eqn}} {{eqn | l = \phi \left({g, \lambda \cdot v }\right) | r = \rho \left({g}\right) \left({\lambda \cdot v}\right) | c = {{Defof|Group Action Associated to Permutation Representation}} }} {{eqn | r = \lambda \cdot \rho\left({g}\right) \left({v}\right) | c = $\rho \left({g}\right)$ is linear }} {{eqn | r = \lambda \cdot \phi \left({g, v}\right) | c = {{Defof|Group Action Associated to Permutation Representation}} }} {{end-eqn}} Therefore $\phi$ is a [[Definition:Linear Group Action|linear group action]] of $G$ on $V$. {{qed|lemma}} === 2 implies 1 === Let $\phi: G \times V \to V$ be a [[Definition:Linear Group Action|linear action]] of $G$ on $V$. Then for all $g \in G$, $v_1, v_2 \in V$: {{begin-eqn}} {{eqn | l = \rho \left({g}\right) \left({v_1 + v_2}\right) | r = \phi \left({g, v_1 + v_2}\right) | c = {{Defof|Permutation Representation Associated to Group Action}} }} {{eqn | r = \phi \left({g, v_1}\right) + \phi \left({g, v_2}\right) | c = {{Defof|Linear Group Action}} }} {{eqn | r = \rho \left({g}\right) \left({v_1}\right) + \rho \left({g}\right) \left({v_2}\right) | c = {{Defof|Permutation Representation Associated to Group Action}} }} {{end-eqn}} and for all $g \in G$, $v \in V$, $\lambda \in k$: {{begin-eqn}} {{eqn | l = \rho \left({g}\right) \left({\lambda \cdot v}\right) | r = \phi \left({g, \lambda \cdot v }\right) | c = {{Defof|Permutation Representation Associated to Group Action}} }} {{eqn | r = \lambda \cdot \phi \left({g, v}\right) | c = {{Defof|Linear Group Action}} }} {{eqn | r = \lambda \cdot \rho\left({g}\right) \left({v}\right) | c = {{Defof|Permutation Representation Associated to Group Action}} }} {{end-eqn}} Therefore $\rho$ is a [[Definition:Linear Representation|linear representation]] of $G$ on $V$. {{qed|lemma}} {{qed}} {{refactor|the below can safely be merged into [[Group Action defines Permutation Representation]]}} Now let $g_1, g_2 \in G$. We have for all $v \in V$: {{begin-eqn}} {{eqn | l = \rho \left({g_1 * g_2}\right) \left({v}\right) | r = \phi \left({g_1 * g_2, v}\right) | c = }} {{eqn | r = \phi \left({g_1, \phi\left({g_2, v}\right) }\right) | c = $\phi$ is an action }} {{eqn | r = \rho \left({g_2}\right) \left({\rho \left({g_1}\right) \left({v}\right) }\right) | c = }} {{eqn | r = \left({ \rho \left({g_2}\right) \circ \rho \left({g_1}\right) }\right) \left({v}\right) | c = where $\circ$ is the [[Definition:Composition of Mappings|composition of mappings]] }} {{end-eqn}} Thus $\rho$ satisfies the [[Definition:Morphism Property|homomorphism property]]. {{refactor|the below is better merged into the theorem statement}} Therefore: :$\hat{} : ($linear representations$) \to ($linear actions$)$ :$\tilde{} : ($linear actions$) \to ($linear representations$)$ give a [[Definition:Bijection|bijection]]. {{qed}}	1
Since $T$ is bounded, it maps bounded sets to bounded sets. Because its range is finite dimensional, the closure of an image of a bounded set is compact. Therefore, $T$ is a compact linear operator.	1
Use induction and the facts $E_{i j} A E_{k l} = A_{j k} E_{i l}$ and $\map \tr {A E_{i j} } = A_{j i}$ (induction basis). {{ProofWanted}} [[Category:Matrix Theory]] 4qzwd4t34nevpnyuos1ofy2hzki679z	1
If $\left({G, +_G: \circ}\right)_R$ is an [[Definition:Module|$R$-module]], then: $\forall x, y, \in G, \forall \lambda, \mu \in R$: : $(1): \quad \lambda \circ \left({x +_G y}\right) = \left({\lambda \circ x}\right) +_G \left({\lambda \circ y}\right)$ : $(2): \quad \left({\lambda +_R \mu}\right) \circ x = \left({\lambda \circ x}\right) +_G \left({\mu \circ x}\right)$ : $(3): \quad \left({\lambda \times_R \mu}\right) \circ x = \lambda \circ \left({\mu \circ x}\right)$ If $\phi: G \to H$ is an [[Definition:R-Algebraic Structure Epimorphism|epimorphism]], then: : $\forall x, y \in G: \phi \left({x +_G y}\right) = \phi \left({x}\right) +_H \phi \left({y}\right)$ : $\forall x \in S: \forall \lambda \in R: \phi \left({\lambda \circ x}\right) = \lambda \circ \phi \left({x}\right)$ : $\forall y \in H: \exists x \in G: y = \phi \left({x}\right)$ As $\phi$ is an epimorphism, we can accurately specify the behaviour of all elements of $H$, as they are the images of elements of $G$. If $\phi$ were not an epimorphism, i.e. not [[Definition:Surjection|surjective]], we would have no way of knowing the behaviour of elements of $H$ outside of the [[Definition:Image of Subset under Mapping|image]] of $G$. Hence the specification that $\phi$ needs to be an epimorphism. Now we check the [[Definition:Module|criteria for $H$ being a module]], in turn. === Module $(1)$ === {{begin-eqn}} {{eqn | l=\lambda \circ \left({\phi \left({x}\right) +_H \phi \left({y}\right)}\right) | r=\lambda \circ \left({\phi \left({x +_G y}\right)}\right) | c= }} {{eqn | r=\phi \left({\lambda \circ \left({x +_G y}\right)}\right) | c= }} {{eqn | r=\phi \left({\left({\lambda \circ x}\right) +_G \left({\lambda \circ y}\right)}\right) | c= }} {{eqn | r=\phi \left({\lambda \circ x}\right) +_H \phi \left({\lambda \circ y}\right) | c= }} {{end-eqn}} Thus $(1)$ is shown to hold for $H$. === Module $(2)$ === {{begin-eqn}} {{eqn | l=\left({\lambda +_R \mu}\right) \circ \phi \left({x}\right) | r=\phi \left({\left({\lambda +_R \mu}\right) \circ x}\right) | c= }} {{eqn | r=\phi \left({\left({\lambda \circ x}\right) +_G \left({\mu \circ x}\right)}\right) | c= }} {{eqn | r=\phi \left({\lambda \circ x}\right) +_H \phi \left({\mu \circ x}\right) | c= }} {{eqn | r=\lambda \circ \phi \left({x}\right) +_H \mu \circ \phi \left({x}\right) | c= }} {{end-eqn}} Thus $(2)$ is shown to hold for $H$. === Module $(3)$ === {{begin-eqn}} {{eqn | l=\left({\lambda \times_R \mu}\right) \circ \phi \left({x}\right) | r=\phi \left({\left({\lambda \times_R \mu}\right) \circ x}\right) | c= }} {{eqn | r=\phi \left({\lambda \circ \left({\mu \circ x}\right)}\right) | c= }} {{eqn | r=\lambda \circ \left({\phi \left({\mu \circ x}\right)}\right) | c= }} {{eqn | r=\lambda \circ \left({\mu \circ \phi \left({x}\right)}\right) | c= }} {{end-eqn}} Thus $(3)$ is shown to hold for $H$. So all the criteria for $H$ to be an [[Definition:Module|$R$-module]] are fulfilled. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Division Ring|division ring]], and denote its [[Definition:Ring Zero|ring zero]] by $0_R$. Then the [[Definition:Trivial Norm on Division Ring|trivial norm]] $\norm {\, \cdot \,}: R \to \R_{\ge 0}$, which is given by: :$\norm x = \begin{cases} 0 & \text { if } x = 0_R\\ 1 & \text { otherwise} \end{cases}$ defines a [[Definition:Norm on Division Ring|norm]] on $R$.	1
First the extreme case: The smallest submodule of $G$ containing $\O$ is $\set {e_G}$. [[Definition:Linear Combination of Empty Set|By definition]], $\set {e_G}$ is the set of all linear combinations of $\varnothing$. Now the general case: Let $\O \subset S \subseteq G$. Let $L$ be the set of all [[Definition:Linear Combination of Subset|linear combinations]] of $S$. Since $G$ is a [[Definition:Unitary Module|unitary $R$-module]], every element $x \in S$ is the linear combination $1_R x$, so $S \subseteq L$. But $L$ is closed for addition and scalar multiplication, so [[Submodule Test|is a submodule]]. Thus $H \subseteq L$. But as every linear combination of $S$ clearly belongs to any submodule of $G$ which contains $S$, we also have $L \subseteq H$.	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $S = \set {a_1, a_2, \dotsc, a_n}$ be a [[Definition:Set|set]] of non-[[Definition:Ring Zero|zero]] [[Definition:Element|elements]] of $D$. Let $y$ be a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] of $S$. Then $y$ is expressible in the form: :$y = d_1 a_1 + d_2 a_2 + \dotsb + d_n a_n$ where $d_1, d_2, \dotsc, d_n \in D$.	1
Let $\mathbf r$ be a [[Definition:Vector Quantity|vector quantity]] embedded in a [[Definition:Cartesian 3-Space|Cartesian $3$-space]]. Let $\mathbf i$, $\mathbf j$ and $\mathbf k$ be the [[Definition:Unit Vector|unit vectors]] in the [[Definition:Positive Direction|positive directions]] of the [[Definition:X-Axis|$x$-axis]], [[Definition:Y-Axis|$y$-axis]] and [[Definition:Z-Axis|$z$-axis]] respectively. Let $\cos \alpha$, $\cos \beta$ and $\cos \gamma$ be the [[Definition:Direction Cosine|direction cosines]] of $\mathbf r$ with respect to the [[Definition:X-Axis|$x$-axis]], [[Definition:Y-Axis|$y$-axis]] and [[Definition:Z-Axis|$z$-axis]] respectively. Let $x$, $y$ and $z$ be the [[Definition:Component of Vector|components]] of $\mathbf r$ in the $\mathbf i$, $\mathbf j$ and $\mathbf k$ [[Definition:Direction|directions]] respectively. Let $r$ denote the [[Definition:Magnitude|magnitude]] of $\mathbf r$, that is: :$r := \size {\mathbf r}$ Then: {{begin-eqn}} {{eqn | l = x | r = r \cos \alpha }} {{eqn | l = y | r = r \cos \beta }} {{eqn | l = z | r = r \cos \gamma }} {{end-eqn}}	1
Let $\R_{\ge 0}$ be the set of [[Definition:Positive Real Number|positive real numbers]]. Then the [[Definition:Module on Cartesian Product|$\R_{\ge 0}$-module $\R_{\ge 0}^n$]] is called the '''positive real ($n$-dimensional) vector space'''.	1
Note that by definition, $\map \DD {\mathbb J} \subseteq \R^{\mathbb J}$. Let $f, g \in \map \DD {\mathbb J}$. Let $\lambda \in \R$. From [[Linear Combination of Derivatives]], we have that: :$f + \lambda g$ is [[Definition:Differentiable Function|differentiable]] on $\mathbb J$. That is: :$f + \lambda g \in \map \DD {\mathbb J}$ So, by [[One-Step Vector Subspace Test]]: :$\struct {\map \DD {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of $\R^{\mathbb J}$. {{qed}}	1
By definition of [[Definition:Sine|sine]] and [[Definition:Cosine|cosine]]: {{begin-eqn}} {{eqn | n = 1 | l = \mathbf u_r | r = \mathbf i \cos \theta + \mathbf j \sin \theta }} {{eqn | n = 2 | l = \mathbf u_\theta | r = -\mathbf i \sin \theta + \mathbf j \cos \theta }} {{end-eqn}} where $\mathbf i$ and $\mathbf j$ are the [[Definition:Unit Vector|unit vectors]] in the [[Definition:X-Axis|$x$-axis]] and [[Definition:Y-Axis|$y$-axis]] respectively. :[[File:DerivativesOfUnitPolarVectors.png|600px]] [[Definition:Differentiation|Differentiating]] $(1)$ and $(2)$ {{WRT}} $\theta$ gives: {{begin-eqn}} {{eqn | l = \dfrac {\mathrm d \mathbf u_r} {\mathrm d \theta} | r = -\mathbf i \sin \theta + \mathbf j \cos \theta | c = }} {{eqn | r = \mathbf u_\theta | c = }} {{eqn | l = \dfrac {\mathrm d \mathbf u_\theta} {\mathrm d \theta} | r = -\mathbf i \cos \theta - \mathbf j \sin \theta | c = }} {{eqn | r = -\mathbf u_r | c = }} {{end-eqn}} {{qed}}	1
By definition, an [[Definition:Open Set in Normed Vector Space|open set]] $S \subseteq X$ is one where every [[Definition:Point|point]] inside it is an [[Definition:Element|element]] of an [[Definition:Open Ball|open ball]] contained entirely within that [[Definition:Set|set]]. That is, there are no [[Definition:Point|points]] in $S$ which have an [[Definition:Open Ball in Normed Vector Space|open ball]] some of whose [[Definition:Element|elements]] are not in $S$. As there are no [[Definition:Element|elements]] in $\O$, the result follows [[Definition:Vacuous Truth|vacuously]]. {{qed}}	1
By the definition of [[Definition:Character (Representation Theory)|character]]: :$\chi \left({g}\right) = \operatorname{Tr} \left({\rho_g}\right)$ {{explain|$\operatorname{Tr} \left({\rho_g}\right)$, and indeed $\rho_g$ itself.}} where: : $\rho \in \hom \left({\C \left[{G}\right], \operatorname{Aut} \left({V}\right)}\right): \vec {e_g} \mapsto \rho_g$ by definition. {{explain|The above definition is not stated in that form on {{ProofWiki}}. Link to $\hom \left({\C \left[{G}\right]$ and $\operatorname{Aut} \left({V}\right)}\right)$, clarify specifically what $\C \left[{G}\right]$ is, and define $\vec {e_g}$.}} Fix an arbitrary $g \in G$. Let $\left\vert{g}\right\vert$ denote the [[Definition:Order of Group Element|order]] of $g$. The [[Definition:Trace (Linear Algebra)|trace]] $\operatorname{Tr} \left({\rho_g}\right)$ of $\rho_g$ is defined as the sum of the [[Definition:Eigenvalue|eigenvalues]] of $\rho_g$. {{explain|The definition of Trace needs to be reviewed.}} From [[Eigenvalues of G-Representation are Roots of Unity]], we have that any [[Definition:Eigenvalue|eigenvalue]] $\lambda$ of $\rho_g$ is a [[Definition:Complex Roots of Unity|root of unity]] whose [[Definition:Order of Root of Unity|order]] is $\left\vert{g}\right\vert$. We have that $\lambda$ satisfies the [[Definition:Monic Polynomial|monic polynomial]] $x^{\left\vert{g}\right\vert} - 1$ Hence we have that $\lambda$ is an [[Definition:Algebraic Integer|algebraic integer]]. From [[Ring of Algebraic Integers]], we have that the sum of the [[Definition:Eigenvalue|eigenvalues]] is also an [[Definition:Algebraic Integer|algebraic integer]]. Thus $\chi \left({g}\right)$ is an [[Definition:Algebraic Integer|algebraic integer]]. {{qed}} [[Category:Module Theory]] [[Category:Group Theory]] [[Category:Complex Analysis]] ncn9awjawffn7x5umv4yc1k7pls6q23	1
The proof proceeds in two stages: :$(1): \quad$ Finding a candidate $v \in V$ where the sum might converge to :$(2): \quad$ Showing that the candidate is indeed sought limit. That $\displaystyle \sum \set {v_i: i \mathop \in I}$ [[Definition:Generalized Sum/Absolute Net Convergence|converges absolutely]] means that $\displaystyle \sum \set {\norm {v_i}: i \mathop \in I}$ [[Definition:Generalized Sum|converges]]. Now, for all $n \in \N$, let $F_n \subseteq I$ be [[Definition:Finite Set|finite]] such that: :$\displaystyle \sum_{i \mathop \in G} \norm {v_i} > \sum \set {\norm {v_i}: i \mathop \in I} - 2^{-n}$ for all [[Definition:Finite Set|finite]] $G$ with $F_n \subseteq G \subseteq I$ It may be arranged that $n \ge m \implies F_m \subseteq F_n$ by passing over to $\displaystyle F'_n = \bigcup_{m \mathop = 1}^n F_m$ if necessary. Define: : $\displaystyle v_n = \sum_{i \mathop \in F_n} v_i$ Next, it is to be shown that the [[Definition:Sequence|sequence]] $\sequence {v_n}_{n \mathop \in \N}$ is [[Definition:Cauchy Sequence|Cauchy]]. So let $\epsilon > 0$, and let $N \in \N$ be such that $2^{-N} < \epsilon$. Then for $m \ge n \ge N$, have: {{begin-eqn}} {{eqn | l = \map d {v_m, v_n} | r = \norm {\paren {\sum_{i \mathop \in F_m} v_i} - \paren {\sum_{i \mathop \in F_n} v_i} } | c = {{Defof|Metric Induced by Norm}} }} {{eqn | r = \norm {\sum_{i \mathop \in F_m \setminus F_n} v_i} | c = $F_n \subseteq F_m$ }} {{eqn | o = \le | r = \sum_{i \mathop \in F_m \setminus F_n} \norm {v_i} | c = [[Definition:Norm on Vector Space|Triangle Inequality]] for $\norm {\, \cdot \,}$ }} {{end-eqn}} Now to estimate this last quantity, observe: {{begin-eqn}} {{eqn | l = \sum \set {\norm {v_i}: i \mathop \in I} - 2^{-n} + \sum_{i \mathop \in F_m \setminus F_n} \norm {v_i} | o = < | r = \sum_{i \mathop \in F_n} \norm {v_i} + \sum_{i \mathop \in F_m \setminus F_n} \norm {v_i} | c = Defining property of $F_n$ }} {{eqn | r = \sum_{i \mathop \in F_m} \norm {v_i} | c = [[Union with Relative Complement]], $F_n \subseteq F_m$ }} {{eqn | o = \le | r = \sum \set {\norm {v_i}: i \mathop \in I} | c = [[Generalized Sum is Monotone]] }} {{eqn | ll= \leadsto | l = \sum_{i \mathop \in F_m \setminus F_n} \norm {v_i} | o = < | r = 2^{-n} }} {{end-eqn}} Finally, by the defining property of $N$, as $n \ge N: :2^{-n} < 2^{-N} < \epsilon$ Combining all of these estimates leads to the conclusion that: :$\map d {v_m, v_n} < \epsilon$ It follows that $\sequence {v_n}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]]. As $V$ is a [[Definition:Banach Space|Banach space]]: :$\displaystyle \exists v \in V: \lim_{n \mathop \to \infty} v_n = v$ Having identified a candidate $v$ for the sum $\displaystyle \sum \set {v_i: i \in I}$ to [[Definition:Generalized Sum|converge]] to, it remains to verify that this is indeed the case. According to the [[Definition:Generalized Sum|definition]] of considered sum, the convergence is [[Definition:Convergent Net|convergence of a net]]. Next, [[Metric Induces Topology]] ensures that we can limit the choice of [[Definition:Open Set (Topology)|opens]] $U$ containing $v$ to [[Definition:Open Ball|open balls]] centered at $v$. Now let $\epsilon > 0$. We want to find a [[Definition:Finite Set|finite]] $F \subseteq I$ such that: :$\map d {\displaystyle \sum_{i \mathop \in G} v_i, v} < \epsilon$ for all [[Definition:Finite Set|finite]] $G$ with $F \subseteq G \subseteq I$. Now let $N \in \N$ such that: :$\forall n \ge N: \map d {v_n, v} < \dfrac \epsilon 2$ with the $v_n$ as above. By taking a larger $N$ if necessary, ensure that $2^{-N} < \dfrac \epsilon 2$ holds as well. Let us verify that the set $F_N$ defined above has sought properties. So let $G$ be [[Definition:Finite Set|finite]] with $F_N \subseteq G \subseteq I$. Then: {{begin-eqn}} {{eqn | l = \map d {\sum_{i \mathop \in G} v_i, v} | r = \norm {\paren {\sum_{i \mathop \in G} v_i} - v} | c = {{Defof|Metric Induced by Norm}} }} {{eqn | o = \le | r = \norm {\paren {\sum_{i \mathop \in G} v_i} - \paren {\sum_{i \mathop \in F_N} v_i} } + \norm {\paren {\sum_{i \mathop \in F_N} v_i} - v} | c = [[Definition:Norm on Vector Space|Triangle inequality]] for $\norm {\, \cdot \,}$ }} {{eqn | o = < | r = \norm {\sum_{i \mathop \in G \setminus F_N} v_i} + \frac \epsilon 2 | c = $F_N \subseteq G$, defining property of $N$ }} {{eqn | o = \le | r = \sum_{i \mathop \in G \setminus F_N} \norm {v_i} + \frac \epsilon 2 | c = [[Definition:Norm on Vector Space|Triangle inequality]] for $\norm {\, \cdot \,}$ }} {{end-eqn}} For the first of these terms, observe: {{begin-eqn}} {{eqn | l = \sum \set {\norm {v_i}: i \in I} - 2^{-N} + \sum_{i \mathop \in G \setminus F_N} \norm {v_i} | o = < | r = \sum_{i \mathop \in F_N} \norm {v_i} + \sum_{i \mathop \in G \setminus F_N} \norm {v_i} | c = defining property of $F_N$ }} {{eqn | r = \sum_{i \mathop \in G} \norm {v_i} | c = [[Union with Relative Complement]], $F_N \subseteq G$ }} {{eqn | o = \le | r = \sum \set {\norm {v_i}: i \in I} | c = [[Generalized Sum is Monotone]] }} {{eqn | ll= \leadsto | l = \sum_{i \mathop \in G \setminus F_N} \norm {v_i} | o = < | r = 2^{-N} }} {{end-eqn}} Using that $2^{-N} < \dfrac \epsilon 2$, combine these inequalities to obtain: :$\displaystyle \map d {\sum_{i \mathop \in G} v_i, v} < \frac \epsilon 2 + \frac \epsilon 2 = \epsilon$ By definition of [[Definition:Convergent Net|convergence of a net]], it follows that: :$\displaystyle \sum \set {v_i: i \in I} = v$ {{qed}}	1
Let $\tuple {\C^n, +, \cdot}_\C$ be a [[Definition:Complex Vector Space|complex vector space]]. Let $S \subseteq \C^n$. Then $S$ is a '''linearly independent set of complex vectors''' if every [[Definition:Finite Sequence|finite]] [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. That is, such that: :$\displaystyle \forall \set {\lambda_k: 1 \le k \le n} \subseteq \C: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0 \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$ where $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ are distinct elements of $S$.	1
Let $\mathbf T_n$ be an [[Definition:Upper Triangular Matrix|upper triangular matrix]] of [[Definition:Order of Square Matrix|order $n$]]. We proceed by [[Principle of Mathematical Induction|induction]] on $n$, the number of [[Definition:Row of Matrix|rows]] of $\mathbf T_n$. === Basis for the Induction === For $n = 1$, the [[Definition:Determinant of Matrix|determinant]] is $a_{11}$, which is clearly also the [[Definition:Diagonal Element|diagonal element]]. This forms the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Fix $n \in \N$. Then, let: :$\mathbf T_n = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ 0 & a_{2 2} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_{n n} \\ \end {bmatrix}$ be an [[Definition:Upper Triangular Matrix|upper triangular matrix]]. Assume that: :$\displaystyle \map \det {\mathbf T_n} = \prod_{k \mathop = 1}^n a_{k k}$ This forms our [[Definition:Induction Hypothesis|induction hypothesis]]. === Induction Step === Let $\mathbf T_{n + 1}$ be an [[Definition:Upper Triangular Matrix|upper triangular matrix]] of [[Definition:Order of Square Matrix|order $n + 1$]]. Then, by the [[Expansion Theorem for Determinants]] (expanding across the $n + 1$th [[Definition:Row of Matrix|row]]): :$\displaystyle D = \map \det {\mathbf T_{n + 1} } = \sum_{k \mathop = 1}^{n + 1} a_{n + 1, k} T_{n + 1, k}$ Because $\mathbf T_{n + 1}$ is [[Definition:Upper Triangular Matrix|upper triangular]], $a_{n + 1, k} = 0$ when $k < n + 1$. Therefore: :$\map \det {\mathbf T_{n + 1} } = a_{n + 1 \, n + 1} T_{n + 1, n + 1}$ By the definition of the [[Definition:Cofactor of Element|cofactor]]: :$T_{n + 1, n + 1} = \paren {-1}^{n + 1 + n + 1} D_{n + 1, n + 1} = D_{n n}$ where $D_{n n}$ is the [[Definition:Order of Determinant|order $n$]] [[Definition:Determinant of Matrix|determinant]] obtained from $D$ by deleting [[Definition:Row of Matrix|row]] $n + 1$ and [[Definition:Column of Matrix|column]] $n + 1$. But $D_{n n}$ is just the [[Definition:Determinant of Matrix|determinant]] of an [[Definition:Upper Triangular Matrix|upper triangular matrix]] $\mathbf T_n$. Therefore: :$\map \det {\mathbf T_{n + 1} } = a_{n + 1, n + 1} \map \det {\mathbf T_n}$ and the result follows by [[Principle of Mathematical Induction|induction]]. {{qed}}	1
For $a + b \sqrt 2$ to be a [[Definition:Unit of Ring|unit]] of $\struct {\Z \sqbrk {\sqrt 2}, +, \times}$, we require that: :$\exists c, d \in \Z: \paren {a + b \sqrt 2} \paren {c + d \sqrt 2} = 1$ In [[Numbers of Type Integer a plus b root 2 are Not a Field]] it is shown that the [[Definition:Product Inverse|product inverse]] of $\paren {a + b \sqrt 2}$ is $\dfrac a {a^2 - 2 b^2} + \dfrac {b \sqrt 2} {a^2 - 2 b^2}$. So if $a^2 - 2 b^2 = \pm 1$ it follows that $c$ and $d$ are [[Definition:Integer|integers]]. Hence the result. {{qed}}	1
Let $S$ be a non-zero [[Definition:Vector Subspace|subspace]] of $\left({\R^2, +, \times}\right)_\R$. Then $S$ contains a [[Definition:Zero Vector|non-zero vector]] $\left({\alpha_1, \alpha_2}\right)$. Hence $S$ also contains $\left\{{\lambda \times \left({\alpha_1, \alpha_2}\right), \lambda \in \R}\right\}$. From [[Equation of Straight Line in Plane]], this set may be described as a line through the origin. Suppose $S$ also contains a [[Definition:Zero Vector|non-zero vector]] $\left({\beta_1, \beta_2}\right)$ which is not on that line. Then $\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1 \ne 0$. Otherwise $\left({\beta_1, \beta_2}\right)$ would be $\zeta \times \left({\alpha_1, \alpha_2}\right)$, where either $\zeta = \beta_1 / \alpha_1$ or $\zeta = \beta_2 / \alpha_2$ according to whether $\alpha_1 \ne 0$ or $\alpha_2 \ne 0$. But then $S = \left({\R^2, +, \times}\right)_\R$. Because, if $\left({\gamma_1, \gamma_2}\right)$ is any vector at all, then: : $\left({\gamma_1, \gamma_2}\right) = \lambda \times \left({\alpha_1, \alpha_2}\right) + \mu \times \left({\beta_1, \beta_2}\right)$ where $\lambda = \dfrac {\gamma_1 \times \beta_2 - \gamma_2 \times \beta_1} {\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1}, \mu = \dfrac {\alpha_1 \times \gamma_2 - \alpha_2 \times \gamma_1} {\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1}$ which we get by solving the simultaneous eqns: {{begin-eqn}} {{eqn | l=\alpha_1 \times \lambda + \beta_1 \times \mu | r=0 | c= }} {{eqn | l=\alpha_2 \times \lambda + \beta_2 \times \mu | r=0 | c= }} {{end-eqn}} The result follows. {{qed}}	1
From [[Matrix Product as Linear Transformation]], $\mathbf {Ax} = \mathbf 0$ defines a [[Definition:Linear Transformation on Vector Space|linear transformation]] from $\R^m$ to $\R^n$. The result then follows from [[Linear Transformation Maps Zero Vector to Zero Vector]].	1
{{begin-eqn}} {{eqn | l = \mathbf v + \paren {-1_F \circ \mathbf v} | r = \paren {1_F \circ \mathbf v} + \paren {-1_F \circ \mathbf v} | c = {{Field-axiom|M3}} }} {{eqn | r = \paren {1_F + \paren {- 1_F} } \circ \mathbf v | c = {{Vector-space-axiom|5}} }} {{eqn | r = 0_F \circ \mathbf v | c = {{Field-axiom|A4}} }} {{eqn | r = \mathbf 0 | c = [[Vector Scaled by Zero is Zero Vector]] }} {{end-eqn}} so $-1_F \circ \mathbf v$ is an [[Definition:Inverse Element|additive inverse]] of $\mathbf v$. From [[Additive Inverse in Vector Space is Unique]]: :$-1_F \circ \mathbf v = -\mathbf v$ {{qed}}	1
Let $e$ be an [[Definition:Elementary Column Operation|elementary column operation]]. Let $\mathbf E$ be the [[Definition:Elementary Column Matrix|elementary column matrix]] of [[Definition:Order of Square Matrix|order]] $n$ defined as: :$\mathbf E = e \paren {\mathbf I}$ where $\mathbf I$ is the [[Definition:Unit Matrix|unit matrix]]. Then for every $m \times n$ [[Definition:Matrix|matrix]] $\mathbf A$: :$e \paren {\mathbf A} = \mathbf A \mathbf E$ where $\mathbf A \mathbf E$ denotes the [[Definition:Matrix Product (Conventional)|conventional matrix product]].	1
The [[Definition:Integer|integers]] $\struct {\Z, +, \times}$ form a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] under [[Definition:Integer Addition|addition]] and [[Definition:Integer Multiplication|multiplication]].	1
{{begin-eqn}} {{eqn | l = \map {\theta_{c, d} \circ \theta_{a, b} } x | r = \map {\theta_{a, b} \circ \theta_{c, d} } x | c = }} {{eqn | ll= \leadsto | l = \theta_{a c, b c + d} | r = \theta_{c a, a d + b} | c = }} {{eqn | ll= \leadsto | l = b c + d | r = a d + b | c = }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Then $R$ is the direct product of two [[Definition:Non-Trivial Ring|non-trivial rings]] {{iff}} $R$ contains an [[Definition:Idempotent Element|idempotent element]] not equal to $0_R$ or $1_R$.	1
Let $\norm {\, \cdot \,}$ be a [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Let $a, b \in \Z_{\ne 0}$ be [[Definition:Coprime|coprime]], $a \perp b$ Then: :$\norm a = 1$ or $\norm b = 1$	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $m,n\geq1$ be [[Definition:Positive Integer|positive integers]]. Let $i, j \in \left\{ {1, \ldots, m}\right\} \times \{ 1, \ldots, n\}$. The '''standard matrix basis''' of $m\times n$ [[Definition:Matrix|matrices]] over $R$ is the [[Definition:Ordered Basis|ordered basis]] of [[Definition:Standard Basis Matrix|standard basis matrices]] ordered by the [[Definition:Colexicographic Order|colexicographic order]] on $\left\{ {1, \ldots, m}\right\} \times \{ 1, \ldots, n\}$.	1
[[Definition:Space of Continuous Functions of Differentiability Class k|Space of Continuously Differentiable on Closed Interval Real-Valued Functions]] with [[Definition:C^k Norm|$C^1$ norm]] forms a [[Definition:Normed Vector Space|normed vector space]].	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $x \in D$ be a [[Definition:Proper Element of Ring|proper element]] of $D$. Let there be two [[Definition:Complete Factorization|complete factorizations]] of $x$: :$x = u_y \circ y_1 \circ y_2 \circ \cdots \circ y_m = F_1$ :$x = u_z \circ z_1 \circ z_2 \circ \cdots \circ z_n = F_2$ Then $F_1$ and $F_2$ are [[Definition:Equivalent Factorizations|equivalent]].	1
For $\struct {S, +, \times}$ to be a [[Definition:Ring (Abstract Algebra)|ring]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is a [[Definition:Semigroup|semigroup]]. For $\struct {S, \times}$ to be a [[Definition:Semigroup|semigroup]], it is a [[Definition:Necessary Condition|necessary condition]] that $\times$ is [[Definition:Associative Operation|associative]] on $S$. However, from [[Vector Cross Product is not Associative]], this is not the case here. The result follows. {{qed}}	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathbf A$ be a [[Definition:Matrix|matrix]] over $R$ of [[Definition:Order of Matrix|order]] $m \times n$ Let $\mathbf 0$ be a [[Definition:Zero Matrix|zero matrix]] whose [[Definition:Order of Matrix|order]] is such that either: :$\mathbf 0 \mathbf A$ is defined or: :$\mathbf A \mathbf 0$ is defined or both. Then: :$\mathbf 0 \mathbf A = \mathbf 0$ or: :$\mathbf A \mathbf 0 = \mathbf 0$ whenever they are defined. The [[Definition:Order of Matrix|order]] of $\mathbf 0$ will be according to the [[Definition:Order of Matrix|orders]] of the factor [[Definition:Matrix|matrices]].	1
Let $\struct {X, \norm {\, \cdot \,}_X}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $Y \subseteq X$ be a [[Definition:Vector Subspace|vector subspace]]. Let $\norm {\, \cdot \,}_Y$ be the [[Definition:Induced Norm|induced norm]] on $Y$. Then $\struct {Y, \norm {\, \cdot \,}_Y}$ is a [[Definition:Normed Vector Space|normed vector space]].	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $K$. Let $A \subseteq V$ be a [[Definition:Subset|subset]] of $V$. Then the '''linear span''' of $A$, denoted $\operatorname{span} A$ or $\map {\operatorname{span} } A$, is the [[Definition:Set|set]]: :$\displaystyle \set {\sum_{k \mathop = 1}^n \alpha_k f_k: n \in \N_{\ge 1}, \alpha_i \in K, f_i \in A}$ The '''linear span''' can be interpreted as the [[Definition:Set|set]] of all [[Definition:Linear Combination|linear combinations]] (of [[Definition:Finite|finite]] [[Definition:Length of Sequence|length]]) of these [[Definition:Vector (Linear Algebra)|vectors]]. === Definition for $\R^n$ === In $\R^n$ (where $n \in \N_{>0}$), above definition translates to: :$\displaystyle \map {\operatorname{span} } {\mathbf v_1, \mathbf v_2, \dotsc, \mathbf v_k} = \set {\sum_{i \mathop = 1}^k \ c_i \ \mathbf v_i: c_i \in \R, \mathbf v_i \in \R^n, 1 \le i \le k}$	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\lambda$ be a [[Definition:Scalar (Matrix Theory)|scalar]]. Let $\lambda \mathbf A$ denote the [[Definition:Matrix Scalar Product|scalar product]] of $\mathbf A$ by $\lambda$. Then: :$\map \det {\lambda \mathbf A} = \lambda^n \map \det {\mathbf A}$ where $\det$ denotes [[Definition:Determinant of Matrix|determinant]].	1
Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]]: :$e := \kappa_k \leftrightarrow \kappa_l$ Thus we have: {{begin-eqn}} {{eqn | l = \kappa'_k | r = \kappa_l | c = }} {{eqn | lo= \text {and} | l = \kappa'_l | r = \kappa_k | c = }} {{end-eqn}} Now let $\map {e'} {\mathbf A'}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A'$ to $\mathbf A''$: :$e' := \kappa'_k \leftrightarrow \kappa'_l$ Applying $e'$ to $\mathbf A'$ we get: {{begin-eqn}} {{eqn | l = \kappa''_k | r = \kappa'_l | c = }} {{eqn | lo= \text {and} | l = \kappa''_l | r = \kappa'_k | c = }} {{eqn | ll= \leadsto | l = \kappa''_k | r = \kappa_k | c = }} {{eqn | lo= \text {and} | l = \kappa''_l | r = \kappa_l | c = }} {{eqn | ll= \leadsto | l = \mathbf A'' | r = \mathbf A | c = }} {{end-eqn}} It is noted that for $e'$ to be an [[Definition:Elementary Column Operation|elementary column operation]], the only possibility is for it to be as defined.	1
The [[Rational Numbers form Metric Space|rational numbers $\Q$ form a metric space]]. We have that the [[Rationals are Everywhere Dense in Topological Space of Reals]]. We also have that the [[Rational Numbers are Countably Infinite]]. The result follows from the definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
From [[Odd-Even Topology is Second-Countable]], $T$ is [[Definition:Second-Countable Space|second-countable]]. The result follows from [[Second-Countable Space is Separable]]. {{qed}}	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime number]] $p$. Let $\size {\,\cdot\,}$ be the [[Definition:Absolute Value|absolute value]] on the [[Definition:Rational Numbers|rationals $\Q$]]. Then $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. That is, the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_p$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\size {\,\cdot\,}$.	1
Let the [[Definition:Field of Real Numbers|field of real numbers]] be denoted $\struct {\R, +, \times}$. From [[Real Vector Space is Vector Space]], we have that $\struct {\R^n, +, \cdot}$ is a [[Definition:Vector Space|vector space]], where: :$\mathbf a + \mathbf b = \tuple {a_1 + b_1, a_2 + b_2, \ldots, a_n + b_n}$ :$\lambda \cdot \mathbf a = \tuple {\lambda \times a_1, \lambda \times a_2, \ldots, \lambda \times a_n}$ where: :$\mathbf a, \mathbf b \in \R^n$ :$\lambda \in \R$ :$\mathbf a = \tuple {a_1, a_2, \ldots, a_n}$ :$\mathbf b = \tuple {b_1, b_2, \ldots, b_n}$ When $n = 1$, the [[Definition:Vector Space|vector space]] degenerates to: :$\mathbf a + \mathbf b = \tuple {a + b}$ :$\lambda \cdot \mathbf a = \tuple {\lambda \times a}$ where: :$\mathbf a, \mathbf b \in \R$ :$\lambda \in \R$ :$\mathbf a = \tuple a$ :$\mathbf b = \tuple b$ Thus it can be seen that the [[Definition:Vector Space|vector space]] $\struct {\R^1, +, \cdot}$ is identical with the [[Definition:Field of Real Numbers|field of real numbers]] denoted by $\struct {\R, +, \times}$. {{qed}}	1
Let $\mathbf I_n$ be the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Then: :$\map \tr {\mathbf I_n} = n$ where $\map \tr {\mathbf I_n}$ denotes the [[Definition:Trace of Matrix|trace]] of $\mathbf I_n$.	1
{{ProofWanted}} [[Category:Bilinear Forms]] 37u4cgm6e1cp6xndr1gf0majk4m78t3	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A2}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Commutative Operation|commutative]]. The result then follows directly from [[Commutativity of Hadamard Product]]. {{qed}}	1
By [[Cauchy Sequence Is Eventually Bounded Away From Non-Limit]] then: :$\exists N_1 \in \N$ and $C \in \R_{\gt 0}: \forall n \ge N_1: \norm {x_n} \gt C$ Since $\sequence {x_n}$ is a [[Definition:Cauchy Sequence inNormed Division Ring|Cauchy sequence]] then: :$\exists N_2 \in \N: \forall n, m \ge N_2: \norm {x_n - x_m} < C$ Let $N = \max \set {N_1, N_2}$. Let $n, m \ge N$. Then: :$\norm {x_n - x_m} < C < \norm {x_n}$ By [[Three Points in Ultrametric Space have Two Equal Distances/Corollary 4|Corollary to Three Points in Ultrametric Space have Two Equal Distances]] then: :$\norm {x_n} = \norm {x_m}$ The result follows. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$ over an [[Definition:Algebraic Structure|algebraic structure]] $\struct {R, +, \circ}$. Let $\mathbf A, \mathbf B \in \map {\MM_R} {m, n}$. Let $\mathbf A + \mathbf B$ be defined as the [[Definition:Matrix Entrywise Addition|matrix entrywise sum]] of $\mathbf A$ and $\mathbf B$. The operation of [[Definition:Matrix Entrywise Addition over Ring|matrix entrywise addition]] satisfies the following properties: :$+$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\map {\MM_R} {m, n}$ :$+$ is [[Definition:Associative|associative]] on $\map {\MM_R} {m, n}$ :$+$ is [[Definition:Commutative Operation|commutative]] on $\map {\MM_R} {m, n}$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $T \in B_0 \left({H}\right)$ be a [[Definition:Compact Linear Operator|compact]], [[Definition:Self-Adjoint Operator|self-adjoint operator]]. Then there exists a (possibly [[Definition:Finite Sequence|finite]]) [[Definition:Sequence|sequence]] $\left({\lambda_n}\right)$ of distinct nonzero [[Definition:Eigenvalue|eigenvalues]] of $T$ such that: * $P_n P_m = P_m P_n = 0$ if $n \ne m$ * $\displaystyle \lim_{k \to \infty} \left\Vert{T - \sum_{n=1}^k \lambda_n P_n}\right\Vert = 0$, that is, $T = \displaystyle \sum_{n=1}^\infty \lambda_n P_n$ where $P_n$ is the [[Definition:Orthogonal Projection|orthogonal projection]] onto the [[Definition:Eigenspace|eigenspace]] of $\lambda_n$, and $\left\Vert{\cdot}\right\Vert$ denotes the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]]. {{refactor|Split corollaries to subpages}} === Corollary 1 === There exists a (possibly [[Definition:Finite Sequence|finite]]) [[Definition:Sequence|sequence]] $\left({\mu_n}\right)$ of [[Definition:Real Number|real numbers]] and a [[Definition:Basis (Hilbert Space)|basis]] $E = \left({e_n}\right)$ for $\left({\operatorname{ker} T}\right)^\perp$ such that: :$\forall h \in H: Th = \displaystyle \sum_{n=1}^\infty \left\langle{h, e_n}\right\rangle_H \mu_n e_n$	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime]] $p > 3$. Then: :$\struct {\Q, \norm {\,\cdot\,}_p}$ is not a [[Definition:Complete Normed Division Ring|complete normed division ring]]. That is, there exists a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {\Q, \norm{\,\cdot\,}_p}$ which does not [[Definition:Convergent Sequence (Normed Division Ring)|converge]] to a [[Definition:Limit of Sequence (Normed Division Ring)|limit]] in $\Q$.	1
Let $\mathbf A = \sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf 0_R | r = \sqbrk a_{m n} + \sqbrk {0_R}_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {a + 0_R}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf A + \mathbf 0_R | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \mathbf 0_R + \mathbf A | r = \sqbrk {0_R}_{m n} + \sqbrk a_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {0_R + a}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf 0_R + \mathbf A | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} {{qed}}	1
'''Matrix algebra''' is the subfield of [[Definition:Algebra (Mathematical Branch)|algebra]] whose subjects of manipulation are [[Definition:Matrix|matrices]].	1
:$\paren {n \cdot 1_R} \circ x = n \cdot x$ that is: :$\paren {\map {\paren {+_R}^n} {1_R} } \circ x = \map {\paren {+_G}^n} x$	1
A [[Definition:Vector Space|vector space]] is a [[Definition:Module|module]], so all results about modules also apply to vector spaces. So from [[Scalar Product with Identity]] it follows directly that: :$\lambda = 0_F \lor \mathbf v = e \implies \lambda \circ \mathbf v = \bszero$ Next, suppose $\lambda \circ \mathbf v = \bszero$ but $\lambda \ne 0_F$. Then: {{begin-eqn}} {{eqn | l = \bszero | r = \lambda^{-1} \circ \bszero | c = [[Zero Vector Scaled is Zero Vector]] }} {{eqn | r = \lambda^{-1} \circ \paren {\lambda \circ \mathbf v} | c = as $\lambda \circ \mathbf v = \bszero$ }} {{eqn | r = \paren {\lambda^{-1} \circ \lambda} \circ \mathbf v | c = {{Vector-space-axiom|7}} }} {{eqn | r = 1 \circ \mathbf v | c = {{Field-axiom|M4}} }} {{eqn | r = \mathbf v | c = {{Vector-space-axiom|8}} }} {{end-eqn}} {{Qed}}	1
{{begin-eqn}} {{eqn | l = \frac a c \circ \frac b d | r = a \circ c^{-1} \circ b \circ d^{-1} | c = {{Defof|Division Product}} }} {{eqn | r = \paren {a \circ b} \circ \paren {d^{-1} \circ c^{-1} } | c = {{Defof|Commutative Operation}} }} {{eqn | r = \paren {a \circ b} \circ \paren {c \circ d}^{-1} | c = [[Inverse of Product]] }} {{eqn | r = \frac {a \circ b} {c \circ d} | c = {{Defof|Division Product}} }} {{end-eqn}} {{qed}}	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n$]]. We proceed by induction on $n$, the number of [[Definition:Row of Matrix|rows]] of $\mathbf A$. === Basis for the Induction === For $n = 1$, we have a matrix of just one [[Definition:Element of Matrix|element]], which is trivially [[Definition:Diagonal Matrix|diagonal]], hence both upper and lower triangular. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Fix $n \in \N$, and assume all $n \times n$-matrices can be upper triangularised by [[Definition:Elementary Row Operation|elementary row operations]]. If $R$ is a [[Definition:Field (Abstract Algebra)|field]], assume all $n \times n$-matrices can be upper triangularised by elementary row operations of type 2. This forms our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]. === Induction Step === Let $\mathbf A = \left[{a}\right]_{n+1}$ be a [[Definition:Square Matrix|square matrix]] of order $n + 1$. When the first [[Definition:Column of Matrix|column]] of $\mathbf A$ contains only zeroes, it is upper triangularisable [[Definition:Iff|iff]] the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ is. Each [[Definition:Elementary Row Operation|elementary row operation]] used in triangularisation process of the submatrix $\mathbf A \left({1; 1}\right)$ will not change the zeros of the first column of $\mathbf A$. So when $\mathbf A \left({1; 1}\right)$ is upper triangularised, then $A$ will also be upper triangularised. From the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]], we conclude that $\mathbf A$ can be upper triangularised by elementary row operations. Now suppose that its first column contains a non-zero value. Suppose that $a_{11} = 0$. Let $j$ be the smallest row index such that $a_{j1} \ne 0$, and note that $j$ exists by assumption. Now apply the following operation of type 2: :$r_1 \to r_1 + r_j$ As $a_{j1} \ne 0$, this enforces $a_{11} \ne 0$, and we continue as in the case below. Suppose $a_{11} \ne 0$. We use the following operations for all $j \in \left\{{2, \ldots, n + 1}\right\}$: :$(1): \quad$ Put $c = a_{j1}$. :$(2): \quad r_j \to a_{11} r_j$ :$(3): \quad r_j \to r_j - c r_1$ This will put the first column to zero (except for the first element, $a_{11}$). It follows that $\mathbf A$ can be upper triangularised precisely when the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ can. Again, the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]] renders $\mathbf A$ upper triangularisable by [[Definition:Elementary Row Operation|elementary row operations]]. This completes the case distinction, and hence the result follows by [[Principle of Mathematical Induction|induction]]. To put the matrix $\mathbf A$ into [[Definition:Lower Triangular Matrix|lower triangular form]], just do the same thing, but start with the last column and the last diagonal element $a_{n + 1 \; n + 1}$. {{qed}}	1
From [[Hilbert Matrix is Cauchy Matrix]], $H_n$ is a special case of a [[Definition:Cauchy Matrix|Cauchy matrix]]: :$\begin{bmatrix} c_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac 1 {x_i + y_j} \end{bmatrix}$ where: :$x_i = i$ :$y_j = j - 1$ Then: {{begin-eqn}} {{eqn | l = \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} | r = \sum_{k \mathop = 1}^n x_k + \sum_{k \mathop = 1}^n y_k | c = [[Sum of Elements in Inverse of Cauchy Matrix]] }} {{eqn | r = \sum_{k \mathop = 1}^n k + \sum_{k \mathop = 1}^n \paren {k - 1} | c = }} {{eqn | r = 2 \sum_{k \mathop = 1}^n k - n | c = }} {{eqn | r = n \paren {n + 1} - n | c = [[Closed Form for Triangular Numbers]] }} {{eqn | r = n^2 | c = }} {{end-eqn}} {{qed}}	1
To show that $\OO$ is a [[Definition:Subring|subring]] the [[Subring Test]] is used by showing: :$(1): \quad \OO \ne \O$ :$(2): \quad \forall x, y \in \OO: x + \paren {-y} \in \OO$ :$(3): \quad \forall x, y \in \OO: x y \in \OO$ '''(1)''' By [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity]], :$\norm{1_R} = 1$ Hence: :$1_R \in \OO \ne \O$ {{qed|lemma}} '''(2)''' Let $x, y \in \OO$. Then: {{begin-eqn}} {{eqn | l = \norm {x + \paren{-y} } | o = \le | r = \max \set {\norm x, \norm{-y} } | c = {{NormAxiom|4}} }} {{eqn | r = \max \set {\norm x, \norm y} | c = [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] }} {{eqn | o = \le | r = 1 | c = Since $x, y \in \OO$ }} {{end-eqn}} Hence: :$x + \paren {-y} \in \OO$ {{qed|lemma}} '''(3)''' Let $x, y \in \OO$. Then: {{begin-eqn}} {{eqn | l = \norm{x y} | o = \le | r = \norm x \norm y | c = {{NormAxiom|2}} }} {{eqn | o = \le | r = 1 | c = Since $x, y \in \OO$ }} {{end-eqn}} Hence: :$x y \in \OO$ {{qed|lemma}} By [[Subring Test]] it follows that $\OO$ is a [[Definition:Subring|subring]] of $R$. Since $1_R \in S$ and $1_R$ is the [[Definition:Unity of Ring|unity]] of $R$ then $1_R$ is the [[Definition:Unity of Ring|unity]] of $\OO$. By [[Division Ring has No Proper Zero Divisors]] then $R$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. Hence $\OO$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. {{qed}}	1
Let $T = \struct {S, \tau}$ be the [[Definition:Arens-Fort Space|Arens-Fort space]]. Then $T$ is a [[Definition:Separable Space|separable space]].	1
This proof assumes that $R$ is a [[Definition:Field (Abstract Algebra)|field]], which makes the triangulation process slightly quicker. By this assumptions, all [[Definition:Element of Matrix|elements]] of $\mathbf A$ have [[Definition:Inverse Element|multiplicative inverses]]. Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n$]]. We proceed by induction on $n$, the number of [[Definition:Row of Matrix|rows]] of $\mathbf A$. === Basis for the Induction === For $n = 1$, we have a matrix of just one [[Definition:Element of Matrix|element]], which is trivially [[Definition:Diagonal Matrix|diagonal]], hence both upper and lower triangular. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Fix $n \in \N$, and assume all $n \times n$-matrices can be upper triangularised by [[Definition:Elementary Row Operation|elementary row operations]]. If $R$ is a [[Definition:Field (Abstract Algebra)|field]], assume all $n \times n$-matrices can be upper triangularised by elementary row operations of type 2. This forms our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]. === Induction Step === Let $\mathbf A = \left[{a}\right]_{n + 1}$ be a [[Definition:Square Matrix|square matrix]] of order $n + 1$. When the first [[Definition:Column of Matrix|column]] of $\mathbf A$ contains only zeroes, it is upper triangularisable {{iff}} the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ is. Each [[Definition:Elementary Row Operation|elementary row operation]] used in triangularisation process of the submatrix $\mathbf A \left({1; 1}\right)$ will not change the zeros of the first column of $\mathbf A$. So when $\mathbf A \left({1; 1}\right)$ is upper triangularised, then $\mathbf A$ will also be upper triangularised. From the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]], we conclude that $\mathbf A$ can be upper triangularised by elementary row operations. Now suppose that its first column contains a non-zero value. Suppose that $a_{11} = 0$. Let $j$ be the smallest row index such that $a_{j1} \ne 0$, and note that $j$ exists by assumption. Now apply the following operation of type 2: :$r_1 \to r_1 + r_j$ As $a_{j1} \ne 0$, this enforces $a_{11} \ne 0$, and we continue as in the case below. Suppose $a_{11} \ne 0$. We use the following operations of type 2: :$\forall j \in \left\{{2, \ldots, n+1}\right\}: r_j \to r_j - \dfrac {a_{j1}} {a_{11}} r_1$ This will put the first column to zero (except for the first element, $a_{11}$). It follows that $\mathbf A$ can be upper triangularised precisely when the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ can. Again, the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]] renders $\mathbf A$ upper triangularisable by [[Definition:Elementary Row Operation|elementary row operations]]. This completes the case distinction, and hence the result follows by [[Principle of Mathematical Induction|induction]]. To put the matrix $\mathbf A$ into [[Definition:Lower Triangular Matrix|lower triangular form]], just do the same thing, but start with the last column and the last diagonal element $a_{n + 1 \; n + 1}$. {{qed}}	1
{{begin-eqn}} {{eqn | l = \sqbrk {\paren {\mathbf A^*}^* }_{i j} | r = \overline {\sqbrk {\mathbf A^*}_{j i} } | c = {{Defof|Hermitian Conjugate}} }} {{eqn | r = \overline {\paren {\overline {\sqbrk {\mathbf A}_{i j} } } } | c = {{Defof|Hermitian Conjugate}} }} {{eqn | r = \sqbrk {\mathbf A}_{i j} | c = [[Complex Conjugation is Involution]] }} {{end-eqn}} So: :$\paren {\mathbf A^*}^* = \mathbf A$ {{qed}} [[Category:Linear Algebra]] [[Category:Involutions]] 1vdgyl2fjuo3qnxxdisxrywovolmnma	1
:$y \in \map {B_r} x \implies \map {B_r} y = \map {B_r} x$	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\, \cdot \,}$. By definition of a [[Definition:Complete Normed Division Ring|complete normed division ring]], the [[Definition:Metric Space|metric space]] $\struct{R, d}$ is [[Definition:Complete Metric Space|complete]]. From [[Leigh.Samphier/Sandbox/Inclusion Mapping on Normed Division Subring is Distance Preserving Monomorphism]], $i : S \to R$ is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]]. From [[Image under Inclusion Mapping]]: :$\map {i^\to} S = S$ Thus $\map {i^\to} S$ is [[Definition:Everywhere Dense|dense]] in $\struct {R, \norm {\, \cdot \,} }$. It follows that $\struct {R, \norm {\, \cdot \,} }$ is a [[Definition:Completion (Normed Division Ring)|completion]] of $\struct {S, \norm {\, \cdot \,}}$ by definition. {{qed}} [[Category:Completion of Normed Division Ring]] aat9tvf411affe5v08pi40bw0ltscgi	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] that [[Definition:Matrix Scalar Product|multiplies]] [[Definition:Row of Matrix|rows]] $i$ by the [[Definition:Scalar (Matrix Theory)|scalar]]$c$. Let $\mathbf B = \map e {\mathbf A}$. Let $\mathbf E$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e$. From [[Elementary Row Operations as Matrix Multiplications]]: :$\mathbf B = \mathbf E \mathbf A$ From [[Determinant of Elementary Row Matrix/Exchange Rows|Determinant of Elementary Row Matrix: Exchange Rows]]: :$\map \det {\mathbf E} = c$ Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf B} | r = \map \det {\mathbf E \mathbf A} | c = [[Determinant of Matrix Product]] }} {{eqn | r = c \map \det {\mathbf A} | c = as $\map \det {\mathbf E} = c$ }} {{end-eqn}} Hence the result. {{qed}}	1
From [[Finite Direct Product of Modules is Module]] we have that $G$ is a [[Definition:Module|module]]. It remains to be shown that: :$\forall x \in G: 1_R \circ x = x$ Let $x = \tuple {x_1, x_2, \ldots, x_n} \in G$. Then: {{begin-eqn}} {{eqn | l = 1_R \circ x | r = 1_R \circ \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = \tuple {1_R \circ x_1, 1_R \circ x_2, \ldots, 1_R \circ x_n} | c = }} {{eqn | r = \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = x | c = }} {{end-eqn}} Hence the result. {{qed}}	1
=== [[Non-Archimedean Norm iff Non-Archimedean Metric/Necessary Condition|Necessary Condition]] === {{:Non-Archimedean Norm iff Non-Archimedean Metric/Necessary Condition}}{{qed|lemma}} === [[Non-Archimedean Norm iff Non-Archimedean Metric/Sufficient Condition|Sufficient Condition]] === {{:Non-Archimedean Norm iff Non-Archimedean Metric/Sufficient Condition}}{{qed}}	1
By definition of [[Definition:Inverse (Bounded Linear Operator)|inverse]], one has $AA^{-1} = I_K$, where $I_K$ is the [[Definition:Identity Operator|identity operator]] on $K$. Now observe from [[Adjoint of Composition]] that: :$I_K = I_K^* = \left({AA^{-1}}\right)^* = \left({A^{-1}}\right)^*A^*$. Similarly, one has: :$I_H = I_H^* = \left({A^{-1}A}\right)^* = A^*\left({A^{-1}}\right)^*$ Hence, by definition of [[Definition:Inverse (Bounded Linear Operator)|inverse]], $\left({A^*}\right)^{-1} = \left({A^{-1}}\right)^*$. This also means that $A^*$ is invertible. {{qed}}	1
{{AimForCont}} that $\set {1, i, j}$ forms a [[Definition:Basis (Linear Algebra)|basis]] for an [[Definition:Algebra over Field|algebra]] of $3$ [[Definition:Dimension (Linear Algebra)|dimensions]] with [[Definition:Real Number|real]] [[Definition:Scalar (Vector Space)|scalars]]. Let $1$ and $i$ have their usual properties as they do as [[Definition:Complex Number|complex numbers]]: :$\forall a: 1 a = a 1 = a$ :$i \cdot i = -1$ Then: :$i j = a_1 + a_2 i + a_3 j$ for some $a_1, a_2, a_3 \in \R$. Multiplying through by $i$: :$(1): \quad i \paren {i j} = \paren {i i} j = -j$ and: {{begin-eqn}} {{eqn | l = i \paren {a_1 + a_2 i + a_3 j} | r = a_1 i - a_2 + a_3 i j | c = }} {{eqn | r = a_1 i - a_2 + a_3 \paren {a_1 + a_2 i + a_3 j} | c = }} {{eqn | r = a_1 i - a_2 + a_1 a_3 + a_2 a_3 i + {a_3}^2 j | c = }} {{eqn | ll= \leadsto | l = 0 | r = \paren {a_1 a_3 - a_2} + \paren {a_1 + a_2 a_3} i + \paren { {a_3}^2 + 1} j | c = from $(1)$ }} {{end-eqn}} But this implies that ${a_3}^2 = -1$, which contradicts our supposition that $a_3 \in \R$. Hence the result by [[Proof by Contradiction]]. {{qed}}	1
Let $n$ be a [[Definition:Positive Integer|positive integer]]. The '''integer lattice''' in $\R^n$ is the [[Definition:Lattice (Group)|lattice]] $\Z^n$. [[Category:Definitions/Linear Algebra]] [[Category:Definitions/Geometry of Numbers]] 01q0zkrsaqbk9hph0uhv482i84e1ows	1
This can be seen to be a special case of [[Minkowski's Inequality]], with $n = 1$. {{qed}}	1
Let $\struct {A_F, \oplus}$ be a [[Definition:Normed Division Algebra|normed division algebra]]. Let the [[Definition:Unit of Algebra|unit]] of $\struct {A_F, \oplus}$ be $1_A$. Then: :$\norm {1_A} = 1$ where $\norm {1_A}$ denotes the [[Definition:Norm on Vector Space|norm]] of $1_A$.	1
Take the version of the [[Definition:Cauchy Matrix|Cauchy matrix]] defined such that $a_{ij} = \dfrac 1 {x_i + y_j}$. Subtract [[Definition:Column of Matrix|column]] $1$ from each of [[Definition:Column of Matrix|column]]s $2$ to $n$. Thus: {{begin-eqn}} {{eqn | l = a_{ij} | o = \gets | r = \frac 1 {x_i + y_j} - \frac 1 {x_i + y_1} | c = }} {{eqn | r = \frac {\left({x_i + y_1}\right) - \left({x_i + y_j}\right)} {\left({x_i + y_j}\right) \left({x_i + y_1}\right)} | c = }} {{eqn | r = \left({\frac {y_1 - y_j}{x_i + y_1} }\right) \left({\frac 1 {x_i + y_j} }\right) | c = }} {{end-eqn}} From [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the [[Definition:Determinant of Matrix|determinant]]. Now: :$1$: extract the factor $\dfrac 1 {x_i + y_1}$ from each [[Definition:Row of Matrix|row]] $1 \le i \le n$ :$2$: extract the factor $y_1 - y_j$ from each [[Definition:Column of Matrix|column]] $2 \le j \le n$. Thus from [[Determinant with Row Multiplied by Constant]] we have the following: :$\displaystyle D_n = \left({\prod_{i \mathop = 1}^n \frac 1 {x_i + y_1}}\right) \left({\prod_{j \mathop = 2}^n y_1 - y_j}\right) \begin{vmatrix} 1 & \dfrac 1 {x_1 + y_2} & \dfrac 1 {x_1 + y_3} & \cdots & \dfrac 1 {x_1 + y_n} \\ 1 & \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ 1 & \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ Now subtract [[Definition:Row of Matrix|row]] $1$ from each of [[Definition:Row of Matrix|row]]s $2$ to $n$. [[Definition:Column of Matrix|Column]] $1$ will go to $0$ for all but the first [[Definition:Row of Matrix|row]]. [[Definition:Column of Matrix|Column]]s $2$ to $n$ will become: {{begin-eqn}} {{eqn | l = a_{ij} | o = \gets | r = \frac 1 {x_i + y_j} - \frac 1 {x_1 + y_j} | c = }} {{eqn | r = \frac {\left({x_1 + y_j}\right) - \left({x_i + y_j}\right)} {\left({x_i + y_j}\right) \left({x_1 + y_j}\right)} | c = }} {{eqn | r = \left({\frac {x_1 - x_i} {x_1 + y_j} }\right) \left({\frac 1 {x_i + y_j} }\right) | c = }} {{end-eqn}} From [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the [[Definition:Determinant of Matrix|determinant]]. Now: :$1$: extract the factor $x_1 - x_i$ from each [[Definition:Row of Matrix|row]] $2 \le i \le n$ :$2$: extract the factor $\dfrac 1 {x_1 + y_j}$ from each [[Definition:Column of Matrix|column]] $2 \le j \le n$. Thus from [[Determinant with Row Multiplied by Constant]] we have the following: :$\displaystyle D_n = \left({\prod_{i \mathop = 1}^n \frac 1 {x_i + y_1}}\right) \left({\prod_{j \mathop = 1}^n \frac 1 {x_1 + y_j}}\right) \left({\prod_{i \mathop = 2}^n x_1 - x_i}\right) \left({\prod_{j \mathop = 2}^n y_1 - y_j}\right) \begin{vmatrix} 1 & 1 & 1 & \cdots & 1 \\ 0 & \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ 0 & \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ From [[Determinant with Unit Element in Otherwise Zero Row]], and tidying up the products, we get: :$D_n = \frac {\displaystyle \prod_{i \mathop = 2}^n \left({x_i - x_1}\right) \left({y_i - y_1}\right)} {\displaystyle \prod_{1 \mathop \le i, j \mathop \le n} \left({x_i + y_1}\right) \left({x_1 + y_j}\right)} \begin{vmatrix} \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ Repeat the process for the remaining [[Definition:Row of Matrix|row]]s and [[Definition:Column of Matrix|column]]s $2$ to $n$. The result follows. {{qed}} A similar process obtains the result for the $a_{ij} = \dfrac 1 {x_i - y_j}$ form.	1
Let $\mathbf a = \left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert\mathbf u$. Then: {{begin-eqn}} {{eqn | l=\cos \angle \mathbf u, \mathbf a | r=\frac {\mathbf u \cdot \mathbf a} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Cosine Formula for Dot Product]] }} {{eqn | r=\frac {\mathbf u \cdot \left({ \left\Vert{ \mathbf u }\right\Vert \mathbf v + \left\Vert{ \mathbf v }\right\Vert \mathbf u }\right)} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a} \right\Vert} | c= }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf u }\right) } {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Properties of Dot Product]] }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf u }\right\Vert^2} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Dot Product of Vector with Itself]] }} {{eqn | r=\frac {\mathbf u \cdot \mathbf v + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert} {\left\Vert{ \mathbf a }\right\Vert} | c= }} {{end-eqn}} {{begin-eqn}} {{eqn | l=\cos \angle \mathbf a, \mathbf v | r=\frac {\mathbf v \cdot \mathbf a} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Cosine Formula for Dot Product]] }} {{eqn | r=\frac {\mathbf v \cdot \left({ \left\Vert{ \mathbf u }\right\Vert \mathbf v + \left\Vert{ \mathbf v }\right\Vert \mathbf u }\right)} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf v \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right)} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Properties of Dot Product]] }} {{eqn | r=\frac {\left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert^2} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= [[Dot Product of Vector with Itself]] }} {{eqn | r=\frac {\mathbf u \cdot \mathbf v + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert} {\left\Vert{ \mathbf a }\right\Vert} | c= }} {{end-eqn}} Comparing the two expressions gives us: :$\cos \angle \mathbf u, \mathbf a = \cos \angle \mathbf a, \mathbf v$ Since the angle used in the dot product is always taken to be between $0$ and $\pi$ and [[Definition:Cosine|cosine]] is [[Definition:Injection|injective]] on this interval (from [[Shape of Cosine Function]]), we have: :$\angle \mathbf u, \mathbf a = \angle \mathbf a, \mathbf v$ The result follows. {{qed}}	1
{{ProofWanted}} [[Category:Module Theory]] 9awn5pi2z2b6ojl6mw92w24270sxhgs	1
Let $z_1, z_2, z_3 \in \C$ be [[Definition:Complex Number|complex numbers]]. Then: :$z_1 \times \paren {z_2 + z_3} = z_1 \times z_2 + z_1 \times z_3$ where $\times$ denotes [[Definition:Complex Cross Product|cross product]].	1
Suppose $x \in B$ is integral over $C$. Certainly $C$ is integral over $A$, so by [[Transitivity of Integrality]], $C[x]$ is integral over $A$. In particular, $x$ is integral over $A$, so $x \in C$. {{Qed}} [[Category:Algebraic Number Theory]] [[Category:Commutative Algebra]] 1k8rp56mvkw2r0m8ykdktnr7yke9yul	1
Let $\mathbf A = \left[{a}\right]_{n}, \mathbf B = \left[{b}\right]_{n}$ be [[Definition:Square Matrix|square matrices]] of order $n$. Let $\mathbf C = \mathbf A + \mathbf B$ be the [[Definition:Matrix Entrywise Addition|matrix entrywise sum]] of $\mathbf A$ and $\mathbf B$. If $\mathbf A$ and $\mathbf B$ are [[Definition:Upper Triangular Matrix|upper triangular matrices]], then so is $\mathbf C$. If $\mathbf A$ and $\mathbf B$ are [[Definition:Lower Triangular Matrix|lower triangular matrices]], then so is $\mathbf C$.	1
Let $\R \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] in $X$ over the [[Definition:Real Number|real numbers]] $\R$. Then the [[Definition:Polynomial in Ring Element|polynomial]] $X^2 + 1$ is an [[Definition:Irreducible Element of Ring|irreducible element]] of $\R \sqbrk X$.	1
Let $\struct {G, +_G}$ be an [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e_G$. Let $\struct {R, +_R, \circ_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}_R$ be the [[Definition:Trivial Module|trivial $R$-module]], such that: :$\forall \lambda \in R: \forall x \in G: \lambda \circ x = e_G$ Then $\struct {G, +_G, \circ}_R$ is a [[Definition:Module|module]].	1
Let $z = x + i y \in \C$ be a [[Definition:Complex Number|complex number]]. Then: :$\size x + \size y \le \sqrt 2 \cmod z$ where: :$\size x$ and $\size y$ denote the [[Definition:Absolute Value|absolute value]] of $x$ and $y$ :$\cmod z$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z$.	1
By the definition of $\alpha$ then: :$\norm {n_0} = n_0^\alpha$ By the definition of $n_0$ then: :$n_0^\alpha > 1$ Let $n \in \N$. By [[Basis Representation Theorem]] then $n$ can be written: :$n = a_0 + a_1 n_0 + a_2 n_0^2 + \cdots + a_s n_0^s$ where $0 \le a_i < n_0$ and $a_s \ne 0$ Since all of the $a_i < n_0$, by choice of $n_0$ then: :$\forall a_i: \norm {a_i} \le 1$ Then: {{begin-eqn}} {{eqn | l = \norm n | o = \le | r = \norm {a_0} + \norm {a_1 n_0} + \norm {a_2 n_0^2} + \cdots + \norm {a_s n_0^s} | c = {{NormAxiom|3}} }} {{eqn | r = \norm {a_0} + \norm {a_1} \norm {n_0} + \norm {a_2} \norm {n_0}^2 + \cdots + \norm {a_s} \norm {n_0}^s | c = {{NormAxiom|2}} }} {{eqn | r = \norm {a_0} + \norm {a_1} n_0^\alpha + \norm {a_2} n_0^{2 \alpha} + \cdots + \norm {a_s} n_0^{s\alpha} | c = as $\norm {n_0} = n_0^\alpha$ }} {{eqn | o = \le | r = 1 + n_0^\alpha + n_0^{2 \alpha} + \cdots + n_0^{s \alpha} | c = as $\forall a_i: \norm {a_i} \le 1$ }} {{eqn | r = n_0^{s \alpha} \paren {1 + n_0^{-\alpha} + n_0^{-2 \alpha} + \cdots + n_0^{-s \alpha} } | c = }} {{eqn | o = \le | r = n_0^{s \alpha} \paren {\sum_{i \mathop = 0}^\infty \paren{\frac 1 {n_0^\alpha} }^i} | c = }} {{eqn | r = n_0^{s \alpha} \paren {\dfrac {n_0^\alpha} {n_0^\alpha - 1} } | c = [[Sum of Infinite Geometric Progression]] (and as $n_0^\alpha > 1$) }} {{eqn | o = \le | r = n^\alpha \paren {\dfrac {n_0^\alpha} {n_0^\alpha - 1} } | c = as $n \ge n_0^s$ }} {{end-eqn}} Let $C = \paren {\dfrac {n_0^\alpha} {n_0^\alpha - 1} }$ Hence: :$\norm n \le C n^\alpha$ As $n \in \N$ was arbitrary: :$\forall n \in N: \norm n \le C n^\alpha$ Let $n, N \in N$ Then: :$\norm {n^N} \le C \paren {n^N}^\alpha$ Now: {{begin-eqn}} {{eqn | l = \norm {n^N} \le C \paren {n^N}^\alpha | o = \leadsto | r = \norm n^N \le C \paren {n^N}^\alpha | c = {{NormAxiom|2}} }} {{eqn | o = \leadsto | r = \norm n^N \le C \paren {n^\alpha}^N | c = }} {{eqn | o = \leadsto | r = \norm n \le \sqrt [N] C n^\alpha | c = taking $N$th roots }} {{end-eqn}} By [[Limit of Root of Positive Real Number]]: :$\sqrt [N] C \to 1$ as $N \to \infty$ By the [[Multiple Rule for Real Sequences]]: :$\sqrt [N] C n^\alpha \to n^\alpha$ as $N \to \infty$ By [[Inequality Rule for Real Sequences]], letting $N \to \infty$ for fixed $n$: :$\norm n \le n^\alpha$ The result follows. {{qed}}	1
=== Existence === Let $\psi \left({r}\right) := r m$. This map is $R$-linear by definition of a module. {{qed|lemma}} === Uniqueness === Let $\psi_1$ and $\psi_2$ be two such morphisms. Then $\psi_1 - \psi_2$ is an $R$-module morphism whose kernel contains $1$. Thus: :$\ker \left({\psi_1 - \psi_2}\right) = R$ and $\psi_1 = \psi_2$. {{qed}} [[Category:Abstract Algebra]] [[Category:Module Theory]] oeq58bgwu5pxrvt0g8obnfyj7226gtm	1
Let $\struct {X, \norm {\,\cdot\,}}$ be a [[Definition:Finite Dimensional Vector Space|finite-dimensional]] [[Definition:Normed Vector Space|normed vector space]]. Let $K \subset X$ be a [[Definition:Compact Space/Normed Vector Space/Subspace|compact]] [[Definition:Subset|subset]]. Then $K$ is [[Definition:Closed Set of Normed Vector Space|closed]] and [[Definition:Bounded Normed Vector Space|bounded]].	1
The [[Definition:Octonion|set of octonions]] $\Bbb O$ forms an [[Definition:Algebra over Field|algebra]] over the [[Definition:Field of Real Numbers|field of real numbers]]. This [[Definition:Algebra over Field|algebra]] is: :$(1): \quad$ An [[Definition:Alternative Algebra|alternative algebra]], but '''not''' an [[Definition:Associative Algebra|associative algebra]]. :$(2): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]]. :$(3): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]].	1
Let $D$ be the [[Definition:Determinant of Matrix|determinant]] of order $n$. Let $D^*$ be the [[Definition:Adjugate Matrix|adjugate]] of $D$. Then $D^* = D^{n-1}$.	1
:$\begin{vmatrix} \left({x + q_2}\right) \left({x + q_3}\right) & \left({x + p_1}\right) \left({x + q_3}\right) & \left({x + p_1}\right) \left({x + p_2}\right) \\ \left({y + q_2}\right) \left({y + q_3}\right) & \left({y + p_1}\right) \left({y + q_3}\right) & \left({y + p_1}\right) \left({y + p_2}\right) \\ \left({z + q_2}\right) \left({z + q_3}\right) & \left({z + p_1}\right) \left({z + q_3}\right) & \left({z + p_1}\right) \left({z + p_2}\right) \end{vmatrix} = \left({x - y}\right) \left({x - z}\right) \left({y - z}\right) \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \left({p_2 - q_3}\right)$ where $\left\vert{\, \cdot \,}\right\vert$ denotes [[Definition:Determinant of Matrix|determinant]].	1
For the result to hold, we need to show that $a \oplus \dfrac {a^*} {\left \Vert {a}\right \Vert^2} = 1 = \dfrac {a^*} {\left \Vert {a}\right \Vert^2} \oplus a$. {{begin-eqn}} {{eqn | o = | r = a \oplus \dfrac {a^*} {\left \Vert {a}\right \Vert^2} | c = }} {{eqn | r = a \oplus a^* \cdot \dfrac 1 {\left \Vert {a}\right \Vert^2} | c = }} {{eqn | r = \left \Vert {a}\right \Vert^2 \cdot \dfrac 1 {\left \Vert {a}\right \Vert^2} | c = Definition of [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]] }} {{eqn | r = 1 | c = }} {{eqn | r = \dfrac 1 {\left \Vert {a}\right \Vert^2} \cdot \left \Vert {a}\right \Vert^2 | c = }} {{eqn | r = \dfrac 1 {\left \Vert {a}\right \Vert^2} \cdot a^* \oplus a | c = Definition of [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]] }} {{eqn | r = \dfrac {a^*} {\left \Vert {a}\right \Vert^2}\oplus a | c = }} {{end-eqn}} {{qed}} Note that this construction works whether $\oplus$ is [[Definition:Associative|associative]] or not.	1
Let $\mathbf v, \mathbf w$ be two non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Linear Algebra)|vectors]] in $\R^2$, and let $p$ be a [[Definition:Point|point]] in $\R^2$. Suppose that the [[Definition:Angle Between Vectors|angle]] between $\mathbf v$ and $\mathbf w$ is a [[Definition:Convex Angle|convex angle]]. Then the [[Definition:Set|set]] :$U = \left\{ {p + st \mathbf v + \left({1-s}\right) t \mathbf w : s \in \left({0\,.\,.\,1}\right) , t \in \R_{>0} }\right\}$ is a [[Definition:Convex Set (Vector Space)|convex set]]. {{expand|It'd be really nice to have a picture of $U$ to support intuition and connect with the page title}}	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_K$ and whose [[Definition:Unity of Field|unity]] is $1_K$. Let $\mathbf A$ be an [[Definition:Invertible Matrix|invertible matrix]] of [[Definition:Order of Square Matrix|order $n$]] over $K$. Then the [[Definition:Determinant of Matrix|determinant]] of its [[Definition:Inverse Matrix|inverse]] is given by: :$\map \det {\mathbf A^{-1} } = \dfrac {1_K} {\map \det {\mathbf A} }$	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]], and let $A: H \to K$ be a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. {{TFAE|def = Norm on Bounded Linear Transformation}} :$(1): \qquad \left\Vert{A}\right\Vert = \sup \left\{{\left\Vert{Ah}\right\Vert_K: \left\Vert{h}\right\Vert_H \le 1}\right\}$ :$(2): \qquad \left\Vert{A}\right\Vert = \sup \left\{{\dfrac {\left\Vert{Ah}\right\Vert_K} {\left\Vert{h}\right\Vert_H}: h \in H, h \ne \mathbf{0}_H}\right\}$ :$(3): \qquad \left\Vert{A}\right\Vert = \sup \left\{{\left\Vert{Ah}\right\Vert_K: \left\Vert{h}\right\Vert_H \le 1}\right\}$ :$(4): \qquad \left\Vert{A}\right\Vert = \inf \left\{{c > 0: \forall h \in H: \left\Vert{Ah}\right\Vert_K \le c \left\Vert{h}\right\Vert_H}\right\}$	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. For $\mathbf A, \mathbf B \in \map {\MM_R} {m, n}$, let $\mathbf A + \mathbf B$ be defined as the [[Definition:Matrix Entrywise Addition over Ring|matrix entrywise sum]] of $\mathbf A$ and $\mathbf B$. The operation $+$ is [[Definition:Associative Operation|associative]] on $\map {\MM_R} {m, n}$. That is: :$\paren {\mathbf A + \mathbf B} + \mathbf C = \mathbf A + \paren {\mathbf B + \mathbf C}$ for all $\mathbf A$, $\mathbf B$ and $\mathbf C$ in $\map {\MM_R} {m, n}$.	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $S\subseteq A$ be a [[Definition:Multiplicatively Closed Subset of Ring|multiplicatively closed subset]]. Let $A \overset \iota \to A_S$ be the [[Definition:Localization of Ring|localization]] at $S$. Let $I$ be the [[Definition:Set|set]] of [[Definition:Saturated Ideal by Multiplicatively Closed Subset|saturated ideals]] of $A$ by $S$. Let $J$ be the [[Definition:Set|set]] of [[Definition:Ideal of Ring|ideals]] of $A_S$.	1
Let $T = \struct {S, \tau}$ be an [[Definition:Uncountable Discrete Topology|uncountable discrete topological space]]. Then $T$ is not [[Definition:Separable Space|separable]].	1
Let $\mathbf A = \paren {a_{i j} }$. Let $A_{i j}$ denote the [[Definition:Cofactor of Element|cofactor]] of $a_{i j} \in \mathbf A$. === Right Multiplication === We show that $\mathbf A \cdot \adj {\mathbf A} = \map \det {\mathbf A} \cdot \mathbf I_n$. Let $i, j \in \set {1, \ldots, n}$. If $i = j$, [[Expansion Theorem for Determinants|expanding]] $\map \det {\mathbf A}$ along [[Definition:Row of Matrix|row]] $i$ shows that: :$\displaystyle \map \det {\mathbf A} = \sum_{k \mathop = 1}^n a_{i k} A_{i k}$ If $i \ne j$, define $\mathbf A'$ as the [[Definition:Matrix|matrix]] obtained by replacing row $j$ of $\mathbf A$ with row $i$ of $\mathbf A$. Then $\mathbf A' = \begin{bmatrix} a' \end{bmatrix}_n$ has two identical rows, so: {{begin-eqn}} {{eqn | l = 0_R | r = \map \det {\mathbf A'} | c = [[Square Matrix with Duplicate Rows has Zero Determinant]] }} {{eqn | r = \sum_{k \mathop = 1}^n a'_{j k} A'_{j k} | c = [[Expansion Theorem for Determinants|expanding]] $\map \det {\mathbf A'}$ along row $j$ }} {{eqn | r = \sum_{k \mathop = 1}^n a_{i k} A_{j k} }} {{end-eqn}} By [[Definition:Matrix Product|definition of matrix product]], [[Definition:Element of Matrix|element]] $\tuple {i, j}$ of $\mathbf A \cdot \adj {\mathbf A}$ is: :$\displaystyle \sum_{k \mathop = 1}^n a_{i k} A_{j k} = \begin{cases} 0_R & \text {for} & i \ne j \\ \map \det {\mathbf A} & \text {for} & i = j \end{cases}$ Hence: :$\mathbf A \cdot \adj {\mathbf A} = \map \det {\mathbf A} \cdot \mathbf I_n$ {{qed|lemma}} === Left Multiplication === We show that $\adj {\mathbf A} \cdot \mathbf A = \map \det {\mathbf A} \cdot \mathbf I_n$. Let $i, j \in \set {1, \ldots, n}$. If $i = j$, [[Expansion Theorem for Determinants|expanding]] $\map \det {\mathbf A}$ along [[Definition:Column of Matrix|column]] $j$ shows that: :$\displaystyle \map \det {\mathbf A} = \sum_{k \mathop = 1}^n a_{k j} A_{k j}$ If $i \ne j$, define $\mathbf A'$ as the [[Definition:Matrix|matrix]] obtained by replacing column $i$ of $\mathbf A$ with column $j$ of $\mathbf A$. Then $\mathbf A' = \begin{bmatrix} a' \end{bmatrix}_n$ has two identical columns, so: {{begin-eqn}} {{eqn | l = 0_R | r = \map \det {\mathbf A'} | c = [[Square Matrix with Duplicate Columns has Zero Determinant]] }} {{eqn | r = \sum_{k \mathop = 1}^n a'_{k i} A'_{k i} | c = [[Expansion Theorem for Determinants|expanding]] $\map \det {\mathbf A'}$ along column $i$ }} {{eqn | r = \sum_{k \mathop = 1}^n a_{k j} A_{k i} }} {{end-eqn}} By definition of [[Definition:Matrix Product|matrix product]], [[Definition:Element of Matrix|element]] $\tuple {i, j}$ of $\adj {\mathbf A} \cdot \mathbf A$ is: :$\displaystyle \sum_{k \mathop = 1}^n A_{k i} a_{k j} = \begin{cases} 0_R & \text {for} & i \ne j \\ \map \det {\mathbf A} & \text {for} & i = j \end{cases}$ Hence: :$\adj {\mathbf A} \cdot \mathbf A = \map \det {\mathbf A} \cdot \mathbf I_n$ {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\struct {U_R, \circ}$ be the [[Definition:Group of Units of Ring|group of units]] of $\struct {R, +, \circ}$. Let $a, b \in R, c, d \in U_R$. Then: :$\dfrac a c \circ \dfrac b d = \dfrac {a \circ b} {c \circ d}$ where $\dfrac x z$ is defined as $x \circ \paren {z^{-1} }$, that is, $x$ [[Definition:Division Product|divided by]] $z$.	1
Let $y \ne 0_R$. By [[Definition:Norm Axioms|Norm axiom (N1) (Positive Definiteness)]] then: :$\norm {y} \ne 0$ So: {{begin-eqn}} {{eqn| l = \norm{x \circ y^{-1} } | r = \norm{x} \norm{y^{-1} } | c = [[Definition:Norm Axioms|Norm axiom (N2) (Multiplicativity)]] }} {{eqn| r = \dfrac {\norm{x} } {\norm{y} } | c = [[Properties of Norm on Division Ring/Norm of Inverse|Norm of Inverse]] }} {{end-eqn}} Similarly: :$\norm{y^{-1}x} = \dfrac {\norm{x}}{\norm{y}}$ {{qed}}	1
By the [[Definition:Norm Axioms|norm axiom (N2) (Multiplicativity)]] then: :$\forall x, y \in R: \norm {x \circ y} = \norm{x} \norm{y}$ In particular: :$\norm{1_R} = \norm{1_R \circ 1_R} = \norm{1_R} \norm{1_R}$ By the [[Definition:Norm Axioms|norm axiom (N1) (Positive defintiteness)]] then: :$\norm{1_R} \ne 0$ So $\norm{1_R}$ has an inverse in $\R$. By multiplying by this inverse, then: :$ \norm{1_R} \norm{1_R} =\norm{1_R} \iff \norm{1_R} = 1$ as desired. {{qed}}	1
Let $M$ be a [[Definition:Smooth Manifold|smooth manifold]]. Let $m \in M$ be a [[Definition:Point|point]]. Let $V$ be an [[Definition:Open Neighborhood|open neighborhood]] of $m$. Let $C^\infty \left({V, \R}\right)$ be defined as the [[Definition:Set|set]] of all [[Definition:Smooth Mapping|smooth mappings]] $f: V \to \R$. {{TFAE|def = Tangent Vector}}	1
:the constant [[Definition:Sequence|sequence]] $\tuple {a, a, a, \dots}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{m n}$ and $\mathbf C = \sqbrk c_{m n}$ be [[Definition:Matrix|matrices]] whose [[Definition:Order of Matrix|order]] is $m \times n$. Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A + \mathbf B} + \mathbf C | r = \paren {\sqbrk a_{m n} + \sqbrk b_{m n} } + \sqbrk c_{m n} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{eqn | r = \sqbrk {a + b}_{m n} + \sqbrk c_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {\paren {a + b} + c}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {a + \paren {b + c} }_{m n} | c = [[Associative Law of Addition]] }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {b + c}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} + \paren {\sqbrk b_{m n} + \sqbrk c_{m n} } | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \mathbf A + \paren {\mathbf B + \mathbf C} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{end-eqn}} {{qed}}	1
Proof by [[Principle of Mathematical Induction|induction]]: Let the [[Vandermonde Determinant]] be presented in the form as defined by [[Vandermonde Determinant#Mirsky|Mirsky]]: Let $V_n = \begin{vmatrix} a_1^{n-1} & a_1^{n-2} & \cdots & a_1 & 1 \\ a_2^{n-1} & a_2^{n-2} & \cdots & a_2 & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_n^{n-1} & a_n^{n-2} & \cdots & a_n & 1 \\ \end{vmatrix}$ For all $n \in \N_{>0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({a_i - a_j}\right)$ $P(1)$ is true, as this just says $\begin{vmatrix} 1 \end{vmatrix} = 1$. === Basis for the Induction === $P(2)$ holds, as it is the case: : $V_2 = \begin{vmatrix} a_1 & 1 \\ a_2 & 1 \end{vmatrix}$ which evaluates to $V_2 = a_1 - a_2$. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: : $\displaystyle V_k = \prod_{1 \mathop \le i \mathop < j \mathop \le k} \left({a_i - a_j}\right)$ Then we need to show: : $\displaystyle V_{k+1} = \prod_{1 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: Take the determinant: :$V_{k+1} = \begin{vmatrix} x^k & x^{k-1} & \cdots & x^2 & x & 1 \\ a_2^k & a_2^{k-1} & \cdots & a_2^2 & a_2 & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ a_{k+1}^k & a_{k+1}^{k-1} & \cdots & a_{k+1}^2 & a_{k+1} & 1 \end{vmatrix}$ Let the [[Expansion Theorem for Determinants]] be used to expand $V_n$ in terms of the first row It can be seen that it is a [[Definition:Real Polynomial Function|polynomial]] in $x$ whose [[Definition:Degree (Polynomial)|degree]] is no greater than $k$. Let that polynomial be denoted $f \left({x}\right)$. Let any $a_r$ be substituted for $x$ in the determinant. Then two of its rows will be the same. From [[Square Matrix with Duplicate Rows has Zero Determinant]], the value of such a determinant will be $0$. Such a substitution in the determinant is equivalent to substituting $a_r$ for $x$ in $f \left({x}\right)$. Thus it follows that: :$f \left({a_2}\right) = f \left({a_3}\right) = \ldots = f \left({a_{k+1}}\right) = 0$ So $f \left({x}\right)$ is divisible by each of the factors $x - a_2, x - a_3, \ldots, x - a_{k+1}$. All these factors are distinct, otherwise the original determinant is zero. So: : $f \left({x}\right) = C \left({x - a_2}\right) \left({x - a_3}\right) \cdots \left({x - a_k}\right) \left({x - a_{k+1}}\right)$ As the degree of $f \left({x}\right)$ is no greater than $k$, it follows that $C$ is independent of $x$. From the [[Expansion Theorem for Determinants]], the coefficient of $x^k$ is: : $\begin{vmatrix} a_2^{k-1} & \cdots & a_2^2 & a_2 & 1 \\ \vdots & \ddots & \vdots & \vdots & \vdots \\ a_{k+1}^{k-1} & \cdots & a_{k+1}^2 & a_{k+1} & 1 \end{vmatrix}$. By the [[Vandermonde Determinant/Proof 2#Induction Hypothesis|induction hypothesis]], this is equal to: : $\displaystyle \prod_{2 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ This must be our value of $C$. So we have: : $\displaystyle f \left({x}\right) = \left({x - a_2}\right) \left({x - a_3}\right) \cdots \left({x - a_k}\right) \left({x - a_{k+1}}\right) \prod_{2 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ Substituting $a_1$ for $x$, we retrieve the proposition $P \left({k+1}\right)$. So $P \left({k}\right) \implies P \left({k+1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: : $\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({a_i - a_j}\right)$ {{qed}}	1
Let $\Omega\subset\R^n$ be [[Definition:Open Set of Real Euclidean Space|open]]. Let $f : \Omega \to \R^k$ be an [[Definition:Submersion|submersion]]. Let $p\in\Omega$. Then $n\geq k$, and there exists a [[Definition:Local Diffeomorphism|local diffeomorphism]] $\phi$ around $f(p)$ such that :$\phi\circ f (x, y) = x$ for all $(x, y)$ in a [[Definition:Neighborhood (Topology)|neighborhood]] of $p$.	1
Let $x_n \to x$ in $V$. We have: :$x_n \to x \implies \norm {x_n - x} \to 0$ By the [[Reverse Triangle Inequality]]: :$\size {\norm {x_n} - \norm x} \le \norm {x_n - x}$ Hence: :$\size {\norm {x_n} - \norm {x_n} } \to 0$ {{mistake|Should the above read $\size {\norm {x_n} - \norm x} \to 0$?}} Thus: :$\norm {x_n} \to \norm x$ {{finish|Explain why, with use of links, this shows continuity.}} [[Category:Norm Theory]] qnjnc1afwm0bbaokk4abjx9wel2yoyw	1
Let $G$ and $H$ be [[Definition:Dimension (Linear Algebra)|$n$-dimensional]] [[Definition:Vector Space|vector spaces]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation|linear transformation]]. Then these statements are equivalent: : $(1): \quad \phi$ is an [[Definition:Vector Space Isomorphism|isomorphism]]. : $(2): \quad \phi$ is a [[Definition:Vector Space Monomorphism|monomorphism]]. : $(3): \quad \phi$ is an [[Definition:Vector Space Epimorphism|epimorphism]]. : $(4): \quad \phi \left({B}\right)$ is a basis of $H$ for every [[Definition:Basis (Linear Algebra)|basis]] $B$ of $G$. : $(5): \quad \phi \left({B}\right)$ is a basis of $H$ for some [[Definition:Basis (Linear Algebra)|basis]] $B$ of $G$.	1
First note that from [[Ring of Integers Modulo Prime is Integral Domain]], $\struct {\Z_m, +, \times}$ is an [[Definition:Integral Domain|integral domain]] only when $m$ is [[Definition:Prime Number|prime]]. So for $m$ [[Definition:Composite Number|composite]] the result holds. If $m$ is [[Definition:Prime Number|prime]], and $\struct {\Z_m, +, \times}$ is therefore an [[Definition:Integral Domain|integral domain]], its [[Definition:Order of Structure|order]] is [[Definition:Finite Structure|finite]]. The result follows from [[Finite Integral Domain cannot be Ordered]]. {{qed}}	1
This proof assumes that $\mathbf A$ and $\mathbf B$ are $n \times n$-[[Definition:Matrix|matrices]] over a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] $\left({R, +, \circ}\right)$. Let $\mathbf C = \left[{c}\right]_n = \mathbf A \mathbf B$. From [[Square Matrix is Row Equivalent to Triangular Matrix]], it follows that $\mathbf A$ can be converted into a [[Definition:Upper Triangular Matrix|upper triangular matrix]] $\mathbf A'$ by a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\hat o_1, \ldots, \hat o_{m'}$. Let $\mathbf C'$ denote the matrix that results from using $\hat o_1, \ldots, \hat o_{m'}$ on $\mathbf C$. From [[Elementary Row Operations Commute with Matrix Multiplication]], it follows that $\mathbf C' = \mathbf A' \mathbf B$. [[Effect of Sequence of Elementary Row Operations on Determinant]] shows that there exists $\alpha \in R$ such that: :$\alpha \det \left({\mathbf A'}\right) = \det \left({\mathbf A}\right)$ :$\alpha \det \left({\mathbf C'}\right) = \det \left({\mathbf C}\right)$ Let $\mathbf B^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] of $B$. From [[Transpose of Matrix Product]], it follows that: : $\left({\mathbf C'}\right)^\intercal = \left({\mathbf A' \mathbf B}\right)^\intercal = \mathbf B^\intercal \left({\mathbf A'}\right)^\intercal$ From [[Square Matrix is Row Equivalent to Triangular Matrix]], it follows that $\mathbf B^\intercal$ can be converted into a [[Definition:Triangular Matrix/Lower Triangular Matrix|lower triangular matrix]] $\left({\mathbf B^\intercal}\right)'$ by a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\hat p_1, \ldots, \hat p_{m''}$. Let $\mathbf C''$ denote the matrix that results from using $\hat p_1, \ldots, \hat p_{m''}$ on $\left({\mathbf C'}\right)^\intercal$. From [[Elementary Row Operations Commute with Matrix Multiplication]], it follows that: : $\mathbf C'' = \left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal$ [[Effect of Sequence of Elementary Row Operations on Determinant]] shows that there exists $\beta \in R$ such that: :$\beta \det \left({\left({\mathbf B^\intercal}\right)'}\right) = \det \left({\mathbf B^\intercal}\right)$ :$\beta \det \left({\mathbf C''}\right) = \det \left({ \left({\mathbf C'}\right)^\intercal }\right)$ From [[Transpose of Upper Triangular Matrix is Lower Triangular]], it follows that $\left({\mathbf A'}\right)^\intercal$ is a lower triangular matrix. Then [[Product of Triangular Matrices]] shows that $\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal$ is a lower triangular matrix whose [[Definition:Diagonal Element|diagonal elements]] are the products of the diagonal elements of $\left({\mathbf B^\intercal}\right)'$ and $\left({\mathbf A'}\right)^\intercal$. From [[Determinant of Triangular Matrix]], we have that $\det \left({\left({\mathbf A'}\right)^\intercal}\right)$, $\det \left({\left({\mathbf B^\intercal}\right)' }\right)$, and $\det \left({\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal }\right)$ are equal to the product of their diagonal elements. Combinining these results shows that: :$\det \left({\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal}\right) = \det \left({\left({\mathbf B^\intercal}\right)'}\right) \det \left({\left({\mathbf A'}\right)^\intercal }\right)$ Then: {{begin-eqn}} {{eqn | l = \det \left({\mathbf C}\right) | r = \alpha \det \left({\mathbf C'}\right) }} {{eqn | r = \alpha \det \left({ \left({\mathbf C'}\right)^\intercal}\right) | c = [[Determinant of Transpose]] }} {{eqn | r = \alpha \beta \det \left({\mathbf C''}\right) }} {{eqn | r = \alpha \beta \det \left({ \left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal}\right) }} {{eqn | r = \alpha \beta \det \left({\left({\mathbf B^\intercal}\right)' }\right) \det \left({\left({\mathbf A'}\right)^\intercal}\right) }} {{eqn | r = \alpha \det \left({\left({\mathbf A'}\right)^\intercal}\right) \beta \det \left({\left({\mathbf B^\intercal}\right)' }\right) | c = [[Definition:Commutative Operation|Commutativity]] of [[Definition:Ring Product|Ring Product]] in $R$ }} {{eqn | r = \alpha \det \left({\mathbf A'}\right) \det \left({\mathbf B^\intercal}\right) | c = [[Determinant of Transpose]] }} {{eqn | r = \det \left({\mathbf A}\right) \det \left({\mathbf B}\right) | c = [[Determinant of Transpose]] }} {{end-eqn}} {{qed}}	1
For every $t\in N$, the [[Definition:Mapping|mapping]]: :$f_t : M \to M : x \mapsto f(x,t)$ is a [[Definition:Contraction Mapping|contraction]]. By the [[Banach Fixed-Point Theorem]], there exists a [[Definition:unique|unique]] $g(t) \in M$ such that $f_t(g(t)) = g(t)$. Let $f$ be [[Definition:Lipschitz Continuous at Point|Lipschitz continuous]] at $(g(t),t)$. We show that $g$ is [[Definition:Lipschitz Continuous at Point|Lipschitz continuous]] at $t$. Let $K<1$ be a [[Definition:Uniform Lipschitz Constant|uniform Lipschitz constant]] for $f$. Let $L$ be a [[Definition:Lipschitz Constant|Lipschitz constant]] for $f$ at $a$. Let $s\in N$. Then {{begin-eqn}} {{eqn | l = d(g(s), g(t)) | r = d(f(g(s), s), f(g(t), t)) | c = Definition of $g$ }} {{eqn | o = \leq | r = d(f(g(s), s), f(g(t), s)) + d(f(g(t), s), f(g(t), t)) | c = {{defof|Metric}} }} {{eqn | o = \leq | r = K \cdot d(g(s), g(t)) + d(f(g(t), s), f(g(t), t)) | c = $f$ is a [[Definition:Uniform Contraction Mapping|uniform contraction]] }} {{end-eqn}} and thus: {{begin-eqn}} {{eqn | l = d(g(s), g(t)) | o = \leq | r = \dfrac1{1-K}d(f(g(t), s), f(g(t), t)) }} {{eqn | o = \leq | r = \dfrac L{1-K} d(s,t) | c = $f$ is [[Definition:Lipschitz Continuous at Point|lipschitz continuous]] at $(g(t),t)$ }} {{end-eqn}} Thus $g$ is [[Definition:Lipschitz Continuous|lipschitz continuous]] at $t$. {{qed}}	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $S \subseteq A$ be a [[Definition:Multiplicatively Closed Subset of Ring|multiplicatively closed subset]]. Let $\left({A_S, \iota}\right)$ and $\left({\tilde A_S, \tilde \iota}\right)$ both satisfy the definition of the [[Definition:Localization of Ring|localization]] of $A$ at $S$. Then there is a [[Definition:Canonical Isomorphism|canonical isomorphism]] $\phi: A_S \to \tilde A_S$.	1
Follows directly from: :$(1) \quad$ the [[Fundamental Theorem of Arithmetic|fundamental theorem of arithmetic]] :$(2) \quad$ the fact that [[Integers form Integral Domain|$\struct {\Z, +, \times}$ is an integral domain]] :$(3) \quad$ the definitions [[Definition:Complete Factorization|complete factorization]] and [[Definition:Equivalent Factorizations|equivalent factorizations]]. {{qed}} [[Category:Integral Domains]] [[Category:Integers]] [[Category:Factorization]] [[Category:Unique Factorization Domains]] 32n4jwtruzn4meukioxmgtt2arwfda5	1
By assumption: :$\forall n \in \N: p^n \divides \paren {x_{n + 1} - x_n}$ By the definition of the [[Definition:P-adic Norm|$p$-adic norm]]: :$\forall n \in \N: \norm {x_{n + 1} - x_n}_p \le \dfrac 1 {p^n}$ By [[Sequence of Powers of Number less than One]]: :$\displaystyle \lim_{n \mathop \to \infty} \dfrac 1 {p^n} = 0$ By the [[Squeeze Theorem for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \norm{x_{n+1} - x_n}_p = 0$. From [[P-adic Norm is Non-Archimedean Norm]], the [[Definition:P-adic Norm|$p$-adic norm]] is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. By [[Characterisation of Cauchy Sequence in Non-Archimedean Norm]]: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. {{qed}} [[Category:Metric Spaces]] [[Category:Normed Spaces]] [[Category:P-adic Number Theory]] hec0ra2qr4h55bn1py1cwb2l55mknq0	1
:$G$ is a [[Definition:Basis of Vector Space|basis]] for $E$ {{iff}} $\card G = n$.	1
Let $\sequence {e_i}_{1 \mathop \le i \mathop \le k}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] that compose $\Gamma_1$. Let $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Row Matrix|elementary row matrices]]. Let $\sequence {f_i}_{1 \mathop \le i \mathop \le l}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] that compose $\Gamma_2$. Let $\sequence {\mathbf F_i}_{1 \mathop \le i \mathop \le l}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Row Matrix|elementary row matrices]]. From [[Row Operation is Equivalent to Pre-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf R_1 \mathbf A = \mathbf B$ where $\mathbf R$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$: :$\mathbf R_1 = \mathbf E_k \mathbf E_{k - 1} \dotsb \mathbf E_2 \mathbf E_1$ Also from [[Row Operation is Equivalent to Pre-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf R_2 \mathbf B = \mathbf C$ where $\mathbf R_2$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf F_i}_{1 \mathop \le i \mathop \le l}$: :$\mathbf R_2 = \mathbf F_l \mathbf F_{l - 1} \dotsb \mathbf F_2 \mathbf F_1$ Hence we have: :$\mathbf R_2 \mathbf R_1 \mathbf A = \mathbf C$ where $\mathbf R := \mathbf R_2 \mathbf R_1$ is the [[Definition:Matrix Product (Conventional)|product]]: :$\mathbf F_l \mathbf F_{l - 1} \dotsb \mathbf F_2 \mathbf F_1 \mathbf E_k \mathbf E_{k - 1} \dotsb \mathbf E_2 \mathbf E_1$ Let $\Gamma$ be the [[Definition:Row Operation|row operation]] composed of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\tuple {e_1, e_2, \ldots, e_{k - 1}, e_k, f_1, f_2, \ldots, f_{l - 1}, f_l}$. Thus $\Gamma$ is a [[Definition:Row Operation|row operation]] which transforms $\mathbf A$ into $\mathbf C$. Hence the result. {{qed}}	1
{{begin-eqn}} {{eqn | l = \dfrac \d {\d x} \left({\mathbf f \cdot \left({\mathbf g \times \mathbf h}\right)}\right) | r = \dfrac {\d \mathbf f} {\d x} \cdot \left({\mathbf g \times \mathbf h}\right) + \mathbf f \cdot \dfrac \d {\d x} \left({\mathbf g \times \mathbf h}\right) | c = [[Derivative of Dot Product of Vector-Valued Functions]] }} {{eqn | r = \dfrac {\d \mathbf f} {\d x} \cdot \left({\mathbf g \times \mathbf h}\right) + \mathbf f \cdot \left({\dfrac {\d \mathbf g} {\d x} \times \mathbf h + \mathbf g \times \dfrac {\d \mathbf h} {\d x} }\right) | c = [[Derivative of Vector Cross Product of Vector-Valued Functions]] }} {{eqn | r = \dfrac {\d \mathbf f} {\d x} \cdot \left({\mathbf g \times \mathbf h}\right) + \mathbf f \cdot \left({\dfrac {\d \mathbf g} {\d x} \times \mathbf h}\right) + \mathbf f \cdot \left({\mathbf g \times \dfrac {\d \mathbf h} {\d x} }\right) | c = [[Dot Product Distributes over Addition]] }} {{end-eqn}} {{qed}}	1
Let $M^\circ$ be the [[Definition:Annihilator on Algebraic Dual|annihilator]] of $M$. Let $N = M^{\circ}$. By [[Results Concerning Annihilator of Vector Subspace]], $N$ is [[Definition:Dimension of Vector Space|one-dimensional]] and $M = \map {J^{-1} } {N^\circ}$. Let $\phi \in N: \phi \ne 0$. Then $N$ is the set of all [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of $\phi$. Because: :$\map {J^{-1} } {N^\circ} = \set {x \in K^n: \forall \psi \in N: \map \psi x = 0}$ it follows that $\map {J^{-1} } {N^\circ}$ is simply the [[Definition:Kernel of Linear Transformation|kernel]] of $\phi$. Hence $(1)$ implies $(2)$. By [[Rank Plus Nullity Theorem]], $(2)$ also implies $(1)$. {{qed|lemma}} Suppose $\sequence {\alpha_n}$ is any sequence of [[Definition:Scalar (Vector Space)|scalars]]. Let $\sequence {e'_n}$ be the [[Definition:Ordered Basis|ordered basis]] of $\paren {K^n}^*$ [[Definition:Ordered Dual Basis|dual to]] the [[Definition:Standard Ordered Basis|standard ordered basis]] of $K^n$. Let $\displaystyle \phi = \sum_{k \mathop = 1}^n \alpha_k e'_k$. Then, by simple calculation: :$\map \ker \phi = \set {\tuple {\lambda_1, \ldots, \lambda_n}: \alpha_1 \lambda_1 + \cdots + \alpha_n \lambda_n = 0}$ {{explain|Prove the above.}} It follows that: :$\phi \ne 0 \iff \exists k \in \closedint 1 n: \alpha_k \ne 0$ Thus $(2)$ and $(3)$ are equivalent. {{qed|lemma}} Suppose $M = \map \ker \psi$, where $\displaystyle \psi = \sum_{k \mathop = 1}^n \beta_k e'_k$. Then $\psi = M^\circ$. As $M^\circ$ is one-dimensional and since $\psi \ne 0$, it follows that: :$\exists \gamma \ne 0: \psi = \gamma \phi$ Therefore: :$\forall k \in \closedint 1 n: \beta_k = \gamma \alpha_k$ {{qed}}	1
Two [[Definition:Vector Quantity|vector quantities]] are [[Definition:Equality|equal]] {{iff}} they have the same [[Definition:Magnitude|magnitude]] and [[Definition:Direction|direction]]. That is: :$\mathbf a = \mathbf b \iff \paren {\size {\mathbf a} = \size {\mathbf b} \land \hat {\mathbf a} = \hat {\mathbf b} }$ where: :$\hat {\mathbf a}$ denotes the [[Definition:Unit Vector|unit vector]] in the [[Definition:Direction|direction]] of $\mathbf a$ :$\size {\mathbf a}$ denotes the [[Definition:Magnitude|magnitude]] of $\mathbf a$.	1
From [[Symmetric Difference on Power Set forms Abelian Group]], $\struct {\powerset S, *}$ is an [[Definition:Abelian Group|abelian group]], where $\O$ is the [[Definition:Identity Element|identity]] and each [[Definition:Element|element]] is [[Definition:Self-Inverse Element|self-inverse]]. From [[Power Set with Intersection is Monoid]], $\struct {\powerset S, \cap}$ is a [[Definition:Commutative Monoid|commutative monoid]] whose [[Definition:Identity Element|identity]] is $S$. Also [[Intersection Distributes over Symmetric Difference]]. Thus $\struct {\powerset S, \cap}$ is a [[Definition:Commutative and Unitary Ring|commutative ring with a unity]] which is $S$. From [[Intersection with Empty Set]]: :$\forall A \in \powerset S: A \cap \O = \O = \O \cap A$ Thus $\O$ is indeed the [[Definition:Ring Zero|zero]]. However, from [[Set Intersection Not Cancellable]], it follows that $\struct {\powerset S, *, \cap}$ is not an [[Definition:Integral Domain|integral domain]]. {{qed}}	1
Let $\mathbf a, \mathbf b, \mathbf c$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]]: Let $\mathbf a \cdot \left({\mathbf b \times \mathbf c}\right)$ denote the [[Definition:Scalar Triple Product|scalar triple product]] of $\mathbf a, \mathbf b, \mathbf c$. Then $\left\lvert{\mathbf a \cdot \left({\mathbf b \times \mathbf c}\right)}\right\rvert$ equals the [[Definition:Volume|volume]] of the [[Definition:Parallelepiped|parallelepiped]] contained by $\mathbf a, \mathbf b, \mathbf c$.	1
Let $T = \struct {S, \tau}$ be a [[Definition:Hausdorff Space|Hausdorff space]] which is [[Definition:Separable Space|separable]]. Then $S$ can have a [[Definition:Cardinality|cardinality]] no greater than $2^{2^{\aleph_0} }$.	1
Let $\left({G, \cdot}\right)$ be a [[Definition:Finite Group|finite group]]. Let $V$ and $V'$ be two [[Definition:Irreducible Linear Representation|irreducible]] [[Definition:G-Module|$G$-modules]]. Let $f: V \to V'$ be a [[Definition:G-Module Homomorphism|homomorphism of $G$-modules]]. Then either: :$f \left({v}\right) = 0$ for all $v \in V$ or: : $f$ is an [[Definition:Module Isomorphism|isomorphism]].	1
Let $T = \struct {S, \tau_p}$ be the [[Definition:Fortissimo Space|fortissimo space]] on an [[Definition:Uncountable Set|uncountable set]] $S$. Then $T$ is not a [[Definition:Separable Space|separable space]].	1
=== Existence === We prove this by [[Proof by Mathematical Induction|induction]] on $k$. ==== Basis for the induction ==== Let $k = 2$. Let $d = \gcd \left({n_1, n_2}\right)$. By [[Bézout's Lemma]], there exist $s, t \in \Z$ such that $d = s n_1 + t n_2$. Let $\left({q_1, r}\right)$ and $\left({q_2, r}\right)$ be the [[Division Theorem|quotient and remainder]] of $b_1$ and $b_2$ upon [[Definition:Integer Division|division]] by $d$. Then $x = q_1 s n_2 + q_2 t n_2 + r$ is a solution. ==== Induction step ==== Let $n = \lcm \left({n_2, \ldots, n_k}\right)$. By the induction hypothesis, the last $k - 1$ congruences are equivalent to a single congruence: :$x \equiv b \pmod n$ for a certain $b \in \Z$ which satisfies $b \equiv b_i \pmod {n_i}$ for all $i \in \left\{ {2, \ldots, k}\right\}$. It remains to be shown that :$\lcm \left({n, n_1}\right) = \lcm \left({n_1, \ldots, n_k}\right)$ :$b \equiv b_1 \pmod {\gcd \left({n, n_1}\right)}$ to apply the case $k = 2$ and conclude. The first statement follows from [[Lowest Common Multiple is Associative]]. From [[GCD and LCM Distribute Over Each Other]]: :$\gcd \left({n, n_1}\right) = \lcm \left({\gcd \left({n_1, n_2}\right), \ldots, \gcd \left({n_1, n_k}\right)}\right)$ For all $i \in \left\{ {2, \ldots, k}\right\}$: : $b \equiv b_i \pmod {n_i}$ and: : $b_i \equiv b_1 \pmod {\gcd \left({n_1, n_i}\right)}$ Thus: : $b \equiv b_1 \pmod {\gcd \left({n_1, n_i}\right)}$ Therefore: :$b \equiv b_1 \pmod {\lcm \left({\gcd \left({n_1, n_2}\right), \ldots, \gcd \left({n_1, n_k}\right)}\right)}$ {{qed|lemma}} === Uniqueness=== {{ProofWanted}} [[Category:Chinese Remainder Theorem]] rblaaqhqiu6lqz5u8gvqms58tp3ug1e	1
{{proof wanted|Use the next result, the spectral theorem}}	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = \sum_{i \mathop = 1}^n u_i^2 | r = 0 | c = {{Defof|Dot Product|index = 1}} }} {{eqn | ll= \leadstoandfrom | lo= \forall i: | l = u_i | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = \mathbf u | r = \bszero | c = {{Defof|Zero Vector}} }} {{end-eqn}} {{qed}}	1
A '''linear differential operator''' is a [[Definition:Differential Operator|differential operator]] $\mathscr L$ with the property that: :$\map {\mathscr L} {\alpha \phi_1 + \beta \phi_2} = \alpha \map {\mathscr L} {\phi_1} + \beta \map {\mathscr L} {\phi_2}$ Thus if $\phi_1$ and $\phi_2$ are [[Definition:Solution to Differential Equation|solutions]] to the [[Definition:Differential Equation|differential equation]] $\map {\mathscr L} {\phi_i} = 0$, then so is any [[Definition:Linear Combination|linear combination]] of $\phi_1$ and $\phi_2$.	1
Let $T = \struct {S, \tau}$ be a [[Definition:Second-Countable Space|second-countable]] [[Definition:Topological Space|topological space]]. Then $T$ is also a [[Definition:Separable Space|separable space]].	1
From [[Structure Induced by Group Operation is Group]], $\struct {H^G, \oplus_H}$ is a [[Definition:Group|group]] Let $\phi, \psi \in \map {\LL_R} {G, H}$. From [[Addition of Linear Transformations]], $\phi \oplus_H \psi \in \map {\LL_R} {G, H}$. From [[Negative Linear Transformation]], $-\phi \in \map {\LL_R} {G, H}$. Thus, from the [[Two-Step Subgroup Test]], $\struct {\map {\LL_R} {G, H}, \oplus_H}$ is a [[Definition:Subgroup|subgroup]] of $\struct {H^G, \oplus_H}$. {{finish|Still need to show that it is abelian}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]]. Let $\map \det {\mathbf A}$ denote the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Take the [[Definition:Elementary Column Operation|elementary column operations]]: {{begin-axiom}} {{axiom | n = \text {ECO} 1 | t = For some $\lambda$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Column of Matrix|column]] $i$ by $\lambda$ | m = \kappa_i \to \lambda \kappa_i }} {{axiom | n = \text {ECO} 2 | t = For some $\lambda$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Column of Matrix|column]] $j$ to [[Definition:Column of Matrix|column]] $i$ | m = \kappa_i \to \kappa_i + \lambda \kappa_j }} {{axiom | n = \text {ECO} 3 | t = Exchange [[Definition:Column of Matrix|columns]] $i$ and $j$ | m = \kappa_i \leftrightarrow \kappa_j }} {{end-axiom}} Applying $\text {ECO} 1$ has the effect of multiplying $\map \det {\mathbf A}$ by $\lambda$. Applying $\text {ECO} 2$ has no effect on $\map \det {\mathbf A}$. Applying $\text {ECO} 3$ has the effect of multiplying $\map \det {\mathbf A}$ by $-1$.	1
Let $A$ be an [[Definition:Integral Domain|integral domain]]. {{TFAE}} : $(1): \quad A$ is a [[Definition:Unique Factorization Domain|unique factorisation domain]] : $(2): \quad A$ is a [[Definition:GCD Domain|GCD domain]] satisfying the [[Definition:Ascending Chain Condition|ascending chain condition]] on [[Definition:Principal Ideal of Ring|principal ideals]]. : $(3): \quad A$ satisfies the [[Definition:Ascending Chain Condition|ascending chain condition]] on [[Definition:Principal Ideal of Ring|principal ideals]] and every [[Definition:Irreducible Element of Ring|irreducible element]] of $A$ is a [[Definition:Prime Element|prime element]] of $A$.	1
{{begin-eqn}} {{eqn | l = \paren {c \mathbf u + \mathbf v} \cdot \mathbf w | r = c \sum_{i \mathop = 1}^n \paren {u_i + v_i} w_i | c = {{Defof|Dot Product|index = 1}} }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {c u_i + v_i} w_i | c = [[Real Multiplication Distributes over Real Addition]] }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {c u_i w_i + v_i w_i} | c = [[Real Multiplication Distributes over Real Addition]] }} {{eqn | r = \sum_{i \mathop = 1}^n c u_i w_i + \sum_{i \mathop = 1}^n v_i w_i | c = [[Real Multiplication is Commutative]] }} {{eqn | r = c \sum_{i \mathop = 1}^n u_i w_i + \sum_{i \mathop = 1}^n v_i w_i | c = [[Real Multiplication Distributes over Real Addition]] }} {{eqn | r = c \paren {\mathbf u \cdot \mathbf w} + \mathbf v \cdot \mathbf w | c = {{Defof|Dot Product|index = 1}} }} {{end-eqn}} {{qed}} [[Category:Dot Product]] 36k7dqpm7b0l2qlcb912iz5izgz60iw	1
Let $M$ be a [[Definition:Riemannian Manifold|Riemannian manifold]] of [[Definition:Dimension of Riemannian Manifold|dimension]] $2$. Then the [[Definition:Riemann-Christoffel Tensor|Riemann-Christoffel tensor]] on $M$ reduces to the [[Definition:Gaussian Curvature|Gaussian curvature]] on $M$.	1
Let $R$ be a [[Definition:Division Ring|division ring]]. Let $\norm {\, \cdot \,}_1: R \to \R_{\ge 0}$ and $\norm {\, \cdot \,}_2: R \to \R_{\ge 0}$ be [[Definition:Norm on Division Ring|norms]] on $R$. Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :for all sequences $\sequence {x_n}$ in $R$: $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$ {{iff}} $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$ Then $\forall x \in R$: :$\norm x_1 < 1 \iff \norm x_2 < 1$	1
Let $n_1, n_2, \ldots, n_k$ be [[Definition:Positive Integer|positive integers]]. Let $b_1, \ldots, b_k$ be integers such that: : $\forall i \ne j: \gcd \left({n_i, n_j}\right) \mathrel \backslash b_i - b_j$ where $\backslash$ denotes [[Definition:Divisor of Integer|divisibility]]. Then the [[Definition:Simultaneous Linear Congruences|system of linear congruences]]: : $x \equiv b_1 \pmod {n_1}$ : $x \equiv b_2 \pmod {n_2}$ : $\ldots$ : $x \equiv b_k \pmod {n_k}$ has a simultaneous solution which is unique [[Definition:Congruence (Number Theory)|modulo]] $\lcm \left({n_1, \ldots, n_k}\right)$.	1
Let $d$ denote the [[Definition:Metric Induced by Norm|metric induced by $\norm {\, \cdot \,}$]], that is: :$\map d {x, y} = \norm {x - y}$ From [[Metric Induced by Norm is Metric]] we have that $d$ is indeed a [[Definition:Metric|metric]]. Then, from the [[Reverse Triangle Inequality]] as applied to [[Definition:Metric Space|metric spaces]]: :$\forall x, y, z \in X: \size {\norm {x - z} - \norm {y - z} } \le \norm {x - y}$ Then: :$\forall x, y \in X: \size {\norm x - \norm y} = \size {\norm {x - 0} - \norm {y - 0} } \le \norm {x - y}$. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Then: :$\forall x, y \in R: \norm {x - y} \ge \bigsize {\norm x - \norm y}$	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Denote by $\left\langle{\cdot, \cdot}\right\rangle_H$ and $\left\langle{\cdot, \cdot}\right\rangle_K$ their respective [[Definition:Inner Product|inner products]]. Let $U: H \to K$ be an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]]. Then $U$ is a [[Definition:Bijection|bijection]].	1
Let $n \in \N_{\gt 0}$. Then: {{begin-eqn}} {{eqn| l = \norm {n \cdot 1_R} | r = = \norm {1_R + 1_R + \dots + 1_R} | o = }} {{eqn| o = }} {{eqn| o = | r = \le \underbrace {\norm {1_R} + \norm {1_R} + \dots + \norm {1_R} }_{n \, times} | c = [[Definition:Norm Axioms|Norm axiom (N3) (Triangle Inequality)]] }} {{eqn| o = }} {{eqn| o = | r = = \underbrace {1 + 1 + \dots + 1 }_{n \, times} | c = [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity]] }} {{eqn| o = }} {{eqn| o = | r = = n }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2}^2 | r = \paren {\cmod {z_1} \cos \theta_1 + \cmod {z_2} \cos \theta_2}^2 + \paren {\cmod {z_1} \sin \theta_1 + \cmod {z_2} \sin \theta_2}^2 | c = {{Defof|Complex Modulus}} }} {{eqn | r = 2 \cmod {z_1} \cmod {z_2} \cos \theta_1 \cos \theta_2 + \cmod {z_1}^2 \cos^2 \theta_1 + \cmod {z_2}^2 \cos^2 \theta_2 | c = }} {{eqn | o = | ro= + | r = 2 \cmod {z_1} \cmod {z_2} \sin \theta_1 \sin \theta_2 + \cmod {z_1}^2 \sin^2 \theta_1 + \cmod {z_2}^2 \sin^2 \theta_2 | c = }} {{eqn | r = 2 \cmod {z_1} \cmod {z_2} \paren {\cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2} + \cmod {z_1}^2 + \cmod {z_2}^2 | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \cmod {z_1} \cmod {z_2} \, \map \cos {\theta_1 - \theta_2} | c = [[Cosine of Difference]] }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $F$, $G$ and $H$ be [[Definition:Free Module|free $R$-modules]] of [[Definition:Finite Dimensional Module|finite dimension]] $p,n,m>0$ respectively. Let $\left \langle {a_p} \right \rangle$, $\left \langle {b_n} \right \rangle$ and $\left \langle {c_m} \right \rangle$ be [[Definition:Ordered Basis|ordered bases]] Let $\operatorname{Hom} \left({G, H}\right)$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Let $\mathcal M_R \left({m, n}\right)$ be the [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Let $\left[{u; \left \langle {c_m} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ be the [[Definition:Relative Matrix|matrix of $u$ relative to $\left \langle {b_n} \right \rangle$ and $\left \langle {c_m} \right \rangle$]]. Let $M: \operatorname{Hom} \left({G, H}\right) \to \mathcal M_R \left({m, n}\right)$ be defined as: :$\forall u \in \operatorname{Hom} \left({G, H}\right): M \left({u}\right) = \left[{u; \left \langle {c_m} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ Then $M$ is an [[Definition:Module Isomorphism|isomorphism of modules]], and: :$\forall u \in \operatorname{Hom} \left({F, G}\right), v \in \operatorname{Hom} \left({G, H}\right): \left[{v \circ u; \left \langle {c_m} \right \rangle, \left \langle {a_p} \right \rangle}\right] = \left[{v; \left \langle {c_m} \right \rangle, \left \langle {b_n} \right \rangle}\right] \left[{u; \left \langle {b_n} \right \rangle, \left \langle {a_p} \right \rangle}\right]$	1
Let $A$ be a [[Definition:Division Algebra|division algebra]] with [[Definition:Real Number|real]] [[Definition:Scalar (Vector Space)|scalars]]. Then the [[Definition:Dimension (Linear Algebra)|dimension]] of $A$ is either: :$1$: the [[Definition:Real Number|real numbers]] $\R$ :$2$: the [[Definition:Complex Number|complex numbers]] $\C$ :$4$: the [[Definition:Quaternion|quaternions]] $\Bbb H$ or: :$8$: the [[Definition:Octonion|octonions]] $\Bbb O$.	1
{{ProofWanted}} {{Namedfor|Carl Friedrich Gauss|name2 = Pierre Ossian Bonnet|cat = Gauss|cat2 = Bonnet}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $I$ be a [[Definition:Set|set]]. Let $R^{\paren I}$ be the [[Definition:Free Module on Set|free $R$-module on $I$]]. Then $R^{\paren I}$ is a [[Definition:Free Module|free $R$-module]].	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Let $\circ': G \times R \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: x \circ' \lambda = \lambda \circ x$ Then $\struct {G, +_G, \circ'}$ is a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$ {{iff}}: :$\forall \lambda, \mu \in R: \forall x \in G: \paren {\lambda \times_R \mu} \circ x = \paren {\mu \times_R \lambda} \circ x$	1
Let $S$ be a '''system of [[Definition:Homogeneous Simultaneous Linear Equations|homogeneous simultaneous linear equations]]''': :$\displaystyle \forall i \in \set {1, 2, \ldots, m}: \sum_{j \mathop = 1}^n \alpha_{i j} x_j = 0$ Consider the [[Definition:Trivial Solution to Homogeneous Simultaneous Linear Equations|trivial solution]] to $A$: :$\tuple {x_1, x_2, \ldots, x_n}$ such that: :$\forall j \in \set {1, 2, \ldots, n}: x_j = 0$ Then the [[Definition:Trivial Solution to Homogeneous Simultaneous Linear Equations|trivial solution]] is indeed a [[Definition:Solution to Simultaneous Linear Equations|solution]] to $S$.	1
Let $V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{vmatrix}$. By [[Multiple of Row Added to Row of Determinant]], we can subtract [[Definition:Row of Matrix|row]] 1 from each of the other rows and leave $V_n$ unchanged: :$V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 0 & x_2 - x_1 & x_2^2 - x_1^2 & \cdots & x_2^{n-2} - x_1^{n-2} & x_2^{n-1} - x_1^{n-1} \\ 0 & x_3 - x_1 & x_3^2 - x_1^2 & \cdots & x_3^{n-2} - x_1^{n-2} & x_3^{n-1} - x_1^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & x_{n-1} - x_1 & x_{n-1}^2 - x_1^2 & \cdots & x_{n-1}^{n-2} - x_1^{n-2} & x_{n-1}^{n-1} - x_1^{n-1} \\ 0 & x_n - x_1 & x_n^2 - x_1^2 & \cdots & x_n^{n-2} - x_1^{n-2} & x_n^{n-1} - x_1^{n-1} \end{vmatrix}$ Similarly without changing the value of $V_n$, we can subtract, in order: :$x_1$ times [[Definition:Column of Matrix|column]] $n-1$ from [[Definition:Column of Matrix|column]] $n$ :$x_1$ times [[Definition:Column of Matrix|column]] $n-2$ from [[Definition:Column of Matrix|column]] $n-1$ and so on, till we subtract: : $x_1$ times [[Definition:Column of Matrix|column]] $1$ from [[Definition:Column of Matrix|column]] $2$. The first row will vanish all apart from the first element $a_{11} = 1$. On all the other rows, we get, with new $i$ and $j$: :$a_{ij} = \left({x_i^{j-1} - x_1^{j-1}}\right) - \left({x_1 x_i^{j-2} - x_1^{j-1}}\right) = \left({x_i - x_1}\right) x_i^{j-2}$: :$V_n = \begin{vmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & x_2 - x_1 & \left({x_2 - x_1}\right) x_2 & \cdots & \left({x_2 - x_1}\right) x_2^{n-3} & \left({x_2 - x_1}\right) x_2^{n-2} \\ 0 & x_3 - x_1 & \left({x_3 - x_1}\right) x_3 & \cdots & \left({x_3 - x_1}\right) x_3^{n-3} & \left({x_3 - x_1}\right) x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & x_{n-1} - x_1 & \left({x_{n-1} - x_1}\right) x_{n-1} & \cdots & \left({x_{n-1} - x_1}\right) x_{n-1}^{n-3} & \left({x_{n-1} - x_1}\right) x_{n-1}^{n-2}\\ 0 & x_n - x_1 & \left({x_n - x_1}\right) x_n & \cdots & \left({x_n - x_1}\right) x_n^{n-3} & \left({x_n - x_1}\right) x_n^{n-2} \end{vmatrix}$ For all rows apart from the first, the $k$th row has the constant factor $\left({x_k - x_1}\right)$. So we can extract all these as factors, and from [[Determinant with Row Multiplied by Constant]], we get: :$\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) \begin{vmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & x_2 & \cdots & x_2^{n-3} & x_2^{n-2} \\ 0 & 1 & x_3 & \cdots & x_3^{n-3} & x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 1 & x_{n-1} & \cdots & x_{n-1}^{n-3} & x_{n-1}^{n-2}\\ 0 & 1 & x_n & \cdots & x_n^{n-3} & x_n^{n-2} \end{vmatrix}$ From [[Determinant with Unit Element in Otherwise Zero Row]], we can see that this directly gives us: :$\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) \begin{vmatrix} 1 & x_2 & \cdots & x_2^{n-3} & x_2^{n-2} \\ 1 & x_3 & \cdots & x_3^{n-3} & x_3^{n-2} \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & \cdots & x_{n-1}^{n-3} & x_{n-1}^{n-2}\\ 1 & x_n & \cdots & x_n^{n-3} & x_n^{n-2} \end{vmatrix}$ and it can be seen that: : $\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) V_{n-1}$ $V_2$, by the time we get to it (it will concern elements $x_{n-1}$ and $x_n$), can be calculated directly using the formula for calculating a [[Definition:Determinant of Order 2|Determinant of Order 2]]: : $V_2 = \begin{vmatrix} 1 & x_{n-1} \\ 1 & x_n \end{vmatrix} = x_n - x_{n-1}$ The result follows. {{qed}}	1
:$\displaystyle \lim_{n \mathop \to \infty} x_n = \lambda$	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\left({G, +_G, \circ}\right)_R$ and $\left({H, +_H, \circ}\right)_R$ be [[Definition:Module|$R$-modules]]. Let $\mathcal L_R \left({G, H}\right)$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Then $\mathcal L_R \left({G, H}\right)$ is a [[Definition:Submodule|submodule]] of the [[Definition:Module of All Mappings|$R$-module $H^G$]]. If $H$ is a [[Definition:Unitary Module|unitary module]], then so is $\mathcal L_R \left({G, H}\right)$.	1
Let $\struct {X, \norm {\, \cdot \,} }$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\map {B_1^-} 0$ be a [[Definition:Closed Unit Ball|closed unit ball]] in $X$. Then $\map {B_1^-} 0$ is [[Definition:Convex Set (Vector Space)|convex]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring |normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. Suppose $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $l \in R$, then: :$\exists K \in \N$ and $C \in \R_{>0}: \forall n > K: C < \norm {x_n - l}$	1
We have that: :$\forall a, b \in S: a +_S b = a + b$ :$\forall a, b \in S: a \times_S b = a \times b$ :$\forall a \in S: \forall x \in G = a \circ_S x = a \circ x$ as $+_S$, $\times_S$ and $\circ_S$ are [[Definition:Restriction of Operation|restrictions]]. Let us verify the [[Definition:Module Axioms|module axioms]]. === {{Module-axiom|1}} === We need to show that: :$\forall a \in S: \forall x, y \in G: a \circ_S \paren {x +_G y} = a \circ_S x +_G a \circ_S y$ We have: {{begin-eqn}} {{eqn | l = a \circ_S \paren {x +_G y} | r = a \circ \paren {x +_G y} }} {{eqn | r = a \circ x +_G a \circ y | c = {{Module-axiom|1}} on $\struct {G, +_G, \circ}_R$ }} {{eqn | r = a \circ_S x +_G a \circ_S y }} {{end-eqn}} {{qed|lemma}} === {{Module-axiom|2}} === We need to show that: :$\forall a,b \in S: \forall x \in G: \paren {a +_S b} \circ_S x = a \circ_S x +_G b \circ_S y$ We have: {{begin-eqn}} {{eqn | l = \paren {a +_S b} \circ_S x | r = \paren {a + b} \circ x }} {{eqn | r = a \circ x + b \circ x | c = {{Module-axiom|2}} on $\struct {G, +_G, \circ}_R$ }} {{eqn | r = a \circ_S x +_G b \circ_S x }} {{end-eqn}} {{qed|lemma}} === {{Module-axiom|3}} === We need to show that: :$\forall a, b \in S: \forall x \in G: \paren {a \times_S b} \circ_S x = a \circ_S \paren {b \circ_S x}$ We have: {{begin-eqn}} {{eqn | l = \paren {a \times_S b} \circ_S x | r = \paren {a \times b} \circ x }} {{eqn | r = a \circ \paren {b \circ x} | c = {{Module-axiom|3}} on $\struct {G, +_G, \circ}_R$ }} {{eqn | r = a \circ_S \paren {b \circ_S x} }} {{end-eqn}} {{qed|lemma}} Thus $\struct {G, +_G, \circ_S}_S$ is an [[Definition:Module|$S$-module]]. {{qed|lemma}} It remains to prove the final statement: If $\struct {G, +_G, \circ}_R$ is a [[Definition:Unitary Module|unitary $R$-module]] and $1_R \in S$, then $\struct {G, +_G, \circ_S}_S$ is also [[Definition:Unitary Module|unitary]]. To show that $\struct {G, +_G, \circ_S}_S$ is [[Definition:Unitary Module|unitary]], we must prove that: :$\forall x \in G: 1_R \circ_S x = x$ Since $1_R \in S$ by assumption, the [[Product Element|product]] $1_R \circ_S x$ is defined. We now have: {{begin-eqn}} {{eqn | l = 1_R \circ_S x | r = 1_R \circ x }} {{eqn | r = x | c = [[Definition:Unitary Module Axioms|Unitary Module Axiom $(4)$]] on $\struct {G, +_G, \circ}_R$ }} {{end-eqn}} {{qed}}	1
Let $e_2$ be the [[Definition:Elementary Row Operation|elementary row operation]] $\text {ERO} 2$: {{begin-axiom}} {{axiom | n = \text {ERO} 2 | t = For some $\lambda$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Row of Matrix|row]] $j$ to [[Definition:Row of Matrix|row]] $i$ | m = r_i \to r_i + \lambda r_j }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_2$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e_2$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_2$ is: :$\map \det {\mathbf E_2} = 1$	1
{{proof wanted|The results to assemble this proof from probably already exist somewhere, as should this proof itself in some format or other in the Linear Algebra category.}}	1
Let $\sequence {a_n}_{n \mathop \in \N}, \sequence {b_n}_{n \mathop \in \N}, \sequence {c_n}_{n \mathop \in \N} \in \ell^\infty$. Let $\lambda, \mu \in \C$. Let $\sequence 0 := \tuple {0, 0, 0, \dots}$ be a [[Definition:Complex-Valued Function|complex-valued function]]. Let us use [[Definition:Complex Number|real number]] [[Definition:Complex Addition|addition]] and [[Definition:Complex Multiplication|multiplication]]. Define [[Definition:Pointwise Addition on Ring of Sequences|pointwise addition]] as: :$\sequence {a_n + b_n}_{n \mathop \in \N} := \sequence {a_n}_{n \mathop \in \N} +_\C \sequence {b_n}_{n \mathop \in \N}$. Define [[Definition:Pointwise Scalar Multiplication on Ring of Sequences|pointwise scalar multiplication]] as: :$\sequence {\lambda \cdot a_n}_{n \mathop \in \N} := \lambda \times_\C \sequence {a_n}_{n \mathop \in \N}$ Let the [[Definition:Ring of Sequences/Additive Inverse|additive inverse]] be $\sequence {-a_n} := - \sequence {a_n}$. === Closure Axiom === By [[Definition:Assumption|assumption]], $\sequence {a_n}_{n \mathop \in \N}, \sequence {b_n}_{n \mathop \in \N} \in \ell^\infty$. By [[Definition:Space of Bounded Sequences|definition]]: :$\displaystyle \sup_{n \mathop \in \N} \size {a_n} < \infty$ :$\displaystyle \sup_{n \mathop \in \N} \size {b_n} < \infty$ Consider the [[Definition:Real Sequence|sequence]] $\sequence {a_n + b_n}$. Then: {{begin-eqn}} {{eqn| l = \sup_{n \mathop \in \N} \size {a_n + b_n} | o = \le | r = \sup_{n \mathop \in \N} \size {\size {a_n} + \size {b_n} } }} {{eqn| o = \le | r = \sup_{n \mathop \in \N} \size {\map \max {\size {a_n}, \size {b_n} } + \map \max {\size {a_n}, \size {b_n} } } | c = {{defof|Max Operation}} }} {{eqn| r = \sup_{n \mathop \in \N} \size {2 \map \max {\size {a_n}, \size {b_n} } } }} {{eqn | o = \le | r = 2 \paren {\sup_{n \mathop \in \N} \size {a_n} + \sup_{n \mathop \in \N} \size {b_n} } }} {{eqn | o = < | r = \infty | c = $\sequence {a_n}, \sequence {b_n} \in \ell^\infty$ }} {{end-eqn}} Hence: :$\sequence {a_n + b_n} \in \ell^\infty$ {{qed|lemma}} === Commutativity Axiom === By [[Pointwise Addition on Ring of Sequences is Commutative]], $\sequence {a_n} + \sequence {b_n} = \sequence {b_n} + \sequence {a_n}$ {{qed|lemma}} === Associativity Axiom === By [[Pointwise Addition on Ring of Sequences is Associative]], $\paren {\sequence {a_n} + \sequence {b_n} } + \sequence {c_n} = \sequence {a_n} + \paren {\sequence {b_n} + \sequence {c_n} }$. {{qed|lemma}} === Identity Axiom === {{begin-eqn}} {{eqn | l = \sequence {0 + a_n} | r = \sequence 0 +_\C \sequence {a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \tuple {0, 0, 0, \dots} +_\C \sequence {a_n} | c = Definition of $\sequence 0$ }} {{eqn | r = \sequence {a_n} }} {{end-eqn}} {{qed|lemma}} === Inverse Axiom === {{begin-eqn}} {{eqn | l = \sequence {a_n + \paren {-a_n} } | r = \sequence {a_n} +_\C \sequence {-a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \sequence {a_n} +_\C \paren {-1} \times_\C \sequence {a_n} | c = Definition of $\sequence {-a_n}$ }} {{eqn | r = 0 }} {{end-eqn}} {{qed|lemma}} === Distributivity over Scalar Addition === {{begin-eqn}} {{eqn | l = \sequence {\paren {\lambda +_\C \mu} \cdot a_n } | r = \paren {\lambda +_\C \mu} \times_\C \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \lambda \times_\C \sequence {a_n} +_\C \mu \times_\C \sequence {a_n} | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \sequence {\lambda \cdot a_n} +_\C \sequence {\mu \cdot a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot a_n + \mu \cdot a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Distributivity over Vector Addition === {{begin-eqn}} {{eqn | l = \lambda \times_\C \sequence {a_n + b_n} | r = \lambda \times_\C \paren {\sequence {a_n} +_\C \sequence {b_n} } | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \lambda \times_\C \sequence {a_n} +_\C \lambda \times_\C \sequence {b_n} | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \sequence {\lambda \cdot a_n} +_\C \sequence {\lambda \cdot b_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot a_n + \mu \cdot b_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Associativity with Scalar Multiplication === {{begin-eqn}} {{eqn | l = \sequence {\paren {\lambda \times_\C \mu} \cdot a_n} | r = \paren {\lambda \times_\C \mu} \times_\C \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \lambda \times_\C \paren {\mu \times_\C \sequence {a_n} } | c = [[Real Multiplication is Associative]] }} {{eqn | r = \lambda \times_\C \sequence {\mu \cdot a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot \paren {\mu \cdot a_n} } | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Identity for Scalar Multiplication === {{begin-eqn}} {{eqn | l = \sequence {1 \cdot a_n} | r = 1 \times_\C \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {a_n} }} {{end-eqn}} {{qed|lemma}} {{qed}}	1
Let $L \in X$. Then: :$\forall \epsilon_a \in \R_{> 0} : \exists N \in \N : \forall n \in \N : n > N \implies \norm {x_n - L}_a < \epsilon_a$ By [[Definition:Equivalence of Norms|equivalence of norms]]: :$\exists M \in \R_{> 0} : \norm {x_n - L}_b \le M \norm {x_n - L}_a < M \epsilon_a$ Let $\epsilon_b := M \epsilon_a$ Then: :$\forall \epsilon_b \in \R_{> 0}: \exists N \in \N : \forall n \in \N : n > N \implies \norm {x_n - L}_b < \epsilon_b$ Therefore, $\sequence{x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $L$ also in $M_b$. {{qed}}	1
Let $\epsilon > 0$ be given. By the definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence]] in $R$ with limit $l$: :$\exists N': \forall n > N', \norm {y_n - l} < \epsilon$ Hence: {{begin-eqn}} {{eqn | lo= \forall n > \paren {N' + N}: | l = \norm {x_n - l} | r = \norm {y_{n - N} - l} | c = $n > N$ }} {{eqn | o = < | r = \epsilon | c = $n - N > N'$ }} {{end-eqn}} The result follows. {{qed}}	1
The [[Definition:Edge of Polyhedron|edge]] of a [[Definition:Polyhedron|polyhedron]] has zero [[Definition:Curvature|curvature]].	1
Let $x \in S_1$. By the definition of [[Definition:Dense|dense subset]]: :$\map \cl {R_1} = S_1$ By [[Closure of Subset of Metric Space by Convergent Sequence]], there exists a [[Definition:Sequence|sequence]] $\sequence {x_n} \subseteq R_1 $ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$, that is: :$\displaystyle \lim_{n \mathop \to \infty} x_n = x$ By [[Isometric Image of Cauchy Sequence is Cauchy Sequence]], $\sequence {\map {\psi'} {x_n} }$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $R_2 \subseteq S_2$. Since $S_2$ is [[Definition:Completion (Metric Space)|complete]] then the [[Definition:Sequence|sequence]] $\sequence {\map {\psi'} {x_n} }$ has a [[Definition:Limit of Sequence|limit]], say $y$. Let $\sequence {x_n}$ and $\sequence {x'_n}$ be [[Definition:Sequence|sequences]] in $\map {\phi_1} R$ such that: :$\displaystyle \lim_{n \mathop \to \infty} x_n = \lim_{n \mathop \to \infty} x'_n = x$ Then: {{begin-eqn}} {{eqn | ll= | l = \lim_{n \mathop \to \infty} x_n - x'_n | r = \paren {\lim_{n \mathop \to \infty} x_n} - \paren {\lim_{n \mathop \to \infty} x'_n} | c = [[Combination Theorem for Sequences/Normed Division Ring/Difference Rule|Difference rule for convergent sequences]] }} {{eqn | ll= | r = x - x }} {{eqn | ll= | r = 0_{S_1} }} {{eqn | o = }} {{eqn | ll= \leadsto | r = \lim_{n \mathop \to \infty} \norm {x_n - x'_n}_1 | l = 0 | c = {{Defof|Convergent Sequence in Normed Division Ring}} }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {\map {\psi'} {x_n} - \map {\psi'} {x'_n} }_2 | c = {{Defof|Isometric Metric Spaces}} }} {{eqn | o = }} {{eqn | ll= \leadsto | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n} - \map {\psi'} {x'_n} | l = 0_{S_2} | c = {{Defof|Convergent Sequence in Normed Division Ring}} }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \map {\psi'} {x_n} } - \paren {\lim_{n \mathop \to \infty} \map {\psi'} {x'_n} } | c = [[Combination Theorem for Sequences/Normed Division Ring/Difference Rule|Difference rule for Convergent Sequences]] }} {{eqn | o = }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} \map {\psi'} {x_n} | r = \lim_{n \mathop \to \infty} \map {\psi'} {x'_n} }} {{end-eqn}} The result follows. {{qed}} [[Category:Completion of Normed Division Ring]] rzmal498jrtaydzxec6a2xtuequuibx	1
By [[Product of Ring Negatives]] then: :$-1_R \circ -1_R = 1_R \circ 1_R=1_R$. So: {{begin-eqn}} {{eqn | l = \norm{-1_R}^{2} | r = \norm{-1_R} \norm{-1_R} }} {{eqn | r = \norm{-1_R \circ -1_R} | c = [[Definition:Norm Axioms|Norm axiom (N2) (Multiplicativity)]] }} {{eqn | r = \norm{1_R} | c = [[Product of Ring Negatives]] }} {{eqn | r = 1 | c = [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity of Division Ring]] }} {{end-eqn}} Thus: :$\norm{-1_R} = \pm 1$ By the [[Definition:Norm Axioms|norm axiom (N1) (Positive defintiteness)]] then: :$\norm{-1_R} \ge 0$ Hence: :$\norm{-1_R} = 1$ {{qed}}	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number|complex numbers]]. Then: :$\overline {z_1} z_2 = \paren {z_1 \circ z_2} + i \paren {z_1 \times z_2}$ where: :$\overline {z_1}$ denotes the [[Definition:Complex Conjugate|complex conjugate]] of $z_1$ :$z_1 \circ z_2$ denotes the [[Definition:Complex Dot Product|complex dot product]] of $z_1$ with $z_2$ :$z_1 \times z_2$ denotes the [[Definition:Complex Cross Product|complex cross product]] of $z_1$ with $z_2$.	1
Let: :$\mathbf r: t \mapsto \left\langle{r_1 \left({t}\right), r_2 \left({t}\right), \ldots, r_n \left({t}\right)}\right\rangle$ be a [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued function]]. The [[Definition:Derivative of Vector-Valued Function|derivative]] of a [[Definition:Vector-Valued Function|vector-valued function]] can be calculated by differentiating each of its [[Definition:Vector-Valued Function/Component Function|component functions]]: :$\dfrac {\d \mathbf r \left({t}\right)} {\d t} = \left\langle{D_t r_1 \left({t}\right), D_t r_2 \left({t}\right), \ldots, D_t r_n \left({t}\right)}\right\rangle$	1
{{begin-eqn}} {{eqn | l = \frac {1_R} {\paren {a / b} } | r = 1_R / \paren {a \circ b^{-1} } | c = {{Defof|Division Product}} }} {{eqn | r = 1_R \circ \paren {a \circ b^{-1} }^{-1} | c = {{Defof|Division Product}} }} {{eqn | r = \paren {a \circ b^{-1} }^{-1} | c = {{Defof|Identity Element}} of $\circ$ }} {{eqn | r = \paren {\frac a b}^{-1} | c = {{Defof|Division Product}} }} {{eqn | r = \paren {a \circ b^{-1} }^{-1} | c = {{Defof|Division Product}} }} {{eqn | r = \paren {b^{-1} }^{-1} \circ a^{-1} | c = [[Inverse of Group Product]] }} {{eqn | r = b \circ a^{-1} | c = [[Inverse of Group Inverse]] }} {{eqn | r = \frac b a | c = {{Defof|Division Product}} }} {{end-eqn}} {{qed}}	1
Let $\struct {\R^n, +, \cdot}_\R$ be a [[Definition:Real Vector Space|real vector space]]. Let $S \subseteq \R^n$. Then $S$ is a '''linearly independent set of real vectors''' if every [[Definition:Finite Sequence|finite]] [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. That is, such that: :$\displaystyle \forall \set {\lambda_k: 1 \le k \le n} \subseteq \R: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0 \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$ where $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ are distinct elements of $S$.	1
Let $\map {\MM_R} {m, n}$ denote the [[Definition:Matrix Space|$m \times n$ metric space]] over $R$. Let $I_n$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Then: :$\forall \mathbf A \in \map {\MM_R} {m, n}: \mathbf A \mathbf I_n = \mathbf A$	1
Let $\GF$ be a [[Definition:Galois Field|Galois field]] with $p$ [[Definition:Element|elements]]: $\card \GF = p$. Let $A = \sqbrk {a_{i j} }_{n, n}$ be a [[Definition:Square Matrix|matrix]] such that $\size A \ne 0$ and $a_{i j} \in \GF$. How many such matrices can be constructed? In order to avoid a zero determinant, the top row of the matrix, $\sequence {a_{1 j} }_{j \mathop = 1, \dotsc, n}$ must have at least one non-zero element. Therefore there are $p^n - 1$ possibilities for the top row: :the $p^n$ possible sequences of $n$ values from $\GF$, minus the one [[Definition:Sequence|sequence]] $\paren {0, 0, \dotsc, 0}$. The only restriction on the second row is that it not be a multiple of the first. Therefore, there are the $p^n$ possible [[Definition:Sequence|sequences]] again, minus the $p$ [[Definition:Sequence|sequences]] which are multiples of the first row. Thus, continuing in this fashion, the $j^{th}$ row can be any of the $p^n$ possible [[Definition:Sequence|sequences]], minus the $p^{\paren {j - 1} }$ [[Definition:Sequence|sequences]] which are linear combinations of previous rows. The number of possible matrices satisfying the conditions of $A$, then, is: :$\displaystyle \prod_{j \mathop = 1}^n \paren {p^n - p^{j - 1} }$ {{qed}} [[Category:Combinatorics]] [[Category:General Linear Group]] [[Category:Galois Fields]] 5dqxuyc8lf2t1unfa2x8z86s9hwle4c	1
By definition, a [[Definition:Unit Matrix|unit matrix]] is a [[Definition:Diagonal Matrix|diagonal matrix]]. From [[Inverse of Diagonal Matrix]], the [[Definition:Inverse Matrix|inverse]] of a [[Definition:Diagonal Matrix|diagonal matrix]]: :$\mathbf D = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_{nn} \\ \end{bmatrix}$ is the [[Definition:Diagonal Matrix|diagonal matrix]]: : $\mathbf D^{-1} = \begin{bmatrix} \dfrac 1 {a_{11}} & 0 & \cdots & 0 \\ 0 & \dfrac 1 {a_{22}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \dfrac 1 {a_{nn}} \\ \end{bmatrix}$ When $\mathbf D$ is the [[Definition:Unit Matrix|unit matrix]] $\mathbf I_n$, all elements $a_{kk}$ are equal to $1$: :$\mathbf I_n = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \\ \end{bmatrix}$ Hence: :$\mathbf I_n^{-1} = \begin{bmatrix} \dfrac 1 1 & 0 & \cdots & 0 \\ 0 & \dfrac 1 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \dfrac 1 1 \\ \end{bmatrix}$ Hence the result. {{qed}} [[Category:Unit Matrices]] [[Category:Inverse Matrices]] rgio764lguz6z2g6wb4yn16z1brin39	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]], and let $A: H \to K$ be a [[Definition:Linear Transformation|linear transformation]]. Then the following four statements are equivalent: :$(1): \quad A$ is [[Definition:Continuous Mapping (Topology)|continuous]] :$(2): \quad A$ is [[Definition:Continuous at Point of Topological Space|continuous]] at $\mathbf 0_H$ :$(3): \quad A$ is [[Definition:Continuous at Point of Topological Space|continuous]] at some point :$(4): \quad \exists c > 0: \forall h \in H: \norm {\map A h}_K \le c \norm h_H$	1
$A = \left({A_F, \oplus}\right)$ be a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]] which is also an [[Definition:Alternative Algebra|alternative algebra]]. Then $A$ is a [[Definition:Normed Division Algebra|normed division algebra]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $\left({G, +_G, \circ}\right)_R$ be a [[Definition:Unitary Module|unitary $R$-module]]. === [[Definition:Basis of Module/Definition 1|Definition 1]] === {{:Definition:Basis of Module/Definition 1}} === [[Definition:Basis of Module/Definition 2|Definition 2]] === {{:Definition:Basis of Module/Definition 2}}	1
Then $\NN$ is a [[Definition:Maximal Ideal of Ring|maximal ring ideal]] of $\CC$.	1
Let: : $\mathcal A = \left \langle {a_m} \right \rangle$ : $\mathcal B = \left \langle {b_n} \right \rangle$ : $\mathcal C = \left \langle {c_p} \right \rangle$ Let: : $\left[{\alpha}\right]_{m n} = \left[{f; \left \langle {b_n} \right \rangle, \left \langle {a_m} \right \rangle}\right]$ and: : $\left[{\beta}\right]_{n p} = \left[{g; \left \langle {c_p} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ Then: {{begin-eqn}} {{eqn | l = \left({g \circ f}\right) \left({a_j}\right) | r = v \left({f \left({a_j}\right)}\right) | c = }} {{eqn | r = v \left({\sum_{k \mathop = 1}^n \alpha_{k j} b_k}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \alpha_{k j} v \left({b_k}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \alpha_{k j} \left({\sum_{i \mathop = 1}^p \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \left({\sum_{i \mathop = 1}^p \alpha_{k j} \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{i \mathop = 1}^p \left({\sum_{k \mathop = 1}^n \alpha_{k j} \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{i \mathop = 1}^p \left({\sum_{k \mathop = 1}^n \alpha_{k j} \beta_{i k} }\right) c_i | c = }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf b \times \mathbf a | r = \begin {bmatrix} b_i \\ b_j \\ b_k \end {bmatrix} \times \begin {bmatrix} a_i \\ a_j \\ a_k \end {bmatrix} }} {{eqn | r = \begin {bmatrix} b_j a_k - a_j b_k \\ b_k a_i - b_i a_k \\ b_i a_j - a_i b_j \end {bmatrix} }} {{eqn | l = \mathbf a \times \mathbf b | r = \begin {bmatrix} a_i \\ a_j \\ a_k \end {bmatrix} \times \begin {bmatrix} b_i \\ b_j \\ b_k \end {bmatrix} }} {{eqn | r = \begin {bmatrix} a_j b_k - a_k b_j \\ a_k b_i - a_i b_k \\ a_i b_j - a_j b_i \end {bmatrix} }} {{eqn | r = \begin {bmatrix} -\paren {a_k b_j - a_j b_k} \\ -\paren {a_i b_k - a_k b_i} \\ -\paren {a_j b_i - a_i b_j} \end {bmatrix} }} {{eqn | r = -1 \begin {bmatrix} b_j a_k - a_j b_k \\ b_k a_i - b_i a_k \\ b_i a_j - a_i b_j \end {bmatrix} }} {{eqn | r = -\paren {\mathbf b \times \mathbf a} }} {{end-eqn}} {{qed}}	1
By [[Null Sequences form Maximal Left and Right Ideal]] then $\NN$ is an [[Definition:Ideal of Ring|ideal]] of the [[Definition:Ring of Sequences|ring $\CC$]] that is also a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]]. By [[Maximal Left and Right Ideal iff Quotient Ring is Division Ring]] then the [[Definition:Quotient Ring|quotient ring]] $\CC / \NN$ is a [[Definition:Division Ring|division ring]] {{qed}}	1
Let $f$ be a [[Definition:Real Function|real function]] which is [[Definition:Continuous on Interval|continuous]] on the [[Definition:Closed Real Interval|closed interval]] $\closedint a b$. Then: :$\displaystyle \size {\int_a^b \map f t \rd t} \le \int_a^b \size {\map f t} \rd t$	1
Let $\mathbf a$ and $\mathbf b$ be expressed in [[Definition:Component of Vector|component form]]: {{begin-eqn}} {{eqn | l = \mathbf a | r = a_1 \mathbf e_1 + a_2 \mathbf e_2 + \cdots + a_n \mathbf e_n | c = }} {{eqn | l = \mathbf b | r = b_1 \mathbf e_1 + b_2 \mathbf e_2 + \cdots + b_n \mathbf e_n | c = }} {{end-eqn}} where $\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n$ denote the [[Definition:Unit Vector|unit vectors]] in the [[Definition:Positive Direction|positive directions]] of the [[Definition:Coordinate Axis|coordinate axes]] of the [[Definition:Cartesian Coordinate System|Cartesian coordinate space]] into which $\mathbf a$ has been embedded. Thus $\mathbf a$ and $\mathbf b$ can be expressed as: {{begin-eqn}} {{eqn | l = \mathbf a | r = \tuple {a_1, a_2, \ldots, a_n} | c = }} {{eqn | l = \mathbf b | r = \tuple {b_1, b_2, \ldots, b_n} | c = }} {{end-eqn}} We have that: {{begin-eqn}} {{eqn | l = \size {\mathbf a} | r = \size {\tuple {a_1, a_2, \ldots, a_n} } | c = }} {{eqn | r = \sqrt {\paren {a_1^2 + a_2^2 + \ldots + a_n^2} } | c = }} {{end-eqn}} and similarly: {{begin-eqn}} {{eqn | l = \size {\mathbf b} | r = \size {\tuple {b_1, b_2, \ldots, b_n} } | c = }} {{eqn | r = \sqrt {\paren {b_1^2 + b_2^2 + \ldots + b_n^2} } | c = }} {{end-eqn}} Also: {{begin-eqn}} {{eqn | l = \hat {\mathbf a} | r = \widehat {\tuple {a_1, a_2, \ldots, a_n} } | c = }} {{eqn | r = \dfrac 1 {\sqrt {\paren {a_1^2 + a_2^2 + \ldots + a_n^2} } } \mathbf a | c = }} {{end-eqn}} and similarly: {{begin-eqn}} {{eqn | l = \hat {\mathbf b} | r = \widehat {\tuple {b_1, b_2, \ldots, b_n} } | c = }} {{eqn | r = \dfrac 1 {\sqrt {\paren {b_1^2 + b_2^2 + \ldots + b_n^2} } } | c = }} {{end-eqn}} Let $\mathbf a = \mathbf b$. Then by [[Equality of Ordered Tuples]]: :$(1): \quad a_1 = b_1, a_2 = b_2, \ldots a_n = b_n$ Then: {{begin-eqn}} {{eqn | l = \size {\mathbf a} | r = \sqrt {\paren {a_1^2 + a_2^2 + \ldots + a_n^2} } | c = }} {{eqn | r = \sqrt {\paren {b_1^2 + b_2^2 + \ldots + b_n^2} } | c = from $(1)$ }} {{eqn | r = \size {\mathbf b} | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \hat {\mathbf a} | r = \dfrac 1 {\sqrt {\paren {a_1^2 + a_2^2 + \ldots + a_n^2} } } \mathbf a | c = }} {{eqn | r = \dfrac 1 {\sqrt {\paren {b_1^2 + b_2^2 + \ldots + b_n^2} } } \mathbf b | c = from $(1)$ }} {{eqn | r = \hat {\mathbf b} | c = }} {{end-eqn}} {{Finish|The other direction now needs to be attended to.}}	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\operatorname O \left({n, k}\right)$ be the $n$th [[Definition:Orthogonal Group|orthogonal group]] on $k$. Let $\operatorname{GL} \left({n, k}\right)$ be the $n$th [[Definition:General Linear Group|general linear group]] on $k$. Then $\operatorname O \left({n, k}\right)$ is a [[Definition:Subgroup|subgroup]] of $\operatorname{GL} \left({n, k}\right)$.	1
By definition, a [[Definition:Basis of Module|basis]] is a [[Definition:Generator of Module|generator]]. By [[Basis of Free Module is No Greater than Generator]], there exist: :an [[Definition:Injection|injection]] $f : B \to C$ :an [[Definition:Injection|injection]] $g : C \to B$ By the [[Cantor-Bernstein-Schröder Theorem]], $B$ and $C$ are [[Definition:Set Equivalence|equivalent]]. {{qed}}	1
By [[Characterisation of Non-Archimedean Division Ring Norms]] then: :$\norm {\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] {{iff}}: ::$\sup \set {\norm {n \cdot 1_R}: n \in \N_{\gt 0}} \le 1$ By [[Properties of Norm on Division Ring/Norm of Unity|norm of unity]] then: :$\norm {1_R} = 1$ The result follows. {{qed}}	1
Let $\norm {\, \cdot \,}$ be a [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Archimedean Division Ring Norm|Archimedean]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Then $\norm {\, \cdot \,}$ is [[Definition:Equivalent Division Ring Norms|equivalent]] to the [[Definition:Absolute Value|absolute value]] $\size {\, \cdot \,}$.	1
{{begin-eqn}} {{eqn | l = x \in U_D | o = \leadsto | r = \exists u \in U_D: u \in \ideal x | c = }} {{eqn | o = \leadsto | r = \ideal x = D | c = [[Ideal of Unit is Whole Ring]] }} {{end-eqn}} Conversely: {{begin-eqn}} {{eqn | l = \ideal x = D | o = \leadsto | r = 1_D \in \ideal x | c = }} {{eqn | o = \leadsto | r = \exists t \in D: 1 = t x | c = }} {{eqn | o = \leadsto | r = u \in U_D | c = }} {{end-eqn}} {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm{\,\cdot\,}$, Let $x, y \in R$ and $\norm x \lt \norm y$. Then: :$\norm {x + y} = \norm {x - y} = \norm {y - x} = \norm y$	1
Let $M$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $S = \left\langle{m_i}\right\rangle_{i \mathop \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Element|elements]] of $M$. Let $\Psi : R^{\left({I}\right)} \to M$ be the [[Definition:Module Homomorphism|module homomorphism]] given by [[Universal Property of Free Module on Set]]. Then the following are equivalent: :$S$ [[Definition:Linearly Independent|linearly independent]] :$\Psi$ is [[Definition:Injection|injective]].	1
A '''linear integral operator''' is a [[Definition:Integral Operator|integral operator]] $\mathscr L$ with the property that: :$\map {\mathscr L} {\alpha \phi_1 + \beta \phi_2} = \alpha \map {\mathscr L} {\phi_1} + \beta \map {\mathscr L} {\phi_2}$ Thus if $\phi_1$ and $\phi_2$ are [[Definition:Solution to Integral Equation|solutions]] to the [[Definition:Integral Equation|integral equation]] $\map {\mathscr L} {\phi_i} = 0$, then so is any [[Definition:Linear Combination|linear combination]] of $\phi_1$ and $\phi_2$.	1
Let $z = a + i b \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\overline z$ denote the [[Definition:Complex Conjugate|complex conjugate]] of $z$. Then: :$z \overline z = a^2 + b^2 = \cmod z^2$ and thus is [[Definition:Wholly Real|wholly real]].	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]] over a [[Definition:Field (Abstract Algebra)|field]]. Let $\lambda: \N_{> 0} \to \N_{> 0}$ be any fixed [[Definition:Permutation on n Letters|permutation on $\N_{> 0}$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\struct {S_n, \circ}$ be the [[Definition:Symmetric Group on n Letters|symmetric group of $n$ letters]]. Then: :$\displaystyle \map \det {\mathbf A} = \sum_{\mu \mathop \in S_n} \paren {\map \sgn \mu \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k, \map \mu k} }$ :$\displaystyle \map \det {\mathbf A} = \sum_{\mu \mathop \in S_n} \paren {\map \sgn \mu \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \mu k, \map \lambda k} }$ where: :the summation $\displaystyle \sum_{\mu \mathop \in S_n}$ goes over all the $n!$ [[Definition:Permutation on n Letters|permutations]] of $\set {1, 2, \ldots, n}$ :$\map \sgn \mu$ is the [[Definition:Sign of Permutation|sign of the permutation]] $\mu$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\C$. Let $A \in \map B H$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Then $A$ is [[Definition:Self-Adjoint Operator|self-adjoint]] {{iff}}: :$\forall h \in H: \innerprod {A h} h_H \mathop \in \R$	1
Let $x, y \in S_1$. Let $\sequence {x_n}$ and $\sequence {y_n}$ be [[Definition:Sequence|sequences]] in $R_1$ such that $\displaystyle \lim_{n \mathop \to \infty} x_n = x, \lim_{n \mathop \to \infty} y_n = y$. Then: {{begin-eqn}} {{eqn | l = x - y | r = \lim_{n \mathop \to \infty} x_n - y_n | c = [[Combination Theorem for Sequences/Normed Division Ring/Difference Rule|Difference Rule for Convergent Sequences]] }} {{eqn | o = }} {{eqn | ll= \leadsto | l = \norm {x - y} | r = \lim_{n \mathop \to \infty} \norm {x_n - y_n} | c = [[Modulus of Limit/Normed Division Ring|Modulus of Limit]] }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {\map {\psi'} {x_n} - \map {\psi'} {y_n} } | c = {{Defof|Isometry (Metric Spaces)}} }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | l = \map \psi x - \map \psi y | r = \paren {\lim_{n \mathop \to \infty} \map {\psi'} {x_n} } - \paren {\lim_{n \mathop \to \infty} \map {\psi'} {y_n} } | c = Definition of $\psi$ }} {{eqn | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n} - \map {\psi'} {y_n} | c = [[Combination Theorem for Sequences/Normed Division Ring/Difference Rule|Difference Rule for Convergent Sequences]] }} {{eqn | o = }} {{eqn | ll= \leadsto | l = \norm {\map \psi x - \map \psi y} | r = \lim_{n \mathop \to \infty} \norm {\map {\psi'} {x_n} - \map {\psi'} {y_n} } | c = [[Modulus of Limit/Normed Division Ring|Modulus of Limit]] }} {{end-eqn}} By [[Convergent Sequence in Metric Space has Unique Limit]]: :$\norm {x - y} = \norm {\map \psi x - \map \psi y}$ It follows that $\psi$ is [[Definition:Distance-Preserving Mapping|distance-preserving]]. By [[Distance-Preserving Surjection is Isometry of Metric Spaces]] then $\psi$ is an [[Definition:Isometry (Metric Spaces)|isometry]]. {{qed}} [[Category:Completion of Normed Division Ring]] powpdrlvt5p4znr4rmcsoi3ipftmw7j	1
Checking in turn each of the critera for [[Definition:Equivalence Relation|equivalence]]: === Reflexive === $\mathbf A = \mathbf{I_n}^{-1} \mathbf A \mathbf{I_n}$ trivially, for all order $n$ [[Definition:Square Matrix|square matrices]] $\mathbf A$. So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetric === Let $\mathbf B = \mathbf P^{-1} \mathbf A \mathbf P$. As $\mathbf P$ is [[Definition:Invertible Matrix|invertible]], we have: {{begin-eqn}} {{eqn | l = \mathbf P \mathbf B \mathbf P^{-1} | r = \mathbf P \mathbf P^{-1} \mathbf A \mathbf P \mathbf P^{-1} | c = }} {{eqn | r = \mathbf{I_n} \mathbf A \mathbf{I_n} | c = }} {{eqn | r = \mathbf A | c = }} {{end-eqn}} So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitive === Let $\mathbf B = \mathbf P_1^{-1} \mathbf A \mathbf P_1$ and $\mathbf C = \mathbf P_2^{-1} \mathbf B \mathbf P_2$. Then $\mathbf C = \mathbf P_2^{-1} \mathbf P_1^{-1} \mathbf A \mathbf P_1 \mathbf P_2$. The result follows from the definition of [[Definition:Invertible Matrix|invertible matrix]], that the product of two invertible matrices is itself invertible. So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} So, by definition, [[Definition:Matrix Similarity|matrix similarity]] is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $A$. Let $v \in H$ be an [[Definition:Eigenvector|eigenvector]] for $\lambda$; i.e. $Av = \lambda v$. Now compute: {{begin-eqn}} {{eqn|l = \lambda \left\langle{v, v}\right\rangle |r = \left\langle{\lambda v, v}\right\rangle |c = Property $(2)$ of an [[Definition:Inner Product|inner product]] }} {{eqn|r = \left\langle{Av, v}\right\rangle |c = $v$ is an [[Definition:Eigenvector|eigenvector]] }} {{eqn|r = \left\langle{v, Av}\right\rangle |c = $A$ is [[Definition:Self-Adjoint Operator|self-adjoint]] }} {{eqn|r = \overline{\left\langle{Av, v}\right\rangle} |c = Property $(1)$ of an [[Definition:Inner Product|inner product]] }} {{eqn|r = \overline{\left\langle{\lambda v, v}\right\rangle} |c = $v$ is an [[Definition:Eigenvector|eigenvector]] }} {{eqn|r = \bar \lambda \left\langle{v, v}\right\rangle |c = Properties $(2)$, $(4)$ of an [[Definition:Inner Product|inner product]] }} {{end-eqn}} Now $v$, being an [[Definition:Eigenvector|eigenvector]], is [[Definition:Zero Vector|non-zero]]. By property $(5)$ of an [[Definition:Inner Product|inner product]], this implies $\left\langle{v, v}\right\rangle \ne 0$. Thence, dividing out $\left\langle{v, v}\right\rangle$, obtain $\lambda = \bar \lambda$. From [[Complex Number equals Conjugate iff Wholly Real]], $\lambda \in \R$. {{qed}}	1
Let $\mathcal L$ be a [[Definition:Straight Line|straight line]] which [[Definition:Intercept|intercepts]] the [[Definition:X-Axis|$x$-axis]] and [[Definition:Y-Axis|$y$-axis]] respectively at $\tuple {a, 0}$ and $\tuple {0, b}$, where $a b \ne 0$. Then $\mathcal L$ can be described by the equation: :$\dfrac x a + \dfrac y a = 1$	1
The '''vector space axioms''' consist of the [[Definition:Abelian Group Axioms|abelian group axioms]]: {{begin-axiom}} {{axiom | n = \text V 0 | lc= [[Definition:Closed Algebraic Structure|Closure]] Axiom | q = \forall \mathbf x, \mathbf y \in G | m = \mathbf x +_G \mathbf y \in G }} {{axiom | n = \text V 1 | lc= [[Definition:Commutative Operation|Commutativity]] Axiom | q = \forall \mathbf x, \mathbf y \in G | m = \mathbf x +_G \mathbf y = \mathbf y +_G \mathbf x }} {{axiom | n = \text V 2 | lc= [[Definition:Associative Operation|Associativity]] Axiom | q = \forall \mathbf x, \mathbf y, \mathbf z \in G | m = \paren {\mathbf x +_G \mathbf y} +_G \mathbf z = \mathbf x +_G \paren {\mathbf y +_G \mathbf z} }} {{axiom | n = \text V 3 | lc= [[Definition:Identity Element|Identity]] Axiom | q = \exists \mathbf 0 \in G: \forall \mathbf x \in G | m = \mathbf 0 +_G \mathbf x = \mathbf x = \mathbf x +_G \mathbf 0 }} {{axiom | n = \text V 4 | lc= [[Definition:Inverse Element|Inverse]] Axiom | q = \forall \mathbf x \in G: \exists \paren {-\mathbf x} \in G | m = \mathbf x +_G \paren {-\mathbf x} = \mathbf 0 }} {{end-axiom}} together with the properties of a [[Definition:Unitary Module|unitary module]]: {{begin-axiom}} {{axiom | n = \text V 5 | lc= [[Definition:Distributive Operation|Distributivity]] over [[Definition:Scalar Addition (Vector Space)|Scalar Addition]] | q = \forall \lambda, \mu \in K: \forall \mathbf x \in G | m = \paren {\lambda + \mu} \circ \mathbf x = \lambda \circ \mathbf x +_G \mu \circ \mathbf x }} {{axiom | n = \text V 6 | lc= [[Definition:Distributive Operation|Distributivity]] over [[Definition:Vector Addition on Vector Space|Vector Addition]] | q = \forall \lambda \in K: \forall \mathbf x, \mathbf y \in G | m = \lambda \circ \paren {\mathbf x +_G \mathbf y} = \lambda \circ \mathbf x +_G \lambda \circ \mathbf y }} {{axiom | n = \text V 7 | lc= [[Definition:Associative Operation|Associativity]] with [[Definition:Scalar Multiplication on Vector Space|Scalar Multiplication]] | q = \forall \lambda, \mu \in K: \forall \mathbf x \in G | m = \lambda \circ \paren {\mu \circ \mathbf x} = \paren {\lambda \cdot \mu} \circ \mathbf x }} {{axiom | n = \text V 8 | lc= [[Definition:Identity Element|Identity]] for [[Definition:Scalar Multiplication on Vector Space|Scalar Multiplication]] | q = \forall \mathbf x \in G | m = 1_K \circ \mathbf x = \mathbf x }} {{end-axiom}}	1
By [[Monomorphism Image is Isomorphic to Domain]], $\phi_1:R \to \map {\phi_1} R$ and $\phi_2:R \to \map {\phi_2} R$ are [[Definition:Ring Isomorphism|ring isomorphisms]]. By [[Distance-Preserving Image Isometric to Domain for Metric Spaces]], $\phi_1:R \to \map {\phi_1} R$ and $\phi_2:R \to \map {\phi_2} R$ are [[Definition:Isometry (Metric Spaces)|isometries]]. By [[Inverse of Algebraic Structure Isomorphism is Isomorphism]], $\phi_1^{-1}:\map {\phi_1} R \to R$ is a [[Definition:Ring Isomorphism|ring isomorphism]]. By [[Inverse of Isometry of Metric Spaces is Isometry]], $\phi_1^{-1}:\map {\phi_1} R \to R$ is a [[Definition:Ring Isomorphism|ring isomorphism]] is an [[Definition:Isometry (Metric Spaces)|isometry]]. By [[Composition of Ring Isomorphisms is Ring Isomorphism]], $\psi'$ is a [[Definition:Ring Isomorphism|ring isomorphism]]. By [[Composition of Isometries is Isometry]], $\psi'$ is an [[Definition:Isometry (Metric Spaces)|isometry]]. {{qed}} [[Category:Completion of Normed Division Ring]] b6j5h8yb3sr3hlymhfcig8m9vaqko0w	1
By definition, an [[Definition:Orthogonal Matrix|orthogonal matrix]] is one such that: :$\mathbf Q^\intercal = \mathbf Q^{-1}$ and so the result follows by definition of [[Definition:Inverse Matrix|inverse]]. {{qed}}	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ be [[Definition:Equivalent Division Ring Norms|equivalent]]. By [[Definition:Equivalent Division Ring Norms/Norm is Power of Other Norm|Norm is Power of Other Norm]] then: :$\exists \alpha \in \R_{\gt 0}: \forall q \in \Q: \norm q_1 = \norm q_2^\alpha$ In particular: :$\exists \alpha \in \R_{\gt 0}: \forall n \in \N: \norm n_1 = \norm n_2^\alpha$	1
Let $\displaystyle \bigoplus_{i \mathop \in I} M_i$ be the [[Definition:Direct Sum of Modules|external direct sum]] of $\left\langle{M_i}\right\rangle_{i \mathop \in I}$. $M$ is the '''internal direct sum''' of $\left\langle{M_i}\right\rangle_{i \mathop \in I}$ {{iff}} the [[Definition:Mapping|mapping]] given by [[Universal Property of Direct Sum of Modules]] is an [[Definition:Module Isomorphism|isomorphism]] onto $M$.	1
Let: :$\mathbf D = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_{nn} \\ \end{bmatrix}$ be an $n \times n$ [[Definition:Diagonal Matrix|diagonal matrix]]. Then its [[Definition:Inverse Matrix|inverse]] is given by: : $\mathbf D^{-1} = \begin{bmatrix} \dfrac 1 {a_{11}} & 0 & \cdots & 0 \\ 0 & \dfrac 1 {a_{22}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \dfrac 1 {a_{nn}} \\ \end{bmatrix}$ provided that none of the [[Definition:Diagonal Element|diagonal elements]] are zero. If any of the diagonal elements are zero, $\mathbf D$ is not [[Definition:Invertible Matrix|invertible]]. {{expand|If any diagonal element isn't a [[Definition:Unit of Ring|unit]]}}	1
Let $0_D$ and $1_D$ be the [[Definition:Ring Zero|zero]] and [[Definition:Unity of Ring|unity]] respectively of $D$. Let $J$ be the [[Definition:Set|set]] of all [[Definition:Linear Combination|linear combinations]] in $D$ of $\set {a_1, a_2, \dotsc, a_n}$. From [[Set of Linear Combinations of Finite Set of Elements of Principal Ideal Domain is Principal Ideal]]: :$J = \ideal x$ for some $x \in D$, where $\ideal x$ denotes the [[Definition:Principal Ideal of Ring|principal ideal]] [[Definition:Generator of Ideal|generated]] by $x$. We have that each $a_i$ can be expressed as a [[Definition:Linear Combination|linear combination]] of $\set {a_1, a_2, \dotsc, a_n}$: :$a_i = 0_D a_1 + 0_d a_2 + \dotsb + 1_D a_i + \dotsb + 0_D a_n$ Thus: :$\forall i \in \set {0, 1, \dotsc, n}: a_i \in J$ and so by definition of $J$: :$\forall i \in \set {0, 1, \dotsc, n}: a_i = t_i x$ for some $t_i \in D$. Thus $x$ is a [[Definition:Common Divisor of Ring Elements|common divisor]] of $a_1, a_2, \dotsc, a_n$. As $x \in \ideal x = J$, we have: :$x = c_1 a_1 + c_2 a_2 + \dotsb + c_n a_n$ for some $c_1, c_2, \dotsc, c_n \in D$. Thus every [[Definition:Common Divisor of Ring Elements|common divisor]] of $a_1, a_2, \dotsc, a_n$ also is a [[Definition:Divisor of Ring Element|divisor]] of $x$. Thus $x$ is a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] of $a_1, a_2, \dotsc, a_n$. {{qed}}	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Then $D$ cannot have an [[Definition:Infinite Sequence|infinite sequence]] of [[Definition:Ideal of Ring|ideals]] $\sequence {j_n}_{n \mathop \in \N}$ such that: :$\forall n \in \N: J_n \subsetneq j_{n + 1}$	1
A [[Definition:Straight Line|straight line]] $\mathcal L$ is the [[Definition:Set|set]] of all $\tuple {x, y} \in \R^2$, where: :$\alpha_1 x + \alpha_2 y = \beta$ where $\alpha_1, \alpha_2, \beta \in \R$ are given, and not both $\alpha_1, \alpha_2$ are [[Definition:Zero (Number)|zero]].	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $J \subseteq R$ be an [[Definition:Ideal of Ring|ideal]] of $R$. Let $\circ_l : R \times J \to J$ be the [[Definition:Restriction of Mapping|restriction]] of $\times$ to $R \times J$. Let $\circ_r : J \times R \to J$ be the [[Definition:Restriction of Mapping|restriction]] of $\times$ to $J \times R$. Then $\struct {J, +, \circ_l, \circ_r}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +, \times}$.	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]] $0$. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by $\norm {\,\cdot\,}$]]. Then: :$\norm {\, \cdot \,}$ is a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] {{iff}} $d$ is a [[Definition:Non-Archimedean Metric|non-Archimedean metric]].	1
The [[Definition:Valuation Ring Induced by Non-Archimedean Norm|valuation ring]] $\mathcal O$ Is the [[Definition:Open Ball|open ball]] ${B_1}^- \paren {0_R}$ by definition. By [[Topological Properties of Non-Archimedean Division Rings/Open Balls are Clopen|Open Balls of Non-Archimedean Division Rings are Clopen]] then $\mathcal O$ is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\norm {\,\cdot\,}$. {{qed}} [[Category:Normed Division Rings]] [[Category:Valuation Ring of Non-Archimedean Division Ring is Clopen]] 3mq7moxsruz5gxr2j57b2c8kntv45zv	1
Let $A$ be a [[Definition:Noetherian Ring|Noetherian ring]]. Let $B$ be a [[Definition:Finitely Generated Algebra|finitely generated]] [[Definition:Unital Associative Commutative Algebra|algebra]] over $A$. Then $B$ is [[Definition:Noetherian Ring|Noetherian]].	1
Let $M$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $\mathcal B = \left\langle{b_i}\right\rangle_{i \mathop \in I}$ be a [[Definition:Indexed Family|family]] of elements of $M$. Let $\Psi: R^{\left({I}\right)} \to M$ be the morphism given by [[Universal Property of Free Module on Set]]. Then the following are equivalent: :$\mathcal B$ is a [[Definition:Basis of Module|basis]] of $M$ :$\Psi$ is an [[Definition:Module Isomorphism|isomorphism]]	1
Let $x \in \R$ be a [[Definition:Real Number|real number]]. Then the [[Definition:Complex Modulus|complex modulus]] of $x$ equals the [[Definition:Absolute Value of Real Number|absolute value]] of $x$.	1
Let $M = \struct{X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Then the [[Definition:Set|set]] $X$ is an [[Definition:Open Set in Normed Vector Space|open set]] of $M$.	1
The [[Definition:Vector Cross Product|vector cross product]] is [[Definition:Distributive Operation|distributive]] over [[Definition:Vector Sum|addition]]. That is, in general: :$\mathbf a \times \paren {\mathbf b + \mathbf c} = \paren {\mathbf a \times \mathbf b} + \paren {\mathbf a \times \mathbf c}$ for $\mathbf a, \mathbf b, \mathbf c \in \R^3$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $M, N$ be [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$. Then $M + N$ is also a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$, where $+$ denotes [[Definition:Setwise Addition|setwise addition]].	1
Let $\mathbf E$ be an [[Definition:Elementary Column Matrix|elementary column matrix]]. Then $\mathbf E$ is [[Definition:Invertible Matrix|invertible]].	1
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\cmod {z_1 + z_2 + \dotsb + z_n} \le \cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_n}$ $\map P 1$ is true by definition of the [[Definition:Usual Ordering|usual ordering on real numbers]]: :$\cmod {z_1} \le \cmod {z_1}$ === Basis for the Induction === $\map P 2$ is the case: :$\cmod {z_1 + z_2} \le \cmod {z_1} + \cmod {z_2}$ which has been proved in [[Triangle Inequality for Complex Numbers]]. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 2$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\cmod {z_1 + z_2 + \dotsb + z_k} \le \cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_k}$ Then we need to show: :$\cmod {z_1 + z_2 + \dotsb + z_{k + 1} } \le \cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_{k + 1} }$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2 + \dotsb + z_{k + 1} } | r = \cmod {\paren {z_1 + z_2 + \dotsb + z_k} + z_{k + 1} } | c = {{Defof|Indexed Summation}} }} {{eqn | o = \le | r = \cmod {z_1 + z_2 + \dotsb + z_k} + \cmod {z_{k + 1} } | c = [[Triangle Inequality/Complex Numbers/General Result#Basis for the Induction|Basis for the Induction]] }} {{eqn | o = \le | r = \paren {\cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_k} } + \cmod {z_{k + 1} } | c = [[Triangle Inequality/Complex Numbers/General Result#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | o = \le | r = \cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_k} + \cmod {z_{k + 1} } | c = {{Defof|Indexed Summation}} }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. {{qed}}	1
$L$ can be expressed by the equations: {{begin-eqn}} {{eqn | l = x - x_1 | r = t \paren {x_2 - x_1} }} {{eqn | l = y - y_1 | r = t \paren {y_2 - y_1} }} {{end-eqn}} These are the '''parametric equations of $L$''', where $t$ is the [[Definition:Parameter|parameter]].	1
Let $H \subseteq \closedint 0 \Omega$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $\closedint 0 \Omega$. From [[Uncountable Open Ordinal Space is not Separable]], there exists an [[Definition:Open Interval|open interval]] $\openint \sigma \Omega$ in the [[Definition:Relative Complement|complement]] of $H^-$ in $\hointr 0 \Omega$, and so also in $\closedint 0 \Omega$. Thus the [[Definition:Closure (Topology)|closure]] of $H$ in $\closedint 0 \Omega$ does not equal $\closedint 0 \Omega$. Thus $H$ is not [[Definition:Everywhere Dense|everywhere dense]] in $\closedint 0 \Omega$. Hence, by definition, $\closedint 0 \Omega$ is not [[Definition:Separable Space|separable]]. {{qed}}	1
Let $V, W$ be [[Definition:Vector Space|vector spaces]] over a [[Definition:Field (Abstract Algebra)|field]] (or, more generally, [[Definition:Division Ring|division ring]]) $K$. A [[Definition:Mapping|mapping]] $A: V \to W$ is a '''linear transformation''' {{iff}}: :$\forall v_1, v_2 \in V, \lambda \in K: \map A {\lambda v_1 + v_2} = \lambda \map A {v_1} + \map A {v_2}$ That is, a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] from one [[Definition:Vector Space|vector space]] to another.	1
:[[File:Angular Bisector Vector Diagram.png|400px]] As shown above: :Let the [[Definition:Angle|angle]] between $\mathbf u$ and $\mathbf v$ be $\gamma$. :Let the angle between $\left\Vert{\mathbf u}\right\Vert \mathbf v$ and $\left\Vert{\mathbf v}\right\Vert \mathbf u$ be $\alpha$. :Let the angle between $\mathbf u$ and $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ be $\beta$. Note that $\left\Vert{ \mathbf u }\right\Vert \mathbf v$ is $\mathbf v$ multiplied by the length of $\mathbf u$. By [[Vector Times Magnitude Same Length As Magnitude Times Vector]] the vectors $\left\Vert{\mathbf u}\right\Vert \mathbf v$ and $\left\Vert{\mathbf v}\right\Vert \mathbf u$ have equal length. So $\left\Vert{\mathbf u}\right\Vert \mathbf v$, $\left\Vert{\mathbf v}\right\Vert \mathbf u$ and $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ make an [[Definition:Isosceles Triangle|isosceles triangle]]. Therefore: {{begin-eqn}} {{eqn | l = 2 \beta + \alpha | r = 180^\circ }} {{eqn | l = \beta | r = 90^\circ - \frac 1 2 \alpha }} {{eqn | l = 2 \beta | r = 180^\circ - \alpha }} {{end-eqn}} But since $\mathbf v$ and $\left\Vert{ \mathbf u }\right\Vert \mathbf v$ are [[Definition:Parallel Vectors|parallel]], we also have: {{begin-eqn}} {{eqn | l = \delta | r = \alpha | c = [[Parallelism implies Equal Corresponding Angles]] }} {{eqn | l = \delta + \gamma | r = 180^\circ }} {{eqn | l = \alpha + \gamma | r = 180^\circ }} {{eqn | l = \gamma | r = 180^\circ - \alpha }} {{end-eqn}} Thus $\gamma = 2 \beta$, and the result follows. {{qed}}	1
Let $\mathbf A$ and $\mathbf B$ be [[Definition:Matrix|$m \times n$ matrices]] over a [[Definition:Field (Abstract Algebra)|field]] $K$ such that $\mathbf A \equiv \mathbf B$. Let $S$ and $T$ be [[Definition:Vector Space|vector spaces]] of [[Definition:Dimension of Vector Space|dimensions]] $n$ and $m$ over $K$. Let $\mathbf A$ be the [[Definition:Relative Matrix|matrix of a linear transformation $u: S \to T$ relative]] to the [[Definition:Ordered Basis|ordered bases]] $\sequence {a_n}$ of $S$ and $\sequence {b_m}$ of $T$. Let $\psi: K^m \to T$ be the [[Definition:Vector Space Isomorphism|isomorphism]] defined as: :$\displaystyle \map \psi {\sequence {\lambda_m} } = \sum_{k \mathop = 1}^m \lambda_k b_k$ Then $\psi$ takes the $j$th column of $\mathbf A$ into $\map u {a_j}$. Hence it takes the [[Definition:Vector Subspace|subspace]] of $K^m$ [[Definition:Generator of Module|generated]] by the [[Definition:Column Matrix|columns]] of $\mathbf A$ onto the [[Definition:Codomain of Mapping|codomain]] of $u$. Thus $\map \rho {\mathbf A} = \map \rho u$, and [[Definition:Matrix Equivalence|equivalent matrices]] over a [[Definition:Field (Abstract Algebra)|field]] have the same [[Definition:Rank of Matrix|rank]]. Now let $\map {\mathcal L} {K^n, K^m}$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $K^n$ to $K^m$. Let $u, v \in \map {\mathcal L} {K^n, K^m}$ such that $\mathbf A$ and $\mathbf B$ are respectively the [[Definition:Relative Matrix|matrices of $u$ and $v$ relative]] to the [[Definition:Standard Ordered Basis|standard ordered bases]] of $K^n$ and $K^m$. Let $r = \map \phi {\mathbf A}$. By [[Linear Transformation from Ordered Basis less Kernel]], there exist ordered bases $\sequence {a_n}, \sequence {a'_n}$ of $K^n$ such that: :$\sequence {\map u {a_r} }$ and $\sequence {\map v {a'_r} }$ are [[Definition:Ordered Basis|ordered bases]] of $\map u {K^n}$ and $\map v {K^n}$ respectively and such that: :$\set {a_k: k \in \closedint {r + 1} n}$ and $\set {a'_k: k \in \closedint {r + 1} n}$ are respectively bases of the [[Definition:Kernel of Linear Transformation|kernels]] of $u$ and $v$. Thus, by [[Results concerning Generators and Bases of Vector Spaces]] there exist ordered bases $\sequence {b_m}$ and $\sequence {b'_m}$ of $K^m$ such that $\forall k \in \closedint 1 r$: {{begin-eqn}} {{eqn | l = b_k | r = \map u {a_k} | c = }} {{eqn | l = b'_k | r = \map v {a'_k} | c = }} {{end-eqn}} Let $z$ be the [[Definition:Vector Space Automorphism|automorphism]] of $K^n$ which satisfies $\forall k \in \closedint 1 n: \map z {a'_k} = a_k$. Let $w$ be the [[Definition:Vector Space Automorphism|automorphism]] of $K^m$ which satisfies $\forall k \in \closedint 1 m: \map w {b'_k} = b_k$. Then:: $\map {\paren {w^{-1} \circ u \circ z} } {a'_k} = \begin{cases} \map {w^{-1} } {b_k} = \map v {a_k} & : k \in \closedint 1 r \\ 0 = \map v {a_k} & : k \in \closedint 1 {r + 1} n \end{cases}$ So $w^{-1} \circ u \circ z = v$. Now let $\mathbf P$ be the matrix of $z$ relative to the [[Definition:Standard Ordered Basis|standard ordered bases]] of $K^n$, and let $\mathbf Q$ be the matrix of $w$ relative to the standard ordered basis of $K^m$. Then $\mathbf P$ and $\mathbf Q$ are [[Definition:Invertible Matrix|invertible]] and: :$\mathbf Q^{-1} \mathbf A \mathbf P = \mathbf B$ and thus: :$\mathbf A \equiv \mathbf B$ {{Qed}} {{Proofread}}	1
Let $0_R$ be the [[Definition:Ring Zero|zero]] of $R$. By the definition of [[Definition:Convergent Sequence in Normed Division Ring|convergence]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = 0_R \iff \lim_{n \mathop \to \infty} \norm {x_n} = 0$ By [[Definition:Norm Axioms|norm axiom (N2) (Multiplicativity)]] then for each $n \in \N$: :$\norm {x_n} = \norm {x^n} = \norm x^n$. So: :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n} = 0 \iff \lim_{n \mathop \to \infty} \norm x^n = 0$ Since $\norm x \in \R_{\ge 0}$, by [[Sequence of Powers of Number less than One]]: :$\displaystyle \lim_{n \mathop \to \infty} \norm x^n = 0 \iff \norm x < 1$ The result follows. {{qed}}	1
We have: {{begin-eqn}} {{eqn | l = 35 | r = 5 \times 7 | c = }} {{eqn | l = 4374 | r = 2 \times 3^7 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 36 | r = 2^2 \times 3^2 | c = }} {{eqn | l = 4375 | r = 5^4 \times 7 | c = }} {{end-eqn}} Thus both pairs of integers can be seen to have the same [[Definition:Prime Factor|prime factors]]: :$2, 3, 5, 7$ {{qed}}	1
The [[Test for Ideal]] is applied to prove the result. === [[Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.1|Lemma 1.1]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.1|Lemma 1}}{{qed|lemma}} === [[Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.2|Lemma 1.2]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.2}}{{qed|lemma}} === [[Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.3|Lemma 1.3]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 1/Lemma 1.3}}{{qed|lemma}} By [[Test for Ideal]] then the result follows. {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ECO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Column of Matrix|column]] $l$ to [[Definition:Column of Matrix|column]] $k$ | m = \kappa_k \to \kappa_k + \lambda \kappa_l }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Column Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Column Operation|elementary column operation]]: :$e' := \kappa_k \to \kappa_k - \lambda \kappa_l$	1
Let $n \in \Z_{\ge 0}$ be an [[Definition:Integer|integer]]. Let $\struct {\Z / n \Z, +, \cdot}$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $n$]]. Let $U = \struct {\paren {\Z / n \Z}^\times, \cdot}$ denote the [[Definition:Group of Units of Ring|group of units]] of this ring. Then: :$\order U = \map \phi n$ where $\phi$ denotes the [[Definition:Euler Phi Function|Euler $\phi$-function]].	1
For $y \in R$ then: {{begin-eqn}} {{eqn | l = \norm y_1 > 1 | r = \dfrac 1 {\norm y_1 } < 1 | o = \leadstoandfrom | c = }} {{eqn | r = \norm {y^{-1} }_1 < 1 | o = \leadstoandfrom | c = [[Norm of Inverse in Division Ring]] }} {{eqn | r = \norm {y^{-1} }_2 < 1 | o = \leadstoandfrom | c = by assumption }} {{eqn | r = \dfrac 1 {\norm y_2 } < 1 | o = \leadstoandfrom | c = [[Norm of Inverse in Division Ring]] }} {{eqn | r = \norm y_2 > 1 | o = \leadstoandfrom | c = }} {{end-eqn}} {{qed}} [[Category:Equivalence of Definitions of Equivalent Division Ring Norms]] 7c3kw7jre137tryfpo9a96n979a9bxq	1
From the [[Definition:Vector Space Axioms|vector space axioms]] we have that $\exists \mathbf 0 \in \mathbf V$. What remains is to prove that $\map T {\mathbf 0} = \mathbf 0'$: {{begin-eqn}} {{eqn | l = \map T {\mathbf 0} | r = \map T {0 \, \mathbf 0} | c = [[Zero Vector Scaled is Zero Vector]] }} {{eqn | r = 0 \, \map T {\mathbf 0} | c = {{Defof|Linear Transformation on Vector Space}} }} {{eqn | r = \mathbf 0' | c = [[Vector Scaled by Zero is Zero Vector]] }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A$ be an [[Definition:Idempotent Operator|idempotent operator]]. Then the [[Definition:Complementary Idempotent|complementary idempotent]] $I - A$ is also [[Definition:Idempotent Operator|idempotent]].	1
Let $\left({R, +_R, \times_R}\right)$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\left({A_1, +_1, \circ_1}\right)_R, \left({A_2, +_2, \circ_2}\right)_R, \left({A_3, +_3, \circ_3}\right)_R$ be [[Definition:Module|$R$-modules]]. Let $\oplus: A_1 \times A_2 \to A_3$ be a [[Definition:Binary Operation|binary operator]] with the property that: $\forall \left({a_1, a_2}\right) \in A_1 \times A_2$: : $a_1 \mapsto a_1 \oplus a_2$ is a [[Definition:Linear Transformation|linear transformation]] from $A_1$ to $A_3$ : $a_2 \mapsto a_1 \oplus a_2$ is a [[Definition:Linear Transformation|linear transformation]] from $A_2$ to $A_3$ Then $\oplus$ is a '''bilinear mapping'''. That is, $\forall a, b \in R, \forall x, y \in A_2, z \in A_3$: : $\left({\left({a \circ_1 x}\right) +_1 \left({y \circ_1 b}\right)}\right) \oplus z = \left({a \circ_3 \left({x \oplus z}\right)}\right) +_3 \left({\left({y \oplus z}\right) \circ_3 b}\right)$ and for all $z \in A_1, x,y \in A_2$: : $z \oplus \left({\left({a \circ_2 x}\right) +_2 \left({y \circ_2 b}\right)}\right) = \left({a \circ_3 \left({z \oplus x}\right)}\right) +_3 \left({\left({z \oplus y}\right) \circ_3 b}\right)$ Equivalently, this can be expressed: : $\left({x +_1 y}\right) \oplus z = \left({x \oplus z}\right) +_3 \left({y \oplus z}\right)$ : $z \oplus \left({x +_2 y}\right) = \left({z \oplus x}\right) +_3 \left({z \oplus y}\right)$ : $\left({a \circ_1 x}\right) \oplus z = a \circ_3 \left({x \oplus z}\right)$ : $z \oplus \left({y \circ_2 b}\right) = \left({z \oplus y}\right) \circ_3 b$ If $\left({A, +, \circ}\right)_R = A_1 = A_2 = A_3$, the notation simplifies considerably: : $\left({\left({a \circ x}\right) + \left({b \circ y}\right)}\right) \oplus z = \left({a \circ \left({x \oplus z}\right)}\right) + \left({b \circ \left({y \oplus z}\right)}\right)$ : $z \oplus \left({\left({a \circ x}\right) + \left({y \circ b}\right)}\right) = \left({a \circ \left({z \oplus x}\right)}\right) + \left({\left({z \oplus y}\right) \circ b}\right)$ or equivalently, more easily digested: : $\left({x + y}\right) \oplus z = \left({x \oplus z}\right) + \left({y \oplus z}\right)$ : $z \oplus \left({x + y}\right) = \left({z \oplus x}\right) + \left({z \oplus y}\right)$ : $\left({a \circ x}\right) \oplus z = a \circ \left({x \oplus z}\right)$ : $z \oplus \left({y \circ b}\right) = \left({z \oplus y}\right) \circ b$ === [[Definition:Bilinear Mapping/Non-Commutative Ring|Non-Commutative Ring]] === {{:Definition:Bilinear Mapping/Non-Commutative Ring}}	1
All $n^2$ [[Definition:Element of Matrix|elements]] of $C_n^{-1}$ have a term $\dfrac {-y} {x \left({x + n y}\right)}$. Further to this, the $n$ elements on the [[Definition:Main Diagonal|main diagonal]] contribute an extra $\dfrac {x + n y} {x \left({x + n y}\right)}$ to the total. Hence: {{begin-eqn}} {{eqn | l = \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} | r = n^2 \dfrac {-y} {x \left({x + n y}\right)} + n \dfrac {x + n y} {x \left({x + n y}\right)} | c = }} {{eqn | r = \dfrac {- n^2 y + n \left({x + n y}\right)} {x \left({x + n y}\right)} | c = }} {{eqn | r = \dfrac {- n^2 y + n x + n^2 y} {x \left({x + n y}\right)} | c = }} {{eqn | r = \dfrac {n x} {x \left({x + n y}\right)} | c = }} {{eqn | r = \dfrac n {x + n y} | c = }} {{end-eqn}} {{qed}}	1
By the definition of a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] then: :$(1): \quad$ there exists a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]] $\phi: R \to R'$. :$(2): \quad$ $\struct {R', \norm {\, \cdot \,}' }$ is a [[Definition:Complete Metric Space|complete metric space]]. :$(3): \quad$ The [[Definition:Image of Mapping|image]] $\phi \sqbrk R$ of $\phi$ is a [[Definition:Everywhere Dense|dense]] [[Definition:Topological Subspace|subspace]] in $\struct {R', \norm {\, \cdot \,}' }$. By [[Ring Homomorphism Preserves Subrings/Corollary|image of a ring homomorphism is a subring]] then $\phi \sqbrk R$ is a [[Definition:Subring|subring]] of $R'$ and $\phi: R \to \phi \sqbrk R$ is an [[Definition:Ring Isomorphism|isomorphism]]. === Necessary Condition === Let $x', y' \in R'$. By the definition of a [[Definition:Dense|dense subset]] then $\map \cl {\phi \sqbrk R} = R'$. By [[Closure of Subset of Metric Space by Convergent Sequence]]: :there exists a [[Definition:Sequence|sequence]] $\sequence {x_n'} \subseteq \phi \sqbrk R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x'$, that is, $\displaystyle \lim_{n \mathop \to \infty} x_n' = x'$ :there exists a [[Definition:Sequence|sequence]] $\sequence {y_n'} \subseteq \phi \sqbrk R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $y'$, that is, $\displaystyle \lim_{n \mathop \to \infty} y_n' = y'$ By [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|product rule for convergent sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n' y_n' = x' y'$ :$\displaystyle \lim_{n \mathop \to \infty} y_n' x_n' = y' x'$ By the definition of a [[Definition:Field (Abstract Algebra)|field]], $R$ is a [[Definition:Commutative Ring|commutative ring]]. Because $\phi: R \to \phi \sqbrk R$ is an [[Definition:Ring Isomorphism|isomorphism]], by [[Isomorphism Preserves Commutativity]], $\phi \sqbrk R$ is a [[Definition:Commutative Ring|commutative ring]]. Because $\sequence {x_n'} \subseteq \phi \sqbrk R$ and $\sequence {y_n'} \subseteq \phi \sqbrk R$: :$\forall n: x_n' y_n' = y_n' x_n'$ Hence: :$\displaystyle x' y' = \lim_{n \mathop \to \infty} x_n' y_n' = \lim_{n \mathop \to \infty} y_n' x_n' = y' x'$ Since $x', y' \in R'$ were arbitrary, then $R'$ is a [[Definition:Commutative Ring|commutative ring]]. The result follows. {{qed|lemma}} === Sufficient Condition === By [[Restriction of Commutative Operation is Commutative]], $\phi \sqbrk R$ is a [[Definition:Commutative Ring|commutative ring]]. By [[Monomorphism Image is Isomorphic to Domain]], $\phi: R \to \phi \sqbrk R$ is a [[Definition:Ring Isomorphism|ring isomorphism]]. By [[Inverse of Algebraic Structure Isomorphism is Isomorphism]], $\phi^{-1}: \phi \sqbrk R \to R$ is a [[Definition:Ring Isomorphism|ring isomorphism]]. By [[Isomorphism Preserves Commutativity]], $R$ is a [[Definition:Commutative Ring|commutative ring]]. The result follows. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $T \in B_0 \left({H}\right)$ be a [[Definition:Compact Linear Operator|compact]], [[Definition:Self-Adjoint Operator|self-adjoint operator]]. Then its [[Definition:Point Spectrum|point spectrum]] $\sigma_p \left({T}\right)$ is [[Definition:Countable|countable]].	1
The [[Definition:Quaternion|set of quaternions]] $\Bbb H$ forms an [[Definition:Algebra over Field|algebra]] over the [[Definition:Field of Real Numbers|field of real numbers]]. This [[Definition:Algebra over Field|algebra]] is: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]], but '''not''' a [[Definition:Commutative Algebra|commutative algebra]]. :$(2): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]]. :$(3): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]].	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $D = \map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$: :$\displaystyle \map \det {\mathbf A} := \sum_{\lambda} \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{k \map \lambda k} } = \sum_\lambda \map \sgn \lambda a_{1 \map \lambda 1} a_{2 \map \lambda 2} \cdots a_{n \map \lambda n}$ where: :the summation $\displaystyle \sum_\lambda$ goes over all the $n!$ [[Definition:Permutation on n Letters|permutations]] of $\set {1, 2, \ldots, n}$ :$\map \sgn \lambda$ is the [[Definition:Sign of Permutation|sign of the permutation]] $\lambda$. Let $a_{p q}$ be an [[Definition:Element of Matrix|element]] of $\mathbf A$. Let $A_{p q}$ be the [[Definition:Cofactor of Element|cofactor]] of $a_{p q}$ in $D$. Then: :$(1): \quad \displaystyle \forall r \in \closedint 1 n: D = \sum_{k \mathop = 1}^n a_{r k} A_{r k}$ :$(2): \quad \displaystyle \forall r \in \closedint 1 n: D = \sum_{k \mathop = 1}^n a_{k r} A_{k r}$ Thus the value of a [[Definition:Determinant of Matrix|determinant]] can be found either by: :multiplying all the [[Definition:Element of Matrix|elements]] in a [[Definition:Row of Matrix|row]] by their [[Definition:Cofactor of Element|cofactor]]s and adding up the products or: :multiplying all the [[Definition:Element of Matrix|elements]] in a [[Definition:Column of Matrix|column]] by their [[Definition:Cofactor of Element|cofactor]]s and adding up the products. The identity: :$\displaystyle D = \sum_{k \mathop = 1}^n a_{r k} A_{r k}$ is known as the '''expansion of $D$ in terms of [[Definition:Row of Matrix|row]] $r$''', while: :$\displaystyle D = \sum_{k \mathop = 1}^n a_{k r} A_{k r}$ is known as the '''expansion of $D$ in terms of [[Definition:Column of Matrix|column]] $r$'''.	1
:$\paren {\mathbf u + \mathbf v} \cdot \mathbf w = \mathbf u \cdot \mathbf w + \mathbf v \cdot \mathbf w$	1
The result follows directly from: :[[Closed Ball in Normed Division Ring is Closed Ball in Induced Metric]] :[[Open Ball in Normed Division Ring is Open Ball in Induced Metric]] :[[Sphere in Normed Division Ring is Sphere in Induced Metric]] :[[Sphere is Set Difference of Closed Ball with Open Ball]] {{qed}} [[Category:Normed Division Rings]] 6vn02xt3symwmexuwwu1ze2ydr4u7br	1
:$\alpha = \beta$	1
Any [[Definition:Linear Transformation|linear transformation]] clearly satisfies the condition. Let $\phi$ be such that the condition is satisfied. Let $\lambda = \mu = 1_R$. Then $\map \phi {x + y} = \map \phi x + \map \phi y$. Now let $\mu = 0_R$. Then $\map \phi {\lambda x} = \lambda \map \phi x$. Thus the conditions are fulfilled for $\phi$ to be a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]], that is, a [[Definition:Linear Transformation|linear transformation]]. {{Qed}}	1
This is a special case of the [[Definition:Vector Space of All Mappings|Vector Space of All Mappings]], where $S$ is the set $\closedint 1 n \subset \N^*$. {{qed}}	1
Let $\struct {\R^n, +, \cdot}_\R$ be a [[Definition:Real Vector Space|real vector space]]. Let $S \subseteq \R^n$. Then $S$ is a '''linearly dependent set''' if there exists a [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ which is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]]. That is, such that: :$\displaystyle \exists \set {\lambda_k: 1 \le k \le n} \subseteq \R: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0$ where $\set {\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n} \subseteq S$, and such that at least one of $\lambda_k$ is not equal to $0$.	1
By [[Quotient Ring of Cauchy Sequences is Division Ring]] then $\CC \,\big / \NN$ is a [[Definition:Division Ring|division ring]]. By [[Cauchy Sequences form Ring with Unity/Corollary|Corollary to Cauchy Sequences form Ring with Unity]] then $\CC$ is a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. By [[Quotient Ring of Commutative Ring is Commutative]] then $\CC \,\big / \NN$ is a [[Definition:Commutative Ring|commutative]] [[Definition:Division Ring|division ring]], that is, a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $m$ be a [[Definition:Positive Integer|positive integer]]. Then the [[Definition:Euclidean Space|Euclidean space]] $\R^m$, along with the [[Definition:Euclidean Norm|Euclidean norm]], forms a [[Definition:Banach Space|Banach space]] over $\R$.	1
Let: {{begin-eqn}} {{eqn | l = C | r = \begin{bmatrix} \dfrac 1 {x_1 - y_1} & \dfrac 1 {x_1 - y_2} & \cdots & \dfrac 1 {x_1 - y_n} \\ \dfrac 1 {x_2 - y_1} & \dfrac 1 {x_2 - y_2} & \cdots & \dfrac 1 {x_2 - y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n - y_1} & \dfrac 1 {x_n - y_2} & \cdots & \dfrac 1 {x_n - y_n} \\ \end{bmatrix} }} {{end-eqn}} To be proved: {{begin-eqn}} {{eqn | l = \det \paren {C} | r = \dfrac {\displaystyle \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right) \left({y_i - y_j}\right)} {\displaystyle \prod_{1 \mathop \le i, \, j \mathop \le n} \left({x_i - y_j}\right)} | c = Knuth (1997) replacing $y_k \to -y_k$ in $C$ and $\det \paren {C}$ }} {{end-eqn}} Assume hereafter that set $\set {x_1,\ldots,x_n,y_1,\ldots,y_n}$ consists of distinct values, because otherwise $\det \paren {C}$ is undefined or zero. '''Preliminaries''': [[Vandermonde Matrix Identity for Cauchy Matrix]] supplies matrix equation :$\displaystyle (1)\quad - C = PV_x^{-1} V_y Q^{-1}$ :Definitions of symbols: ::$\displaystyle V_x=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ x_1 & x_2 & \cdots & x_n \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1} \\ \end{smallmatrix} },\quad V_y=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ y_1 & y_2 & \cdots & y_n \\ \vdots & \vdots & \ddots & \vdots \\ y_1^{n-1} & y_2^{n-1} & \cdots & y_n^{n-1} \\ \end{smallmatrix} }$ [[Definition:Vandermonde Matrix|Vandermonde matrices]] ::$\displaystyle P= \paren {\begin{smallmatrix} p_1(x_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p_n(x_n) \\ \end{smallmatrix} }, \quad Q= \paren {\begin{smallmatrix} p(y_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p(y_n) \\ \end{smallmatrix} }$ [[Definition:Diagonal Matrix|Diagonal matrices]] ::$\displaystyle p(x) = \prod_{i \mathop = 1}^n \paren {x - x_i}, \quad \displaystyle p_k(x) = \prod_{i \mathop = 1,i \mathop \ne k}^n \, \paren {x - x_i}, \quad 1 \mathop \le k \mathop \le n$ [[Definition:Polynomial/Complex Numbers|Polynomials]] '''Determinant of $C$ Calculation''': {{begin-eqn}} {{eqn | l = C | r = \paren {-I}\,P\,V_x^{-1}\, V_y\, Q^{-1} | c = [[Vandermonde Matrix Identity for Cauchy Matrix]] Symbol $I$ is the $n\times n$ identity matrix. }} {{eqn | l = (2)\quad \det \paren {C} | r = \det \paren { -I } \det \paren {P} \det \paren { V_x^{-1} } \det \paren { V_y } \det \paren { Q^{-1} } | c = [[Determinant of Matrix Product]] }} {{eqn | r = \paren { -1 }^n \det \paren { I } \det \paren {P} \det \paren { V_x^{-1} } \det \paren { V_y } \det \paren { Q^{-1} } | c = [[Effect of Elementary Row Operations on Determinant]] Factor constant $-1$ from all rows of $-I$. }} {{eqn | r = \paren { -1 }^n \det \paren { I } \dfrac { \det \paren {P} \det \paren { V_y } } { \det \paren { Q } \det \paren { V_x } } | c = [[Matrix Product with Adjugate Matrix]] and [[Determinant of Matrix Product]] }} {{end-eqn}} '''Lemma''': $\det \paren {P} = \paren {-1}^m \paren { \det \paren {V_x} }^2$ where $m=\frac 1 2 n \paren {n-1}$ :'''Details''': Determinant $\det \paren {P}$ expands to: {{begin-eqn}} {{eqn | l = \,\,\prod_{j \mathop = 1}^n \map {p_j} {x_j} | r = \prod_{j \mathop = 1}^n \,\, \prod_{k \mathop = 1,\, k \mathop \neq j}^n \paren { x_j - x_k } | c = Definition of polynomials $\map {p_j} x$. }} {{end-eqn}} :Pair factors $\paren {x_r - x_s}$ and $\paren {x_s - x_r}$ into factor $- \paren {x_s - x_r}^2$, then: {{begin-eqn}} {{eqn | l = \,\,\det \paren {P} | r = \paren {-1}^m \, \paren { \prod_{1 \mathop \leq r \mathop \lt s \mathop \le n} \paren { x_s - x_r }^2 } | c = where $m = 1 + \cdots + \paren {n-1} = \frac 1 2 n \paren {n-1}$ }} {{eqn | r = \paren {-1}^m \, \paren { \det \paren {V_x} }^2 | c = [[Vandermonde Determinant]] }} {{end-eqn}} {{qed|lemma}} Apply the '''Lemma''' to equation (2): {{begin-eqn}} {{eqn | l = \det \paren {C} | r = \paren { -1 }^{n+m} \, \paren { \dfrac { \det \paren {V_x} \det \paren { V_y } } { \det \paren { Q } } } | c = $m = \frac 1 2 n \paren {n-1}$ }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \, \prod_{1 \mathop \le m \mathop \lt k \mathop \le n} \paren { y_k - y_m } } { \displaystyle \prod_{k \mathop = 1}^n \map {p} {y_k} } | c = [[Vandermonde Determinant]] and [[Determinant of Diagonal Matrix]] }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_k - y_m } } { \displaystyle \prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { y_k - x_j } } | c = Definition of $\map p x$. }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \paren {-1}^{m}\prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_m - y_k } } { \displaystyle \paren {-1}^{n^2}\,\,\prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { x_j - y_k } } | c = Factor out changed signs. }} {{eqn | r = \left. \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_m - y_k } } { \displaystyle \prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { x_j - y_k } } \right. | c = All signs cancel. }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be an [[Definition:Module|$R$-module]]. Let $S \subseteq M$ be a [[Definition:Subset|subset]]. === [[Definition:Generator of Module/Definition 1|Definition 1]] === {{:Definition:Generator of Module/Definition 1}} === [[Definition:Generator of Module/Definition 2|Definition 2]] === {{:Definition:Generator of Module/Definition 2}} === [[Definition:Generator of Module/Definition 3|Definition 3]] === {{:Definition:Generator of Module/Definition 3}}	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]]: Let $\mathbf a \times \mathbf b$ denote the [[Definition:Vector Cross Product|vector cross product]] of $\mathbf a$ with $\mathbf b$. Then $\left\lvert{\mathbf a \times \mathbf b}\right\rvert$ equals the [[Definition:Area|area]] of the [[Definition:Parallelogram|parallelogram]] two of whose [[Definition:Side of Polygon|sides]] are $\mathbf a$ and $\mathbf b$.	1
Proof by [[Principle of Mathematical Induction|induction]] on $m$, the number of [[Definition:Elementary Row Operation|elementary row operations]] in the [[Definition:Finite Sequence|sequence]] $\hat o_1, \ldots, \hat o_m$. === Basis for the Induction === Suppose $m = 1$, so there is only one [[Definition:Elementary Row Operation|elementary row operation]] $\hat o$ in the [[Definition:Finite Sequence|sequence]]. Let $r_i$ denote the $i$'th [[Definition:Row of Matrix|row]] of $\mathbf A$. Suppose that $\hat o$ is of the type $r_i \to a r_i$, where $a \in R$ and $i \in \set {1, \ldots, n}$. From [[Effect of Elementary Row Operations on Determinant]], it follows that: :$\map \det {\mathbf A} = a \map \det {\mathbf A'}$ Suppose that $\hat o$ is of the type $r_i \to r_i + ar_j$, where $a \in R$ and $i, j \in \set {1, \ldots, n}, i \ne j$. From [[Effect of Elementary Row Operations on Determinant]], it follows that :$\map \det {\mathbf A} = \map \det {\mathbf A'} = 1_R \map \det {\mathbf A'}$ where $1_R$ denotes the [[Definition:Identity Element|identity element]] of $\struct {R, \circ}$. Suppose that $\hat o$ is of the type $r_i \leftrightarrow r_j$. From [[Effect of Elementary Row Operations on Determinant]], it follows that :$\map \det {\mathbf A} = -\map \det {\mathbf A'} = -1_R \map \det {\mathbf A'}$ where the last equality follows from [[Product with Ring Negative/Corollary|Product with Ring Negative: Corollary]]. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === For $m \in \N$, let $\hat o_1, \ldots, \hat o_m$ be a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]]. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: There exists $c \in R$ such that for all [[Definition:Square Matrix|matrices]] of [[Definition:Order of Square Matrix|order $n$]] $\mathbf A$: :$\map \det {\mathbf A} = c \map \det {\mathbf A'}$ where $\mathbf A'$ is the [[Definition:Square Matrix|matrix]] of [[Definition:Order of Square Matrix|order $n$]] that results from using the [[Definition:Elementary Row Operation|elementary row operations]] $\hat o_1, \ldots, \hat o_m$ on $\mathbf A$. === Induction Step === This is the [[Definition:Induction Step|induction step]]: Let $\hat o_1, \ldots, \hat o_m, \hat o_{m + 1}$ be a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]]. Let $r_i$ denote the $i$'th [[Definition:Row of Matrix|row]] of $\mathbf A'$. Let $\mathbf A''$ denote the [[Definition:Square Matrix|matrix]] of [[Definition:Order of Square Matrix|order $n$]] that results from using the [[Definition:Elementary Row Operation|elementary row operation]] $\hat o_{m + 1}$ on $A'$. Then $\mathbf A''$ is equal to the [[Definition:Square Matrix|matrix]] that results from using the [[Definition:Elementary Row Operation|elementary row operations]] $\hat o_1, \ldots, \hat o_m, \hat o_{m + 1}$ on $A$. Suppose that $\hat o_{m + 1}$ is of the type $r_i \to ar_i$, where $a \in R$ and $i \in \set {1, \ldots, n}$. Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A} | r = c \map \det {\mathbf A'} | c = by the [[Effect of Sequence of Elementary Row Operations on Determinant#Induction Hypothesis|induction hypothesis]] }} {{eqn | r = \paren {c a} \map \det {\mathbf A''} | c = [[Effect of Elementary Row Operations on Determinant]] }} {{end-eqn}} Suppose that $\hat o_{m + 1}$ is of the type $r_i \to r_i + a r_j$, where $a \in R$ and $i, j \in \set {1, \ldots, n}, i \ne j$. Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A} | r = c \map \det {\mathbf A'} | c = by the [[Effect of Sequence of Elementary Row Operations on Determinant#Induction Hypothesis|induction hypothesis]] }} {{eqn | r = c 1_R \map \det {\mathbf A''} | c = [[Effect of Elementary Row Operations on Determinant]] }} {{eqn | r = c \map \det {\mathbf A''} | c = {{Defof|Identity Element}}, since $R$ is [[Definition:Commutative Ring|commutative]] }} {{end-eqn}} Suppose that $\hat o_{m + 1}$ is of the type $r_i \leftrightarrow r_j$. Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A} | r = c \map \det {\mathbf A'} | c = by the [[Effect of Sequence of Elementary Row Operations on Determinant#Induction Hypothesis|induction hypothesis]] }} {{eqn | r = c \paren {-1_R} \map \det {\mathbf A''} | c = [[Effect of Elementary Row Operations on Determinant]] }} {{eqn | r = \paren {-c} \map \det {\mathbf A''} | c = [[Product with Ring Negative/Corollary|Product with Ring Negative: Corollary]] }} {{end-eqn}} Then the [[Definition:Induction Step|induction step]] is proved for all three types of [[Definition:Elementary Row Operation|elementary row operations]]. {{qed}} [[Category:Determinants]] [[Category:Elementary Row Operations]] [[Category:Proofs by Induction]] lxp6dfv6nef3s6nkzxs4nrdn7e7j5vo	1
$\norm {\,\cdot\,}$ is [[Definition:Archimedean Division Ring Norm|Archimedean]] {{iff}}: :$\sup \set {\norm {n \cdot 1_R}: n \in \N_{\gt 0} } = +\infty$	1
:$* : \struct {R \times R, d_p} \to \struct {R, d}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]].	1
{{begin-eqn}} {{eqn | l = \nabla \times \paren {g \, \mathbf f} | r = \paren {\dfrac {\partial g f_z} {\partial y} - \dfrac {\partial g f_y} {\partial z} } \mathbf i + \paren {\dfrac {\partial g f_x} {\partial z} - \dfrac {\partial g f_z} {\partial x} } \mathbf j + \paren {\dfrac {\partial g f_y} {\partial x} - \dfrac {\partial g f_x} {\partial y} } \mathbf k | c = {{Defof|Curl Operator}} }} {{eqn | r = \paren {g \dfrac {\partial f_z} {\partial y} + \dfrac {\partial g} {\partial y} f_z - g \dfrac {\partial f_y} {\partial z} - \dfrac {\partial g} {\partial z} f_y} \mathbf i | c = [[Product Rule for Derivatives]] }} {{eqn | o = | ro= + | r = \paren {g \dfrac {\partial f_x} {\partial z} + \dfrac {\partial g} {\partial z} f_x - g \dfrac {\partial f_z} {\partial x} - \dfrac {\partial g} {\partial x} f_z} \mathbf j | c = }} {{eqn | o = | ro= + | r = \paren {g \dfrac {\partial f_y} {\partial x} + \dfrac {\partial g} {\partial x} f_y - g \dfrac {\partial f_x} {\partial y} - \dfrac {\partial g} {\partial y} f_x } \mathbf k | c = }} {{eqn | r = \map g {\paren {\dfrac {\partial f_z} {\partial y} - \dfrac {\partial f_y} {\partial z} } \mathbf i + \paren {\dfrac {\partial f_x} {\partial z} - \dfrac {\partial f_z} {\partial x} } \mathbf j + \paren {\dfrac {\partial f_y} {\partial x} - \dfrac {\partial f_x} {\partial y} } \mathbf k} | c = rearrangement }} {{eqn | o = | ro= + | r = \paren {\dfrac {\partial g} {\partial y} f_z - \dfrac {\partial g} {\partial z} f_y} \mathbf i + \paren {\dfrac {\partial g} {\partial z} f_x - \dfrac {\partial g} {\partial x} f_z} \mathbf j + \paren {\dfrac {\partial g} {\partial x} f_y - \dfrac {\partial g} {\partial y} f_x} \mathbf k | c = }} {{eqn | r = \map g {\nabla \times \mathbf f} + \paren {\dfrac {\partial g} {\partial y} f_z - \dfrac {\partial g} {\partial z} f_y} \mathbf i + \paren {\dfrac {\partial g} {\partial z} f_x - \dfrac {\partial g} {\partial x} f_z} \mathbf j + \paren {\dfrac {\partial g} {\partial x} f_y - \dfrac {\partial g} {\partial y} f_x} \mathbf k | c = {{Defof|Curl Operator}} }} {{eqn | r = \map g {\nabla \times \mathbf f} + \paren {\dfrac {\partial g} {\partial x} \mathbf i + \dfrac {\partial g} {\partial y} \mathbf j + \dfrac {\partial g} {\partial z} \mathbf k} \times \paren {f_x \mathbf i + f_y \mathbf j + f_z \mathbf k} | c = {{Defof|Cross Product}} }} {{eqn | r = \map g {\nabla \times \mathbf f} + \paren {\nabla g} \times \mathbf f | c = {{Defof|Gradient Operator}}, {{Defof|Vector (Linear Algebra)|Vector}} }} {{end-eqn}} {{qed}}	1
Let $f: S \to S$ be a [[Definition:Mapping|mapping]] on an [[Definition:Algebraic Structure|algebraic structure]] $\struct {S, +}$. Then $f$ is an '''additive function''' {{iff}} it preserves the [[Definition:Addition|addition]] operation: :$\forall x, y \in S: \map f {x + y} = \map f x + \map f y$	1
Let $\left({S, \ast_1, \ast_2, \ldots, \ast_n, \circ}\right)_R$ and $\left({T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}\right)_R$ be [[Definition:R-Algebraic Structure|$R$-algebraic structures]]. Then $\phi: S \to T$ is an '''$R$-algebraic structure monomorphism''' {{iff}}: : $(1): \quad \phi$ is an [[Definition:Injection|injection]] : $(2): \quad \forall k: k \in \left[{1 \,.\,.\, n}\right]: \forall x, y \in S: \phi \left({x \ast_k y}\right) = \phi \left({x}\right) \odot_k \phi \left({y}\right)$ : $(3): \quad \forall x \in S: \forall \lambda \in R: \phi \left({\lambda \circ x}\right) = \lambda \otimes \phi \left({x}\right)$. That is, {{iff}}: : $(1): \quad \phi$ is an [[Definition:Injection|injection]] : $(2): \quad \phi$ is an [[Definition:R-Algebraic Structure Homomorphism|$R$-algebraic structure homomorphism]]. This definition continues to apply when $S$ and $T$ are [[Definition:Module|modules]], and also when they are [[Definition:Vector Space|vector spaces]]. === [[Definition:Vector Space Monomorphism|Vector Space Monomorphism]] === {{:Definition:Vector Space Monomorphism}}	1
Let $\mathbf 0$ denote a [[Definition:Zero Vector Quantity|zero vector]]. {{AimForCont}} $\mathbf 0$ has a [[Definition:Direction|direction]]. Then $\mathbf 0$ can be [[Definition:Arrow Representation of Vector Quantity|represented as an arrow]] in a [[Definition:Real Vector Space|real vector space]] $\R^n$ with a [[Definition:Cartesian Coordinate System|Cartesian frame]]. Let $\mathbf 0$ be so embedded. Thus it consists of a [[Definition:Line Segment|line segment]] between two [[Definition:Point|points]] with an [[Definition:Initial Point of Vector|initial point]] $A$ and a [[Definition:Terminal Point of Vector|terminal point]] $B$. The [[Definition:Initial Point of Vector|initial point]] and a [[Definition:Terminal Point of Vector|terminal point]] are [[Definition:Distinct Elements|distinct]] from each other. Let these [[Definition:Point|points]] be identified as: {{begin-eqn}} {{eqn | l = A | r = \tuple {a_1, a_2, \ldots, a_n} }} {{eqn | l = B | r = \tuple {b_1, b_2, \ldots, b_n} }} {{end-eqn}} Hence we have that the [[Definition:Vector Length|length]] of $\mathbf 0$ is defined as: :$\norm {\mathbf 0} = \ds \sqrt {\sum_{i \mathop = 1}^n \paren {a_i - b_i}^2} > 0$ which means that at least one of $a_i - b_i$ is non-[[Definition:Zero (Number)|zero]]. But this [[Definition:Contradiction|contradicts]] the definition of $\mathbf 0$ being the [[Definition:Zero Vector Quantity|zero vector]]. It follows by [[Proof by Contradiction]] that our assumption that $\mathbf 0$ has a [[Definition:Direction|direction]] must be false. Hence the result. {{qed}}	1
For any $d \in F \sqbrk X$, let $\ideal d$ denote the [[Definition:Principal Ideal of Ring|principal ideal of $F \sqbrk X$ generated by $d$]]. Let $J$ be any [[Definition:Ideal of Ring|ideal]] of $F \sqbrk X$. What we need to prove is that $J$ is a [[Definition:Principal Ideal of Ring|principal ideal]]. Let us first [[Definition:Distinguish|distinguish]] the following two cases for $J$: :If $J = \set {0_F}$, then by [[Zero Element Generates Null Ideal]] $J = \ideal {0_F}$, and hence is a [[Definition:Principal Ideal of Ring|principal ideal]]. :If $J = F \sqbrk X$, then by [[Ideal of Unit is Whole Ring/Corollary|Ideal of Unit is Whole Ring: Corollary]] $J = \ideal {1_F}$, and hence is a [[Definition:Principal Ideal of Ring|principal ideal]]. Now suppose $J \ne \set {0_F}$ and $J \ne F \sqbrk X$. Then $J$ necessarily contains a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]]. By the [[Well-Ordering Principle]], we can introduce the lowest [[Definition:Degree of Polynomial over Field|degree]] of a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] of $J$. Denote this [[Definition:Degree of Polynomial over Field|degree]] by $n$. If $n = 0$, then $J$ contains a [[Definition:Polynomial over Ring in One Variable|polynomial]] of [[Definition:Degree of Polynomial over Field|degree]] $0$. This is a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] of $F$. As $F$ is a [[Definition:Field (Abstract Algebra)|field]], this is therefore a [[Definition:Unit of Ring|unit]] of $F$, and thus by [[Ideal of Unit is Whole Ring]], $J = F \sqbrk X$. Because the [[Definition:Degree of Polynomial over Field|degree]] of a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] is a [[Definition:Natural Numbers|natural number]], we conclude that $n \ge 1$. Now let $d$ be a [[Definition:Polynomial over Field|polynomial]] of degree $n$ in $J$, and let $f \in J$. By [[Division Theorem for Polynomial Forms over Field]], $f = q \circ d + r$ for some $q, r \in F \sqbrk X$ where either: :$r = 0_F$ or: :$r$ is a [[Definition:Polynomial over Field|polynomial]] of [[Definition:Degree of Polynomial over Field|degree]] smaller than $n$. Because $J$ is an [[Definition:Ideal of Ring|ideal]] and $d \in J$, it follows that: :$q \circ d \in J$ Since $f \in J$, we also conclude: :$r = f - q \circ d \in J$ From the construction of $d$, it follows that we must have $r = 0_F$. Therefore: :$f = q \circ d$ and thus: :$f \in \ideal d$. This reasoning shows that: :$J \subseteq \ideal d$ From property $(3)$ of the [[Definition:Principal Ideal of Ring|principal ideal]] $\ideal d$, we conclude that: :$\ideal d \subseteq J$ as $d \in J$. Hence $J = \ideal d$. These $2$ [[Definition:Distinguish|distinguished cases]] cover all of the possible [[Definition:Ideal of Ring|ideals]] of $F \sqbrk X$. Hence $F \sqbrk X$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. {{qed}}	1
We have that $\sequence {x_n}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $M_a$. Then: :$\forall \epsilon_a \in \R_{> 0} : \exists N \in \N : \forall n, m \in \N : n, m > N \implies \norm {x_n - x_m}_a < \epsilon_a$ By [[Definition:Equivalence of Norms|equivalence of norms]]: :$\exists M \in \R_{> 0} : \norm {x_n - x_m}_b \le M \norm {x_n - x_m}_a < M \epsilon_a$ Let $\epsilon_b := M \epsilon_a$ Then: :$\forall \epsilon_b \in \R_{> 0} : \exists N \in \N : \forall n \in \N : n, m > N \implies \norm {x_n - x_m}_b < \epsilon_b$ Therefore, $\sequence {x_n}_{n \mathop \in \N}$ is also a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $M_b$. {{qed}}	1
Let $\mathbf r$ be a [[Definition:Vector Quantity|vector quantity]] embedded in a [[Definition:Cartesian 3-Space|Cartesian $3$-space]]. Let $\mathbf r$ be expressed in terms of its [[Definition:Component of Vector|components]]: :$\mathbf r = x \mathbf i + y \mathbf j + z \mathbf k$ Let $\mathbf r$ be the [[Definition:Zero Vector Quantity|zero vector]]. Then: :$x = y = z = 0$	1
Let $G$ be a [[Definition:Finite Group|finite]] [[Definition:Abelian Group|abelian group]]. Let $G^*$ be the dual group of [[Definition:Character (Number Theory)|characters]] $G \to \C^\times$. Let $\eta: G \to \C$ be a [[Definition:Mapping|mapping]] from $G$ to the set of [[Definition:Complex Number|complex numbers]]. Then for all $x \in G$: :$\displaystyle \eta \left({x}\right) = \frac 1 {\phi \left({q}\right)} \sum_{\chi \mathop \in G^*} \langle \eta, \chi \rangle_G \chi(x)$ where: :$\displaystyle \langle \eta, \chi \rangle_G = \sum_{x \mathop \in G} \eta \left({x}\right) \overline{\chi} \left({x}\right)$	1
Let $z_1 := r_1 e^{i \theta_1}, z_2 := r_2 e^{i \theta_2} \in \C$ be [[Definition:Exponential Form of Complex Number|complex numbers expressed in exponential form]]. Then: :$z_1 \circ z_2 = r_1 r_2 \, \map \cos {\theta_2 - \theta_1}$ where $z_1 \circ z_2$ denotes the [[Definition:Complex Dot Product|dot product]] of $z_1$ and $z_2$.	1
From the definition of [[Definition:Matrix Product (Conventional)|matrix product]], we have: :$\displaystyle \forall i, j \in \closedint 1 n: c_{i j} = \sum_{k \mathop = 1}^n a_{i k} b_{k j}$ Now when $i = j$ (as on the [[Definition:Diagonal Element|main diagonal]]): :$\displaystyle c_{j j} = \sum_{k \mathop = 1}^n a_{j k} b_{k j}$ Now both $\mathbf A$ and $\mathbf B$ are [[Definition:Upper Triangular Matrix|upper triangular]]. Thus: :if $k > j$, then $b_{k j} = 0$ and thus $a_{j k} b_{k j} = 0$ :if $k < j$, then $a_{j k} = 0$ and thus $a_{j k} b_{k j} = 0$. So $a_{j k} b_{k j} \ne 0$ only when $j = k$. So: :$\displaystyle c_{j j} = \sum_{k \mathop = 1}^n a_{j k} b_{k j} = a_{j j} b_{j j}$ Now if $i > j$, it follows that either $a_{i k}$ or $b_{k j}$ is [[Definition:Zero Element|zero]] for all $k$, and thus $c_{i j} = 0$. Thus $\mathbf C$ is [[Definition:Upper Triangular Matrix|upper triangular]]. The same argument can be used for when $\mathbf A$ and $\mathbf B$ are both [[Definition:Lower Triangular Matrix|lower triangular matrices]]. {{Qed}} [[Category:Triangular Matrices]] [[Category:Conventional Matrix Multiplication]] kfypsdf63kqm9gggh85dbj560bfaa6i	1
Let $V$ be a [[Definition:Normed Vector Space|normed vector space]] with [[Definition:Norm on Vector Space|norm]] $\left\Vert{\cdot}\right\Vert$ over $\R$ or $\C$. An [[Definition:Open Ball|open ball]] in the [[Definition:Metric Induced by Norm|metric induced by $\left\Vert{\cdot}\right\Vert$]] is a [[Definition:Convex Set (Vector Space)|convex set]].	1
From the [[Equation of Straight Line in Plane/Normal Form|Normal Form of Equation of Straight Line in Plane]], a general [[Definition:Straight Line|straight line]] can be expressed in the form: :$x \cos \alpha + y \sin \alpha = p$ where: :$p$ is the [[Definition:Length of Line|length]] of a [[Definition:Perpendicular|perpendicular]] $\mathcal P$ from $\mathcal L$ to the [[Definition:Origin|origin]]. :$\alpha$ is the [[Definition:Angle|angle]] made between $\mathcal P$ and the [[Definition:X-Axis|$x$-axis]]. As $\mathcal L$ is [[Definition:Vertical Line|vertical]], then by definition $\mathcal P$ is [[Definition:Horizontal Line|horizontal]]. By definition, the [[Definition:Horizontal Line|horizontal line]] through the [[Definition:Origin|origin]] is the [[Definition:X-Axis|$x$-axis]] itself. Thus $\alpha = 0$ and $p = a$ Hence the [[Definition:Equation of Geometric Figure|equation]] of $\mathcal L$ becomes: {{begin-eqn}} {{eqn | l = x \cos 0 + y \sin 0 | r = a | c = }} {{eqn | ll= \leadsto | l = x | r = a | c = [[Sine of Zero is Zero]], [[Cosine of Zero is One]] }} {{end-eqn}} Hence the result. {{qed}} [[Category:Equations of Straight Lines in Plane]] oee6f8ls5odc5y8hh8dzf1vgjk3lax1	1
Let $\struct {X, \Sigma, \mu}$ be a [[Definition:Measure Space|measure space]]. Let $f, g: X \to \R$ be [[Definition:Square Integrable Function|$\mu$-square integrable functions]], that is $f, g \in \map {\LL^2} \mu$, [[Definition:Lebesgue Space|Lebesgue $2$-space]]. Then: :$\displaystyle \int \size {f g} \rd \mu \le \norm f_2^2 \cdot \norm g_2^2$ where $\norm {\, \cdot \,}_2$ is the [[Definition:P-Norm|$2$-norm]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $L$ be a [[Definition:Bounded Linear Functional|bounded linear functional]] on $H$. {{explain|We can weaken the assumption that $H$ be a Hilbert space by saying that it is just a normed space, I think. It is not strictly necessary to consider a Hilbert space.}} Define the following [[Definition:Norm on Bounded Linear Functional|norms]] of $L$: :$(1): \quad \left\|{L}\right\|_1 = \sup \left\{{\left|{Lh}\right|: \left\|{h}\right\| \le 1}\right\}$ :$(2): \quad \left\|{L}\right\|_2 = \sup \left\{{\left|{Lh}\right|: \left\|{h}\right\| = 1}\right\}$ :$(3): \quad \left\|{L}\right\|_3 = \displaystyle \sup \left\{{\dfrac {\left|{Lh}\right|} {\left\|{h}\right\|}: h \in H\setminus \left\{\mathbf 0 \right\}}\right\}$ :$(4): \quad \left\|{L}\right\|_4 = \inf \left\{{c > 0: \forall h \in H: \left|{Lh}\right| \le c \left\|{h}\right\|}\right\}$ Then: : $\left\|{L}\right\|_1 = \left\|{L}\right\|_2 = \left\|{L}\right\|_3 = \left\|{L}\right\|_4$	1
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N_{> 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: : $\cmod {z_1 z_2 \cdots z_n} = \cmod {z_1} \cdot \cmod {z_2} \cdots \cmod {z_n}$ $P \left({1}\right)$ is trivially true: :$\cmod {z_1} = \cmod {z_1}$ === Basis for the Induction === $P \left({2}\right)$ is the case: : $\cmod {z_1 z_2} = \cmod {z_1} \cdot \cmod {z_2}$ which has been proved in [[Complex Modulus of Product of Complex Numbers]]. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: : $\cmod {z_1 z_2 \cdots z_k} = \cmod {z_1} \cdot \cmod {z_2} \cdots \cmod {z_k}$ Then we need to show: : $\cmod {z_1 z_2 \cdots z_{k + 1} } = \cmod {z_1} \cdot \cmod {z_2} \cdots \cmod {z_{k + 1} }$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \cmod {z_1 z_2 \cdots z_{k + 1} } | r = \cmod {\left({z_1 z_2 \cdots z_k}\right) z_{k + 1} } | c = }} {{eqn | r = \cmod {z_1 z_2 \cdots z_k} \cdot \cmod {z_{k + 1} } | c = [[Complex Modulus of Product of Complex Numbers/General Result#Basis for the Induction|Basis for the Induction]] }} {{eqn | r = \cmod {z_1} \cdot \cmod {z_2} \cdots \cmod {z_k} \cdot \cmod {z_{k + 1} } | c = [[Complex Modulus of Product of Complex Numbers/General Result#Induction Hypothesis|Induction Hypothesis]] }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: : $\forall n \in \N_{>0}: \cmod {z_1 z_2 \cdots z_n} = \cmod {z_1} \cdot \cmod {z_2} \cdots \cmod {z_n}$ {{qed}}	1
Let $G$ be an [[Definition:Module|$R$-module]]. Let $\O \subset S \subseteq G$. Let $b \in G$ be a [[Definition:Linear Combination of Sequence|linear combination of some sequence]] $\sequence {a_n}$ of [[Definition:Element|elements]] of $S$. Then $b$ is a '''linear combination of $S$'''.	1
Suppose: :$\displaystyle \sum_{k \mathop = 1}^r \lambda_k \phi \left({a_k}\right) = 0$ Then: :$\displaystyle \phi \left({\sum_{k \mathop = 1}^r \lambda_k a_k}\right) = 0$ So $\displaystyle \sum_{k \mathop = 1}^r \lambda_k \phi \left({a_k}\right)$ belongs to the [[Definition:Kernel of Linear Transformation|kernel]] of $\phi$ and hence is also a [[Definition:Linear Combination|linear combination]] of $\left\{{a_k: r + 1 \le k \le n}\right\}$. Thus $\forall k \in \left[{1 \,.\,.\, r}\right]: \lambda_k = 0$ since $\left \langle {a_n} \right \rangle$ is [[Definition:Linearly Independent Sequence|linearly independent]]. Thus the sequence $\left \langle {\phi \left({a_r}\right)} \right \rangle$ is linearly independent. We have $\forall k \in \left[{r + 1 \,.\,.\, n}\right]: \phi \left({a_k}\right) = 0$. So let $x \in G$. Let $\displaystyle x = \sum_{k \mathop = 1}^n \mu_k a_k$. Then: : $\displaystyle \phi \left({x}\right) = \sum_{k \mathop = 1}^n \mu_k \phi \left({a_k}\right) = \sum_{k \mathop = 1}^r \mu_k \phi \left({a_k}\right)$ Therefore $\left \langle {\phi \left({a_r}\right)} \right \rangle$ is an [[Definition:Ordered Basis|ordered basis]] of the [[Definition:Image of Mapping|image]] of $\phi \left({G}\right)$. {{Qed}}	1
Let $M \in \map {\mathrm O} {n, \R}$ be an [[Definition:Orthogonal Matrix|orthogonal matrix]]. Let $\lambda^n$ be [[Definition:Lebesgue Measure|$n$-dimensional Lebesgue measure]]. Then the [[Definition:Pushforward Measure|pushforward measure]] $M_* \lambda^n$ equals $\lambda^n$.	1
Let $T$ be [[Definition:Separable Space|separable]]. By [[Space is Separable iff Density not greater than Aleph Zero]]: :$\map d T \le \aleph_0$ where: :$\map d T$ denotes the [[Definition:Density of Topological Space|density]] of $T$ :$\aleph$ denotes the [[Definition:Aleph Mapping|aleph mapping]]. By definition of [[Definition:Density of Topological Space|density]]: :$\exists A \subseteq S: A$ is [[Definition:Everywhere Dense|dense]] $\land \map d T = \card A$ where $\card A$ denotes the [[Definition:Cardinality|cardinality]] of $A$. By definition of [[Definition:Everywhere Dense|dense set]]: :$A^- = S$ where $A^-$ denotes the [[Definition:Closure (Topology)|closure]] of $A$. By [[Set in Discrete Topology is Clopen]]: :$A$ is [[Definition:Closed Set (Topology)|closed]] Then by [[Set is Closed iff Equals Topological Closure]]: :$A^- = A$ Thus by [[Countable iff Cardinality not greater than Aleph Zero]]: :$S$ is [[Definition:Countable Set|countable]] {{qed}}	1
Let $\struct {G, +_G, \circ}_R$ and $\struct {H, +_H, \circ}_R$ be [[Definition:Module|$R$-modules]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation|linear transformation]]. Let $\map Z R$ be the [[Definition:Center of Ring|center]] of the [[Definition:Scalar Ring of Module|scalar ring]] $R$. Let $\lambda \in \map Z R$. Then $\lambda \circ \phi$ is a [[Definition:Linear Transformation|linear transformation]].	1
It needs to be demonstrated that there exists a [[Definition:Separable Space|separable topological space]] which has a [[Definition:Topological Subspace|subspace]] which is [[Definition:Closed Set (Topology)|closed]] but not [[Definition:Separable Space|separable]]. Consider an [[Definition:Uncountable Particular Point Space|uncountable particular point space]] $T = \struct {S, \tau_p}$. From [[Particular Point Space is Separable]], $T$ is [[Definition:Separable Space|separable]]. By definition, the [[Definition:Particular Point Topology|particular point $p$]] is an [[Definition:Open Point|open point]] of $T$. Thus the [[Definition:Subset|subset]] $S \setminus \set p$ is by definition [[Definition:Closed Set (Topology)|closed]] in $T$. But from [[Separability in Uncountable Particular Point Space]], $S \setminus \set p$ is not [[Definition:Separable Space|separable]]. Thus by [[Proof by Counterexample]], [[Definition:Separable Space|separability]] is not [[Definition:Weakly Hereditary Property|weakly hereditary]]. {{qed}}	1
From [[External Direct Product of Abelian Groups is Abelian Group]] it follows that $(M,+)$ is an [[Definition:Abelian Group|abelian group]]. We need to show that: $\forall x, y, \in M, \forall \lambda, \mu \in R$: : $(1): \quad \lambda \circ \left({x + y}\right) = \left({\lambda \circ x}\right) + \left({\lambda \circ y}\right)$ : $(2): \quad \left({\lambda +_R \mu}\right) \circ x = \left({\lambda \circ x}\right) + \left({\mu \circ x}\right)$ : $(3): \quad \left({\lambda \times_R \mu}\right) \circ x = \lambda \circ \left({\mu \circ x}\right)$ Checking the criteria in order: === Criterion 1 === : $(1): \quad \lambda \circ \left({x + y}\right) = \left({\lambda \circ x}\right) + \left({\lambda \circ y}\right)$ Let $x = \left({x_i}\right)_{i\in I}, y = \left({y_i}\right)_{i\in I} \in M$. {{begin-eqn}} {{eqn | l = \lambda \circ \left({x + y}\right) | r = \lambda \circ \left({ \left({x_i}\right)_{i\in I} + \left({y_i}\right)_{i\in I} }\right) | c = }} {{eqn | r = \lambda \circ \left({x_i+_iy_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda\circ_i x_i+_i \lambda\circ_i y_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda\circ_i x_i}\right)_{i\in I}+\left({\lambda\circ_i y_i}\right)_{i\in I} | c = }} {{eqn | r = \lambda\circ\left({x_i}\right)_{i\in I}+\lambda\circ\left({y_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda \circ x}\right) + \left({\lambda \circ y}\right) | c = }} {{end-eqn}} So $(1)$ holds. {{qed|lemma}} === Criterion 2 === : $(2): \quad \left({\lambda +_R \mu}\right) \circ x = \left({\lambda \circ x}\right) + \left({\mu \circ x}\right)$ Let $x = \left({x_i}\right)_{i\in I} \in M$. {{begin-eqn}} {{eqn | l = \left({\lambda +_R \mu}\right) \circ x | r = \left({\lambda +_R \mu}\right) \circ \left({x_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\left({\lambda +_R \mu}\right) \circ_i x_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda \circ_i x_i +_i \mu\circ_i x_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda \circ_i x_i}\right)_{i\in I} + \left({\mu\circ_i x_i}\right)_{i\in I} | c = }} {{eqn | r = \lambda \circ \left({x_i}\right)_{i\in I} + \lambda \circ \left({y_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda \circ x}\right) + \left({\mu \circ x}\right) | c = }} {{end-eqn}} So $(2)$ holds. {{qed|lemma}} === Criterion 3 === : $(3): \quad \left({\lambda \times_R \mu}\right) \circ x = \lambda \circ \left({\mu \circ x}\right)$ Let $x = \left({x_i}\right)_{i\in I} \in M$. {{begin-eqn}} {{eqn | l = \left({\lambda \times_R \mu}\right) \circ x | r = \left({\lambda \times_R \mu}\right) \circ \left({x_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\left({\lambda \times_R \mu}\right)\circ_i x_i}\right)_{i\in I} | c = }} {{eqn | r = \left({\lambda\circ_i\left({\mu\circ_i x_i}\right)}\right)_{i\in I} | c = }} {{eqn | r = \lambda \circ \left({\mu\circ_i x_i}\right)_{i\in I} | c = }} {{eqn | r = \lambda \circ \left({ \mu \circ \left({x_i}\right)_{i\in I} }\right) | c = }} {{eqn | r = \lambda \circ \left({\mu \circ x}\right) | c = }} {{end-eqn}} So $(3)$ holds. {{qed|lemma}} Thus all criteria are seen to hold. The result follows. {{Qed}}	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $K$. Then $V$ has a [[Definition:Basis of Vector Space|basis]].	1
Let $\sequence {a_n}_{n \mathop \in \N}, \sequence {b_n}_{n \mathop \in \N}, \sequence {c_n}_{n \mathop \in \N} \in \ell^p$. Let $\lambda, \mu \in \R$. Let $\sequence 0 := \tuple {0, 0, 0, \dots}$ be a [[Definition:Real-Valued Function|real-valued function]]. Let us use [[Definition:Real Number|real number]] [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]]. Define [[Definition:Pointwise Addition on Ring of Sequences|pointwise addition]] as: :$\sequence {a_n + b_n}_{n \mathop \in \N} := \sequence {a_n}_{n \mathop \in \N} +_\R \sequence {b_n}_{n \mathop \in \N}$. Define [[Definition:Pointwise Scalar Multiplication on Ring of Sequences|pointwise scalar multiplication]] as: :$\sequence {\lambda \cdot a_n}_{n \mathop \in \N} := \lambda \times_\R \sequence {a_n}_{n \mathop \in \N}$ Let the [[Definition:Ring of Sequences/Additive Inverse|additive inverse]] be $\sequence {-a_n} := - \sequence {a_n}$. === Closure Axiom === By [[Definition:Assumption|assumption]], $\sequence {a_n}_{n \mathop \in \N}, \sequence {b_n}_{n \mathop \in \N} \in \ell^p$. By [[Definition:P-Sequence Space|definition]]: :$\displaystyle \sum_{n \mathop = 1}^\infty \size {a_n}^p < \infty$ :$\displaystyle \sum_{n \mathop = 1}^\infty \size {b_n}^p < \infty$ Consider the [[Definition:Real Sequence|sequence]] $\sequence {a_n + b_n}$. Then: {{begin-eqn}} {{eqn| l = \sum_{n \mathop = 1}^\infty \size {a_n + b_n}^p | o = \le | r = \sum_{n \mathop = 1}^\infty \paren {\size {a_n} + \size {b_n} }^p }} {{eqn| o = \le | r = \sum_{n \mathop = 1}^\infty \paren {\map \max {\size {a_n}, \size {b_n} } + \map \max {\size {a_n}, \size {b_n} } }^p | c = {{defof|Max Operation}} }} {{eqn| r = \sum_{n \mathop = 1}^\infty 2^p \paren {\map \max {\size {a_n}, \size {b_n} } }^p }} {{eqn | o = \le | r = 2^p \sum_{n \mathop = 1}^\infty \paren {\size {a_n}^p + \size {b_n}^p} }} {{eqn | o = < | r = \infty | c = $\sequence {a_n}, \sequence {b_n} \in \ell^p$ }} {{end-eqn}} Hence: :$\sequence {a_n + b_n} \in \ell^p$ {{qed|lemma}} === Commutativity Axiom === By [[Pointwise Addition on Ring of Sequences is Commutative]], $\sequence {a_n} + \sequence {b_n} = \sequence {b_n} + \sequence {a_n}$ {{qed|lemma}} === Associativity Axiom === By [[Pointwise Addition on Ring of Sequences is Associative]], $\paren {\sequence {a_n} + \sequence {b_n} } + \sequence {c_n} = \sequence {a_n} + \paren {\sequence {b_n} + \sequence {c_n} }$. {{qed|lemma}} === Identity Axiom === {{begin-eqn}} {{eqn | l = \sequence {0 + a_n} | r = \sequence 0 +_\R \sequence {a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \tuple {0, 0, 0, \dots} +_\R \sequence {a_n} | c = Definition of $\sequence 0$ }} {{eqn | r = \sequence {a_n} }} {{end-eqn}} {{qed|lemma}} === Inverse Axiom === {{begin-eqn}} {{eqn | l = \sequence {a_n + \paren {-a_n} } | r = \sequence {a_n} +_\R \sequence {-a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \sequence {a_n} +_\R \paren {-1} \times_\R \sequence {a_n} | c = Definition of $\sequence {-a_n}$ }} {{eqn | r = 0 }} {{end-eqn}} {{qed|lemma}} === Distributivity over Scalar Addition === {{begin-eqn}} {{eqn | l = \sequence {\paren {\lambda +_\R \mu} \cdot a_n } | r = \paren {\lambda +_\R \mu} \times_\R \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \lambda \times_\R \sequence {a_n} +_\R \mu \times_\R \sequence {a_n} | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \sequence {\lambda \cdot a_n} +_\R \sequence {\mu \cdot a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot a_n + \mu \cdot a_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Distributivity over Vector Addition === {{begin-eqn}} {{eqn | l = \lambda \times_\R \sequence {a_n + b_n} | r = \lambda \times_\R \paren {\sequence {a_n} +_\R \sequence {b_n} } | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{eqn | r = \lambda \times_\R \sequence {a_n} +_\R \lambda \times_\R \sequence {b_n} | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \sequence {\lambda \cdot a_n} +_\R \sequence {\lambda \cdot b_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot a_n + \mu \cdot b_n} | c = {{Defof|Pointwise Addition on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Associativity with Scalar Multiplication === {{begin-eqn}} {{eqn | l = \sequence {\paren {\lambda \times_\R \mu} \cdot a_n} | r = \paren {\lambda \times_\R \mu} \times_\R \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \lambda \times_\R \paren {\mu \times_\R \sequence {a_n} } | c = [[Real Multiplication is Associative]] }} {{eqn | r = \lambda \times_\R \sequence {\mu \cdot a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {\lambda \cdot \paren {\mu \cdot a_n} } | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{end-eqn}} {{qed|lemma}} === Identity for Scalar Multiplication === {{begin-eqn}} {{eqn | l = \sequence {1 \cdot a_n} | r = 1 \times_\R \sequence {a_n} | c = {{Defof|Pointwise Scalar Multiplication on Ring of Sequences}} }} {{eqn | r = \sequence {a_n} }} {{end-eqn}} {{qed|lemma}} {{qed}}	1
We will demonstrate this for each of the $3$ types of [[Definition:Elementary Row Operation|elementary row operation]]. Let $\sim$ denote [[Definition:Row Equivalence|row equivalence]]. Let $\mathbf I$ denote the [[Definition:Unit Matrix|unit matrix]]. === $\text {ERO} 1$: Scalar Product of Row === Let $e_1$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e_1 := r_i \to \lambda r_i$ for an arbitrary [[Definition:Row of Matrix|row]] $r_i$ of $\mathbf I$. Let $\mathbf E_1$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] created by applying $e_1$ to $\mathbf I$. From [[Existence of Inverse Elementary Row Operation/Scalar Product of Row|Existence of Inverse Elementary Row Operation: Scalar Product of Row]], the [[Definition:Inverse of Elementary Row Operation|inverse]] $e_1'$ of $e_1$ is given by: :$e_1' := r_i \to \dfrac 1 \lambda r_i$ Thus applying $e_1'$ to $\mathbf E_1$ transforms $\mathbf E_1$ back to $\mathbf I$. From [[Elementary Row Operations as Matrix Multiplications]], for every [[Definition:Elementary Row Operation|elementary row operation]] there exists a corresponding [[Definition:Elementary Row Matrix|elementary row matrix]]. Let $\mathbf E_1'$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] created by applying $e_1'$ to $\mathbf I$. Then: :$\mathbf E_1 \mathbf E_1' = \mathbf E_1' \mathbf E_1 = \mathbf I$ === $\text {ERO} 2$: Add Scalar Product of Row to Another === Let $e_2$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e_2 := r_i \to r_i + \lambda r_j$ for arbitrary [[Definition:Row of Matrix|row]] $r_i$ and $r_j$ of $\mathbf I$ such that $i \ne j$. Let $\mathbf E_2$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] created by applying $e_2$ to $\mathbf I$. From [[Existence of Inverse Elementary Row Operation/Add Scalar Product of Row to Another|Existence of Inverse Elementary Row Operation: Add Scalar Product of Row to Another]], the [[Definition:Inverse of Elementary Row Operation|inverse]] $e_2'$ of $e_2$ is given by: :$e_2' := r_i \to r_i - \lambda r_j$ Thus applying $e_2'$ to $\mathbf E_2$ transforms $\mathbf E_2$ back to $\mathbf I$. From [[Elementary Row Operations as Matrix Multiplications]], for every [[Definition:Elementary Row Operation|elementary row operation]] there exists a corresponding [[Definition:Elementary Row Matrix|elementary row matrix]]. Let $\mathbf E_2'$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] created by applying $e_2'$ to $\mathbf I$. Then: :$\mathbf E_2 \mathbf E_2' = \mathbf E_2' \mathbf E_2 = \mathbf I$ === $\text {ERO} 3$: Exchange Rows === Let $e_3$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e_3 := r_i \leftrightarrow r_j$ for arbitrary [[Definition:Row of Matrix|row]] $r_i$ and $r_j$ of $\mathbf I$ such that $i \ne j$. Let $\mathbf E_3$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] created by applying $e_3$ to $\mathbf I$. From [[Existence of Inverse Elementary Row Operation/Exchange Rows|Existence of Inverse Elementary Row Operation: Exchange Rows]], the [[Definition:Inverse of Elementary Row Operation|inverse]] $e_3'$ of $e_3$ is $e_3$ itself: :$e_3' := r_i \leftrightarrow r_j = e_3$ Thus applying $e_3$ to $\mathbf E_3$ transforms $\mathbf E_3$ back to $\mathbf I$. :$\mathbf E_3 \mathbf E_3 = \mathbf I$ Hence the result, from [[Proof by Cases]]. {{qed}}	1
It is shown that $\struct{G, +_G, \circ’}$ satisfies the [[Definition:Right Module Axioms|right module axioms]] By definition of the [[Definition:Opposite Ring|opposite ring]]: :$\forall x, y \in R: x *_R y = y \times_R x$. === $(RM \, 1)$ : Scalar Multiplication (Right) Distributes over Module Addition === Let $\lambda \in R$ and $x, y \in G$. {{begin-eqn}} {{eqn | l = \paren{x +_G y} \circ’ \lambda | r = \lambda \circ \paren{x +_G y} | c = Definition of $\circ’$ }} {{eqn | r = \lambda \circ x +_G \lambda \circ y | c = [[Definition:Left Module Axioms|Left module axiom $(M \, 1)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = x \circ’ \lambda +_G y \circ’ \lambda | c = Definition of $\circ’$ }} {{end-eqn}} {{qed|lemma}} === $(RM \, 2)$ : Scalar Multiplication (Left) Distributes over Scalar Addition === Let $\lambda, \mu \in R$ and $x \in G$. {{begin-eqn}} {{eqn | l = x \circ’ \paren {\lambda +_S \mu} | r = \paren {\lambda +_R \mu} \circ x | c = Definition of $\circ’$ }} {{eqn | r = \lambda \circ x +_G \mu \circ x | c = [[Definition:Left Module Axioms|Left module axiom $(M \, 2)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = x \circ’ \lambda +_G x \circ’ \mu | c = Definition of $\circ’$ }} {{end-eqn}} {{qed|lemma}} === $(RM \, 3)$ : Associativity of Scalar Multiplication === Let $\lambda, \mu \in S$ and $x \in G$. {{begin-eqn}} {{eqn | l = x \circ’ \paren {\lambda *_R \mu} | r = \paren {\lambda *_R \mu} \circ x | c = Definition of $\circ’$ }} {{eqn | r = \paren {\mu \times_R \lambda} \circ x | c = Definition of $*_R$ }} {{eqn | r = \mu \circ \paren {\lambda \circ x} | c = [[Definition:Left Module Axioms|Left module axiom $(M \, 3)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = \mu \circ \paren{x \circ’ \lambda} | c = Definition of $\circ’$ }} {{eqn | r = \paren{x \circ’ \lambda} \circ’ \mu | c = Definition of $\circ’$ }} {{end-eqn}} {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\struct {U_R, \circ}$ be the [[Definition:Group of Units of Ring|group of units]] of $\struct {R, +, \circ}$. Let $a, b \in U_R$. Then: :$\paren {\dfrac a b}^{-1} = \dfrac {1_R} {\paren {a / b}} = \dfrac b a$ where $\dfrac x z$ is defined as $x \circ \paren {z^{-1} }$, that is, $x$ [[Definition:Division Product|divided by]] $z$.	1
Let $X = D^-$. We have to show, that for every $x \in X$ there is an [[Definition:Open Ball in Normed Vector Space|open ball]] with an [[Definition:Element|element]] from $D^-$. We have that $X = D \cup \paren {X \setminus D}$. Suppose $x \in X \setminus D$. Then $x \in D^- \setminus D$. Hence, $x$ is a [[Definition:Limit Point (Normed Vector Space)|limit point]] of $D$. Therefore, there is a [[Definition:Sequence|sequence]] $\sequence {d_n}_{n \mathop \in \N}$ in $D$ which [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $x$. Thus: :$\forall \epsilon \in \R_{> 0} : \exists N \in \N : \norm {x - d_N} < \epsilon$ In other words: :$\displaystyle d_N \in D \implies d_N \in \map {B_\epsilon} x$ Therefore: :$d_N \in D \cap \map {B_\epsilon} x$. Suppose $x \in D$. Let $\epsilon > 0$. Then $x \in \map {B_\epsilon} x \cap D$. From both parts and definition we conclude that $D^-$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] in $X$.	1
{{ProofWanted|from [[Scalar Triple Product equals Determinant]], and a result about $3 \times 3$ determinants.}}	1
Let $M$ be a [[Definition:Topological Space|topological space]]. Let $\mathscr F, \mathscr G$ be $d$-[[Definition:Dimension of Atlas|dimensional]] [[Definition:Atlas|atlases]] of [[Definition:Class of Atlas|class]] $C^k$ on $M$. {{TFAE|def = Compatible Atlases}}	1
Let $V$ be a [[Definition:Vector Space|$K$-vector space]]. Let $\phi: V \to V$ be a [[Definition:Vector Space Isomorphism|vector space isomorphism]] to itself. Then $\phi$ is a '''vector space automorphism'''.	1
Let $\alpha_1, \alpha_2, \ldots, \alpha_n$ be a [[Definition:Generator of Vector Space|generator]] of $V$. Let $\xi_1, \xi_2, \ldots, \xi_r$ be a [[Definition:Linearly Independent Set|linearly independent set]] of [[Definition:Element|elements]] of $V$. Hence the [[Definition:Sequence|sequence]] $\sequence {\xi_1, \alpha_1, \alpha_2, \ldots, \alpha_n}$ is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]] of [[Definition:Element|elements]] of $V$. One of these [[Definition:Element|elements]], which cannot be $\xi_1$, is a [[Definition:Linear Combination of Subset|linear combination]] of the preceding [[Definition:Element|elements]]. Let this [[Definition:Element|elements]] be $\alpha_i$. So we can omit $\alpha_i$ from that [[Definition:Sequence|sequence]], and the remaining [[Definition:Set|set]] is still a [[Definition:Generator of Vector Space|generator]] of $V$. Therefore $\xi_2$ is a [[Definition:Linear Combination of Subset|linear combination]] of these. Thus $\sequence {\xi_2, \xi_1, \alpha_1, \alpha_2, \ldots, \alpha_{i - 1}, \alpha {i + 1}, \ldots, \alpha_n}$ is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]] of [[Definition:Element|elements]] of $V$. Again, one of them is a [[Definition:Linear Combination of Subset|linear combination]] of the preceding [[Definition:Element|elements]]. This cannot be $\xi_2$, as $\xi_2$ has no preceding [[Definition:Element|elements]]. Neither can it be $\xi_1$, as $\xi_1$ and $\xi_2$ are [[Definition:Linearly Independent Set|linearly independent]]. Thus we can omit whichever $\alpha_j$ it is, and we have a new [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$. This consists of $\xi_1$, $\xi_2$ and whichever $n - 2$ of the remaining [[Definition:Element|elements]] of $\set {\alpha_1, \alpha_2, \ldots, \alpha_n}$. After $p$ such steps, we have a [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$ which consists of: :$\xi_1, \xi_2, \ldots, \xi_p$ and $n - p$ of the [[Definition:Element|elements]] of $\set {\alpha_1, \alpha_2, \ldots, \alpha_n}$. {{AimForCont}} suppose $n < r$. Then when $p = n$, the remaining [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$ consists of: :$\xi_1, \xi_2, \ldots, \xi_n$ and there is at least one more [[Definition:Element|element]] $\xi_{n + 1}$. This is a [[Definition:Linear Combination of Subset|linear combination]] of $\set {\xi_1, \xi_2, \ldots, \xi_n}$. But this [[Definition:Contradiction|contradicts]] the supposition that $\set {\xi_1, \xi_2, \ldots, \xi_n, \xi_{n + 1} }$ is a [[Definition:Linearly Independent Set|linearly independent set]]. Hence, by [[Proof by Contradiction]], $n \ge r$. The result follows. {{qed}}	1
Let $x, y, z \in V$. Because $f$ is [[Definition:Bilinear Form|bilinear]]: :$\map f {x, \map f {x, y} z - \map f {x, z} y} = \map f {x, y} \, \map f {x, z} - \map f {x, z} \, \map f {x, y} = 0$ Because $f$ is [[Definition:Reflexive Bilinear Form|reflexive]]: :$\map f {\map f {x, y} z - \map f {x, z} y, x} = 0$ Because $f$ is [[Definition:Bilinear Form|bilinear]]: :$(1): \quad \map f {x, y} \, \map f {z, x} = \map f {x, z} \, \map f {y, x}$ Letting $z = x$, we obtain: :$\map f {x, x} \, \map f {x, y} = \map f {x, x} \, \map f {y, x}$ Thus: :$(2): \quad$ if $\map f {x, x} \ne 0$, then $\map f {x, y} = \map f {y, x}$ for all $y \in V$. Suppose $f$ is not [[Definition:Symmetric Bilinear Form|symmetric]]. We show that $\map f {u, u} = 0$ for all $u \in V$. Let $v, w \in V$ with $\map f {v, w} \ne \map f {w, v}$. By $(2)$, $\map f {v, v} = \map f {w, w} = 0$. Suppose $u \in V$ with $\map f {u, u} \ne 0$. By $(2)$, $\map f {u, v} = \map f {v, u}$ and $\map f {u, w} = \map f {w, u}$. By $(1)$, we have: :$\begin{cases} \map f {v, w} \, \map f {u, v} = \map f {v, u} \, \map f {w, v} \\ \map f {w, v} \, \map f {u, w} = \map f {w, u} \, \map f {v, w} \end{cases}$. Because $\map f {v, w} \ne \map f {w, v}$: :$\begin{cases} \map f {u, v} = \map f {v, u} = 0 \\ \map f {u, w} = \map f {w, u} = 0 \end{cases}$. Because $f$ is [[Definition:Bilinear Form|bilinear]], $\map f {u + v, w} = \map f {v, w} \ne \map f {v, w} = \map f {u + v, w}$. By $(2)$, $\map f {u + v, u + v} = 0$. Because $f$ is [[Definition:Bilinear Form|bilinear]] and $\map f {u, v} = \map f {v, u} = \map f {v, v} = 0$, $\map f {u, u} = 0$. This [[Definition:Contradiction|contradicts]] the assumption $\map f {u, u} \ne 0$. Thus $f$ is [[Definition:Alternating Bilinear Form|alternating]]. {{qed}}	1
Matrices $P$ and $Q$ are invertible because all diagonal elements are nonzero. For $1\le i \le n$ express polynomial $p_i$ as: :$\displaystyle \map {p_i} {x} = \sum_{k \mathop = 1}^n a_{ik} x^{k-1}$ Then: {{begin-eqn}} {{eqn | l = \left( \map {p_i} {x_j} \right) | r = \left( a_{ij} \right)V_x | c = [[Definition:Matrix Multiplication]] }} {{eqn | l = P | r = \left( a_{ij} \right) V_x | c = Because $\map {p_i} {x_j} = 0$ for $i \ne j$. }} {{eqn | l = \left( a_{ij} \right) | r = P V_x^{-1} | c = Solve for matrix $\left( a_{ij} \right)$ }} {{eqn | l = \left( \map {p_i} {y_j} \right) | r = \left( a_{ij} \right)V_y | c = [[Definition:Matrix Multiplication]] }} {{eqn | l = \left( p_i\left( y_j \right) \right) | r = P V_x^{-1} V_y | c = Substitute $\left( a_{ij} \right) = P V_x^{-1}$. }} {{end-eqn}} Use second equation $\map {p_i} {y_j} = \dfrac{ \map {p} {y_j} }{y_j - x_i}$: {{begin-eqn}} {{eqn | l = \left( \map {p_i} {y_j} \right) | r = -CQ | c = [[Definition:Matrix Multiplication]] }} {{eqn | l = -CQ | r = P V_x^{-1} V_y | c = Equate competing equations for $\left( \map {p_i} {y_j} \right)$. }} {{eqn | l = C | r = -P V_x^{-1} V_y Q^{-1} | c = Solve for $C$. }} {{end-eqn}} {{qed}}	1
Let $A$ be a [[Definition:Non-Trivial Ring|non-trivial]] [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Then its [[Definition:Maximal Spectrum of Ring|maximal spectrum]] is [[Definition:Non-Empty Set|non-empty]]: :$\operatorname {Max} \Spec A \ne \O$	1
Let $x_0 \in R$. Let $\epsilon \in \R_{>0}$. Let $x \in R: \norm {x - x_0} < \epsilon$. Then: {{begin-eqn}} {{eqn | l = \size {\norm x - \norm {x_0} } | o = \le | r = \norm {x - x_0} | c = [[Reverse Triangle Inequality/Normed Division Ring|Reverse triangle inequality]] }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} By the definition of [[Definition:Metric Induced by Norm|metric induced by a norm]] and the definition of a [[Definition:Continuous Mapping (Metric Space)|continuous mapping]], $\norm {\,\cdot\,}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]]. {{qed}} [[Category:Normed Division Rings]] [[Category:Norm Theory]] 81dpy5f0xjivpcxu1h0pvr5llskobwv	1
{{ProofWanted}} {{Namedfor|Semyon Aranovich Gershgorin|cat = Gershgorin}}	1
From [[Matrix Multiplication is Homogeneous of Degree 1|Matrix Multiplication is Homogeneous of Degree $1$]]: :$\forall \lambda \in \mathbb F \in \set {\R, \C}: \mathbf A \paren {\lambda \mathbf x} = \lambda \paren {\mathbf A \mathbf x}$ From [[Matrix Multiplication Distributes over Matrix Addition]]: :$\forall \mathbf x, \mathbf y \in \R^m: \mathbf A \paren {\mathbf x + \mathbf y} = \mathbf A \mathbf x + \mathbf A \mathbf y$ Hence the result, from the definition of [[Definition:Linear Transformation on Vector Space|linear transformation]]. {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $\map B {H, K}$ denote the [[Definition:Space of Bounded Linear Transformations|space of bounded linear transformations from $H$ to $K$]]. Let $\Bbb F \in \set {\R, \C}$ denote the [[Definition:Ground Field|ground field]] of $K$. Now $\map B {H, K} \subseteq K^H$, the [[Definition:Set of All Mappings|set of mappings from $H$ to $K$]]. Therefore, $\map B {H, K}$ can be endowed with [[Definition:Pointwise Addition of Mappings|pointwise addition]] ($+$) and [[Definition:Pointwise Scalar Multiplication of Mappings|($\Bbb F$)-scalar multiplication]] ($\circ$). Let $\norm{\,\cdot\,}$ denote the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear transformations]]. Then $\norm{\,\cdot\,}$ is a [[Definition:Norm on Vector Space|norm]] on $\map B {H, K}$. Furthermore, $B \left({H, K}\right)$ is a [[Definition:Banach Space|Banach space]] with respect to this [[Definition:Norm on Vector Space|norm]].	1
Let $M = \left({A, d}\right)$ be a [[Definition:Totally Bounded Metric Space|totally bounded metric space]]. By the definition of [[Definition:Totally Bounded Metric Space|total boundedness]], we can use the [[Axiom:Axiom of Countable Choice|axiom of countable choice]] to construct a [[Definition:Sequence|sequence]] $\left\langle{F_n}\right\rangle_{n \ge 1}$ such that: :For all [[Definition:Natural Numbers|natural numbers]] $n \ge 1$, $F_n$ is a [[Definition:Finite Net|finite $\left({1/n}\right)$-net]] for $M$. Let $\displaystyle S = \bigcup_{n \mathop \ge 1} F_n$. From [[Countable Union of Countable Sets is Countable]], it follows that $S$ is [[Definition:Countable Set|countable]]. It suffices to prove that $S$ is [[Definition:Everywhere Dense|everywhere dense]] in $M$. Let $S^-$ denote the [[Definition:Closure (Topology)|closure]] of $S$. Let $x \in X$. Let $U \subseteq X$ be [[Definition:Open Set of Metric Space|open]] in $M$ such that $x \in U$. By definition, there exists a [[Definition:Strictly Positive Real Number|strictly positive real number]] $\epsilon$ such that $B_{\epsilon} \left({x}\right) \subseteq U$. That is, the [[Definition:Open Ball of Metric Space|open $\epsilon$-ball of $x$ in $M$]] is [[Definition:Subset|contained]] in $U$. By the [[Archimedean Principle]], there exists a [[Definition:Natural Number|natural number]] $n > \dfrac 1 \epsilon$. That is, $\dfrac 1 n < \epsilon$, and so $B_{1 / n} \left({x}\right) \subseteq B_{\epsilon} \left({x}\right)$. Since [[Subset Relation is Transitive|$\subseteq$ is a transitive relation]], we have $B_{1/n} \left({x}\right) \subseteq U$. By the definition of a [[Definition:Net (Metric Space)|net]], there exists a $y \in F_n$ such that $x \in B_{1/n} \left({y}\right)$. By [[Definition:Metric Space Axioms|axiom $\left({M3}\right)$ for a metric]], it follows from the definition of an [[Definition:Open Ball of Metric Space|open ball]] that $y \in B_{1/n} \left({x}\right)$. Since $y \in S \cap U$, it follows that $x$ is an [[Definition:Adherent Point|adherent point]] of $S$. By definition of [[Definition:Adherent Point|adherent point]], we have $x \in S^-$. That is, $X \subseteq S^-$, and so $S$ is [[Definition:Everywhere Dense|everywhere dense]] in $M$. {{qed}} {{ACC}} [[Category:Separable Spaces]] [[Category:Metric Spaces]] [[Category:Totally Bounded Metric Spaces]] ejynvni81404dwnx6gyadutj5l949qs	1
By assumption: :$\forall n \in \N: p^n \divides \paren {x_n^k - a}$ By the definition of the [[Definition:P-adic Norm|$p$-adic norm]]: :$\forall n \in \N: \norm {x_n^k - a}_p \le \dfrac 1 {p^n}$ By [[Sequence of Powers of Number less than One]]: :$\displaystyle \lim_{n \mathop \to \infty} \dfrac 1 {p^n} = 0$ By [[Squeeze Theorem for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n^k - a}_p = 0$. By the definition of [[Definition:Convergent Sequence in Normed Division Ring|convergence]] in $\struct {\Q, \norm {\,\cdot\,}_p}$ then: :$\displaystyle \lim_{n \mathop \to \infty} x_n^k = a$ {{qed}} [[Category:P-adic Norm not Complete on Rational Numbers]] orpqibd6mumv5ml5n87nontx9ivpf76	1
Let $\struct {K, +, \circ}$ be a [[Definition:Division Ring|division ring]]. Let $\struct {G, +_G, \circ}_K$ be a [[Definition:Vector Space|$K$-vector space]]. Let $S$ be a set. Let $\struct {G^S, +_G', \circ}_R$ be the [[Definition:Vector Space of All Mappings|vector space of all mappings]] from $S$ to $G$. Then $\struct {G^S, +_G', \circ}_K$ is a [[Definition:Vector Space|$K$-vector space]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in \map B H$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Then the following are equivalent: :$(1): \quad A$ is a [[Definition:Unitary Operator|unitary operator]] :$(2): \quad A^* A = A A^* = I$, where $A^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$, and $I$ denotes the [[Definition:Identity Operator|identity operator]] :$(3): \quad A$ is a [[Definition:Normal Operator|normal]] [[Definition:Isometry (Hilbert Spaces)|isometry]]	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector Quantity|vector quantities]]. Then by [[Vector Quantity can be Expressed as Sum of 3 Non-Coplanar Vectors]], $\mathbf a$ and $\mathbf b$ can be expressed uniquely as [[Definition:Component of Vector|components]]. So if $\mathbf a$ and $\mathbf b$ then the [[Definition:Component of Vector|components]] of $\mathbf a$ are the same as the [[Definition:Component of Vector|components]] of $\mathbf b$ Suppose $\mathbf a$ and $\mathbf b$ have the same [[Definition:Component of Vector|components]]: $\mathbf x$, $\mathbf y$ and $\mathbf z$. Then by definition: :$\mathbf a = \mathbf x + \mathbf y + \mathbf z$ and also: :$\mathbf b = \mathbf x + \mathbf y + \mathbf z$ and trivially: :$\mathbf a = \mathbf b$ {{qed}}	1
:$a \in \Z_{> 0}: \nexists \,c \in \Z : c^k = a$	1
By [[Krull's Theorem]], $A$ has a [[Definition:Maximal Ideal of Ring|maximal ideal]]. By [[Maximal Ideal of Commutative and Unitary Ring is Prime Ideal]], $A$ has a [[Definition:Prime Ideal of Ring|prime ideal]]. {{qed}}	1
=== Necessary Condition === Let $\struct {G, +_G, \circ'}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Then: {{begin-eqn}} {{eqn | l = x \circ \paren {\lambda \times_R \mu} | r = \paren {\lambda \times_R \mu} \circ' x | c = Definition of $\circ'$ }} {{eqn | r = \lambda \circ' \paren {\mu \circ' x} | c = [[Definition:Left Module Axioms|Left module axiom $(\text M 3)$]]: [[Definition:Associateve Operation|Associativity]] of [[Definition:Scalar Multiplication on Module|Scalar Multiplication]] }} {{eqn | r = \paren {x \circ \mu} \circ \lambda | c = Definition of $\circ'$ }} {{eqn | r = x \circ \paren {\mu \times_R \lambda} | c = [[Definition:Right Module Axioms|Right module axiom $(\text {RM} 3)$]]: [[Definition:Associateve Operation|Associativity]] of [[Definition:Scalar Multiplication on Module|Scalar Multiplication]] }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === Let the [[Definition:Scalar Multiplication on Module|scalar multiplication]] $\circ$ satisfy: :$\forall \lambda, \mu \in R: \forall x \in G: x \circ \paren {\lambda \times_R \mu} = x \circ \paren {\mu \times_R \lambda}$ It needs to be shown that $\struct {G, +_G, \circ'}$ satisfies the [[Definition:Left Module Axioms|left module axioms]]. ==== $(\text M 1)$ : Scalar Multiplication (Left) Distributes over Module Addition ==== Let $\lambda, \mu \in R, x \in G$. Then: {{begin-eqn}} {{eqn | l = \lambda \circ' \paren {x +_G y} | r = \paren {x +_G y} \circ \lambda | c = Definition of $\circ'$ }} {{eqn | r = x \circ \lambda +_G y \circ \lambda | c = [[Definition:Right Module Axioms|Right module axiom $(\text {RM} 1)$]] on $\circ$ }} {{eqn | r = \lambda \circ' x +_G \lambda \circ' y | c = Definition of $\circ'$ }} {{end-eqn}} {{qed|lemma}} ==== $(\text M 2)$ : Scalar Multiplication (Right) Distributes over Scalar Addition ==== Let $\lambda \in R, x, y \in G$. Then: {{begin-eqn}} {{eqn | l = \paren{\lambda +_R \mu} \circ' x | r = x \circ \paren{\lambda +_R \mu} | c = Definition of $\circ'$ }} {{eqn | r = x \circ \lambda +_G x \circ \mu | c = [[Definition:Right Module Axioms|Right module axiom $(\text {RM} 2)$]] on $\circ$ }} {{eqn | r = \lambda \circ' x +_G \mu \circ' x | c = Definition of $\circ'$ }} {{end-eqn}} {{qed|lemma}} ==== $(\text M 3)$ : Associativity of Scalar Multiplication ==== Let $\lambda, \mu \in R, x \in G$. Then: {{begin-eqn}} {{eqn | l = \paren{\lambda \times_R \mu} \circ' x | r = x \circ \paren{\lambda \times_R \mu} | c = Definition of $\circ'$ }} {{eqn | r = x \circ \paren{\mu \times_R \lambda} | c = Assumption }} {{eqn | r = \paren{x \circ \mu} \circ \lambda | c = [[Definition:Right Module Axioms|Right module axiom $(\text {RM} 3)$]] on $\circ$ }} {{eqn | r = \lambda \circ' \paren {\mu \circ' x} | c = Definition of $\circ'$ }} {{end-eqn}} {{qed}}	1
=== Existence === === Uniqueness === {{ProofWanted}} [[Category:Manifolds]] lygxq4j52mhi1204vkwcnszqeea56pu	1
Let $M = \struct {X, d}$ be a [[Definition:Metric Space|metric space]]. Then: :$\forall x, y, z \in X: \size {\map d {x, z} - \map d {y, z} } \le \map d {x, y}$ === [[Reverse Triangle Inequality/Normed Division Ring|Normed Division Ring]] === {{:Reverse Triangle Inequality/Normed Division Ring}} === [[Reverse Triangle Inequality/Normed Vector Space|Normed Vector Space]] === {{:Reverse Triangle Inequality/Normed Vector Space}} === [[Reverse Triangle Inequality/Real and Complex Fields|Real and Complex Numbers]] === {{:Reverse Triangle Inequality/Real and Complex Fields}}	1
The result follows from [[Vector Space has Basis between Linearly Independent Set and Spanning Set]]. It suffices to find a [[Definition:Linearly Independent Set|linearly independent]] [[Definition:Subset|subset]] $L \subseteq V$ that is [[Definition:Contain|contained]] in a [[Definition:Spanning Set|spanning set]] $S \subseteq V$. By [[Empty Set is Linearly Independent]], $L$ can be taken to be the [[Definition:Empty Set|empty set]]. Or if $V$ is nonzero, by [[Singleton is Linearly Independent]], $L$ can be taken to be any [[Definition:Singleton|singleton]] of $V$. $S$ can be taken to be $V$, since $V$ trivially [[Definition:Spanning Set|spans]] itself. Therefore, $L$ and $S$ exist and $L \subseteq S$ so $V$ has a [[Definition:Basis of Vector Space|basis]] $B$ with $L \subseteq B \subseteq S$. {{qed}}	1
Let $T = \struct {S, \tau_p}$ be a [[Definition:Fort Space|Fort space]] on an [[Definition:Uncountable Set|uncountable set]] $S$. Then $T$ is not a [[Definition:Separable Space|separable space]].	1
The [[Definition:Integer|integers]]: :$35, 4374$ have the same [[Definition:Prime Factor|prime factors]] between them as the [[Definition:Integer|integers]]: :$36, 4375$	1
By [[Elementary Matrix corresponding to Elementary Row Operation/Scale Row and Add|Elementary Matrix corresponding to Elementary Row Operation: Scale Row and Add]], $\mathbf E_2$ is of the form: :$E_{a b} = \delta_{a b} + \lambda \cdot \delta_{a i} \cdot \delta_{j b}$ where: :$E_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf E$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$ :$\delta_{a b}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: ::$\delta_{a b} = \begin {cases} 1 & : \text {if $a = b$} \\ 0 & : \text {if $a \ne b$} \end {cases}$ Because $i \ne j$ it follows that: :if $a = i$ and $b = j$ then $a \ne b$ Hence when $a = b$ we have that: :$\delta_{a i} \cdot \delta_{j b} = 0$ Hence the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf E_2$ are all equal to $1$. We also have that $\delta_{a i} \cdot \delta_{j b} = 1$ {{iff}} $a = i$ and $b = j$. Hence, all [[Definition:Element of Matrix|elements]] of $\mathbf E_2$ apart from the [[Definition:Diagonal Element|diagonal elements]] and $a_{i j}$ are equal to $0$. Thus $\mathbf E_2$ is a [[Definition:Triangular Matrix|triangular matrix]] (either [[Definition:Upper Triangular Matrix|upper]] or [[Definition:Lower Triangular Matrix|lower]]). From [[Determinant of Triangular Matrix]], $\map \det {\mathbf E_2}$ is equal to the [[Definition:Multiplication|product]] of all the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf E_2$. But as we have seen, these are all equal to $1$. Hence the result. {{qed}}	1
=== Necessary Condition === Let $z \in \map { {B_r}^-} x \cap \map { {B_s}^-} y$. If $r \le s$ then: {{begin-eqn}} {{eqn| l = \map { {B_r}^-} x | r = \map { {B_r}^-} z | c = [[Topological Properties of Non-Archimedean Division Rings/Centers of Open Balls|Every element in an open ball is the center]] }} {{eqn| o = \subseteq | r = \map { {B_s}^-} z | c = as $r \le s$ }} {{eqn| r = \map { {B_s}^-} y | c = [[Topological Properties of Non-Archimedean Division Rings/Centers of Open Balls|Every element in an open ball is the center]] }} {{end-eqn}} Similarly, if $s \le r$ then: {{begin-eqn}} {{eqn| l = \map { {B_s}^-} y | o = \subseteq | r = \map { {B_r}^-} x }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === Let: :$\map { {B_r}^-} x \subseteq \map { {B_s}^-} y$ or: :$\map { {B_s}^-} y \subseteq \map { {B_r}^-} x$ By the definition of an [[Definition:Open Ball|open ball]] then: :$x \in \map { {B_r}^-} x \ne \O$ :$y \in \map { {B_s}^-} y \ne \O$ The result follows. {{qed}}	1
{{proof wanted|Induction}}	1
=== [[P-adic Norm not Complete on Rational Numbers/Proof 1/Case 1|Case: $p \gt 3$]] === {{:P-adic Norm not Complete on Rational Numbers/Proof 1/Case 1}}{{qed|lemma}} === [[P-adic Norm not Complete on Rational Numbers/Proof 1/Case 2|Case: $p = 2$ or $3$]] === {{:P-adic Norm not Complete on Rational Numbers/Proof 1/Case 2}}{{qed}}	1
Let $h \in H, k \in K$. Then: {{begin-eqn}} {{eqn|l = \left\langle{Ah, k}\right\rangle_K |r = \left\langle{h, A^*k}\right\rangle_H |c = Definition of [[Definition:Adjoint Linear Transformation|adjoint]] of $A$ }} {{eqn|r = \left\langle{A^{**}h, k}\right\rangle_K |c = Definition of [[Definition:Adjoint Linear Transformation|adjoint]] of $A^*$ }} {{end-eqn}} Thus, by [[Existence and Uniqueness of Adjoint]], $A^{**} = A$. {{qed}}	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \N_{> 1}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. First note that if $n$ is [[Definition:Prime Number|prime]], the result is immediate. === Basis for the Induction === $\map P 2$ is the case: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. As $2$ itself is a [[Definition:Prime Number|prime number]], and the result is immediate. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P j$ is true, for all $j$ such that $2 \le j \le k$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :For all $j \in \N$ such that $2 \le j \le k$, $j$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. from which it is to be shown that: :$k + 1$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: If $k + 1$ is [[Definition:Prime Number|prime]], then the result is immediate. Otherwise, $k + 1$ is [[Definition:Composite|composite]] and can be expressed as: :$k + 1 = r s$ where $2 \le r < k + 1$ and $2 \le s < k + 1$ That is, $2 \le r \le k$ and $2 \le s \le k$. Thus by the [[Integer is Expressible as Product of Primes/Proof 3#Induction Hypothesis|induction hypothesis]], both $r$ and $s$ can be expressed as a product of [[Definition:Prime Number|primes]]. So $k + 1 = r s$ can also be expressed as a product of [[Definition:Prime Number|primes]]. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Second Principle of Mathematical Induction]]. Therefore, for all $n \in \N_{> 1}$: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]].	1
Let $M$ be a [[Definition:Locally Euclidean Space|locally Euclidean space]] of some dimension $d$. Then $M$ is [[Definition:Locally Compact Space|locally compact]].	1
Let $B$ be the [[Definition:Set|set]] of all the [[Definition:Identity Mapping|identity functions]] $I^n$ on $\R^n$ where $n \in \N^*$. Then $B$ is a [[Definition:Basis of Vector Space|basis]] of the [[Definition:Vector Space|$\R$-vector space]] $\map P \R$ of all [[Definition:Real Polynomial Function|polynomial functions]] on $\R$.	1
We have that [[Linear Function on Real Numbers is Bijection]]. Let $y = \map f x$. Then: {{begin-eqn}} {{eqn | l = y | r = \map f x | c = }} {{eqn | r = a x + b | c = }} {{eqn | ll= \leadsto | l = x | r = \dfrac {y - b} a | c = }} {{end-eqn}} and so: :$\forall y \in \R: \map {f^{-1} } y = \dfrac {y - b} a$ {{qed}}	1
Let $\left({A_R, \oplus}\right)$ be an [[Definition:Algebra over Ring|algebra over a ring]]. Then the [[Definition:Commutator/Algebra|commutator]] on $\left({A_R, \oplus}\right)$ is an [[Definition:Alternating Bilinear Mapping|alternating bilinear mapping]]: :$\forall a, b \in A_R: \left[{a, b}\right] = -\left[{b, a}\right]$	1
[[Hensel's Lemma/First Form|Hensel's Lemma]] is used to prove the existence of a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] that does not [[Definition:Convergent Sequence in Normed Division Ring|converge]]. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 1|Lemma 1]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 1}} Let $x_1 \in \Z_{>0}: p \nmid x_1, x_1 \ge \dfrac {p + 1} 2$ Let $k$ be a [[Definition:Positive Integer|positive integer]] such that $k \ge 2, p \nmid k$. Let $a = x_1^k + p$. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2|Lemma 2]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2}} Let $\map f X \in \Z \sqbrk X$ be the [[Definition:Polynomial (Abstract Algebra)|polynomial]]: :$X^k - a$ ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 3|Lemma 3]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 3}} Let $\map {f'} X \in \Z \sqbrk X$ be the [[Definition:Formal Derivative of Polynomial|formal derivative]] of $\map f X$. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 4|Lemma 4]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 4}} By [[Hensel's Lemma/First Form|Hensel's Lemma]] there exists a [[Definition:Sequence|sequence]] of [[Definition:Integer|integers]] $\sequence {x_n}$ such that: :$(1) \quad \forall n: \map f {x_n} \equiv 0 \pmod {p^n}$ :$(2) \quad \forall n: x_{n + 1} \equiv x_n \pmod {p^n}$ ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 5|Lemma 5]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 5}} By [[Characterisation of Cauchy Sequence in Non-Archimedean Norm/Corollary 1|corollary of Characterisation of Cauchy Sequence in Non-Archimedean Norm]] then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {\Q, \norm {\,\cdot\,}_p}$ {{AimForCont}} $\sequence {x_n}$ is a [[Definition:Sequence|sequence]] such that for some $c \in \Q$: :$\displaystyle \lim_{n \mathop \to \infty} x_n = c$ in $\struct {\Q, \norm {\,\cdot\,}_p}$ By [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|product rule for convergent sequences]] then: :$\displaystyle \lim_{n \mathop \to \infty} x_n^k = c^k$ Hence: :$c^k = a \in \Z$. By [[Nth Root of Integer is Integer or Irrational]] then: :$c \in \Z$ This [[Definition:Contradiction|contradicts]] [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2|Lemma 2]]. So the [[Definition:Sequence|sequence]] $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] in $\struct {\Q, \norm{\,\cdot\,}_p}$. The result follows. {{qed}}	1
By [[Quotient Ring of Cauchy Sequences is Division Ring]] the [[Definition:Ring Zero|zero]] of $\CC \,\big / \NN$ is $\eqclass {0_R} {}$. {{begin-eqn}} {{eqn | l = \norm {\eqclass {0_R} {} }_1 = 0 | o = \leadstoandfrom | r = \lim_{n \mathop \to \infty} \norm {x_n} = 0 | c = Definition of $\norm {\,\cdot\,}_1$ }} {{eqn | o = \leadstoandfrom | r = \sequence {x_n} \in \NN | c = Definition of $\NN$ }} {{eqn | o = \leadstoandfrom | r = \eqclass {x_n} {} = \eqclass {0_R} {} | c = [[Left Cosets are Equal iff Product with Inverse in Subgroup]] }} {{end-eqn}} The result follows. {{qed}}	1
Let $T = \struct {S, \tau}$ be a [[Definition:Discrete Space|discrete]] [[Definition:Topological Space|topological space]]. Let $T$ be [[Definition:Separable Space|separable]]. Then $S$ is [[Definition:Countable Set|countable]].	1
Let $A \subseteq B$ be an [[Definition:Ring Extension|extension]] of [[Definition:Commutative and Unitary Ring|commutative rings with unity]]. Let $C$ be the [[Definition:Integral Closure|integral closure]] of $A$ in $B$. Then $C$ is [[Definition:Integrally Closed|integrally closed]].	1
A '''system of simultaneous equations''' is a set of equations: :$\forall i \in \set {1, 2, \ldots, m} : \map {f_i} {x_1, x_2, \ldots x_n} = \beta_i$ That is: {{begin-eqn}} {{eqn | l = \beta_1 | r = \map {f_1} {x_1, x_2, \ldots x_n} }} {{eqn | l = \beta_2 | r = \map {f_2} {x_1, x_2, \ldots x_n} }} {{eqn | o = \cdots}} {{eqn | l = \beta_m | r = \map {f_m} {x_1, x_2, \ldots x_n} }} {{end-eqn}}	1
From [[Matrix Form of Quaternion]] it is clear that the [[Definition:Quaternion|quaternions]] $\mathbb H$ can be expressed in [[Definition:Matrix|matrix]] form, as elements of $\mathcal M_\C \left({2}\right)$. Thus $\mathbb H \subseteq \mathcal M_\C \left({2}\right)$. As [[Ring of Quaternions|the quaternions form a ring]], the result follows by definition of [[Definition:Subring|subring]]. {{qed}}	1
Let $\left \langle {a_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ be a [[Definition:Sequence|sequence of elements]] of $G$ such that: : $\displaystyle \exists \left \langle {\lambda_k} \right \rangle_{1 \mathop \le k \mathop \le n} \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e$ where not all of $\lambda_k$ are equal to $0_R$. That is, it is possible to find a [[Definition:Linear Combination|linear combination]] of $\left \langle {a_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ which equals $e$. Such a sequence is '''linearly dependent'''. === [[Definition:Linearly Dependent/Sequence/Real Vector Space|Linearly Dependent Sequence on a Real Vector Space]] === {{:Definition:Linearly Dependent/Sequence/Real Vector Space}}	1
We can see the truth of this by writing them out in full. :$\displaystyle \sum_{j \mathop = 1}^n {\alpha_{i j} x_j} = \beta_i$ can be written as: {{begin-eqn}} {{eqn | l = \alpha_{1 1} x_1 + \alpha_{1 2} x_2 + \ldots + \alpha_{1 n} x_n | r = \beta_1 | c = }} {{eqn | l = \alpha_{2 1} x_1 + \alpha_{2 2} x_2 + \ldots + \alpha_{2 n} x_n | r = \beta_2 | c = }} {{eqn | o = \vdots }} {{eqn | l = \alpha_{m 1} x_1 + \alpha_{m 2} x_2 + \ldots + \alpha_{m n} x_n | r = \beta_m | c = }} {{end-eqn}} while $\sqbrk \alpha_{m n} \sqbrk x_{n 1} = \sqbrk \beta_{m 1}$ can be written as: :$\begin {bmatrix} \alpha_{1 1} & \alpha_{1 2} & \cdots & \alpha_{1 n} \\ \alpha_{2 1} & \alpha_{2 2} & \cdots & \alpha_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ \alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m n} \end {bmatrix} \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix} = \begin {bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end {bmatrix}$ So the question: :Find a solution to the following system of $m$ [[Definition:Simultaneous Linear Equations|simultaneous linear equations]] in $n$ variables is equivalent to: :Given the following element $\mathbf A \in \map {\MM_K} {m, n}$ and $\mathbf b \in \map {\MM_K} {m, 1}$, find the set of all $\mathbf x \in \map {\MM_K} {n, 1}$ such that $\mathbf A \mathbf x = \mathbf b$ where $\map {\MM_K} {m, n}$ is the [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. {{qed}}	1
Let $\mathbf E$ be an [[Definition:Elementary Row Matrix|elementary row matrix]]. Then $\mathbf E$ is [[Definition:Invertible Matrix|invertible]].	1
=== 1 implies 2 === Follows immediately from the definition of $C^k$-[[Definition:Atlas|atlas]]. {{qed|lemma}} === 2 implies 1 === Let $(U,\phi)$ and $(V,\psi)$ be [[Definition:Chart|charts]] in $\mathscr F \cup \mathscr G$. If they are both in $\mathscr F$, they are [[Definition:Compatible Charts|$C^k$-compatible]] because $\mathscr F$ is a $C^k$-[[Definition:Atlas|atlas]]. If they are both in $\mathscr G$, they are [[Definition:Compatible Charts|$C^k$-compatible]] because $\mathscr G$ is a $C^k$-[[Definition:Atlas|atlas]]. If $(U,\phi) \in \mathscr F$ and $(V,\psi) \in \mathscr G$, they are [[Definition:Compatible Charts|$C^k$-compatible]] by assumption. Thus $\mathscr F \cup \mathscr G$ is a $C^k$-[[Definition:Atlas|atlas]]. {{qed}} [[Category:Manifolds]] kbyy48ho66z0akiplglybgcbh9iadz9	1
:$x \ne 0_R \implies \norm {x^{-1} } = \dfrac 1 {\norm x}$	1
By [[Reduced Residue System under Multiplication forms Abelian Group]], $U$ is equal to the [[Definition:Integers Modulo m|set of integers modulo $n$]] which are [[Definition:Coprime Integers|coprime]] to $n$. It follows by [[Cardinality of Reduced Residue System]]: :$\order U = \map \phi n$ {{qed}}	1
Follows directly from [[Vector Subspace Dimension One Less]]. {{qed}}	1
Let $G$ and $H$ be [[Definition:Unitary Module|unitary $R$-modules]]. Let $\phi: G \to H$ be a non-zero [[Definition:Linear Transformation|linear transformation]]. Let $G$ be [[Definition:Dimension of Module|$n$-dimensional]]. Let $\left \langle {a_n} \right \rangle$ be any [[Definition:Ordered Basis|ordered basis]] of $G$ such that $\left\{{a_k: r + 1 \le k \le n}\right\}$ is the [[Definition:Basis (Linear Algebra)|basis]] of the [[Definition:Kernel of Linear Transformation|kernel]] of $\phi$. Then $\left \langle {\phi \left({a_r}\right)} \right \rangle$ is an [[Definition:Ordered Basis|ordered basis]] of the [[Definition:Image of Mapping|image]] of $\phi$.	1
Let: :$\mathbf a = \begin{bmatrix} a_x \\ a_y \\a_z \end{bmatrix}$, $\mathbf b = \begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix}$, $\mathbf c = \begin{bmatrix} c_x \\ c_y \\ c_z \end{bmatrix}$ be [[Definition:Vector (Euclidean Space)|vectors in $\R^3$]]. Then: {{begin-eqn}} {{eqn | l = \mathbf a \times \paren {\mathbf b + \mathbf c} | r = \begin{bmatrix} a_x \\ a_y \\a_z \end{bmatrix} \times \paren {\begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix} + \begin{bmatrix} c_x \\ c_y \\ c_z \end{bmatrix} } }} {{eqn | r = \begin{bmatrix} a_x \\ a_y \\a_z \end{bmatrix} \times {\begin{bmatrix} b_x + c_x \\ b_y + c_y \\ b_z + c_z \end{bmatrix} } | c = {{Defof|Vector Sum}} }} {{eqn | r = \begin{bmatrix} a_y \paren {b_z + c_z} - a_z \paren {b_y + c_y} \\ a_z \paren {b_x + c_x} - a_x \paren {b_z + c_z} \\ a_x \paren {b_y + c_y} - a_y \paren {b_x + c_x} \end{bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = \begin{bmatrix} a_y b_z + a_y c_z - a_z b_y - a_z c_y \\ a_z b_x + a_z c_x - a_x b_z - a_x c_z \\ a_x b_y + a_x c_y - a_y b_x - a_y c_x \end{bmatrix} | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \begin{bmatrix} a_y b_z - a_z b_y + a_y c_z - a_z c_y \\ a_z b_x - a_x b_z + a_z c_x - a_x c_z \\ a_x b_y - a_y b_x + a_x c_y - a_y c_x \end{bmatrix} | c = [[Real Addition is Commutative]] }} {{eqn | r = \begin{bmatrix} a_y b_z - a_z b_y \\ a_z b_x - a_x b_z \\ a_x b_y - a_y b_x \end{bmatrix} + \begin{bmatrix} a_y c_z - a_z c_y \\ a_z c_x - a_x c_z \\ a_x c_y - a_y c_x \end{bmatrix} | c = {{Defof|Vector Sum}} }} {{eqn | r = \paren {\begin{bmatrix}a_x \\ a_y \\ a_z \end{bmatrix} \times \begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix} } + \paren {\begin{bmatrix} a_x \\ a_y \\ a_z \end{bmatrix} \times \begin{bmatrix} c_x \\ c_y \\ c_z \end{bmatrix} } | c = {{Defof|Vector Cross Product}} }} {{eqn|r = \paren {\mathbf a \times \mathbf b} + \paren {\mathbf a \times \mathbf c} }} {{end-eqn}} {{qed}}	1
By [[Set of Linear Subspaces is Closed under Intersection]], $M$ is a [[Definition:Vector Subspace|linear subspace]] of $V$. By [[Topology Defined by Closed Sets]], the [[Definition:Set Intersection|intersection]] of [[Definition:Closed Set (Topology)|closed sets]] is again [[Definition:Closed Set (Topology)|closed]]. As the $M_i$ are all [[Definition:Closed Set (Topology)|closed]], it follows that $M$ is [[Definition:Closed Set (Topology)|closed]]. Hence $M$ is a [[Definition:Closed Linear Subspace|closed linear subspace]] of $V$. {{qed}} [[Category:Vector Subspaces]] 4qr0useq9yg5nggqucqyd0u0twbljkt	1
:$n_0$ is a [[Definition:Prime Number|prime number]].	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $A \in B \left({H, K}\right), B \in B \left({K, H}\right)$ be [[Definition:Bounded Linear Transformation|bounded linear transformations]]. Let $u, v: H \times K \to \Bbb F$ be defined by: :$u \left({h, k}\right) := \left\langle{Ah, k}\right\rangle_K$ :$v \left({h, k}\right) := \left\langle{h, Bk}\right\rangle_H$ Then $u$ and $v$ are [[Definition:Bounded Sesquilinear Form|bounded sesquilinear forms]].	1
Follows immediately from [[Countable Space is Separable]]. {{qed}}	1
This proof assumes that $\mathbf A$ and $\mathbf B$ are $n \times n$-[[Definition:Matrix|matrices]] over a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] $\left({R, +, \circ}\right)$. Let $\mathbf C = \left[{c}\right]_n = \mathbf A \mathbf B$. From [[Square Matrix is Row Equivalent to Triangular Matrix]], it follows that $\mathbf A$ can be converted into a [[Definition:Upper Triangular Matrix|upper triangular matrix]] $\mathbf A'$ by a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\hat o_1, \ldots, \hat o_{m'}$. Let $\mathbf C'$ denote the matrix that results from using $\hat o_1, \ldots, \hat o_{m'}$ on $\mathbf C$. From [[Elementary Row Operations Commute with Matrix Multiplication]], it follows that $\mathbf C' = \mathbf A' \mathbf B$. [[Effect of Sequence of Elementary Row Operations on Determinant]] shows that there exists $\alpha \in R$ such that: :$\alpha \det \left({\mathbf A'}\right) = \det \left({\mathbf A}\right)$ :$\alpha \det \left({\mathbf C'}\right) = \det \left({\mathbf C}\right)$ Let $\mathbf B^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] of $B$. From [[Transpose of Matrix Product]], it follows that: : $\left({\mathbf C'}\right)^\intercal = \left({\mathbf A' \mathbf B}\right)^\intercal = \mathbf B^\intercal \left({\mathbf A'}\right)^\intercal$ From [[Square Matrix is Row Equivalent to Triangular Matrix]], it follows that $\mathbf B^\intercal$ can be converted into a [[Definition:Triangular Matrix/Lower Triangular Matrix|lower triangular matrix]] $\left({\mathbf B^\intercal}\right)'$ by a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\hat p_1, \ldots, \hat p_{m''}$. Let $\mathbf C''$ denote the matrix that results from using $\hat p_1, \ldots, \hat p_{m''}$ on $\left({\mathbf C'}\right)^\intercal$. From [[Elementary Row Operations Commute with Matrix Multiplication]], it follows that: : $\mathbf C'' = \left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal$ [[Effect of Sequence of Elementary Row Operations on Determinant]] shows that there exists $\beta \in R$ such that: :$\beta \det \left({\left({\mathbf B^\intercal}\right)'}\right) = \det \left({\mathbf B^\intercal}\right)$ :$\beta \det \left({\mathbf C''}\right) = \det \left({ \left({\mathbf C'}\right)^\intercal }\right)$ From [[Transpose of Upper Triangular Matrix is Lower Triangular]], it follows that $\left({\mathbf A'}\right)^\intercal$ is a lower triangular matrix. Then [[Product of Triangular Matrices]] shows that $\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal$ is a lower triangular matrix whose [[Definition:Diagonal Element|diagonal elements]] are the products of the diagonal elements of $\left({\mathbf B^\intercal}\right)'$ and $\left({\mathbf A'}\right)^\intercal$. From [[Determinant of Triangular Matrix]], we have that $\det \left({\left({\mathbf A'}\right)^\intercal}\right)$, $\det \left({\left({\mathbf B^\intercal}\right)' }\right)$, and $\det \left({\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal }\right)$ are equal to the product of their diagonal elements. Combinining these results shows that: :$\det \left({\left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal}\right) = \det \left({\left({\mathbf B^\intercal}\right)'}\right) \det \left({\left({\mathbf A'}\right)^\intercal }\right)$ Then: {{begin-eqn}} {{eqn | l = \det \left({\mathbf C}\right) | r = \alpha \det \left({\mathbf C'}\right) }} {{eqn | r = \alpha \det \left({ \left({\mathbf C'}\right)^\intercal}\right) | c = [[Determinant of Transpose]] }} {{eqn | r = \alpha \beta \det \left({\mathbf C''}\right) }} {{eqn | r = \alpha \beta \det \left({ \left({\mathbf B^\intercal}\right)' \left({\mathbf A'}\right)^\intercal}\right) }} {{eqn | r = \alpha \beta \det \left({\left({\mathbf B^\intercal}\right)' }\right) \det \left({\left({\mathbf A'}\right)^\intercal}\right) }} {{eqn | r = \alpha \det \left({\left({\mathbf A'}\right)^\intercal}\right) \beta \det \left({\left({\mathbf B^\intercal}\right)' }\right) | c = [[Definition:Commutative Operation|Commutativity]] of [[Definition:Ring Product|Ring Product]] in $R$ }} {{eqn | r = \alpha \det \left({\mathbf A'}\right) \det \left({\mathbf B^\intercal}\right) | c = [[Determinant of Transpose]] }} {{eqn | r = \det \left({\mathbf A}\right) \det \left({\mathbf B}\right) | c = [[Determinant of Transpose]] }} {{end-eqn}} {{qed}}	1
Let $B = \set {\phi_{i j}: i \in \closedint 1 n, j \in \closedint 1 m}$. Let $\displaystyle \sum_{j \mathop = 1}^m \sum_{i \mathop = 1}^n \lambda_{i j} \phi_{i j} = 0$. Then: :$\displaystyle \forall k \in \closedint 1 n: 0 = \sum_{j \mathop = 1}^m \sum_{i \mathop = 1}^n \lambda_{i j} \map {\phi_{i j} } {a_k} = \sum_{j \mathop = 1}^m \lambda_{k j} b_j$ So: :$\forall j \in \closedint 1 n: \lambda_{k j} = 0$ Hence $B$ is [[Definition:Linearly Independent Set|linearly independent]]. Now let $\phi \in \map {\LL_R} {G, H}$. Let $\tuple {\alpha_{i 1}, \alpha_{i 2}, \ldots, \alpha_{i m} }$ be the [[Definition:Sequence|sequence]] of [[Definition:Scalar (Module)|scalars]] that satisfies: :$\displaystyle \forall i \in \closedint 1 n: \map \phi {a_i} = \sum_{j \mathop = 1}^m \alpha_{i j} b_j$ Then: :$\displaystyle \forall k \in \closedint 1 n: \map \phi {a_k} = \map {\paren {\sum_{j \mathop = 1}^m \sum_{i \mathop = 1}^n \alpha_{i j} u_{i j} } } {a_k}$ by a calculation similar to the preceding. So, by [[Linear Transformation of Generated Module]]: :$\displaystyle u = \sum_{j \mathop = 1}^m \sum_{i \mathop = 1}^n \alpha_{i j} u_{i j}$ Thus $B$ is a [[Definition:Generator of Module|generator]] for $\phi \in \map {\LL_R} {G, H}$. {{Qed}}	1
Let $\left({G, +, \circ}\right)_R$ be an [[Definition:Module|$R$-module]]. Let $H$ and $K$ be [[Definition:Submodule|submodules]] of $G$. Then $H + K$ and $H \cap K$ are also [[Definition:Submodule|submodules]] of $G$. The [[Definition:Set Intersection|intersection]] of any [[Definition:Set|set]] of submodules of $G$ is a [[Definition:Submodule|submodule]]. Thus if $S \subseteq G$, the [[Definition:Set Intersection|intersection]] of all submodules of $G$ containing $S$ is the smallest submodule of $G$ containing $S$.	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $\R$ or $\C$. Let $\mathcal C$ be a family of [[Definition:Convex Set (Vector Space)|convex subsets]] of $V$. Then the intersection $\displaystyle \bigcap \mathcal C$ is also a [[Definition:Convex Set (Vector Space)|convex subset]] of $V$.	1
From [[Ring of Polynomial Forms over Field is Vector Space]] we note that $\struct {F, +, \times}$ is a [[Definition:Vector Space|vector space over $F$]]. The remaining question is that $S$ remains [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Polynomial Addition|polynomial addition]] and [[Definition:Scalar Multiplication on Vector Space|scalar multiplication]]. Let $\mathbf x, \mathbf y \in S$ such that $\map \deg {\mathbf x} = m$ and $\map \deg {\mathbf y} = n$. We have: {{begin-eqn}} {{eqn | l = \mathbf x + \mathbf y | r = \sum_{j \mathop = 0}^m x_j X^j + \sum_{k \mathop = 0}^n y_k X^k | c = }} {{eqn | r = \sum_{k \mathop = 0}^{\max \set {m, n} } \paren {x_k + y_k} X^k | c = {{Defof|Polynomial Addition}} }} {{end-eqn}} As both $m < d$ and $n < d$ by dint of them belonging to $S$, it follows that $\max \set {m, n} < d$. Hence $\mathbf x + \mathbf y \in S$ and so $S$ is [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Polynomial Addition|polynomial addition]]. Let the [[Definition:Binary Operation|operation]] $\times': F \to F \sqbrk X$ be defined as follows. Let $\lambda \in F$. Let $\mathbf x \in F \sqbrk X$ be defined as: :$\mathbf x = \displaystyle \sum_{k \mathop = 0}^n x_k X^k$ where $n = \map \deg {\mathbf x}$ denotes the [[Definition:Degree of Polynomial|degree]] of $\mathbf x$. Thus: :$\lambda \times' \mathbf x := \displaystyle \lambda \times' \sum_{k \mathop = 0}^n x_k X^k = \displaystyle \sum_{k \mathop = 0}^n \paren {\lambda \times x_k} X^k$ We have that $\times': F \to F \sqbrk X$ is an instance of [[Definition:Polynomial Multiplication|polynomial multiplication]] where the [[Definition:Multiplier|multiplier]] $\lambda$ is a [[Definition:Polynomial|polynomial]] of [[Definition:Degree of Polynomial|degree]] $0$. By definition of [[Definition:Polynomial Multiplication|polynomial multiplication]]: {{begin-eqn}} {{eqn | l = \map \deg {\lambda \times' \mathbf x} | r = \map \deg \lambda + \map \deg {\mathbf x} | c = }} {{eqn | r = 0 + \map \deg {\mathbf x} | c = }} {{eqn | o = < | r = d | c = as $\mathbf x \in S$ }} {{end-eqn}} The result follows. {{qed}}	1
Let $\mathbf A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}\end{bmatrix}$ and $\mathbf A^* = \begin{bmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn}\end{bmatrix}$. Thus $\left({\mathbf A^*}\right)^\intercal = \begin{bmatrix} A_{11} & A_{21} & \cdots & A_{n1} \\ A_{12} & A_{22} & \cdots & A_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1n} & A_{2n} & \cdots & A_{nn}\end{bmatrix}$ is the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A^*$. Let $c_{ij}$ be the typical element of $\mathbf A \left({\mathbf A^*}\right)^\intercal$. Then $\displaystyle c_{ij} = \sum_{k \mathop = 1}^n a_{ik} A_{jk}$ by definition of [[Definition:Matrix Product (Conventional)|matrix product]]. Thus by the [[Expansion Theorem for Determinants/Corollary|corollary of the Expansion Theorem for Determinants]], $c_{ij} = \delta_{ij} D$. So $\det \left({\mathbf A \left({\mathbf A^*}\right)^\intercal}\right) = \begin{vmatrix} D & 0 & \cdots & 0 \\ 0 & D & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & D\end{vmatrix} = D^n$ by [[Determinant of Diagonal Matrix]]. From [[Determinant of Matrix Product]], $\det \left({\mathbf A}\right) \det \left({\left({\mathbf A^*}\right)^\intercal}\right) = \det \left({\mathbf A \left({\mathbf A^*}\right)^\intercal}\right)$ From [[Determinant of Transpose]]: : $\det \left({\left({\mathbf A^*}\right)^\intercal}\right) = \det \left({\mathbf A^*}\right)$ Thus as $D = \det \left({\mathbf A}\right)$ and $D^* = \det \left({\mathbf A^*}\right)$ it follows that $DD^* = D^n$. Now if $D \ne 0$, the result follows. However, if $D = 0$ we need to show that $D^* = 0$. Let $D^* = \begin{vmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn}\end{vmatrix}$. Suppose that at least one element of $\mathbf A$, say $a_{rs}$, is non-zero (otherwise the result follows immediately). By the [[Expansion Theorem for Determinants]] and its corollary, we can expand $D$ by row $r$, and get: :$\displaystyle D = 0 = \sum_{j \mathop = 1}^n A_{ij} t_j, \forall i = 1, 2, \ldots, n$ for all $t_1 = a_{r1}, t_2 = a_{r2}, \ldots, t_n = a_{rn}$. But $t_s = a_{rs} \ne 0$. So, by '''(work in progress)''': :$D^* = \begin{vmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn}\end{vmatrix} = 0$ {{WIP|One result to document, I've got to work out how best to formulate it.}} [[Category:Determinants]] bkm55tefjheohcpkb9z8u2t6jx7dz5p	1
Let $\displaystyle N = \prod_{i \mathop = 1}^r n_i$ and $N_k = \dfrac N {n_k}$ for $k = 1, 2, \ldots, r$. Because $n_i \perp n_k$ for each $i \ne k$, we have: :$\gcd \set {N_k, n_k} = \gcd \set {n_1 n_2 \cdots n_{k - 1} n_{k + 1} \cdots n_r, n_k} = 1$ So by [[Solution of Linear Congruence]], the [[Definition:Linear Congruence|linear congruence]] $N_k x \equiv 1 \pmod {n_k}$ has a unique solution modulo $n_k$. Let this solution be $x_k$. So $N_k x_k \equiv 1 \pmod {n_k}$ We are going to show that $x_0 = b_1 N_1 x_1 + b_2 N_2 x_2 + \cdots + b_r N_r x_r$ satisfies each of the congruences. So, we evaluate $x_0$ modulo $n_k$ for each $k = 1, 2, 3, \ldots, r$. We start by noting that: :$i \ne k \implies n_k \divides N_i$ so: :$N_i \equiv 0 \pmod {n_k}$ So for each $k = 1, 2, \ldots, r$ we have $x_0 \equiv b_k N_k x_k$ because each of the remaining terms in the sum is congruent modulo $n_k$ to $0$. Finally, since $x_k$ was found such that $N_k x_k \equiv 1 \pmod {n_k}$ we have $x_0 \equiv b_k \pmod {n_k}$ as we claimed. All we need to do now is show that this solution we have discovered is unique modulo $N$. So, suppose that $x'$ is a second solution of the system. That is: :$x_0 \equiv x' \equiv b_k \pmod {n_k}$ for each $k = 1, 2, \ldots, r$. So each $n_k$ divides $x' - x_0$. But because each of the moduli are pairwise [[Definition:Coprime Integers|coprime]], we have: :$\displaystyle \prod_{i \mathop = 1}^r n_i \divides x' - x_0$ That is: :$\displaystyle x' \equiv x_0 \pmod {\prod_{i \mathop = 1}^r n_i}$ as we wanted to show. {{qed}}	1
Let $M = \struct {I^\omega, d_2}$ be the [[Definition:Hilbert Cube|Hilbert cube]]. Then $M$ is a [[Definition:Separable Space|separable space]].	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $F$. Let $\mathbf a, \mathbf b \in V$. Let $\mathbf a + \mathbf b = \mathbf a$. Then: :$\mathbf b = \bszero$ where $\bszero$ is the [[Definition:Zero Vector|zero vector]] of $V$.	1
Follows directly from [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|Product Rule for Normed Division Ring Sequences]], setting: :$\sequence {y_n} := \sequence {x_n}$ and: :$\sequence {x_n} := \tuple {\lambda, \lambda, \lambda, \ldots}$ {{qed}}	1
From [[Ring of Integers Modulo m is Ring]], $\left({\Z_m, +_m, \times_m}\right)$ is a [[Definition:Commutative and Unitary Ring|commutative ring with unity $\left[\!\left[{1}\right]\!\right]_m$]]. Thus by definition $\left({\Z_m, \times_m}\right)$ is a [[Definition:Commutative Monoid|commutative monoid]]. The result follows from [[Multiplicative Inverse in Monoid of Integers Modulo m]]. {{qed}}	1
:$\sequence {\lambda x_n + \mu y_n }$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] and $\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x_n + \mu y_n} = \lambda l + \mu m$	1
{{begin-eqn}} {{eqn | l = \cmod z | r = \cmod {a + b i} | c = Definition of $z$ }} {{eqn | r = +\sqrt {a^2 + b^2} | c = {{Defof|Complex Modulus}} }} {{eqn | o = \ge | r = 0 | c = {{Defof|Positive Square Root}} }} {{end-eqn}} {{qed}}	1
Let $S$ be one of the [[Definition:Standard Number Field|standard number fields]] $\Q$, $\R$, $\C$. Let $S^n$ be a [[Definition:Cartesian Space|cartesian space]] for $n \in \N_{\ge 1}$. Let $M = \left({S^n, d}\right)$ be a [[Definition:Euclidean Space|Euclidean space]]. The [[Definition:Topology Induced by Metric|topology $\tau_d$ induced]] by the [[Definition:Euclidean Metric|Euclidean metric]] $d$ is called the '''Euclidean topology'''.	1
Let $P = \tuple {x, y}$ be a [[Definition:Point|point]] in the [[Definition:Cartesian Plane|cartesian plane]]. Then $P$ is at a [[Definition:Length (Linear Measure)|distance]] of $\sqrt {x^2 + y^2}$ from the [[Definition:Origin|origin]].	1
From [[Determinant of Diagonal Matrix]], it follows directly that: :$\map \det {r \, \mathbf I_n} = \displaystyle \prod_{i \mathop = 1}^n r = r^n$ {{qed}} [[Category:Determinants]] 2xs8qedaiszlly34e7fjppgn0psrdr9	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $G$ be a [[Definition:Dimension (Linear Algebra)|finite-dimensional]] [[Definition:Unitary Module|unitary $R$-module]]. Let $A = \sequence {a_n}$ and $B = \sequence {b_n}$ be [[Definition:Ordered Basis|ordered bases]] of $G$. {{TFAE|def = Change of Basis Matrix}}	1
{{improve}} Let $\mathcal V$ be an [[Definition:Inner Product Space|inner product space]]. {{explain|The theorem calls for a [[Definition:Hilbert Space|Hilbert space]]. What is actually required?}} Let $T: \mathcal V \to \mathcal V$ be a [[Definition:Normal Operator|normal linear operator]]. Requisite knowledge: $T^*$ is the [[Definition:Adjoint Linear Transformation|adjoint]] of $T$ and is defined by the fact that for any $u, w \in \mathcal V$, we have :$\left\langle{T u, w}\right\rangle = \left\langle{T^* w}\right\rangle$ It is important to note the existence and uniqueness of adjoint operators. {{explain|Link to the required proofs and/or definitions as given above.}} '''Claim''': We know that for $v \in \mathcal V$: :$T v = \lambda v \iff T^* v = \overline \lambda v$ This is true because for all [[Definition:Normal Operator|normal operators]], by definition: :$T^* T = T T*$ and so: {{begin-eqn}} {{eqn | l = \left\Vert{T v}\right\Vert^2 | r = \left\langle{T v, T v}\right\rangle | c = }} {{eqn | r = \left\langle{T^* T v, v}\right\rangle | c = }} {{eqn | r = \left\langle{T T^* v, v}\right\rangle | c = }} {{eqn | r = \left\langle{T^* v, T^*v}\right\rangle | c = }} {{eqn | r = \left\Vert{T^* v}\right\Vert^2 | c = }} {{end-eqn}} Since for [[Definition:Normal Operator|normal]] $T$, $\left({T - \lambda I}\right)$ is [[Definition:Normal Operator|normal]], we have: {{explain|Link to a page demonstrating the above}} {{begin-eqn}} {{eqn | l = T v | r = \lambda v | c = }} {{eqn | ll= \iff | l = \left\Vert{\left({T - \lambda I}\right) v}\right\Vert | r = 0 | c = }} {{eqn | ll= \iff | l = \left\Vert{\left({T - \lambda I}\right)^* v}\right\Vert | r = 0 | c = }} {{eqn | ll= \iff | l = \left\Vert{T^* v - \overline \lambda v}\right\Vert | r = 0 | c = }} {{eqn | ll= \iff | l = T^* v | r = \overline \lambda v | c = }} {{end-eqn}} Now, if $T v_1 = \lambda_1 v_1$ and $T v_2 = \lambda_2 v_2$, where $\lambda_1 \ne \lambda_2$ and $v_1, v_2$ are eigenvectors (i.e. $v_1, v_2 \ne \vec 0$), we have: {{begin-eqn}} {{eqn | l = \lambda_1 \left\langle{v_1, v_2}\right\rangle | r = \left\langle{\lambda_1 v_1, v_2}\right\rangle | c = }} {{eqn | r = \left\langle{T v_1, v_2}\right\rangle | c = }} {{eqn | r = \left\langle{v_1, T^* v_2}\right\rangle | c = }} {{eqn | r = \left\langle{v_1, \overline{\lambda_2} v_2}\right\rangle | c = }} {{eqn | r = \lambda_2 \left\langle{v_1, v_2}\right\rangle | c = }} {{end-eqn}} Since $\lambda_1 \ne \lambda_2$, this is only possible if $\left\langle{v_1, v_2}\right\rangle = 0$, which means the [[Definition:Eigenvector|eigenvectors]] of our [[Definition:Normal Operator|normal operator]] are [[Definition:Orthogonal (Hilbert Space)#Sets|orthogonal]]. {{qed}}	1
Let $R$ be a [[Definition:Normed Division Ring|normed division ring]] with a [[Definition:Submultiplicative Norm on Ring|submultiplicative norm]] $\norm {\, \cdot \,}_R$. Let $V$ be a [[Definition:Vector Space|vector space]] that is not a [[Definition:Trivial Vector Space|trivial vector space]]. Let $\norm {\, \cdot \,}: V \to \R_{\ge 0}$ be a mapping from $V$ to the [[Definition:Positive Real Number|positive real numbers]] satisfying the [[Definition:Norm Axioms (Vector Space)|vector space norm axioms]]. Then $\norm {\, \cdot \,}_R$ is a [[Definition:Multiplicative Norm on Ring|multiplicative norm]]. That is: :$\forall r, s \in R: \norm {r s}_R = \norm r_R \norm s_R$	1
Let $z_1 = x_1 + i y_1$ and $z_2 = x_2 + i y_2$. Then: {{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2} | r = \cmod {z_1 - z_2} | c = }} {{eqn | ll= \leadsto | l = \paren {x_1 + x_2}^2 + \paren {y_1 + y_2}^2 | r = \paren {x_1 - x_2}^2 + \paren {y_1 - y_2}^2 | c = {{Defof|Complex Modulus}} }} {{eqn | ll= \leadsto | l = {x_1}^2 + 2 x_1 x_2 + {x_2}^2 + {y_1}^2 + 2 y_1 y_2 + {y_2}^2 | r = {x_1}^2 - 2 x_1 x_2 + {x_1}^2 + {y_1}^2 - 2 y_1 y_2 + {y_1}^2 | c = [[Square of Sum]], [[Square of Difference]] }} {{eqn | ll= \leadsto | l = 4 x_1 x_2 + 4 y_1 y_2 | r = 0 | c = simplifying }} {{eqn | ll= \leadsto | l = x_1 x_2 + y_1 y_2 | r = 0 | c = simplifying }} {{end-eqn}} Now we have: {{begin-eqn}} {{eqn | l = \dfrac {z_1} {z_2} | r = \frac {x_1 + i y_1} {x_2 + i y_2} | c = }} {{eqn | r = \frac {\paren {x_1 + i y_1} \paren {x_2 - i y_2} } { {x_2}^2 + {y_2}^2} | c = {{Defof|Complex Division}} }} {{eqn | r = \frac {x_1 x_2 + y_1 y_2} { {x_2}^2 + {y_2}^2} + \frac {i \paren {x_2 y_1 - x_1 y_2} } { {x_2}^2 + {y_2}^2} | c = {{Defof|Complex Multiplication}} }} {{end-eqn}} But we have: :$x_1 x_2 + y_1 y_2 = 0$ Thus: :$\dfrac {z_1} {z_2} = \dfrac {i \paren {x_2 y_1 - x_1 y_2} } { {x_2}^2 + {y_2}^2}$ which is [[Definition:Wholly Imaginary|wholly imaginary]]. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ and $\sequence {y_n}$ be [[Definition:Sequence|sequences in $R$]]. Let $\sequence {x_n}$ and $\sequence {y_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\,\cdot\,}$ to the following [[Definition:Limit of Sequence (Normed Division Ring)|limits]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = l$ :$\displaystyle \lim_{n \mathop \to \infty} y_n = m$ Then: {{:Combination Theorem for Sequences/Normed Division Ring/Product Rule}}	1
Let $\mathbf a, \mathbf b, \mathbf c, \mathbf d$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] $\mathbf V$ of [[Definition:Dimension of Vector Space|$3$ dimensions]]: {{begin-eqn}} {{eqn | l = \mathbf a | r = a_1 \mathbf e_1 + a_2 \mathbf e_2 + a_3 \mathbf e_3 }} {{eqn | l = \mathbf b | r = b_1 \mathbf e_1 + b_2 \mathbf e_2 + b_3 \mathbf e_3 }} {{eqn | l = \mathbf c | r = c_1 \mathbf e_1 + c_2 \mathbf e_2 + c_3 \mathbf e_3 }} {{eqn | l = \mathbf d | r = d_1 \mathbf e_1 + d_2 \mathbf e_2 + d_3 \mathbf e_3 }} {{end-eqn}} where $\left({\mathbf e_1, \mathbf e_2, \mathbf e_3}\right)$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\mathbf V$. Let $\mathbf a \times \mathbf b$ denote the [[Definition:Vector Cross Product|vector cross product]] of $\mathbf a$ with $\mathbf b$. Let $\mathbf a \cdot \mathbf b$ denote the [[Definition:Dot Product|dot product]] of $\mathbf a$ with $\mathbf b$. Then: {{begin-eqn}} {{eqn | l = \left({\mathbf a \times \mathbf b}\right) \times \left({\mathbf c \times \mathbf d}\right) | r = \mathbf c \left({\mathbf a \cdot \left({\mathbf b \times \mathbf d}\right)}\right) - \mathbf d \left({\mathbf a \cdot \left({\mathbf b \times \mathbf c}\right)}\right) | c = }} {{eqn | r = \mathbf b \left({\mathbf a \cdot \left({\mathbf c \times \mathbf d}\right)}\right) - \mathbf a \left({\mathbf b \cdot \left({\mathbf c \times \mathbf d}\right)}\right) | c = }} {{end-eqn}}	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for $p = 2$ or $3$. Then: :$\struct {\Q, \norm {\,\cdot\,}_p}$ is not a [[Definition:Complete Normed Division Ring|complete normed division ring]]. That is, there exists a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {\Q, \norm{\,\cdot\,}_p}$ which does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to a [[Definition:Limit of Sequence (Normed Division Ring)|limit]] in $\Q$.	1
From [[Finite Set of Elements in Principal Ideal Domain has GCD]] we have that at least one such [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] exists. So, let $y_1$ and $y_2$ be [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisors]] of $S$. Then: {{begin-eqn}} {{eqn | l = y_1 | o = \divides | r = y_2 | c = as $y_2$ is a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] }} {{eqn | l = y_2 | o = \divides | r = y_1 | c = as $y_1$ is a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] }} {{end-eqn}} Thus we have: :$y_1 \divides y_2$ and $y_2 \divides y_1$ where $\divides$ denotes [[Definition:Divisor of Ring Element|divisibility]]. Hence the result, by definition of [[Definition:Associate in Integral Domain|associates]]. {{qed}}	1
Let $\sequence {x_n}$ be the constant [[Definition:Sequence|sequence]]: :$\forall n, x_n = \lambda$ Given $\epsilon \gt 0$, then: :$\forall n \ge 1: \norm {x_n - \lambda} = \norm {\lambda - \lambda} = \norm {0} = 0 < \epsilon$ The result follows. {{qed}} [[Category:Sequences]] [[Category:Normed Division Rings]] h5vohsh8cy0tm21orkru2mv48ys7ish	1
Let $I$ be an [[Definition:Indexing Set|indexing set]] with [[Definition:Uncountable Set|uncountable cardinality]]. Let $\family {\struct {S_\alpha, \tau_\alpha} }_{\alpha \mathop \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Topological Space|topological spaces]] [[Definition:Indexed Family|indexed]] by $I$. Let $\displaystyle \struct {S, \tau} = \prod_{\alpha \mathop \in I} \struct {S_\alpha, \tau_\alpha}$ be the [[Definition:Product Space of Topological Spaces|product space]] of $\family {\struct {S_\alpha, \tau_\alpha} }_{\alpha \mathop \in I}$. Let each of $\struct {S_\alpha, \tau_\alpha}$ be a [[Definition:Separable Space|separable space]]. Then it is not necessarily the case that $\struct {S, \tau}$ is also a [[Definition:Separable Space|separable space]].	1
Let $G$ and $H$ be [[Definition:Module|$R$-modules]]. Then $\map {\mathrm {Hom}_R} {G, H}$ is the '''set of all [[Definition:Linear Transformation|linear transformations]]''' from $G$ to $H$: :$\map {\mathrm {Hom}_R} {G, H} := \set {\phi: G \to H: \phi \mbox{ is a linear transformation} }$ If it is clear (and therefore does not need to be stated) that the [[Definition:Scalar Ring of Module|scalar ring]] is $R$, then this can be written $\map {\mathrm {Hom} } {G, H}$. Similarly, $\map {\mathrm {Hom}_R} G$ is the set of all [[Definition:Linear Operator|linear operators]] on $G$: :$\map {\mathrm {Hom}_R} G := \set {\phi: G \to G: \phi \text{ is a linear operator} }$ Again, this can also be written $\map {\mathrm {Hom} } G$.	1
Let $\begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix}$ be a [[Definition:Determinant of Matrix|determinant]]. Then $\begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} + a'_{r1} & \cdots & a_{rs} + a'_{rs} & \cdots & a_{rn} + a'_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix} = \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix} + \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a'_{r1} & \cdots & a'_{rs} & \cdots & a'_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix}$. Similarly: Then $\begin{vmatrix} a_{11} & \cdots & a_{1s} + a'_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a_{rs} + a'_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} + a'_{ns} & \cdots & a_{nn} \end{vmatrix} = \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix} + \begin{vmatrix} a_{11} & \cdots & a'_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a'_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a'_{ns} & \cdots & a_{nn} \end{vmatrix}$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \subseteq H$ be a [[Definition:Subset|subset]] of $H$. Then the [[Definition:Orthocomplement|orthocomplement]] $A^\perp$ of $A$ is a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$.	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]] Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence in Normed Division Ring|null sequences]]. For all $\sequence {x_n} \in \CC$, let $\eqclass {x_n} {}$ denote the [[Definition:Left Coset|left coset]] $\sequence {x_n} + \NN$ Let $\norm {\, \cdot \,}_1: \CC \,\big / \NN \to \R_{\ge 0}$ be defined by: :$\displaystyle \forall \eqclass {x_n} {} \in \CC \,\big / \NN: \norm {\eqclass {x_n} {} }_1 = \lim_{n \mathop \to \infty} \norm {x_n}$ Then: :$\struct {\CC \,\big / \NN, \norm {\, \cdot \,}_1 }$ is a [[Definition:Normed Division Ring|normed division ring]].	1
From the [[Cauchy-Schwarz Inequality/Complex Numbers|Complex Number form of the Cauchy-Schwarz Inequality]], we have: :$\displaystyle \sum \left|{w_i}\right|^2 \left|{z_i}\right|^2 \ge \left|{\sum w_i z_i}\right|^2$ where all of $w_i, z_i \in \C$. As elements of $\R$ are also elements of $\C$, it follows that: :$\displaystyle \sum \left|{r_i}\right|^2 \left|{s_i}\right|^2 \ge \left|{\sum r_i s_i}\right|^2$ where all of $r_i, s_i \in \R$. But from the [[Definition:Modulus of Complex Number|definition of modulus]], it follows that: :$\displaystyle \forall r_i \in \R: \left|{r_i}\right|^2 = r_i^2$ Thus: :$\displaystyle \sum {r_i^2} \sum {s_i^2} \ge \left({\sum {r_i s_i}}\right)^2$ where all of $r_i, s_i \in \R$. {{qed}}	1
Let $D$ be an [[Definition:Integral Domain|integral domain]]. Let $D \sqbrk X$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms]] in $X$ over $D$. Let $D sqbrk X$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]; Then $D$ is a [[Definition:Field (Abstract Algebra)|field]].	1
Let $m \in \Z: m \ge 2$. Let $\struct {\Z_m, +, \times}$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. Let $m$ be a [[Definition:Composite Number|composite number]]. Then $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H}\right)$ be a [[Definition:Normal Operator|normal operator]]. Let $\lambda, \mu$ be distinct [[Definition:Eigenvalue|eigenvalues]] of $A$. Then: :$\ker \left({A - \lambda}\right) \perp \ker \left({A - \mu}\right)$ where: : $\ker$ denotes [[Definition:Kernel of Linear Transformation|kernel]] : $\perp$ denotes [[Definition:Orthogonal (Hilbert Space)#Sets|orthogonality]].	1
Let $n$ be a [[Definition:Positive Integer|positive integer]]. An '''integral lattice''' in $\R^n$ is a [[Definition:Subgroup|subgroup]] that is [[Definition:Group Isomorphism|isomorphic]] to the [[Definition:Integer Lattice|integer lattice]] $\Z^n$.	1
{{missingLinks}} By [[Noether Normalization Lemma]], we find a finite and injective morphism: :$\alpha: k \left[{x_1, \ldots, x_n}\right] \to L$ If we can prove that $n = 0$, the proof is complete. Let $n > 0$. Then: :$x_1 \in k \left[{x_1, \dotsc, x_n}\right]$ and: :$\alpha \left({x_1}\right) \ne 0$ We have that $\alpha \left({x_1}\right)^{-1}$ is integral over $k \left[{x_1, \dotsc, x_n}\right]$. Thus there exists a $m \in \N$ and $a_0, \dotsc, a_{m-1} \in k \left[{x_1, \dotsc, x_n}\right]$ such that: :$\displaystyle \alpha \left({x_1}\right)^{-m} + \sum_{i \mathop = 0}^{m-1} \alpha \left({a_i}\right) \alpha \left({x_1}\right)^{-i} = 0$ If we multiply this by $\alpha \left({x_1}\right)^m$, we find that: {{begin-eqn}} {{eqn | l = 0 | r = 1 + \sum_{i \mathop = 0}^{m - 1} \alpha \left({a_i}\right) \alpha \left({x_1}\right)^{m - i} }} {{eqn | r = \alpha\left({1 + x_1 \left({\sum_{i \mathop = 0}^{m - 1} a_i x_1^{m - i - 1} }\right)}\right) }} {{end-eqn}} and thus, since $\alpha$ is injective, we find that: :$\displaystyle 1 = x_1 \left({- \sum_{i \mathop = 0}^{m - 1} a_i x_1^{m - i - 1} }\right)$ which means that $x_1$ is invertible. This contradiction shows that $n = 0$. {{qed}} {{Namedfor|Oscar Zariski}} [[Category:Commutative Algebra]] [[Category:Field Extensions]] 6yb9f0ademarqp30nbp22rgs0xsbytb	1
Let $S$ be a [[Definition:Simultaneous Linear Equations|system of simultaneous linear equations]]: :$\displaystyle \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$ Let $S$ be expressed in [[Definition:Matrix Representation of Simultaneous Linear Equations|matrix form]] as: :$\mathbf A \mathbf x = \mathbf b$ where: :$\mathbf A = \begin {pmatrix} \alpha_{1 1} & \alpha_{1 2} & \cdots & \alpha_{1 n} \\ \alpha_{2 1} & \alpha_{2 2} & \cdots & \alpha_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ \alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m n} \\ \end {pmatrix}$, $\mathbf x = \begin {pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $\mathbf b = \begin {pmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end {pmatrix}$ Then $S$ has at least one [[Definition:Solution to System of Simultaneous Equations|solution]] {{iff}}: :$\map \rho {\mathbf A} = \map \rho {\begin {array} {c|c} \mathbf A & \mathbf b \end {array} }$ where: :$\map \rho {\mathbf A}$ denotes the [[Definition:Rank of Matrix|rank]] of $\mathbf A$ :$\paren {\begin {array} {c|c} \mathbf A & \mathbf b \end {array} }$ denotes the [[Definition:Augmented Matrix of Simultaneous Linear Equations|augmented matrix]] of $S$.	1
Let $\mathbf V$ be a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$n$ dimensions]]. Let $\left({\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}\right)$ be the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis of $\mathbf V$]]. Let $f \left({x_1, x_2, \ldots, x_n}\right), g \left({x_1, x_2, \ldots, x_n}\right): \mathbf V \to \R$ be [[Definition:Real-Valued Function|real-valued functions]] on $\mathbf V$. Let $\nabla f$ denote the [[Definition:Gradient Operator|gradient]] of $f$. Then: :$\nabla \left({f + g}\right) = \nabla f + \nabla g$	1
A [[Definition:Banach Space|Banach space]] is a [[Definition:Normed Vector Space|normed vector space]], where a [[Definition:Cauchy Sequence|Cauchy sequence]] [[Definition:Convergent Sequence in Normed Vector Space|converges]] {{WRT}} the supplied [[Definition:Norm on Vector Space|norm]]. To prove the theorem, we need to show that a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $\struct {\map \CC I, \norm {\,\cdot\,}_\infty}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]]. We take a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] $\sequence {x_n}_{n \mathop \in \N}$ in $\struct {\map \CC I, \norm {\,\cdot\,}_\infty}$. Then we fix $t \in I$ and show, that a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] $\sequence {\map {x_n} t}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$ with the [[Definition:Limit of Real Sequence|limit]] $\map x t$. Then we prove the [[Definition:Continuous Real Function/Point/Definition by Epsilon-Delta|continuity]] of $\map x t$. Finally, we show that $\sequence {x_n}_{n \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] in $\struct {\map \CC I, \norm {\,\cdot\,}_\infty}$ with the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $\map x t$. === $\sequence {\map {x_n} t}_{n \mathop \in \N}$ is a Cauchy sequence === Let $\sequence{x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\map \CC I$: :$\forall \epsilon \in \R_{> 0} : \exists N \in \N : \forall n, m > N : \norm{x_n − x_m}_\infty < \epsilon$ Fix $t \in I$. Then: {{begin-eqn}} {{eqn | l = \size {\map {x_n} t - \map {x_m} t} | o = \le | r = \max_{\tau \mathop \in I} \size {\map {x_n} \tau - \map {x_m} \tau} }} {{eqn | r = \norm {x_n - x_m}_\infty | c = {{Defof|Supremum Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Hence $\sequence{\map {x_n} t}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\R, \size {\, \cdot \,}}$. {{qed|lemma}} === $\sequence {\map {x_n} t}_{n \mathop \in \N}$ converges in $\struct {\R,\size {\, \cdot \,}}$ === From [[Real Number Line is Complete Metric Space]], $\R$ is a [[Definition:Complete Metric Space|complete metric space]]. Therefore, $\sequence{\map {x_n} t}_{n \mathop \in \N}$ is [[Definition:Convergent Sequence in Normed Vector Space|convergent]]. Denote the [[Definition:Limit of Sequence in Normed Vector Space|limit]] by $\map x t : I \to \R$: :$\displaystyle \lim_{n \mathop \to \infty} \map {x_n} t = \map x t$ {{qed|lemma}} === $\map x t$ is continuous === Choose $N$ such that: :$\forall n, m > N : \norm{x_n - x_m} < \dfrac \epsilon 3$ Let $\tau \in I$. Then $\forall n > N$ and $m = N + 1 > N$: {{begin-eqn}} {{eqn | l = \size {\map {x_n} \tau - \map {x_{N + 1} } \tau } | o = \le | r = \norm {x_n - x_{N + 1} }_\infty }} {{eqn | o = < | r = \frac \epsilon 3 }} {{end-eqn}} Take the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $n \to \infty$: {{begin-eqn}} {{eqn | r = \lim_{n \mathop \to \infty} \size {\map {x_n} \tau - \map {x_{N + 1} } \tau} | l = \size {\map x \tau - \map {x_{N + 1} } \tau} | c = $\sequence {\map {x_n} t}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R,\size {\, \cdot \,} }$ }} {{eqn | o = < | r = \frac \epsilon 3 }} {{end-eqn}} By assumption, $\map {x_{N + 1} } t \in \map \CC I$. Then: :$\forall t, \tau \in I : \exists \delta > 0: \size {\tau - t} < \delta \implies \size {\map {x_{N + 1} } t - \map {x_{N + 1} } \tau} < \dfrac \epsilon 3$ Thus: {{begin-eqn}} {{eqn | l = \size {\map x \tau - \map x t} | r = \size {\map x \tau - \map {x_{N + 1} } \tau + \map {x_{N + 1} } \tau - \map {x_{N + 1} } t + \map {x_{N + 1} } t - \map x t} }} {{eqn | o = < | r = \size {\map x \tau - \map {x_{N + 1} } \tau} + \size {\map {x_{N + 1} } \tau - \map {x_{N + 1} } t} + \size {\map {x_{N + 1} } t - \map x t} | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | o = < | r = \frac \epsilon 3 + \frac \epsilon 3 + \frac \epsilon 3 }} {{eqn | r = \epsilon}} {{end-eqn}} Hence, $\map x t$ is [[Definition:Continuous Real Function/Point|continuous]] in $I$. {{qed|lemma}} === $\sequence {x_n}_{n \mathop \in \N}$ converges to $x$=== Let $\epsilon > 0$. Choose $N$ such that: :$\forall n, m > N : \norm {x_n - x_m}_\infty < \epsilon$ Fix $n > N$. Let $t \in I$. Then, $\forall m > N$: {{begin-eqn}} {{eqn | l = \size {\map {x_n} t - \map {x_m} t} | o = \le | r = \norm {x_n - x_m}_\infty }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Take the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $m \to \infty$: {{begin-eqn}} {{eqn | l = \lim_{m \mathop \to \infty} \size {\map {x_n} t - \map {x_m} t} | r = \size {\map {x_n} t - \map x t} | c = $\sequence {\map {x_n} t}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R,\size {\, \cdot \,} }$ }} {{eqn | o = \le | r = \max_{t \mathop \in I} \size {\map {x_n} t - \map x t} }} {{eqn | r = \norm {x_n - x}_\infty | c = {{Defof|Supremum Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} This holds for every $n > N$. Repeat the argument for all $\epsilon > 0$: :$\forall \epsilon > 0 : \exists N \in \N : \forall n \in \N : \forall n > N : \norm {x_n - x}_\infty < \epsilon$. Therefore, $x_n$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $x$ in [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|$\struct {\map \CC I, \norm {\,\cdot\,}_\infty}$]]: :$\displaystyle \lim_{x \mathop \to \infty} x_n = x$ {{qed}}	1
If ones appear in a row of $\mathbf A$, then replace $\mathbf A$ by $\mathbf A^T$ and $\mathbf B$ by $\mathbf B^T$. Assume $\mathbf A$ has a column of ones. Apply [[Sum of Elements of Invertible Matrix]] to the [[Definition:Inverse Matrix|inverse]] $\mathbf B = \mathbf A^{-1}$: :$\displaystyle \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n b_{i j} = 1 - \map \det {\mathbf B} \map \det {\mathbf B^{-1} - \mathbf J_n}$ where $\mathbf J_n$ denotes the [[Definition:Square Ones Matrix|square ones matrix]] of [[Definition:Order of Square Matrix|order]] $n$. If $\mathbf A = \mathbf B^{-1}$ has a column of ones, then $\mathbf B^{-1} - \mathbf J_n$ has a column of zeros, implying determinant zero. Substitute $\map \det {\mathbf B^{-1} - \mathbf J_n} = 0$ in [[Sum of Elements of Invertible Matrix]]: :$\displaystyle \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n b_{i j} = 1 - 0$ which implies the statement. {{qed}}	1
From [[Subsequence of Cauchy Sequence in Normed Division Ring is Cauchy Sequence]]: :$\sequence {x_{m_n} }$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] Let $\epsilon > 0$. By definition of a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]: :$\exists N: \forall n, m > N: \norm {x_n - x_m } < \epsilon$ From [[Index of Subsequence not Less than its Index]]: $\forall n \in \N : m_n \ge n$ Thus: :$\exists N: \forall n > N: \norm {x_n - x_{m_n} } < \epsilon$ By definition of [[Definition:Convergent Sequence in Normed Division Ring|convergence]]: :$\displaystyle \lim_{n \mathop \to \infty} {x_n - x_{m_n} } = 0$ {{qed}} [[Category:Cauchy Sequences]] [[Category:Normed Division Rings]] 9kmhm554fykctwjkx1wpwshgnupjzs3	1
Now suppose $m \in \Z: m \ge 2$ be [[Definition:Composite Number|composite]]. Then: : $\exists k, l \in \N_{> 0}: 1 < k < m, 1 < l < m: m = k \times l$ Thus: {{begin-eqn}} {{eqn | l = \eqclass 0 m | r = \eqclass m m | c = }} {{eqn | r = \eqclass {k l} m | c = }} {{eqn | r = \eqclass k m \times \\eqclass l m | c = }} {{end-eqn}} So $\struct {\Z_m, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]] with [[Definition:Zero Divisor of Ring|zero divisors]]. So by definition $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]]. {{qed}}	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $\R$ or $\C$. Let $A \subseteq V$ be a [[Definition:Empty Set|non-empty]] [[Definition:Convex Set (Vector Space)|convex set]]. Then $A$ is a [[Definition:Star Convex Set|star convex set]], and every point in $A$ is a [[Definition:Star Convex Set|star center]].	1
By definition of the [[Definition:Unit Matrix|unit matrix]]: :$I_{a b} = \delta_{a b}$ where: :$I_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf I$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$. By definition, $\mathbf E$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $m$]] formed by applying $e$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$. That is, all [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $k$ of $\mathbf I$ are to be [[Definition:Ring Product|multiplied]] by $\lambda$. By definition of [[Definition:Unit Matrix|unit matrix]], all [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $k$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{k k}$, which is $1$. Thus in $\mathbf E$: :$E_{k k} = \lambda \cdot 1 = \lambda$ The [[Definition:Element of Matrix|elements]] in all the other [[Definition:Column of Matrix|columns]] of $\mathbf E$ are the same as the corresponding [[Definition:Element of Matrix|elements]] of $\mathbf I$. Hence the result. {{qed}} [[Category:Elementary Matrix corresponding to Elementary Column Operation]] jb6yvslnfj0bla5ztd3536s3d31e4o2	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {R', \norm {\, \cdot \,}' }$ be a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] of $\struct {R, \norm {\, \cdot \,} }$ Then: :$\norm {\, \cdot \,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]] {{iff}} $\norm {\, \cdot \,}'$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]].	1
Let $\mathbf a, \mathbf b$ be [[Definition:Vector Quantity|vector quantities]]. Then: :$\mathbf a + \mathbf b = \mathbf b + \mathbf a$	1
Let $\mathbf C := \mathbf A \mathbf B$. By definition of [[Definition:Matrix Product (Conventional)|matrix product]]: :$\ds c_{i k} = \sum_{j \mathop = 1}^n a_{i j} b_{j k}$ Thus for the [[Definition:Diagonal Element|diagonal elements]]: :$\ds c_{i i} = \sum_{j \mathop = 1}^n a_{i j} b_{j i}$ By definition of [[Definition:Trace of Matrix|trace]]: :$\ds \map \tr {\mathbf C} = \sum_{i \mathop = 1}^n c_{i i}$ Hence the result. {{qed}}	1
By definition of [[Definition:Contour (Complex Plane)|contour]], $C$ is a [[Definition:Concatenation of Contours (Complex Plane)|concatenation]] of a [[Definition:Finite Sequence|finite sequence]] $C_1, \ldots, C_n$ of [[Definition:Directed Smooth Curve (Complex Plane)|directed smooth curves]]. Let $C_i$ be [[Definition:Parameterization of Directed Smooth Curve (Complex Plane)|parameterized]] by the [[Definition:Smooth Path (Complex Analysis)|smooth path]] $\gamma_i: \closedint {a_i} {b_i} \to \C$ for all $i \in \set {1, \ldots, n}$. Then: {{begin-eqn}} {{eqn | l = \size {\int_C \map f z \rd z} | r = \size {\sum_{i \mathop = 1}^n \int_{a_i}^{b_i} \map f {\map {\gamma_i} t} \map {\gamma_i'} t \rd t} | c = {{Defof|Complex Contour Integral}} }} {{eqn | o = \le | r = \sum_{i \mathop = 1}^n \size {\int_{a_i}^{b_i} \map f {\map {\gamma_i} t} \map {\gamma_i'} t \rd t} | c = [[Triangle Inequality for Complex Numbers]] }} {{eqn | o = \le | r = \sum_{i \mathop = 1}^n \int_{a_i}^{b_i} \size {\map f {\map {\gamma_i} t} } \size {\map {\gamma_i'} t} \rd t | c = [[Modulus of Complex Integral]] }} {{eqn | o = \le | r = \sum_{i \mathop = 1}^n \max_{t \mathop \in \closedint {a_i} {b_i} } \size {\map f {\map {\gamma_i} t} } \int_{a_i}^{b_i} \size {\map {\gamma_i'} t} \rd t | c = [[Linear Combination of Integrals]] }} {{eqn | o = \le | r = \sum_{i \mathop = 1}^n \max_{z \mathop \in \Img C} \size {\map f z} \int_{a_i}^{b_i} \size {\map {\gamma_i'} t} \rd t | c = as $\map {\gamma_i} t \in \Img C$ }} {{eqn | r = \max_{z \mathop \in \Img C} \size {\map f z} \map L C | c = {{Defof|Length of Contour (Complex Plane)}} }} {{end-eqn}} {{qed}}	1
Let $\mathbf C = \mathbf A_1 \mathbf A_2 \cdots \mathbf A_m$ From [[Product of Finite Sequence of Matrices]], the general element of $\mathbf C$ is given in the [[Definition:Einstein Summation Convention|Einstein summation convention]] by: :$\map c {i_1, j} = \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \cdots \map {a_{m - 1} } {i_{m - 1}, i_m} \map {a_m} {i_m, j}$ Thus for the [[Definition:Diagonal Element|diagonal elements]]: :$\ds \map c {i_1, i_1} = \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \cdots \map {a_{m - 1} } {i_{m - 1}, i_m} \map {a_m} {i_m, i_1}$ which is the [[Definition:Einstein Summation Convention|Einstein summation convention]] for the [[Definition:Trace of Matrix|trace]] of $\mathbf C$. {{qed}}	1
The [[Definition:Real Number|set of real numbers]] $\R$, with the operations of [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]], forms a [[Definition:Vector Space|vector space]].	1
Let $G$ be an [[Definition:Dimension (Linear Algebra)|$n$-dimensional]] [[Definition:Module|$R$-module]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. Then $G^*$ and $G^{**}$ are also [[Definition:Dimension (Linear Algebra)|$n$-dimensional]].	1
Suppose, $F$ is [[Definition:Empty Set|empty]]. By [[Empty Set is Closed in Normed Vector Space]], $F$ is [[Definition:Closed Set in Normed Vector Space|closed]]. Suppose, for some $n \in \N$: :$\displaystyle F = \bigcup_{i \mathop = 1}^n \set {x_i}$ We have that [[Singleton in Normed Vector Space is Closed]]. Hence, $F$ is a [[Definition:Finite Union|finite union]] of [[Definition:Closed Set in Normed Vector Space|closed sets]]. By [[Finite Union of Closed Sets is Closed in Normed Vector Space ]], $F$ is [[Definition:Closed Set in Normed Vector Space|closed]]. {{qed}}	1
'''Vector analysis''' is the branch of [[Definition:Linear Algebra|linear algebra]] concerned with [[Definition:Differentiation|differentiation]] and [[Definition:Integration|integration]] of [[Definition:Vector Space|vector spaces]], primarily in [[Definition:Euclidean Space|Euclidean space]] of [[Definition:Dimension (Geometry)|$3$-dimensional space]].	1
$M$ is the '''internal direct sum''' of $(M_i)_{i\in I}$ {{Iff}} every $m\in M$ can be written uniquely as a sum $\sum m_i$ with each $m_i\in M_i$.	1
Let $\struct {R, \norm {\,\cdot\,}_R}$ and $\struct {S, \norm {\,\cdot\,}_S}$ be [[Definition:Normed Division Ring|normed division rings]]. Let $\phi:R \to S$ be an [[Definition:Isometric Isomorphism|isometric isomorphism]]. Then: :$\norm {\,\cdot\,}_R$ is a [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]] {{iff}} $\norm {\,\cdot\,}_S$ is a [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]].	1
Let $\struct {\R \setminus \Q, \tau_d}$ be the [[Definition:Irrational Number Space|irrational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Then $\struct {\R \setminus \Q, \tau_d}$ is [[Definition:Separable Space|separable]].	1
Let $G$ and $H$ be [[Definition:Module|$R$-modules]]. Let $\phi$ and $\psi$ be [[Definition:Linear Transformation|linear transformations]] $G$ into $H$. Let $S$ be a [[Definition:Generator of Module|generator]] for $G$. Suppose that $\forall x \in S: \map \phi x = \map \psi x$. Then $\phi = \psi$.	1
A vector space has unique additive identity.	1
Let $\struct {\R, \size {\, \cdot \,}}$ be the [[Real Numbers with Absolute Value form Normed Vector Space|normed vector space of real numbers]]. Let $\R \setminus \Q$ be the [[Definition:Irrational Number|set of irrational numbers]]. Then $\R \setminus \Q$ are [[Definition:Everywhere Dense/Normed Vector Space|everywhere dense]] in $\struct {\R, \size {\, \cdot \,}}$	1
Let $H, K, L$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $A \in \map B {K, L}, B \in \map B {H, K}$ be [[Definition:Bounded Linear Transformation|bounded linear transformations]]. Then $\paren {A B}^* = B^* A^*$, where $^*$ denotes [[Definition:Adjoint Linear Transformation|adjoining]].	1
=== Axiom 1 === Let $x, y \in T$ with [[Definition:Module Direct Product|$x = (s_i)_{i\in I}$ and $y = (t_i)_{i\in I}$.]] Let $\lambda\in R$. Then: {{begin-eqn}} {{eqn | l = \lambda \circ (x + y) | r = \lambda \circ ((s_i)_{i\in I} + (t_i)_{i\in I}) | c = By definition of elements in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = \lambda \circ (s_i + t_i)_{i\in I} | c = By addition in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = (\lambda \circ s_i + \lambda \circ t_i)_{i\in I} | c = By $R$-action in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = (\lambda \circ s_i)_{i\in I} + (\lambda \circ t_i)_{i\in I} | c = By addition in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = \lambda \circ (s_i)_{i\in I} + \lambda \circ (t_i)_{i\in I} | c = By $R$-action in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = \lambda \circ x + \lambda \circ y | c = By definition of elements in [[Definition:Module Direct Sum|direct sum]] }} {{end-eqn}} {{qed|lemma}} === Axiom 2 === Let $x \in T$ with $x = (s_i)_{i\in I}$ Let $\lambda, \mu \in R$. Then: {{begin-eqn}} {{eqn | l = \left({\lambda + \mu}\right) \circ x | r = (\lambda+\mu) \circ (s_i)_{i\in I} | c = By definition of elements in [[Definition:Module Direct Sum|direct sum]] }} {{eqn | r = ((\lambda + \mu) \circ s_i)_{i\in I} | c = By definition or $R$-action in direct sum }} {{eqn | r = (\lambda \circ s_i + \mu\circ s_i)_{i\in I} | c = By definition or $R$-action in modules }} {{eqn | r = (\lambda \circ s_i)_{i\in I} + (\mu\circ s_i)_{i\in I} | c = By definition or sum in direct sum }} {{eqn | r = \lambda \circ (s_i)_{i\in I} + \mu\circ (s_i)_{i\in I} | c = By definition of $R$-action on direct sum }} {{eqn | r = \lambda \circ x + \mu\circ x | c = By original equality }} {{end-eqn}} {{qed|lemma}} === Axiom 3 === Let $x\in T$ with $x = (s_i)_{i\in I}$. Let $\lambda, \mu \in R$. Then: {{begin-eqn}} {{eqn | l = (\lambda \times \mu) \circ x | r = (\lambda \times \mu) \circ (s_i)_{i\in I} | c = By original equality }} {{eqn | r = ((\lambda \times\mu) \circ s_i)_{i\in I} | c = By Definition of $R$-action on direct sum }} {{eqn | r = (\lambda \circ (\mu\circ s_i))_{i\in I} | c = By definition of modules }} {{eqn | r = \lambda \circ (\mu \circ s_i)_{i\in I} | c = Definition of $R$-action on direct sum }} {{eqn | r = \lambda \circ \left({\mu \circ x}\right) | c = By original equality }} {{end-eqn}} {{qed}} [[Category:Tensor Algebra]] [[Category:Module Theory]] dzo7uolnglmq24k5wog3smx85vt5h3o	1
[[Hensel's Lemma/First Form|Hensel's Lemma]] is used to prove the existence of a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] that does not [[Definition:Convergent Sequence in Normed Division Ring|converge]]. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 1|Lemma 1]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 1}} Let $x_1 \in \Z_{>0}: p \nmid x_1, x_1 \ge \dfrac {p + 1} 2$ Let $k$ be a [[Definition:Positive Integer|positive integer]] such that $k \ge 2, p \nmid k$. Let $a = x_1^k + p$. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2|Lemma 2]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2}} Let $\map f X \in \Z \sqbrk X$ be the [[Definition:Polynomial (Abstract Algebra)|polynomial]]: :$X^k - a$ ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 3|Lemma 3]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 3}} Let $\map {f'} X \in \Z \sqbrk X$ be the [[Definition:Formal Derivative of Polynomial|formal derivative]] of $\map f X$. ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 4|Lemma 4]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 4}} By [[Hensel's Lemma/First Form|Hensel's Lemma]] there exists a [[Definition:Sequence|sequence]] of [[Definition:Integer|integers]] $\sequence {x_n}$ such that: :$(1) \quad \forall n: \map f {x_n} \equiv 0 \pmod {p^n}$ :$(2) \quad \forall n: x_{n + 1} \equiv x_n \pmod {p^n}$ ==== [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 5|Lemma 5]] ==== {{:P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 5}} By [[Characterisation of Cauchy Sequence in Non-Archimedean Norm/Corollary 1|corollary of Characterisation of Cauchy Sequence in Non-Archimedean Norm]] then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {\Q, \norm {\,\cdot\,}_p}$ {{AimForCont}} $\sequence {x_n}$ is a [[Definition:Sequence|sequence]] such that for some $c \in \Q$: :$\displaystyle \lim_{n \mathop \to \infty} x_n = c$ in $\struct {\Q, \norm {\,\cdot\,}_p}$ By [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|product rule for convergent sequences]] then: :$\displaystyle \lim_{n \mathop \to \infty} x_n^k = c^k$ Hence: :$c^k = a \in \Z$. By [[Nth Root of Integer is Integer or Irrational]] then: :$c \in \Z$ This [[Definition:Contradiction|contradicts]] [[P-adic Norm not Complete on Rational Numbers/Proof 2/Lemma 2|Lemma 2]]. So the [[Definition:Sequence|sequence]] $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] in $\struct {\Q, \norm{\,\cdot\,}_p}$. The result follows. {{qed}}	1
Let $S$ be one of the [[Definition:Standard Number Field|standard number fields]] $\Q$, $\R$, $\C$. Let $S^n$ be a [[Definition:Cartesian Space|cartesian space]] for $n \in \N_{\ge 1}$. Let $d: S^n \times S^n \to \R$ be the [[Definition:Usual Metric|usual (Euclidean) metric]] on $S^n$. Then $\tuple {S^n, d}$ is a '''Euclidean space'''.	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. By the definition of a [[Definition:Closed Ball of Normed Division Ring|closed ball]] in $\norm {\,\cdot\,}$ then: :$\map { {B_r}^-} x$ is a [[Definition:Closed Ball|closed ball]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$. By [[Closed Ball is Closed in Metric Space]] then $\map { {B_r}^-} c$ is [[Definition:Closed Set of Metric Space|closed]] in $d$. So it remains to show that $\map { {B_r}^-} x$ is [[Definition:Open Set of Metric Space|open]] in $d$. Let $y \in \map { {B_r}^-} x$. By [[Topological Properties of Non-Archimedean Division Rings/Centers of Closed Balls|Centers of Closed Balls]] then: :$\map { {B_r}^-} y = \map { {B_r}^-} x$ By the definition of an [[Definition:Open Ball|open ball]] then: :$y \in \map {B_r} y \subseteq \map { {B_r}^-} y = \map { {B_r}^-} x$ By the definition of an [[Definition:Open Set of Metric Space|open set]] in a [[Definition:Metric Space|metric space]], $\map { {B_r}^-} x$ is [[Definition:Open Set of Metric Space|open]]. {{qed}}	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $-z$ be the [[Definition:Negative of Complex Number|negative]] of $z$: :$z + \paren {-z} = 0$ Then: :$\cmod z = \cmod {\paren {-z} }$ where $\cmod z$ denotes the [[Definition:Complex Modulus|modulus]] of $z$.	1
Suppose $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0$. Then: :$\exists K \in \N: \forall n > K : x_n \ne 0$ and the [[Definition:Sequence|sequence]]: :$\sequence {\paren {x_{K + n} }^{-1} }_{n \mathop \in \N}$ is well-defined and a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. The $n$th [[Definition:Special Orthogonal Group|orthogonal group]] on $k$ is a [[Definition:Group|group]].	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]], and let $B_0 \left({H, K}\right)$ be the [[Definition:Space of Compact Linear Transformations|space of compact linear transformations from $H$ to $K$]]. Let $\Bbb F \in \left\{{\R, \C}\right\}$ be the [[Definition:Ground Field|ground field]] of $K$. Now $B_0 \left({H, K}\right) \subseteq K^H$, the [[Definition:Set of All Mappings|set of mappings from $H$ to $K$]]. Therefore, $B_0 \left({H, K}\right)$ can be endowed with [[Definition:Pointwise Addition of Mappings|pointwise addition]] ($+$) and [[Definition:Pointwise Scalar Multiplication of Mappings|($\Bbb F$)-scalar multiplication]] ($\circ$). Let $\left\Vert{\cdot}\right\Vert$ denote the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]]. Then $\left\Vert{\cdot}\right\Vert$ is a [[Definition:Norm on Vector Space|norm]] on $B_0 \left({H, K}\right)$. Furthermore, $B_0 \left({H, K}\right)$ is a [[Definition:Banach Space|Banach space]] with respect to this norm.	1
=== Necessary Condition === Let $\struct {R, +, \circ}$ be a [[Definition:Field (Abstract Algebra)|field]]. The result follows from [[Field has 2 Ideals]]. {{qed|lemma}} === Sufficient Condition === Suppose that the only [[Definition:Ideal of Ring|ideals]] of $\struct {R, +, \circ}$ are $\struct {R, +, \circ}$ and $\set {0_R}$. The result follows from [[Commutative and Unitary Ring with 2 Ideals is Field]] {{Qed}}	1
Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Let $R^n$ be the [[Definition:Module on Cartesian Product|$R$-module $R^n$]]. Let $\psi: R^n \to G$ be defined as: :$\displaystyle \map \psi {\sequence {\lambda_k}_{1 \mathop \le k \mathop \le n} } = \sum_{k \mathop = 1}^n \lambda_k a_k$ Then $\psi$ is an [[Definition:R-Algebraic Structure Isomorphism|isomorphism]].	1
:[[File:Rotation-of-cartesian-plane.png|400px]] Let $\mathbf r$ be represented by a [[Definition:Directed Line Segment|directed line segment]] whose [[Definition:Initial Point of Vector|initial point]] coincides with the [[Definition:Origin|origin]] $O$. Let the [[Definition:Terminal Point of Vector|terminal point]] of $\mathbf r$ be identified with the [[Definition:Point|point]] $P$. Let $\CC$ be rotated to $\CC'$ through an [[Definition:Angle|angle]] $\varphi$ as shown, keeping $P$ fixed. We have that: {{begin-eqn}} {{eqn | l = x' | r = OA + BP | c = by inspection }} {{eqn | r = x \cos \varphi + y \sin \varphi | c = {{Defof|Cosine}}, {{Defof|Sine}} }} {{eqn | l = y' | r = xB - xA | c = by inspection }} {{eqn | r = y \cos \varphi - x \sin \varphi | c = {{Defof|Cosine}}, {{Defof|Sine}} }} {{end-eqn}} hence the result. {{qed}}	1
By the definition of [[Definition:Null Space|null space]]: :$\mathbf x \in \map {\mathrm N} {\mathbf A} \iff \mathbf A \mathbf x = \mathbf 0$ From the [[Row Equivalent Matrix for Homogeneous System has same Solutions/Corollary|corollary to Row Equivalent Matrix for Homogeneous System has same Solutions]]: :$\mathbf A \mathbf x = \mathbf 0 \iff \map {\mathrm {rref} } {\mathbf A} \mathbf x = \mathbf 0$ Hence the result, by the definition of [[Definition:Set Equality|set equality]]. {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Complete Normed Division Ring|complete normed division ring]]. Let $\struct {S, \norm {\, \cdot \,}}$ be a [[Definition:Everywhere Dense|dense]] [[Definition:Normed Division Subring|normed division subring]] of $\struct {R, \norm {\, \cdot \,}}$. Then for all $x \in R$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $S$: :$x = \displaystyle \lim_{n \mathop \to \infty} x_n$ and :$\norm x = \displaystyle \lim_{n \mathop \to \infty} \norm {x_n}$	1
The [[Definition:Octonion|octonions]] $\Bbb O$ are formed by the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]] from the [[Definition:Quaternion|quaternions]] $\Bbb H$. From [[Quaternions form Algebra]], we have that $\Bbb H$ forms: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]] :$(2): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]] :$(3): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. From [[Cayley-Dickson Construction forms Star-Algebra]], $\Bbb O$ is a [[Definition:Star-Algebra|$*$-algebra]]. From [[Cayley-Dickson Construction from Nicely Normed Algebra is Nicely Normed]], $\Bbb O$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. From [[Nicely Normed Cayley-Dickson Construction from Associative Algebra is Alternative]], $\Bbb O$ is an [[Definition:Alternative Algebra|alternative algebra]]. Now suppose $\Bbb O$ formed an [[Definition:Associative Algebra|associative algebra]]. Then from [[Cayley-Dickson Construction from Commutative Associative Algebra is Associative]], that would mean $\Bbb H$ is a [[Definition:Commutative Algebra|commutative algebra]]. But from [[Quaternions form Algebra]] it is explicitly demonstrated that $\Bbb H$ is '''not''' a [[Definition:Commutative Algebra|commutative algebra]]. So $\Bbb O$ can not be a [[Definition:Associative Algebra|associative algebra]]. === Proof of Normed Division Algebra === {{finish|Might not need this section. Might find a better way of processing this bit.}} {{qed}}	1
Recall that [[Real Numbers form Field]]. Thus by definition, $\R$ is also a [[Definition:Division Ring|division ring]]. Thus we only need to show that [[Definition:Module|$\R$-module]] $\C$ is a [[Definition:Unitary Module|unitary module]], by demonstrating the module properties: $\forall x, y, \in \C, \forall \lambda, \mu \in \R$: :$(1): \quad \lambda \paren {x + y} = \paren {\lambda x} + \paren {\lambda y}$ :$(2): \quad \paren {\lambda + \mu} x = \paren {\lambda x} + \paren {\mu x}$ :$(3): \quad \paren {\lambda \mu} x = \lambda \paren {\mu x}$ :$(4): \quad 1 x = x$ As $\lambda, \mu \in \R$ it follows that $\lambda, \mu \in \C$. Thus from [[Complex Multiplication Distributes over Addition]], $(1)$ and $(2)$ immediately follow. $(3)$ follows from [[Complex Multiplication is Associative]]. $(4)$ follows from [[Complex Multiplication Identity is One]], as $1 + 0 i$ is the [[Definition:Unity of Field|unity]] of $\C$. {{qed}}	1
Let $y = \map f x$ be the [[Definition:Equation of Geometric Figure|equation]] of a [[Definition:Straight Line|straight line]] $\mathcal L$. From [[Line in Plane is Straight iff Gradient is Constant]], $\mathcal L$ has [[Definition:Constant|constant]] [[Definition:Slope of Straight Line|slope]]. Thus the [[Definition:Derivative|derivative]] of $y$ {{WRT|Differentiation}} $x$ will be of the form: :$y' = c$ Thus: {{begin-eqn}} {{eqn | l = y | r = \int c \rd x | c = [[Fundamental Theorem of Calculus]] }} {{eqn | r = c x + K | c = [[Primitive of Constant]] }} {{end-eqn}} where $K$ is [[Definition:Arbitrary Constant (Calculus)|arbitrary]]. Taking the equation: :$\alpha_1 x + \alpha_2 y = \beta$ it can be seen that this can be expressed as: :$y = - \dfrac {\alpha_1} {\alpha_2} x + \dfrac {\beta} {\alpha_2}$ thus demonstrating that $\alpha_1 x + \alpha_2 y = \beta$ is of the form $y = c x + K$ for some $c, K \in \R$. {{qed}}	1
Follows from [[Row Equivalent Matrix for Homogeneous System has same Solutions]] and from [[Matrix is Row Equivalent to Reduced Echelon Matrix]]. {{qed}} [[Category:Matrix Theory]] [[Category:Linear Algebra]] dla471omeqku92w6a8j8ujatufa5qe7	1
Suppose $m \ne 0$. Then: :$\exists k \in \N : \forall n \in \N: y_{k + n} \ne 0$ and the [[Definition:Sequence|sequences]]: :$\sequence {x_{k + n} \ {y_{k + n} }^{-1} }$ and $\sequence { {y_{k + n} }^{-1} \ x_{k + n} }$ are well-defined and [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} x_{k + n} \ {y_{k + n} }^{-1} = l m^{-1}$ :$\displaystyle \lim_{n \mathop \to \infty} {y_{k + n} }^{-1} \ x_{k + n} = m^{-1} l$	1
From the [[Reverse Triangle Inequality/Real and Complex Fields/Corollary/Proof 2|proof 2 of the corollary to this result]], which is derived independently: :$\size {x - y} \ge \size x - \size y$ There are two cases: $(1): \quad \size x \ge \size y$ We have : :$\size {\size x - \size y} = \size x - \size y$ and the proof is finished. {{qed|lemma}} $(2): \quad \size y \ge \size x$ We have: :$\size {y - x} \ge \size y - \size x = \size {\size y - \size x}$ But: :$\size {y - x} = \size {x - y}$ and: :$\size {\size y - \size x} = \size {\size x - \size y}$ From this we have: :$-\size {\size x - \size y} \ge -\size {x - y}$ Since, by [[Negative of Absolute Value]], we have that: :$\size x - \size y \ge -\size {\size x - \size y}$ it follows that: :$-\size {x - y} \le \size x - \size y \le \size {x - y}$ The result follows. {{qed}}	1
From [[Special Linear Group is Subgroup of General Linear Group]] we have that $\SL {n, K}$ is a [[Definition:Group|group]]. From [[Matrix Multiplication is not Commutative]] it follows that $\SL {n, K}$ is not [[Definition:Abelian Group|abelian]]. {{qed}}	1
Let $\mathbf P$ and $\mathbf Q$ be [[Definition:Orthogonal Matrix|orthogonal matrices]]. Let $\mathbf P \mathbf Q$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf P$ and $\mathbf Q$. Then $\mathbf P \mathbf Q$ is an [[Definition:Orthogonal Matrix|orthogonal matrix]].	1
Let $\struct {R, +, \circ, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {R^\N, +, \circ}$ be the [[Definition:Ring of Sequences|ring of sequences over $R$]] with [[Definition:Ring with Unity|unity]] $\tuple {1, 1, 1, \dotsc}$. Let $\CC \subset R^\N$ be the [[Definition:Set|set]] of [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequences]] on $R$. Then: :$\struct {\CC, +, \circ}$ is a [[Definition:Subring|subring]] of $R^\N$ with [[Definition:Unity of Ring|unity]] $\tuple {1, 1, 1, \dotsc}$.	1
Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then for every $\mathbf v \in \mathbf V$, the [[Definition:Inverse Element|additive inverse]] of $\mathbf v$ is [[Definition:Unique|unique]]: :$\forall \mathbf v \in \mathbf V: \exists! \paren {-\mathbf v} \in \mathbf V: \mathbf v + \paren {-\mathbf v} = \mathbf 0$	1
=== [[Equivalence of Definitions of Equivalent Division Ring Norms/Topologically Equivalent implies Convergently Equivalent|Topologically Equivalent implies Convergently Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Topologically Equivalent implies Convergently Equivalent}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Convergently Equivalent implies Null Sequence Equivalent|Convergently Equivalent implies Null Sequence Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Convergently Equivalent implies Null Sequence Equivalent}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Null Sequence Equivalent implies Open Unit Ball Equivalent|Null Sequence Equivalent implies Open Unit Ball Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Null Sequence Equivalent implies Open Unit Ball Equivalent}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm|Open Unit Ball Equivalent implies Norm is Power of Other Norm]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Norm is Power of Other Norm implies Topologically Equivalent|Norm is Power of Other Norm implies Topologically Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Norm is Power of Other Norm implies Topologically Equivalent}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Norm is Power of Other Norm implies Cauchy Sequence Equivalent|Norm is Power of Other Norm implies Cauchy Sequence Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Norm is Power of Other Norm implies Cauchy Sequence Equivalent}}{{qed|lemma}} === [[Equivalence of Definitions of Equivalent Division Ring Norms/Cauchy Sequence Equivalent implies Open Unit Ball Equivalent|Cauchy Sequence Equivalent implies Open Unit Ball Equivalent]] === {{:Equivalence of Definitions of Equivalent Division Ring Norms/Cauchy Sequence Equivalent implies Open Unit Ball Equivalent}}{{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Then $\struct {R, +, \circ}_R$ is an [[Definition:Module|$R$-module]]. If $\struct {R, +, \circ}$ has a [[Definition:Unity of Ring|unity]], then $\struct {R, +, \circ}_R$ is [[Definition:Unitary Module|unitary]].	1
This follows by [[Principle of Mathematical Induction|induction]] from {{Module-axiom|1}}, as follows: For all $m \in \N_{>0}$, let $\map P m$ be the [[Definition:Proposition|proposition]]: :$\ds \lambda \circ \paren {\sum_{k \mathop = 1}^m x_k} = \sum_{k \mathop = 1}^m \paren {\lambda \circ x_k}$ === Basis for the Induction === $\map P 1$ is true, as this just says: :$\lambda \circ x_1 = \lambda \circ x_1$ This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P n$ is true, where $n \ge 1$, then it logically follows that $\map P {n + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\ds \lambda \circ \paren {\sum_{k \mathop = 1}^n x_k} = \sum_{k \mathop = 1}^n \paren {\lambda \circ x_k}$ Then we need to show: :$\ds \lambda \circ \paren {\sum_{k \mathop = 1}^{n + 1} x_k} = \sum_{k \mathop = 1}^{n + 1} \paren {\lambda \circ x_k}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \lambda \circ \paren {\sum_{k \mathop = 1}^{n + 1} x_k} | r = \lambda \circ \paren {\sum_{k \mathop = 1}^n x_k + x_{n + 1} } | c = }} {{eqn | r = \lambda \circ \paren {\sum_{k \mathop = 1}^n x_k} + \lambda \circ x_{n + 1} | c = {{Module-axiom|1}} }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {\lambda \circ x_k} + \lambda \circ x_{n + 1} | c = [[Scalar Product with Sum#Induction Hypothesis|Induction hypothesis]] }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \paren {\lambda \circ x_k} | c = }} {{end-eqn}} So $\map P n \implies \map P {n + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\ds \forall m \in \N_{>0}: \lambda \circ \paren {\sum_{k \mathop = 1}^m x_k} = \sum_{k \mathop = 1}^m \paren {\lambda \circ x_k}$ {{qed}}	1
Let $p: H \to V$ be the orthogonal projection of $H$ onto $V$. Then the orthogonal projection of $H$ onto $V^\perp$ is $\mathbf 1 - p$, where $\mathbf 1$ is the [[Definition:Identity_Mapping|identity map]] of $H$. The fact that $t$ stabilizes $V$ can be expressed as: :$\left({\mathbf 1 - p}\right) t p = 0$ or: :$p t p = t p$ The goal is to show that: :$p t \left({\mathbf 1 - p}\right) = 0$ We have that $\left({a, b}\right) \mapsto \operatorname{tr} \left({a b^*}\right)$ is an [[Definition:Inner Product|inner product]] on the space of endomorphisms of $H$. Here, $b^*$ denotes the [[Definition:Adjoint_Operator|adjoint operator]] of $b$. Thus it will suffice to show that $\operatorname{tr} \left({x x^*}\right) = 0$ for $x = p t \left({\mathbf 1 - p}\right)$. This follows from a direct computation, using properties of the [[Definition:Trace (Linear Algebra)|trace]] and orthogonal projections: {{begin-eqn}} {{eqn | l = x x^* | r = p t \left({\mathbf 1 - p}\right)^2 t^* p | c = }} {{eqn | r = p t \left({\mathbf 1 - p}\right) t^* p | c = }} {{eqn | r = p t t^* p - p t p t^* p | c = }} {{eqn | ll= \implies | l = \operatorname{tr} \left({x x^*}\right) | r = \operatorname{tr} \left({p t t^* p}\right) - \operatorname{tr} \left({p t p t^* p}\right) | c = }} {{eqn | r = \operatorname{tr} \left({p^2 t t^*}\right) - \operatorname{tr} \left({p^2 t p t^*}\right) | c = }} {{eqn | r = \operatorname{tr} \left({p t t^*}\right) - \operatorname{tr} \left({\left({p t p}\right) t^*}\right) | c = }} {{eqn | r = \operatorname{tr} \left({p t t^*}\right) - \operatorname{tr} \left({t p t^*}\right) | c = }} {{eqn | r = \operatorname{tr} \left({p t t^*}\right) - \operatorname{tr} \left({p t^* t}\right) | c = }} {{eqn | r = \operatorname{tr} \left({p \left({t t^* - t^*t}\right)}\right) | c = }} {{eqn | r = \operatorname{tr} \left({0}\right) | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed}} [[Category:Hilbert Spaces]] ln3fp79opuvp5z9ckxy0ptr7tuexxoc	1
* $(1)$ implies $(2)$ by definition. * $(2)$ implies $(4)$ by [[Linear Transformation of Vector Space Monomorphism]] and [[Results concerning Generators and Bases of Vector Spaces]]. * $(4)$ implies $(5)$ by basic logic. * Suppose $\phi \left({B}\right)$ is a [[Definition:Basis (Linear Algebra)|basis]] of $H$. Then the [[Definition:Image of Mapping|image]] of $\phi$ is a [[Definition:Vector Subspace|subspace]] of $H$ [[Definition:Generator|generating]] $H$ and hence is $H$ itself. Thus $(5)$ implies $(3)$. * Finally, $(3)$ implies that $\phi$ is [[Definition:Injection|injective]]. If $\phi$ is [[Definition:Surjection|surjective]], the [[Definition:Dimension (Linear Algebra)|dimension]] of its [[Definition:Kernel of Linear Transformation|kernel]] is $0$ by [[Rank Plus Nullity Theorem]]. Hence $\phi$ is an [[Definition:Vector Space Isomorphism|isomorphism]] and therefore $(3)$ implies $(1)$. {{qed}}	1
Let $m\in M$, and $[m]_{\mathcal A}$ be its [[Definition:Coordinate Vector|coordinate vector]] with respect to $\mathcal A$. On the one hand: {{begin-eqn}} {{eqn | l = [g(f(m))]_{\mathcal C} | r = \mathbf M_{(g\mathop\circ f), \mathcal C, \mathcal A} \cdot [m]_{\mathcal A} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $g\circ f$ }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | l = [g(f(m))]_{\mathcal C} | r = \mathbf M_{g, \mathcal C, \mathcal B} \cdot [f(m)]_{\mathcal B} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $g$ }} {{eqn | l = | r = \mathbf M_{g, \mathcal C, \mathcal B} \cdot \mathbf M_{f, \mathcal B, \mathcal A} \cdot [m]_{\mathcal A} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $f$ }} {{end-eqn}} Thus $(\mathbf M_{(g\mathop\circ f), \mathcal C, \mathcal A} - \mathbf M_{g, \mathcal C, \mathcal B} \cdot \mathbf M_{f, \mathcal B, \mathcal A}) \cdot [m]_{\mathcal A} = 0$ for all $m\in M$. The result follows. {{explain|find a link why}} {{qed}}	1
{{begin-eqn}} {{eqn | o = | r = \begin{vmatrix} \left({x + q_2}\right) \left({x + q_3}\right) & \left({x + p_1}\right) \left({x + q_3}\right) & \left({x + p_1}\right) \left({x + p_2}\right) \\ \left({y + q_2}\right) \left({y + q_3}\right) & \left({y + p_1}\right) \left({y + q_3}\right) & \left({y + p_1}\right) \left({y + p_2}\right) \\ \left({z + q_2}\right) \left({z + q_3}\right) & \left({z + p_1}\right) \left({z + q_3}\right) & \left({z + p_1}\right) \left({z + p_2}\right) \end{vmatrix} | c = }} {{eqn | r = \begin{vmatrix} \left({x + q_2}\right) \left({x + q_3}\right) & \left({p_1 - q_2}\right) \left({x + q_3}\right) & \left({p_1 - q_3}\right) \left({x + p_2}\right) \\ \left({y + q_2}\right) \left({y + q_3}\right) & \left({p_1 - q_2}\right) \left({y + q_3}\right) & \left({p_1 - q_3}\right) \left({y + p_2}\right) \\ \left({z + q_2}\right) \left({z + q_3}\right) & \left({p_1 - q_2}\right) \left({z + q_3}\right) & \left({p_1 - q_3}\right) \left({z + p_2}\right) \end{vmatrix} | c = [[Multiple of Row Added to Row of Determinant]] }} {{eqn | r = \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \begin{vmatrix} \left({x + q_2}\right) \left({x + q_3}\right) & x + q_3 & x + p_2 \\ \left({y + q_2}\right) \left({y + q_3}\right) & y + q_3 & y + p_2 \\ \left({z + q_2}\right) \left({z + q_3}\right) & z + q_3 & z + p_2 \end{vmatrix} | c = [[Determinant with Row Multiplied by Constant]] }} {{eqn | r = \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \begin{vmatrix} x \left({x + q_3}\right) & x + q_3 & p_2 - q_3 \\ y \left({y + q_3}\right) & y + q_3 & p_2 - q_3 \\ z \left({z + q_3}\right) & z + q_3 & p_2 - q_3 \end{vmatrix} | c = [[Multiple of Row Added to Row of Determinant]] }} {{eqn | r = \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \left({p_2 - q_3}\right) \begin{vmatrix} x \left({x + q_3}\right) & x + q_3 & 1\\ y \left({y + q_3}\right) & y + q_3 & 1\\ z \left({z + q_3}\right) & z + q_3 & 1 \end{vmatrix} | c = [[Determinant with Row Multiplied by Constant]] }} {{eqn | r = \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \left({p_2 - q_3}\right) \begin{vmatrix} x^2 & x & 1\\ y^2 & y & 1\\ z^2 & z & 1 \end{vmatrix} | c = [[Multiple of Row Added to Row of Determinant]] }} {{eqn | r = \left({p_1 - q_2}\right) \left({p_1 - q_3}\right) \left({p_2 - q_3}\right) \left({x - y}\right) \left({y - z}\right) \left({x - z}\right) | c = [[Vandermonde Determinant]] }} {{end-eqn}} {{qed}} {{Namedfor|Christian Friedrich Krattenthaler|cat = Krattenthaler}}	1
Let $0_R$ be the [[Definition:Ring Zero|zero]] of $R$, then: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $0_R$ in $\norm {\, \cdot \,}_1 \iff \sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $0_R$ in $\norm {\, \cdot \,}_2$ Hence: :$\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm{\,\cdot\,}_1 \iff \sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm{\,\cdot\,}_2$ {{qed}}	1
=== Positive definiteness === Let $x \in \map {\CC^k} I$. Then: {{begin-eqn}} {{eqn | l = \norm x_{\map {C^k} I} | r = \sum_{i \mathop = 0}^k \norm {x^{\paren i} }_\infty }} {{eqn | o = \ge | r = \sum_{i \mathop = 0}^k 0 | c = [[Supremum Norm is Norm]], [[Definition:Norm Axioms (Vector Space)|Norm Axiom $\paren {N1}:$ Positive Definiteness]] }} {{eqn | r = 0 }} {{end-eqn}} Suppose $\norm x_{\map {C^k} I} = 0$. We have that the [[Sum of Nonnegative Real Numbers is Zero iff Every Element is Zero|sum of non-negatives is zero if every element is zero]]. Hence: :$\forall i \in \N : 0 \le i \le k : \norm {x^{\paren i}}_\infty = 0$ Namely, $\norm x_\infty = 0$. By [[Supremum Norm is Norm]] and [[Definition:Norm Axioms (Vector Space)|Norm Axiom $\paren {N1}:$ Positive Definiteness]]: :$\forall t \in I : \map x t = 0$. === Positive homogeneity === Let $x \in \map {\CC^k} I$, $\alpha \in \R$. Then: {{begin-eqn}} {{eqn | l = \norm {\alpha x}_{\map {C^k} I} | r = \sum_{i \mathop = 0}^k \norm {\paren {\alpha x}^{\paren i} }_\infty | c = {{defof|C^k Norm}} }} {{eqn | r = \sum_{i \mathop = 0}^k \norm {\alpha x^{\paren i} }_\infty | c = {{defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \size \alpha \sum_{i \mathop = 0}^k \norm {x^{\paren i} }_\infty | c = [[Supremum Norm is Norm/Continuous on Closed Interval Real-Valued Function|Supremum norm on continuous real-valued functions is a norm: positive homogeneity]] }} {{eqn | r = \size \alpha \norm {x}_{\map {C^k} I} | c = {{defof|C^k Norm}} }} {{end-eqn}} === Triangle inequality === Let $x, y \in \map {\CC^k} I$ {{begin-eqn}} {{eqn | l = \norm {x + y}_{\map {C^k} I} | r = \sum_{i \mathop = 0}^k \norm {\paren {x + y}^{\paren i} }_\infty | c = {{defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | r = \sum_{i \mathop = 0}^k \norm {x^{\paren i} + y^{\paren i} }_\infty | c = {{defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | o = \le | r = \sum_{i \mathop = 0}^k \norm {x^{\paren i} }_\infty + \sum_{i \mathop = 0}^k \norm {y^{\paren i} }_\infty | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | r = \norm x_{\map {C^k} I} + \norm y_{\map {C^k} I} | c = {{defof|C^k Norm}} }} {{end-eqn}} {{qed}}	1
Let: : $z_1 := x_1 + i y_1, z_2 = x_2 + i y_2$ Then: {{begin-eqn}} {{eqn | l = z_1 \times z_2 | r = x_1 y_2 - y_1 x_2 | c = {{Defof|Vector Cross Product|subdef = Complex|index = 1|Complex Cross Product}} }} {{eqn | r = -\left({x_2 y_1 - y_2 x_1}\right) | c = [[Real Addition is Commutative]] and [[Real Multiplication is Commutative]] }} {{eqn | r = -\left({z_2 \times z_1}\right) | c = {{Defof|Vector Cross Product|subdef = Complex|index = 1|Complex Cross Product}} }} {{end-eqn}} {{qed}}	1
By [[Characterisation of Non-Archimedean Division Ring Norms]] then: :$\norm {\,\cdot\,}_1$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] $\iff \forall n \in \N_{>0}: \norm{n \cdot 1_R}_1 \le 1$. By the definition of [[Definition:Equivalent Division Ring Norms by Cauchy Sequence|norm equivalence]] then: :$\forall n \in \N: \norm {n \cdot 1_R}_1 \le 1 \iff \norm {n \cdot 1_R}_2 \le 1$ Similarly, by [[Characterisation of Non-Archimedean Division Ring Norms]] then: :$\forall n \in \N_{>0}: \norm {n \cdot 1_R}_2 \le 1 \iff \norm {\,\cdot\,}_2$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. The result follows. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $L$ be a [[Definition:Bounded Linear Functional|bounded linear functional]] on $H$. Then there is a unique $h_0 \in H$ such that: :$\forall h \in H: L h = \innerprod h {h_0}$	1
Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be a [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] of $A$. Let $\sequence {\lambda_k}_{1 \mathop \le k \mathop \le n}$ a [[Definition:Sequence|sequence]] of [[Definition:Scalar (Module)|scalars]]. Then: $\displaystyle \sum_{k \mathop = 1}^n \lambda_k f_{a_k}$ is the [[Definition:Mapping|mapping]] whose value at $a_k$ is $\lambda_k$ and whose value at any $x$ not in $\set {a_1, a_2, \ldots, a_n}$ is zero. Hence $B$ is a [[Definition:Generator of Module|generator]] of $R^{\left({A}\right)}$ which is [[Definition:Linearly Independent Set|linearly independent]]. Thus, by definition, $B$ is a [[Definition:Basis (Linear Algebra)|basis]] of $R^{\paren A}$. If $A = \closedint 1 n$, then $B$ is the [[Definition:Standard Basis|standard basis]] of $R^n$. {{Qed}}	1
If two [[Definition:Row of Matrix|rows]] of a [[Definition:Matrix|matrix]] with [[Definition:Determinant of Matrix|determinant]] $D$ are [[Definition:Transposition|transposed]], its determinant becomes $-D$.	1
Let $z = a + i b$ be a [[Definition:Complex Number|complex number]]. Let $\cmod z$ be the [[Definition:Complex Modulus|modulus]] of $z$. Let $\overline z$ be the [[Definition:Complex Conjugate|conjugate]] of $z$. Then: :$\cmod z^2 = z \overline z$	1
Let $S$ be the [[Definition:Set|set]] of [[Definition:Vector (Linear Algebra)|vectors]] in $3$ [[Definition:Dimension (Linear Algebra)|dimensional]] [[Definition:Euclidean Space|Euclidean space]]. Let $\times$ denote the [[Definition:Vector Cross Product|vector cross product]] on $S$. Then $\struct {S, \times}$ is a [[Definition:Lie Algebra|Lie algebra]].	1
In the following, $\mathbf A$, $\mathbf B$ and $\mathbf C$ denote arbitrary [[Definition:Matrix|matrices]] in a given [[Definition:Matrix Space|matrix space]] $\map \MM {m, n}$ for $m, n \in \Z{>0}$. We check in turn each of the conditions for [[Definition:Equivalence Relation|equivalence]]: === Reflexive === Let $r_i$ denote an arbitrary [[Definition:Row of Matrix|row]] of $\mathbf A$. Let $e$ denote the [[Definition:Elementary Row Operation|elementary row operation]] $r_i \to 1 r_i$ applied to $\mathbf A$. Then trivially: :$\map e {\mathbf A} = \mathbf A$ and so $\mathbf A$ is trivially [[Definition:Row Equivalence|row equivalent]] to itself. So [[Definition:Row Equivalence|row equivalence]] has been shown to be [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetric === Let $\mathbf A$ be [[Definition:Row Equivalence|row equivalent]] to $\mathbf B$. Let $\Gamma$ be the [[Definition:Row Operation|row operation]] that transforms $\mathbf A$ into $\mathbf B$. From [[Row Operation has Inverse]] there exists a [[Definition:Row Operation|row operation]] $\Gamma'$ which transforms $\mathbf B$ into $\mathbf A$. Thus $\mathbf B$ is [[Definition:Row Equivalence|row equivalent]] to $\mathbf A$. So [[Definition:Row Equivalence|row equivalence]] has been shown to be [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitive === Let $\mathbf A$ be [[Definition:Row Equivalence|row equivalent]] to $\mathbf B$, and let $\mathbf B$ be [[Definition:Row Equivalence|row equivalent]] to $\mathbf C$. Let $\Gamma_1$ be the [[Definition:Row Operation|row operation]] that transforms $\mathbf A$ into $\mathbf B$. Let $\Gamma_2$ be the [[Definition:Row Operation|row operation]] that transforms $\mathbf B$ into $\mathbf C$. From [[Sequence of Row Operations is Row Operation]], $\mathbf C$ is [[Definition:Row Equivalence|row equivalent]] to $\mathbf A$. So [[Definition:Row Equivalence|row equivalence]] has been shown to be [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} [[Definition:Row Equivalence|Row equivalence]] has been shown to be [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. Hence by definition it is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $R^{\left({I}\right)}$ be the [[Definition:Free Module on Set|free $R$-module on $I$]]. Let $M$ be an [[Definition:Module|$R$-module]]. Let $\left\langle{m_i}\right\rangle_{i \mathop \in I}$ be a [[Definition:Indexed Family|family]] of elements of $M$. Then there exists a unique [[Definition:R-Algebraic Structure Homomorphism|$R$-module morphism]]: :$\Psi: R^{\left({I}\right)}\to M$ that sends the $i$th [[Definition:Free Module on Set/Canonical Basis|canonical basis element]] to $m_i$, for all $i\in I$. Moreover: :$\displaystyle \Psi((r_i)_{i \mathop \in I}) = \sum_{i \mathop \in I} r_i m_i$	1
Define: :$\mathbb K := \leftset {K \supseteq H: K}$ is [[Definition:Closed Set (Topology)|closed]] in $\rightset T$ That is, let $\mathbb K$ be the [[Definition:Set|set]] of all [[Definition:Superset|supersets]] of $H$ that are [[Definition:Closed Set (Topology)|closed]] in $T$. The claim is that $H^-$ is the [[Definition:Smallest Set by Set Inclusion|smallest set]] of $\mathbb K$. From [[Set is Subset of its Topological Closure]]: :$H \subseteq H^-$ From [[Topological Closure is Closed]], $H^-$ is [[Definition:Closed Set (Topology)|closed]] in $T$. Thus $H^- \in \mathbb K$. Let $K \in \mathbb K$. From [[Set Closure as Intersection of Closed Sets]]: :$\displaystyle H^- = \bigcap \mathbb K$ Therefore, from [[Intersection is Subset/General Result|Intersection is Subset: General Result]]: :$H^- \subseteq K$ Thus by definition $H^-$ is the [[Definition:Smallest Set by Set Inclusion|smallest set]] of $\mathbb K$. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A:H \to H$ be a [[Definition:Linear Operator|linear operator]] on $H$. Then $A$ is [[Definition:Diagonalizable Operator|diagonalizable]] iff there exists a [[Definition:Basis (Hilbert Space)|basis]] $E$ of $H$, consisting of [[Definition:Eigenvector|eigenvectors]] for $A$.	1
Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf 0 | r = \sqbrk a_{m n} + \sqbrk 0_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {a + 0}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = [[Identity Element of Addition on Numbers]] }} {{eqn | ll= \leadsto | l = \mathbf A + \mathbf 0 | r = \mathbf A | c = {{Defof|Zero Matrix}} }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \mathbf 0 + \mathbf A | r = \sqbrk 0_{m n} + \sqbrk a_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {0 + a}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = [[Identity Element of Addition on Numbers]] }} {{eqn | ll= \leadsto | l = \mathbf 0 + \mathbf A | r = \mathbf A | c = {{Defof|Zero Matrix}} }} {{end-eqn}} {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $n$ be a [[Definition:Positive Integer|positive integer]]. For each $j \in \closedint 1 n$, let $e_j$ be the [[Definition:Ordered Tuple|ordered $n$-tuple]] of [[Definition:Element|elements]] of $R$ whose $j$th entry is $1_R$ and all of whose other entries is $0_R$. Then $\sequence {e_n}$ is an [[Definition:Ordered Basis|ordered basis]] of the [[Definition:Module on Cartesian Product|$R$-module $R^n$]]. This [[Definition:Ordered Basis|ordered basis]] is called the '''[[Definition:Standard Ordered Basis|standard ordered basis of $R^n$]]'''. The corresponding [[Definition:Set|set]] $\set {e_1, e_2, \ldots, e_n}$ is called the '''[[Definition:Standard Basis|standard basis of $R^n$]]'''.	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\cmod z$ denote the [[Definition:Modulus of Complex Number|modulus]] of $z$. Then: :$\cmod {z_1 + z_2} \le \cmod {z_1} + \cmod {z_2}$	1
From [[Normed Vector Space is Open in Itself]], $X$ is [[Definition:Open Set in Normed Vector Space|open]] in $M$. But: :$\varnothing = \relcomp X X$ where $\complement_X$ denotes the [[Definition:Relative Complement|set complement relative to $X$]]. The result follows by definition of [[Definition:Closed Set in Normed Vector Space|closed set]]. {{qed}}	1
Let $m$ be [[Definition:Prime Number|prime]]. From [[Ring of Integers Modulo Prime is Integral Domain]], $\struct {\Z_m, +, \times}$ is an [[Definition:Integral Domain|integral domain]]. From [[Finite Integral Domain is Galois Field]], $\struct {\Z_m, +, \times}$ is a [[Definition:Field (Abstract Algebra)|field]]. {{qed|lemma}} Now suppose $m \in \Z: m \ge 2$ is [[Definition:Composite Number|composite]]. From [[Ring of Integers Modulo Composite is not Integral Domain]], $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]]. From [[Field is Integral Domain]] $\struct {\Z_m, +, \times}$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $z = a + i b$. {{begin-eqn}} {{eqn | l = \cmod {\paren {-z} } | r = \cmod {\paren {-a - i b} } | c = {{Defof|Negative of Complex Number}} }} {{eqn | r = \sqrt {\paren {-a}^2 + \paren {-b}^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \sqrt {a^2 + b^2} | c = [[Even Power of Negative Real Number]] }} {{eqn | r = \cmod {a + i b} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \cmod z | c = Definition of $z$ }} {{end-eqn}} {{qed}} [[Category:Complex Modulus]] 6wt6y75bct78sw0c4q7wfse098r4xw7	1
Since $V$ is not a [[Definition:Trivial Vector Space|trivial vector space]]: :$\exists \mathbf v \in V: \mathbf v \ne 0$ By [[Definition:Norm Axioms (Vector Space)|Norm axiom (N1)]]: :$\norm {\mathbf v} > 0$ Let $r, s \in R$: {{begin-eqn}} {{eqn | l = \norm {r s}_R \norm {\mathbf v} | r = \norm {\paren {r s} \mathbf v} | c = [[Definition:Norm Axioms (Vector Space)|Norm axiom (N2)]] }} {{eqn | r = \norm {r \paren {s \mathbf v} } | c = {{Vector-space-axiom|7}} }} {{eqn | r = \norm r_R \norm {s \mathbf v} | c = [[Definition:Norm Axioms (Vector Space)|Norm axiom (N2)]] }} {{eqn | r = \norm r_R \norm s_R \norm {\mathbf v} | c = [[Definition:Norm Axioms (Vector Space)|Norm axiom (N2)]] }} {{end-eqn}} By dividing both sides of the equation by $\norm {\mathbf v}$ then: :$\norm {r s}_R = \norm r_R \norm s_R$ The result follows. {{qed}} [[Category:Normed Division Rings]] [[Category:Norm Theory]] n9j7rsaatesqxu9zjp0c8vke73w4pay	1
Let $G$ be the [[Definition:Cartesian Product|cartesian product]] of a [[Definition:Sequence|sequence]] $\sequence {G_n}$ of [[Definition:Module|$R$-modules]]. Then for each $j \in \closedint 1 n$, the [[Definition:Projection (Mapping Theory)|projection]] $\pr_j$ on the $j$th co-ordinate is an [[Definition:R-Algebraic Structure Epimorphism|epimorphism]] from $G$ onto $G_j$.	1
Let $\R$ be the set of [[Definition:Real Number|real numbers]]. Let $\C$ be the set of [[Definition:Complex Number|complex numbers]]. Then the [[Definition:Module|$\R$-module]] $\C$ is a [[Definition:Vector Space|vector space]].	1
First: {{begin-eqn}} {{eqn | l = \norm f_\infty | r = 0 }} {{eqn | ll= \leadstoandfrom | l = \sup_{x \mathop \in S} \norm {\map f x} | r = 0 }} {{eqn | ll= \leadstoandfrom | lo= \forall x \in S: | l = \norm {\map f x} | r = 0 | c = since $\norm {\, \cdot \,}$ is a [[Definition:Norm on Vector Space|norm]], and hence non-negative }} {{eqn | ll= \leadstoandfrom | lo= \forall x \in S: | l = \map f x | r = 0 | c = since $\norm x = 0 \iff x = 0$ }} {{eqn | ll= \leadstoandfrom | l = f | r = 0 | c = }} {{end-eqn}} Now let $\lambda \in K, f \in \BB$ We have: {{begin-eqn}} {{eqn | l = \norm {\lambda f}_\infty | r = \sup_{x \mathop \in S} \norm {\lambda \map f x} | c = }} {{eqn | r = \size \lambda_K \sup_{x \mathop \in S} \norm {\map f x} | c = [[Multiple of Supremum]], and because $\norm {\, \cdot \,}$ is a [[Definition:Norm on Vector Space|norm]] }} {{eqn | r = \size \lambda_K \, \norm f_\infty | c = }} {{end-eqn}} Finally let $f, g \in \BB$. We have: {{begin-eqn}} {{eqn | l = \norm {f + g}_\infty | r = \sup_{x \mathop \in S} \norm {\map f x + \map g x} | c = }} {{eqn | o = \le | r = \sup_{x \mathop \in S} \paren {\norm {\lambda \map f x} + \norm {\lambda \map g x} } | c = because $\norm {\, \cdot \,}$ is a [[Definition:Norm on Vector Space|norm]] }} {{eqn | o = \le | r = \sup_{x \mathop \in S} \norm {\lambda \map f x} + \sup_{x \mathop \in S} \norm {\lambda \map g x} | c = [[Supremum of Sum]] }} {{eqn | r = \norm f_\infty + \norm g_\infty | c = }} {{end-eqn}} Thus $\norm {\, \cdot \,}_\infty$ has the defining properties of a [[Definition:Norm on Vector Space|norm]]. {{qed}}	1
Let $z = a + i b$. {{begin-eqn}} {{eqn | l = \cmod {\frac 1 z} | r = \cmod {\frac 1 {a + i b} } | c = }} {{eqn | r = \cmod {\frac {a - i b} {a^2 + b^2} } | c = [[Inverse for Complex Multiplication]] }} {{eqn | r = \cmod {\frac a {a^2 + b^2} + i \frac {-b} {a^2 + b^2} } | c = }} {{eqn | r = \sqrt {\paren {\frac a {a^2 + b^2} }^2 + \paren {\frac {-b} {a^2 + b^2} }^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \frac {\sqrt {a^2 + b^2} } {a^2 + b^2} | c = }} {{eqn | r = \frac 1 {\sqrt {a^2 + b^2} } | c = }} {{eqn | r = \frac 1 {\cmod z} | c = {{Defof|Complex Modulus}} }} {{end-eqn}} {{qed}}	1
Let $m, n \in \Z_{>1}$. Let $\struct {\Z_m, +_m, \times_m}$ and $\struct {\Z_n, +_n, \times_n}$ be the [[Definition:Ring of Integers Modulo m|rings of integers modulo $m$ and $n$]] respectively. Let $\struct {\Z_m \times \Z_n}$ be the [[Definition:Ring Direct Product|direct product]] of $\Z_m$ and $\Z_n$. Let $\struct {\Z_{m n}, +_{m n}, \times_{m n} }$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $mn$]]. Then $\struct {\Z_m \times \Z_n}$ is [[Definition:Ring Isomorphism|isomorphic]] to $\struct {\Z_{m n}, +_{m n}, \times_{m n} }$ {{iff}} $m$ and $n$ are [[Definition:Coprime Integers|coprime]].	1
By the [[Definition:Complex Number|definition of a complex number]], we have: :$z = \map \Re z + i \map \Im z$ {{begin-eqn}} {{eqn | l = \cmod z | r = \sqrt {\paren {\map \Re z}^2 + \paren {\map \Im z}^2} | c = {{Defof|Complex Modulus}} }} {{eqn | o = \ge | r = \sqrt {\paren {\map \Im z}^2 } | c = [[Square of Real Number is Non-Negative]], as $\map \Re z$ is [[Definition:Real Number|real]] }} {{eqn | r = \size {\map \Im z} | c = [[Square of Real Number is Non-Negative]], as $\map \Im z$ is [[Definition:Real Number|real]] }} {{end-eqn}} {{qed}}	1
Let $x \in X \setminus D$. Suppose $D$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] in $X$. Then: :$\forall n \in N : \exists d_n \in D : d_n \in \map {B_{\frac 1 n}} x$ where $\displaystyle \map {B_{\frac 1 n}} x$ is an [[Definition:Open Ball in Normed Vector Space|open ball]]. Let $\sequence {d_n}_{n \mathop \in \N}$ be a [[Definition:Sequence|sequence]] in $D$. Then: :$\forall n \in \N : \norm {x - d_n} < \frac 1 n$ Hence, $x$ is a [[Definition:Limit Point (Normed Vector Space)|limit point]] of $D$. In other words, $x \in D^-$. We have just shown that: :$x \in X \setminus D \implies x \in D^-$ Hence: :$X \setminus D \subseteq D^-$. By definition of [[Definition:Closure/Normed Vector Space|closure]]: :$D \subseteq D^-$ Therefore: {{begin-eqn}} {{eqn | l = X | r = D \cup \paren {X \setminus D} }} {{eqn | o = \subseteq | r = D^- }} {{eqn | o = \subseteq | r = X }} {{end-eqn}} Thus: :$X = D^-$.	1
=== [[Definition:Scalar Multiplication/R-Algebraic Structure|$R$-Algebraic Structure]] === {{:Definition:Scalar Multiplication/R-Algebraic Structure}} === [[Definition:Scalar Multiplication/Module|Module]] === {{:Definition:Scalar Multiplication/Module}} === [[Definition:Scalar Multiplication/Vector Space|Vector Space]] === {{:Definition:Scalar Multiplication/Vector Space}}	1
From: :[[Integers form Ring]] :[[Rational Numbers form Ring]] :[[Real Numbers form Ring]] :[[Complex Numbers form Ring]] the [[Definition:Standard Number System|standard number systems]] $\Z$, $\Q$, $\R$ and $\C$ are [[Definition:Ring (Abstract Algebra)|rings]]. Hence we can apply [[Matrix Entrywise Addition over Ring is Commutative]]. {{qed|lemma}} The above cannot be applied to the [[Definition:Natural Numbers|natural numbers]] $\N$, as they do not form a [[Definition:Ring (Abstract Algebra)|ring]]. However, from [[Natural Numbers under Addition form Commutative Monoid]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\N, +}$ is a [[Definition:Commutative Monoid|commutative monoid]]. By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Addition|addition of numbers]]. The result follows from [[Commutativity of Hadamard Product]]. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] with associated norm $\norm {\, \cdot \,}$. Let $f, g \in H$ be arbitrary. Then: :$\norm {f + g}^2 + \norm {f - g}^2 = 2 \paren {\norm f^2 + \norm g^2}$	1
Let $e$ be an [[Definition:Elementary Row Operation|elementary row operation]]. Let $\mathbf E$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] of [[Definition:Order of Square Matrix|order]] $m$ defined as: :$\mathbf E = e \paren {\mathbf I}$ where $\mathbf I$ is the [[Definition:Unit Matrix|unit matrix]]. Then for every $m \times n$ [[Definition:Matrix|matrix]] $\mathbf A$: :$e \paren {\mathbf A} = \mathbf {E A}$ where $\mathbf {E A}$ denotes the [[Definition:Matrix Product (Conventional)|conventional matrix product]].	1
Let: :$\mathbf r:x \mapsto \mathbf z$ be a [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued function]], where: :$\mathbf{z} = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{bmatrix}$ such that: :$z_1, z_2, \cdots, z_n$ are ([[Definition:Image of Mapping|images]] of) [[Definition:Differentiable Real Function|differentiable]] [[Definition:Real Function|real functions]]. Let: :$f: x \mapsto y$ be a [[Definition:Differentiable Real Function|differentiable]] [[Definition:Real Function|real function]]. Then: :$D_x \left({y \, \mathbf z}\right) = \dfrac {\d y} {\d x} \mathbf z + y \dfrac {\d \mathbf z} {\d x}$	1
Let $n\geq1$ and $k\geq1$ be [[Definition:Natural Number|natural numbers]]. Let $\Omega\subset \R^n$ be [[Definition:Open Set of Real Euclidean Space|open]]. $f : \Omega \to \R^n$ be a [[Definition:Vector-Valued Function|vector-valued function]] of [[Definition:Differentiability Class|class]] $C^k$. Let $a\in\Omega$. Let the [[Definition:Differential of Vector-Valued Function|differential]] $Df(a)$ of $f$ at $a$ be [[Definition:Invertible Linear Operator|invertible]]. Then there exist [[Definition:Open Set of Real Euclidean Space|open sets]] $U\subset\Omega$ and $V\subset\R^n$ such that the [[Definition:Restriction|restriction]] of $f$ to $U$ is a $C^k$-[[Definition:Diffeomorphism|diffeomorphism]] $f:U\to V$.	1
=== Lemma === Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $E_{ij}$ denote the [[Definition:Matrix|matrix]] with only zeroes except a $1$ at the $\left({i, j}\right)$th position. Let $X, Y \in R^{2 n \times 2 n}$. Let $X = \begin {pmatrix} X_{11} & X_{12} \\ X_{21} & X_{22} \end{pmatrix}$ and $Y = \begin{pmatrix} Y_{11} & Y_{12} \\ Y_{21} & Y_{22}\end{pmatrix}$ Then: :$\displaystyle \sum_{i, j \mathop = 1}^{2 n} \operatorname {tr} \left({\left({X E_{i j} Y}\right)^t E_{i j} }\right) = \operatorname{tr} \left({Y}\right) \operatorname{tr} \left({X}\right)$ :$\displaystyle \sum_{i, j \mathop = 1}^n \operatorname{tr} \left({\left({X E_{ij} Y}\right)^t E_{j + n, i + n} }\right) = \operatorname{tr} \left({Y_{1 2}^t X_{2 1} }\right)$ === Proof === Use [[Trace of Alternating Product of Matrices and Almost Zero Matrices]]. ---- Use [[Definition:Frobenius Inner Product]] and [[Trace in Terms of Orthonormal Basis]] and the fact that the $\left({E_{i j} }\right)_{i \le n, j \ge n + 1}, \left({E_{i j} }\right)_{i \ge n + 1, j \le n}, \left({E_{i j} - E_{j + n, i + n} }\right) / \sqrt2$ are an [[Definition:Orthonormal Basis|orthonormal basis]] of $\mathfrak{sp}_{2 n}$. {{ProofWanted}}	1
Let $z_1$ and $z_2$ be represented by the [[Definition:Point|points]] $A = \tuple {x_1, y_1}$ and $B = \tuple {x_2, y_2}$ respectively in the [[Definition:Complex Plane|complex plane]]. Let $z$ be an arbitrary [[Definition:Point|point]] on $L$ represented by the [[Definition:Point|point]] $P$. :[[File:Perpendicular Bisector of Two Points in Complex Plane.png|400px]] We have that $L$ passes through the [[Definition:Point|point]]: :$\dfrac {z_1 + z_2} 2$ and is [[Definition:Perpendicular|perpendicular]] to the [[Definition:Straight Line|straight line]]: :$z = z_1 + t \paren {z_2 - z_1}$ {{ProofWanted}} [[Category:Equation for Perpendicular Bisector of Two Points in Complex Plane]] azrcyl0u9al6hkpf7oqidu3zb161zkd	1
From [[Dot Product Operator is Bilinear]]: :$\left({c \mathbf u + \mathbf v}\right) \cdot \mathbf w = c \left({\mathbf u \cdot \mathbf w}\right) + \left({\mathbf v \cdot \mathbf w}\right)$ Setting $\mathbf v = 0$ and renaming $\mathbf w$ yields the result. {{qed}}	1
By definition of [[Definition:Like Vector Quantities|like vector quantities]], $\mathbf a$ and $\mathbf b$ have the same [[Definition:Direction|direction]]. By definition of [[Definition:Unit Vector|unit vector]], $\mathbf {\hat a}$ and $\mathbf {\hat b}$ are both of [[Definition:Magnitude|magnitude]] $1$. Hence the result, by [[Equality of Vector Quantities]]. {{qed}} [[Category:Vectors]] 4dc6u0f04r1xrbj90fu5p7eaj30d0uk	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $1 \le r < s \le n$. Let $\mathbf B$ be $\mathbf A$ with [[Definition:Column of Matrix|columns]] $r$ and $s$ [[Definition:Transposition|transposed]]. Consider: :the [[Definition:Transpose of Matrix|transpose]] $\mathbf A^\intercal$ of $\mathbf A$ :the [[Definition:Transpose of Matrix|transpose]] $\mathbf B^\intercal$ of $\mathbf B$. hence $\mathbf B^\intercal$ is $\mathbf A^\intercal$ with [[Definition:Row of Matrix|rows]] $r$ and $s$ [[Definition:Transposition|transposed]]. From [[Determinant with Rows Transposed]]: :$\map \det {\mathbf B^\intercal} = -\map \det {\mathbf A^\intercal}$ From from [[Determinant of Transpose]]: :$\map \det {\mathbf B^\intercal} = \map \det {\mathbf B}$ :$\map \det {\mathbf A^\intercal} = \map \det {\mathbf A}$ and the result follows. {{qed}}	1
Let $\mathbf a, \mathbf b, \mathbf c$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] $\mathbf V$ of [[Definition:Dimension of Vector Space|$3$ dimensions]]: {{begin-eqn}} {{eqn | l = \mathbf a | r = a_i \mathbf i + a_j \mathbf j + a_k \mathbf k }} {{eqn | l = \mathbf b | r = b_i \mathbf i + b_j \mathbf j + b_k \mathbf k }} {{eqn | l = \mathbf c | r = c_i \mathbf i + c_j \mathbf j + c_k \mathbf k }} {{end-eqn}} where $\left({\mathbf i, \mathbf j, \mathbf k}\right)$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\mathbf V$. Let $\mathbf a \times \mathbf b$ denote the [[Definition:Vector Cross Product|vector cross product]] of $\mathbf a$ with $\mathbf b$. Let $\mathbf a \cdot \mathbf b$ denote the [[Definition:Dot Product|dot product]] of $\mathbf a$ with $\mathbf b$. Then: :$\mathbf a \cdot \left({\mathbf b \times \mathbf c}\right) = \begin{vmatrix} a_i & a_j & a_k \\ b_i & b_j & b_k \\ c_i & c_j & c_k \end{vmatrix}$	1
{{begin-eqn}} {{eqn | l = \alpha_1 x + \alpha_2 y | r = \beta | c = }} {{eqn | ll= \leadsto | l = \alpha_2 y | r = y_1 - \alpha_1 x + \beta | c = }} {{eqn | n = 1 | ll= \leadsto | l = y | r = -\dfrac {\alpha_1} {\alpha_2} x + \dfrac {\beta} {\alpha_2} | c = }} {{end-eqn}} Setting $x = 0$ we obtain: :$y = \dfrac {\beta} {\alpha_2}$ which is the [[Definition:Y-Intercept|$y$-intercept]]. [[Definition:Differentiation|Differentiating]] $(1)$ {{WRT|Differentiation}} $x$ gives: :$y' = -\dfrac {\alpha_1} {\alpha_2}$ By definition, this is the [[Definition:Slope of Straight Line|slope]] of $\mathcal L$ and is seen to be [[Definition:Constant|constant]]. The result follows by setting: {{begin-eqn}} {{eqn | l = m | r = -\dfrac {\alpha_1} {\alpha_2} | c = }} {{eqn | l = c | r = \dfrac {\beta} {\alpha_2} | c = }} {{end-eqn}} {{qed}}	1
=== [[Equivalent Norms on Rational Numbers/Necessary Condition|Necessary Condition]] === {{:Equivalent Norms on Rational Numbers/Necessary Condition}}{{qed|lemma}} === [[Equivalent Norms on Rational Numbers/Sufficient Condition|Sufficient Condition]] === {{:Equivalent Norms on Rational Numbers/Sufficient Condition}}{{qed}}	1
Let $V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{vmatrix}$. Start by replacing number $x_n$ in $V_n$ with the unknown $x$. Thus $V_n$ is made into a function of $x$. :$P \left({x}\right) = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x & x^2 & \cdots & x^{n-2} & x^{n-1} \end{vmatrix}$. Let $x$ equal a value from the set $\set {x_1,\ldots,x_{n-1} }$. Then determinant $\map P {x}$ has [[Square Matrix with Duplicate Rows has Zero Determinant|equal rows]], giving: {{begin-eqn}} {{eqn | l = \map P {x} | r = 0 | c = for $x = x_1, \ldots, x_{n-1}$ }} {{end-eqn}} Perform row expansion by the last row. Then $P \left({x}\right)$ is seen to be a polynomial of degree $n-1$: :$P \left({x}\right) = \begin{vmatrix} x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \end{vmatrix} + \begin{vmatrix} 1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \end{vmatrix}x \ \ + \ \ \cdots \ \ + \ \ \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} \end{vmatrix}x^{n-1}$. <!--- We can see that statement $P \left({x}\right) = 0$ holds true for all $x_1, x_2, \cdots x_{n-1}$ because if $x=x_1, x_2, \cdots x_{n-1}$ starting determinant would have two equal rows and by [[Square Matrix with Duplicate Rows has Zero Determinant]] would $V_n = 0$. {{explain|The meaning of the above statement needs to be made clearer, and the logical sense turned into a linear flow.}} Moved statement higher up so it reads linearly. Nov 6 2019 GBGustafson Also fixed one typo $x_{n-1} \to x^{n-1}$ and changed "returning" to "evaluating at". The cofactor expansion is missing checkerboard signs but the minors are correct. Not fixed. ---> By the [[Polynomial Factor Theorem]]: :$P \left({x}\right) = C \left({x - x_1}\right) \left({x - x_2}\right) \dotsm \left({x - x_{n-1} }\right)$ where $C$ is the leading coefficient (with $x^{n-1}$ power). Thus: :$P \left({x}\right) = V_{n-1} \left({x - x_1}\right) \left({x - x_2}\right) \dotsm \left({x - x_{n-1} }\right)$ which by evaluating at $x = x_n$ gives: :$V_n = V_{n-1} \left({x_n - x_1}\right) \left({x_n - x_2}\right) \dotsm \left({x_n - x_{n-1} }\right)$ Repeating the process: {{begin-eqn}} {{eqn | l = V_n | r = \prod_{1 \mathop \le i \mathop < n} \left({x_n - x_i}\right) V_{n-1} | c = }} {{eqn | r = \prod_{1 \mathop \le i \mathop < n} \left({x_n - x_i}\right) \prod_{1 \mathop \le i \mathop < n-1} \left({x_{n-1} - x_i}\right) V_{n-2} | c = }} {{eqn | r = \dotsm | c = }} {{eqn | r = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right) | c = }} {{end-eqn}} which establishes the solution. {{qed}}	1
Let $\left({G, +, \circ}\right)_R$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $H$ be a [[Definition:Empty Set|non-empty]] [[Definition:Subset|subset]] of $G$. Then $\left({H, +, \circ}\right)_R$ is a [[Definition:Submodule|submodule]] of $G$ iff: :$\forall x, y \in H: \forall \lambda \in R: x + y \in H, \lambda \circ x \in H$	1
Let $i \in \set {1, 2, \ldots, m}$. We have: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 1}^n \alpha_{i j} x_j | r = \sum_{j \mathop = 1}^n \alpha_{i j} \times 0 | c = }} {{eqn | r = \sum_{j \mathop = 1}^n 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} This holds for all $i \in \set {1, 2, \ldots, m}$. Hence: :$\displaystyle \forall i \in \set {1, 2, \ldots, m}: \sum_{j \mathop = 1}^n \alpha_{i j} x_j = 0$ and the result follows. {{qed}} [[Category:Simultaneous Linear Equations]] 6rej10oi787y9zs6gtvdti9fplbgdqm	1
From [[Scalar Product with Identity]], $\forall \lambda: \lambda \circ e = e$. Let $H \subseteq G$ such that $e \in H$. Consider any [[Definition:Sequence|sequence]] $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ in $H$ which includes $e$. So, let $a_j = e$ for some $j \in \closedint 1 n$. Let $c \in R \ne 0_R$. Consider the [[Definition:Sequence|sequence]] $\sequence {\lambda_k}_{1 \mathop \le k \mathop \le n}$ of [[Definition:Element|elements]] of $R$ defined as: :$\lambda_k = \begin{cases} c & : k \ne j \\ 0_R & : k= j \end{cases}$ Then: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \lambda_k \circ a_k | r = \lambda_1 \circ a_1 + \lambda_2 \circ a_2 + \cdots + \lambda_j \circ a_j + \cdots + \lambda_n \circ a_n | c = }} {{eqn | r = 0_R \circ a_1 + 0_R \circ a_2 + \cdots + c \circ e + \cdots + 0_R \circ a_n | c = }} {{eqn | r = e + e + \cdots + e + \cdots + e | c = }} {{eqn | r = e | c = }} {{end-eqn}} Thus there exists a [[Definition:Sequence|sequence]] $\sequence {\lambda_k}_{1 \mathop \le k \mathop \le n}$ in which not all $\lambda_k = 0_R$ such that: :$\displaystyle \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e$ Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. === [[Definition:Linear Combination/Sequence|Linear Combination of Sequence]] === {{:Definition:Linear Combination/Sequence}} === [[Definition:Linear Combination/Subset|Linear Combination of Subset]] === {{:Definition:Linear Combination/Subset}} === [[Definition:Linear Combination/Empty Set|Linear Combination of Empty Set]] === {{:Definition:Linear Combination/Empty Set}}	1
Let $R$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Then $R$ fulfills the [[Definition:Ascending Chain Condition|ascending chain condition]].	1
Let $a, b, c, d \in \R$ be [[Definition:Real Number|real numbers]]. Let $\theta_{a, b}: \R \to \R$ be the [[Definition:Real Function|real function]] defined as: :$\forall x \in \R: \map {\theta_{a, b} } x = a x + b$ Let $\theta_{c, d} \circ \theta_{a, b}$ denote the [[Definition:Composition of Mappings|composition]] of $\theta_{c, d}$ with $\theta_{a, b}$. Then: :$\theta_{c, d} \circ \theta_{a, b} = \theta_{a c, b c + d}$	1
{{AimForCont}} that: :$\exists \lambda \in F: \exists \mathbf v \in \mathbf V: \lambda \circ \mathbf v = \bszero \land \lambda \ne 0_F \land \mathbf v \ne \bszero$ which is the [[Definition:Negation|negation]] of the exposition of the theorem. Utilizing the [[Definition:Vector Space Axioms|vector space axioms]]: {{begin-eqn}} {{eqn | l = \lambda \circ \mathbf v | r = \bszero }} {{eqn | ll= \leadsto | l = \lambda^{-1} \circ \paren {\lambda \circ \mathbf v} | r = \lambda^{-1} \circ \mathbf 0 | c = multiplying both sides by $\lambda^{-1}$ }} {{eqn | ll= \leadsto | l = \bszero | r = \lambda^{-1} \circ \paren {\lambda \circ \mathbf v} | c = [[Zero Vector Scaled is Zero Vector]] }} {{eqn | r = \paren {\lambda^{-1} \cdot \lambda} \circ \mathbf v }} {{eqn | r = 1_F \circ \mathbf v }} {{eqn | r = \mathbf v }} {{end-eqn}} which [[Proof by Contradiction|contradicts]] the assumption that $\mathbf v \ne \mathbf 0$. {{qed}}	1
The [[Definition:Empty Set|empty set]] is a [[Definition:Linearly Independent Set|linearly independent set]].	1
Let $f_1$ and $f_2$ be [[Definition:Generator of Ideal|generators]] of $J$. Then $f_1$ and $f_2$ are unit multiples of each other. The [[Definition:Unit of Ring|units]] of $F \sqbrk X$ are the non-[[Definition:Field Zero|zero]] [[Definition:Element|elements]] of $F$. {{explain|The above needs to be properly derived.}}	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] $\mathbf V$ of [[Definition:Dimension of Vector Space|$3$ dimensions]]: :$\mathbf a = a_i \mathbf i + a_j \mathbf j + a_k \mathbf k$ :$\mathbf b = b_i \mathbf i + b_j \mathbf j + b_k \mathbf k$ where $\left({\mathbf i, \mathbf j, \mathbf k}\right)$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\mathbf V$.	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $V$, $W$ be [[Definition:Finite|finite]] [[Definition:Dimension of Vector Space|dimensional]] $K$-[[Definition:Vector Space|vector spaces]]. Suppose that $\dim_K V = \dim_K W$. Then: :$V \cong W$ That is, $V$ and $W$ are [[Definition:Vector Space Isomorphism|isomorphic]].	1
Let $V$ be a [[Definition:Banach Space|Banach space]] with [[Definition:Norm on Vector Space|norm]] $\norm {\, \cdot \,}$. Let $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ be an [[Definition:Absolutely Convergent Series|absolutely convergent series]] in $V$. Then $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Convergent Series|convergent]].	1
Let $m \in M$ be arbitrary. By definition there is an [[Definition:Open Neighborhood|open neighborhood]] $U$ of $m$, [[Definition:Homeomorphic Metric Spaces|homeomorphic]] to an [[Definition:Open Set of Metric Space|open subset]] of $\R^d$. By the definition of an [[Definition:Open Set of Metric Space|open set]], there is some [[Definition:Open Ball of Metric Space|open ball]]: :$B = B_\delta \left({\phi \left({m}\right)}\right) = \left\{{x \in \R^d: \left|{x - \phi \left({m}\right)}\right| < \delta}\right\}$ of [[Definition:Radius of Open Ball|radius]] $\delta$ containing $\phi \left({m}\right)$, contained in $U$. By [[Closure of Open Ball in Metric Space]] and [[Topological Closure is Closed]], the set: :$C = \left\{{x \in \R^d: \left|{x - \phi \left({m}\right)}\right| \le \dfrac \delta 2}\right\}$ is [[Definition:Closed Set (Topology)|closed]], and $C \subseteq B \subseteq U$. Moreover, $C$ is trivially [[Definition:Bounded Metric Space|bounded]], hence [[Definition:Compact Topological Space|compact]] by the [[Heine-Borel Theorem]]. Now if $\phi$ is a [[Definition:Homeomorphism (Metric Spaces)|homeomorphism]] $U \to \R^d$, then by definition $\phi^{-1}$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. Therefore by [[Continuous Image of Compact Space is Compact]], $\phi^{-1} \left({C}\right) \subseteq M$ is [[Definition:Compact Topological Space|compact]]. Furthermore $m \in \phi^{-1} \left({C}\right)$ because $\phi \left({m}\right) \in C$. Thus every point of $M$ has a [[Definition:Compact Topological Subspace|compact]] [[Definition:Neighborhood (Topology)|neighborhood]]. {{Qed}} {{explain|Clarification is needed as to why this result should be categorised in [[:Category:Manifolds]].}} [[Category:Manifolds]] atj03zp0r9bpm24yi9z708tx91dy20y	1
From [[Element of Principal Ideal Domain is Finite Product of Irreducible Elements]], each element which is neither $0$ nor a [[Definition:Unit of Ring|unit]] of a [[Definition:Principal Ideal Domain|principal ideal domain]] has a [[Definition:Factorization|factorization]] of [[Definition:Irreducible Element of Ring|irreducible elements]]. {{proof wanted|Need to prove that the factorization is unique}}	1
Let $V$ be a [[Definition:Banach Space|Banach space]]. Let $\norm {\, \cdot \,}$ denote the [[Definition:Norm on Vector Space|norm]] on $V$. Let $d$ denote the corresponding [[Definition:Metric Induced by Norm|induced metric]]. Let $\family {v_i}_{i \mathop \in I}$ be an [[Definition:Indexed Set|indexed]] [[Definition:Subset|subset]] of $V$ such that the [[Definition:Generalized Sum|generalized sum]] $\displaystyle \sum_{i \mathop \in I} \set {v_i}$ [[Definition:Generalized Sum/Absolute Net Convergence|converges absolutely]]. Then the [[Definition:Generalized Sum|generalized sum]] $\displaystyle \sum \set {v_i: i \in I}$ [[Definition:Convergent Net|converges]].	1
From [[Dot Product Operator is Bilinear]]: :$\left({c \mathbf u + \mathbf v}\right) \cdot \mathbf w = c \left({\mathbf u \cdot \mathbf w}\right) + \left({\mathbf v \cdot \mathbf w}\right)$ Setting $\mathbf v = 0$ and renaming $\mathbf w$ yields the result. {{qed}}	1
We have that: :[[Integers form Commutative Ring|$\struct {\Z, +, \times}$ form a commutative ring]]. :$\struct {\Z, +, \times}$ has a [[Definition:Unity of Ring|unity]], and the [[Integer Multiplication Identity is One|unity is $1$]]. {{Qed}}	1
Let $V$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\norm{\,\cdot\,}$ denote its [[Definition:Norm on Vector Space|norm]]. Let $d$ be the [[Definition:Metric Induced by Norm|metric induced by $\norm{\,\cdot\,}$]]. Then $d$ is a [[Definition:Metric|metric]].	1
=== [[Norms Equivalent to Absolute Value on Rational Numbers/Necessary Condition|Necessary Condition]] === {{:Norms Equivalent to Absolute Value on Rational Numbers/Necessary Condition}}{{qed|lemma}} === [[Norms Equivalent to Absolute Value on Rational Numbers/Sufficient Condition|Sufficient Condition]] === {{:Norms Equivalent to Absolute Value on Rational Numbers/Sufficient Condition}}{{qed}}	1
We are given that $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible]]. From [[Product of Matrices is Invertible iff Matrices are Invertible]], $\mathbf A \mathbf B$ is also [[Definition:Invertible Matrix|invertible]]. By the definition of [[Definition:Inverse Matrix|inverse matrix]]: :$\mathbf A \mathbf A^{-1} = \mathbf A^{-1} \mathbf A = \mathbf I$ and :$\mathbf B \mathbf B^{-1} = \mathbf B^{-1} \mathbf B = \mathbf I$ Now, observe that: {{begin-eqn}} {{eqn | l = \paren {\mathbf A \mathbf B} \paren {\mathbf B^{-1} \mathbf A^{-1} } | r = \paren {\mathbf A \paren {\mathbf B \mathbf B^{-1} } } \mathbf A^{-1} | c = [[Matrix Multiplication is Associative]] }} {{eqn | r = \paren {\mathbf A \mathbf I} \mathbf A^{-1} }} {{eqn | r = \mathbf A \mathbf A^{-1} | c = {{Defof|Identity Element}} }} {{eqn | r = \mathbf I }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \paren {\mathbf B^{-1} \mathbf A^{-1} } \paren {\mathbf A \mathbf B} | r = \paren {\mathbf B^{-1} \paren {\mathbf A^{-1} \mathbf A} } \mathbf B | c = [[Matrix Multiplication is Associative]] }} {{eqn | r = \paren {\mathbf B^{-1} \mathbf I} \mathbf B }} {{eqn | r = \mathbf B^{-1} \mathbf B | c = {{Defof|Identity Element}} }} {{eqn | r = \mathbf I }} {{end-eqn}} The result follows from the definition of [[Definition:Inverse Matrix|inverse]]. {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring |normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]] Let $\NN = \set {\sequence {x_n}: \displaystyle \lim_{n \mathop \to \infty} x_n = 0}$ Let $\norm {\, \cdot \,}: \CC \, \big / \NN \to \R_{\ge 0}$ be the [[Quotient Ring of Cauchy Sequences is Normed Division Ring|norm on the quotient ring $\CC \, \big / \NN$]] defined by: :$\displaystyle \forall \sequence {x_n} + \NN: \norm {\sequence {x_n} + \NN} = \lim_{n \mathop \to \infty} \norm {x_n}$ Let $\phi: R \to \CC \, \big / \NN$ be the mapping from $R$ to the [[Definition:Quotient Ring|quotient ring]] $\CC \, \big / \NN$ defined by: :$\forall a \in R: \map \phi a = \sequence {a, a, a, \dotsc} + \NN$ where $\sequence {a, a, a, \dotsc} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {a, a, a, \dotsc}$. Then: :$\phi$ is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]].	1
Let $\ell^p$ be the [[Definition:P-Sequence Space|p-sequence space]]. Let $\struct {\R, +_\R, \times_\R}$ be the [[Definition:Field of Real Numbers|field of real numbers]]. Let $\paren +$ be the [[Definition:Pointwise Addition on Ring of Sequences|pointwise addition on the ring of sequences]]. Let $\paren {\, \cdot \,}$ be the [[Definition:Pointwise Multiplication on Ring of Sequences|pointwise multiplication on the ring of sequences]]. Then $\struct {\ell^p, +, \, \cdot \,}_\R$ is a [[Definition:Vector Space|vector space]].	1
Let $p$ be [[Definition:Prime Number|prime]]. From [[Irreducible Elements of Ring of Integers]], we have that $p$ is [[Definition:Irreducible Element of Ring|irreducible]] in the [[Definition:Ring of Integers|ring of integers]] $\struct {\Z, +, \times}$. From [[Ring of Integers is Principal Ideal Domain]], $\struct {\Z, +, \times}$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. Thus by [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]], $\ideal p$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $\struct {\Z, +, \times}$. Hence by [[Maximal Ideal iff Quotient Ring is Field]], $\Z / \ideal p$ is a [[Definition:Field (Abstract Algebra)|field]]. But $\Z / \ideal p$ is exactly $\struct {\Z_p, +, \times}$. {{qed|lemma}} Let $p$ be [[Definition:Composite Number|composite]]. Then $p$ is not [[Definition:Irreducible Element of Ring|irreducible]] in $\struct {\Z, +, \times}$. Thus by [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]], $\ideal p$ is not a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $\struct {\Z, +, \times}$. Hence by [[Maximal Ideal iff Quotient Ring is Field]], $\Z / \ideal p$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $p_1 ,p_2 \in U$. Then for $i \in \left\{ {1, 2}\right\}$, $p_i = p + s_i t_i \mathbf v + \left({1 - s_i}\right) t_i \mathbf w$ for some $s_i \in \left({0\,.\,.\,1}\right) , t_i \in \R_{>0}$. [[Definition:WLOG|WLOG]] assume that $t_1 \le t_2$. Suppose that $q \in \R^2$ lies on the [[Definition:Convex Set (Vector Space)/Line Segment|line segment]] joining $p_1$ and $p_2$, so: {{begin-eqn}} {{eqn |l= q |r= p + s_1 t_1 \mathbf v + \left({1 - s_1}\right) t_1 \mathbf w + s \left({ p + s_2 t_2 \mathbf v + \left({1 - s_2}\right) t_2 \mathbf w - p - s_1 t_1 \mathbf v - \left({1 - s_1}\right) t_1 \mathbf w }\right) |c= for some $s \in \left({0\,.\,.\,1}\right)$ }} {{eqn |r= p + \left({ \left({1 - s}\right) s_1 t_1 + s s_2 t_2}\right) \mathbf v + \left({ \left({1 - s}\right) \left({1 - s_1}\right) t_1 + s \left({1 - s_2}\right) t_2}\right) \mathbf w }} {{eqn |r= p + \dfrac{\left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r}r \mathbf v + \dfrac{t_1 + st_2 - st_1 - \left({1 - s}\right) s_1 t_1 - s s_2 t_2}{r} r \mathbf w |c= where $r = t_1 + s \left({t_2 - t_1}\right)$ }} {{eqn |r= p + \dfrac{\left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r}r \mathbf v + \left({ 1 - \dfrac{\left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r} }\right) r \mathbf w }} {{end-eqn}} As $t_1 \le t_2$, it follows that $r \in \R_{>0}$. We have $\dfrac{ \left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r}> 0$, and: {{begin-eqn}} {{eqn |l= 1 - \dfrac{\left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r} |r= \dfrac{ \left({1 - s}\right) \left({1 - s_1}\right) t_1 + s \left({1 - s_2}\right) t_2}{r} }} {{eqn |o= > |r= 0 }} {{end-eqn}} It follows that $\dfrac{ \left({1 - s}\right) s_1 t_1 + s s_2 t_2}{r} \in \left({0\,.\,.\,1}\right)$. Then $q \in U$. By [[Definition:Convex Set (Vector Space)|definition of convex set]], it follows that $U$ is convex. {{qed}} [[Category:Vector Spaces]] 9bmr4r3gkhrw6nlhcxeeovr0al1t8ls	1
Let $c = \displaystyle \sup_{n \mathop \in \N} \, \left\Vert{T_n}\right\Vert$. By assumption, $c < \infty$. Let $h = \left({h_n}\right)_{n \in \N} \in H$ be arbitrary. Then: {{begin-eqn}} {{eqn|l = \left\Vert{ T h }\right\Vert_H^2 |r = \left\Vert{ \left({T_n h_n}\right)_{n \in \N} }\right\Vert_H^2 |c = Definition of $T$ }} {{eqn|r = \sum_{n \mathop = 1}^\infty \left\Vert{ T_n h_n }\right\Vert_{H_n}^2 |c = Definition of $\left\Vert{\cdot}\right\Vert_H$ }} {{eqn|o = \le |r = \sum_{n \mathop = 1}^\infty \left\Vert{ T_n }\right\Vert^2 \left\Vert{ h_n }\right\Vert_{H_n}^2 }} {{eqn|o = \le |r = \sum_{n \mathop = 1}^\infty c^2 \left\Vert{ h_n }\right\Vert_{H_n}^2 }} {{eqn|r = c^2 \sum_{n \mathop = 1}^\infty \left\Vert{ h_n }\right\Vert_{H_n}^2 }} {{eqn|r = c^2 \left\Vert{ h }\right\Vert_H^2 }} {{end-eqn}} {{MissingLinks|results for some steps}} In summary: :$\left\Vert{ T h }\right\Vert_H^2 \le c^2 \left\Vert{ h }\right\Vert_H^2$ It follows that $T$ is [[Definition:Bounded Linear Transformation|bounded]]. {{qed}} [[Category:Linear Transformations on Hilbert Spaces]] t68mo69inf7su8yo7n236b3e1laax81	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Ring Zero|zero]] is $0_K$ and [[Definition:Unity of Field|unity]] is $1_K$. Let $\GL {n, K}$ be the [[Definition:General Linear Group|general linear group of order $n$ over $K$]]. Then $\GL {n, K}$ is not an [[Definition:Abelian Group|abelian group]].	1
When $1_R$ denotes the [[Definition:Unity of Ring|unity]] of $R$, we have: {{begin-eqn}} {{eqn | l = 1_R | r = \map \det {\mathbf I_n} | c = [[Determinant of Unit Matrix]] }} {{eqn | r = \map \det {\mathbf A \mathbf B} | c = by assumption }} {{eqn | r = \map \det {\mathbf A} \map \det {\mathbf B} | c = [[Determinant of Matrix Product]] }} {{end-eqn}} From [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], it follows that $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible]]. Then: {{begin-eqn}} {{eqn | l = \mathbf B | r = \mathbf I_n \mathbf B | c = [[Unit Matrix is Unity of Ring of Square Matrices]] }} {{eqn | r = \paren {\mathbf A^{-1} \mathbf A} \mathbf B | c = {{Defof|Inverse Matrix}} }} {{eqn | r = \mathbf A^{-1} \paren {\mathbf A \mathbf B} | c = [[Matrix Multiplication is Associative]] }} {{eqn | r = \mathbf A^{-1} \mathbf I_n | c = by assumption }} {{eqn | r = \mathbf A^{-1} | c = [[Unit Matrix is Unity of Ring of Square Matrices]] }} {{end-eqn}} {{qed}}	1
Let $\mathbb J = \set {x \in \R: a \le x \le b}$ be a [[Definition:Closed Real Interval|closed interval]] of the [[Definition:Real Number Line|real number line]] $\R$. Let $\map \CC {\mathbb J}$ be the set of all [[Definition:Continuous Real Function|continuous real functions]] on $\mathbb J$. Then $\struct {\map \CC {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of the [[Definition:Vector Space|$\R$-vector space]] $\struct {\R^{\mathbb J}, +, \times}_\R$.	1
From [[Determinant of Orthogonal Matrix is Plus or Minus One]] and [[Matrix is Invertible iff Determinant has Multiplicative Inverse]] it follows that both $\mathbf P$ and $\mathbf Q$ are [[Definition:Invertible Matrix|invertible]]. Thus: {{begin-eqn}} {{eqn | l = \paren {\mathbf P \mathbf Q}^{-1} | r = \mathbf Q^{-1} \mathbf P^{-1} | c = [[Inverse of Matrix Product]] }} {{eqn | r = \mathbf Q^\intercal \mathbf P^\intercal | c = {{Defof|Orthogonal Matrix}} }} {{eqn | r = \paren {\mathbf P \mathbf Q}^\intercal | c = [[Transpose of Matrix Product]] }} {{end-eqn}} Hence the result, by definition of [[Definition:Orthogonal Matrix|orthogonal matrix]]. {{qed}}	1
Let $p_1$ and $p_2$ be [[Definition:Prime Number|prime numbers]] such that $p_1 \neq p_2$. Let $\norm {\,\cdot\,}_{p_1}$ and $\norm {\,\cdot\,}_{p_2}$ be the [[Definition:P-adic Norm|$p$-adic norms]] on the [[Definition:Rational Numbers|rationals $\Q$]]. Then $\norm {\,\cdot\,}_{p_1}$ and $\norm {\,\cdot\,}_{p_2}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. That is, the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_{p_1}$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_{p_2}$.	1
{{AimForCont}} this supposition is false. Let $m$ be the smallest [[Definition:Integer|integer]] which can not be expressed as the [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. As a [[Definition:Prime Number|prime number]] is trivially a [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]], $m$ can not itself be [[Definition:Prime Number|prime]]. Hence: :$\exists r, s \in \Z: 1 < r < m, 1 < s < m: m = r s$ As $m$ is our [[Principle of Least Counterexample|least counterexample]], both $r$ and $s$ can be expressed as the [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. Say $r = p_1 p_2 \cdots p_k$ and $s = q_1 q_2 \cdots q_l$, where all of $p_1, \ldots, p_k, q_1, \ldots, q_l$ are [[Definition:Prime Number|prime]]. Hence $m = r s = p_1 p_2 \cdots p_k q_1 q_2 \cdots q_l$, which is a [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. Hence there is no such counterexample. {{qed}}	1
The only [[Definition:Invertible Element|invertible elements]] of $\Z$ for [[Definition:Integer Multiplication|multiplication]] (that is, [[Definition:Unit of Ring|units of $\Z$]]) are $1$ and $-1$.	1
Let $d_R$ and $d_S$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norms]] $\norm {\,\cdot\,}_R$ and $\norm {\,\cdot\,}_S$ respectively. === Necessary Condition === Let $\phi: R \to S$ be an [[Definition:Isometric Isomorphism|isometric isomorphism]]. Then for $x \in R$: {{begin-eqn}} {{eqn | l = \norm {\map \phi x}_S | r = \norm {\map \phi x - 0_S}_S | c = $0_S$ is the [[Definition:Ring Zero|zero]] of $S$ }} {{eqn | r = \norm {\map \phi x - \map \phi {0_R} }_S | c = [[Ring Homomorphism Preserves Zero]] }} {{eqn | r = \map {d_S} {\map \phi x, \map \phi {0_R} } | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \map {d_R} {x, {0_R} } | c = {{Defof|Isometry (Metric Spaces)}} }} {{eqn | r = \norm {x - {0_R} }_R | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm x_R | c = $0_R$ is the [[Definition:Ring Zero|zero]] of $R$ }} {{end-eqn}} The result follows. {{qed|lemma}} === Sufficient Condition === Let $\phi: R \to S$ satisfy: :$\forall x \in R: \norm {\map \phi x}_S = \norm x_R$ Then for $x, y \in R$: {{begin-eqn}} {{eqn | l = \map {d_S} {\map \phi x, \map \phi y} | r = \norm {\map \phi x - \map \phi y}_S | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm {\map \phi {x - y} }_S | c = $\phi$ is a [[Definition:Ring Isomorphism|Ring Isomorphism]] }} {{eqn | r = \norm {x - y}_R | c = by hypothesis }} {{eqn | r = \map {d_R} {x, y} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{end-eqn}} The result follows. {{qed}} [[Category:Normed Division Rings]] lry0tga0ndyp1ra3oqf88egnkmdshws	1
Consider a [[Definition:Particle|particle]] $p$ moving in the [[Definition:Plane|plane]]. Let the [[Definition:Position|position]] of $p$ at [[Definition:Time|time]] $t$ be given in [[Definition:Polar Coordinates|polar coordinates]] as $\left\langle{r, \theta}\right\rangle$. Then the [[Definition:Velocity|velocity]] $\mathbf v$ of $p$ can be expressed as: :$\mathbf v = r \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_\theta + \dfrac {\mathrm d r} {\mathrm d t} \mathbf u_r$ where: :$\mathbf u_r$ is the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Radial Coordinate|radial coordinate]] of $p$ :$\mathbf u_\theta$ is the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Angular Coordinate|angular coordinate]] of $p$	1
:$\sequence {x_n - y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Let $s_\beta: \R^2 \to \R^2$ be a [[Similarity Mapping of Plane is Linear Operator|similarity]] of $\R^2$. Then $s_{-1}$ is the same as the [[Rotation of Plane about Origin is Linear Operator|rotation]] $r_{\pi}$ of [[Definition:The Plane|the plane]] about the [[Definition:Origin|origin]] one half turn. If $\beta \ge 1$, then $s_\beta$ is called a '''stretching''', and if $0 < \beta \le 1$, $s_\beta$ is called a '''contraction'''. If $\beta < 0$, then $s_\beta$ is a stretching or contraction followed by a [[Rotation of Plane about Origin is Linear Operator|rotation]] one half turn. It is also the same as a [[Rotation of Plane about Origin is Linear Operator|rotation]] one half turn followed by a stretching or contraction.	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n \in \N_{>0}$. Let $\struct {R^n, +, \times}_R$ be the '''[[Definition:Module on Cartesian Product|$R$-module $R^n$]]'''. Then $\struct {R^n, +, \times}_R$ is a [[Definition:Unitary Module|unitary $R$-module]].	1
Let $\GL {n, \R}$ denote the [[Definition:General Linear Group|general linear group]] of degree $n$ over $\R$. Let $\SL {n, \R}$ denote the [[Definition:Special Linear Group|special linear group]] of degree $n$ over $\R$. Then the [[Definition:Quotient Group|quotient group]] $\GL {n, \R} / \SL {n, \R}$ is the [[Definition:Multiplicative Group of Real Numbers|multiplicative group of real numbers]] $\struct {\R_{\ne 0}, \times}$.	1
The [[Definition:Transpose of Matrix|transpose]] of an [[Definition:Upper Triangular Matrix|upper triangular matrix]] is a [[Definition:Lower Triangular Matrix|lower triangular matrix]].	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\cmod z$ be the [[Definition:Complex Modulus|modulus]] of $z$. Then: :$\cmod {z_1 z_2} = \cmod {z_1} \cdot \cmod {z_2}$	1
Let's apply the [[Definition:Linearly Independent|definition of linear independence]]. Assume a [[Definition:Linear Combination|linear combination]] of the functions $f_1,\ldots,f_n$ is the zero function: {{begin-eqn}} {{eqn | n = 1 | l = \displaystyle \sum_{i \mathop = 1}^n c_i \, \map {f_i} x | r = 0 | c = for all $x$ }} {{end-eqn}} Let $\vec c$ have components $c_1, \ldots, c_n$. For $i = 1, \ldots, n$ replace $x = x_i$ in (1). There are $n$ [[Definition:Homogeneous Linear Equations|linear homogeneous algebraic equations]], written as: :$S \vec c = \vec 0$ Because $S$ is invertible, then $\vec c = \vec 0$. The functions are [[Definition:Linearly Independent|linearly independent]]. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\struct {U_R, \circ}$ be the [[Definition:Group of Units of Ring|group of units]] of $\struct {R, +, \circ}$. Let $a, c \in R, b, d \in U_R$. Then: :$\dfrac a b + \dfrac c d = \dfrac {a \circ d + b \circ c} {b \circ d}$ where $\dfrac x z$ is defined as $x \circ \paren {z^{-1} }$, that is, $x$ [[Definition:Division Product|divided by]] $z$. The [[Definition:Binary Operation|operation]] $+$ is [[Definition:Well-Defined Operation|well-defined]]. That is: :$\dfrac a b = \dfrac {a'} {b'}, \dfrac c d = \dfrac {c'} {d'} \implies \dfrac a b + \dfrac c d = \dfrac {a'} {b'} + \dfrac {c'} {d'}$ {{questionable|This is an existing operation and thus already well-defined. See talk.}}	1
Let $\mathbb A$ be one of the [[Definition:Standard Number System|standard number systems]] $\N, \Z, \Q, \R, \C$. Let $S$ be a [[Definition:Finite Set|finite set]]. Let $f : S \to \mathbb A$ be a [[Definition:Mapping|mapping]]. Let $\left\vert{\, \cdot\,}\right\vert$ denote the [[Definition:Standard Absolute Value|standard absolute value]]. Let $\left\vert{f}\right\vert$ be the [[Definition:Absolute Value of Mapping|absoute value]] of $f$. Then we have the [[Definition:Inequality|inequality]] of [[Definition:Summation|summations]] on [[Definition:Finite Set|finite sets]]: :$\displaystyle \left\vert \sum_{s \mathop \in S} f(s) \right\vert \leq \sum_{s \mathop \in S} \vert f(s) \vert$	1
{{begin-eqn}} {{eqn | o = | r = \paren {z_1 \circ z_2} + i \paren {z_1 \times z_2} | c = }} {{eqn | r = \paren {z_1 \circ z_2} + i \frac {\overline {z_1} z_2 - z_1 \overline {z_2} } {2 i} | c = {{Defof|Vector Cross Product|subdef = Complex|index = 4}} }} {{eqn | r = \frac {\overline {z_1} z_2 + z_1 \overline {z_2} } 2 + \frac {\overline {z_1} z_2 - z_1 \overline {z_2} } 2 | c = {{Defof|Dot Product|subdef = Complex|index = 4}} }} {{eqn | r = \frac {\overline {z_1} z_2 + z_1 \overline {z_2} + \overline {z_1} z_2 - z_1 \overline {z_2} } 2 | c = }} {{eqn | r = \overline {z_1} z_2 | c = }} {{end-eqn}} {{qed}}	1
Let $n \in \Z_{\ge 0}$ be a [[Definition:Positive Integer|positive integer]]. Let $\struct {\Z / n \Z, +, \times}$ be the [[Definition:Ring (Abstract Algebra)|ring]] of [[Definition:Integers Modulo m|integers modulo $n$]]. Let $U = \struct {\paren {\Z / n \Z}^\times, \times}$ denote the [[Definition:Group of Units of Ring|group of units]] of $\struct {\Z / n \Z, +, \times}$. Then $U$ is [[Definition:Cyclic Group|cyclic]] {{iff}} either: :$n = p^\alpha$ or: :$n = 2 p^\alpha$ where $p \ge 3$ is [[Definition:Prime Number|prime]] and $\alpha \ge 0$.	1
:$x \divides y \iff \ideal y \subseteq \ideal x$	1
Let $\rho: G \to \operatorname{GL} \left({V}\right)$ be a [[Definition:Linear Representation|representation]]. Let $f: V \to V$ be a [[Definition:Linear Mapping|linear mapping]]. Let: :$\forall g \in G: \rho \left({g}\right) \circ f = f \circ \rho \left({g}\right)$ Then $f: V \to V$ is a [[Definition:G-Module Homomorphism|$G$-module homomorphism]]. {{explain|between which $G$-modules? (probably $\left({G, \rho}\right) \to \left({G, \rho}\right)$, which makes it rather tautologous)}}	1
[[Definition:Row Equivalence|Row equivalence]] is an [[Definition:Equivalence Relation|equivalence relation]].	1
Let $m$ be [[Definition:Prime Number|prime]]. From [[Ring of Integers Modulo Prime is Integral Domain]], $\struct {\Z_m, +, \times}$ is an [[Definition:Integral Domain|integral domain]]. From [[Finite Integral Domain is Galois Field]], $\struct {\Z_m, +, \times}$ is a [[Definition:Field (Abstract Algebra)|field]]. {{qed|lemma}} Now suppose $m \in \Z: m \ge 2$ is [[Definition:Composite Number|composite]]. From [[Ring of Integers Modulo Composite is not Integral Domain]], $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]]. From [[Field is Integral Domain]] $\struct {\Z_m, +, \times}$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
If the conditions are fulfilled, then: :$x \in H \implies -x = \left({- 1_R}\right) \circ x \in H$ Thus $H$ is a [[Definition:Subgroup|subgroup]] of $\left({G, +}\right)$ by the [[Two-Step Subgroup Test]], and hence a [[Definition:Submodule|submodule]]. {{qed}}	1
:$G$ is a [[Definition:Left Ideal|left ideal]] of $\struct {\map {\MM_S} 2, +, \times}$.	1
From [[Power Set is Closed under Symmetric Difference]] and [[Power Set is Closed under Intersection]], we have that both $\struct {\powerset S, *}$ and $\struct {\powerset S, \cap}$ are [[Definition:Closed Algebraic Structure|closed]]. Hence $\powerset S$ is a [[Definition:Ring of Sets|ring of sets]], and hence a [[Ring of Sets is Commutative Ring|commutative ring]]. From [[Intersection with Subset is Subset]], we have $A \subseteq S \iff A \cap S = A$. Thus we see that $S$ is the [[Definition:Unity of Ring|unity]]. Also during the proof of [[Power Set with Intersection is Monoid]], it was established that $S$ is the [[Definition:Identity Element|identity]] of $\struct {\powerset S, \cap}$. We also note that [[Set Intersection Not Cancellable|set intersection is not cancellable]], so $\struct {\powerset S, *, \cap}$ is not an [[Definition:Integral Domain|integral domain]]. The result follows. {{qed}}	1
Let $\mathbf x$ be a [[Definition:Vector (Linear Algebra)|vector]] in a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]]: : $\mathbf x = x_i \mathbf i + x_j \mathbf j + x_k \mathbf k$ Then: :$\mathbf x \times \mathbf x = \mathbf 0$ where $\times$ denotes [[Definition:Vector Cross Product|vector cross product]].	1
Let: : $B = \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} + a'_{r1} & \cdots & a_{rs} + a'_{rs} & \cdots & a_{rn} + a'_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix} = \begin{vmatrix} b_{11} & \cdots & b_{1s} & \cdots & b_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ b_{r1} & \cdots & b_{rs} & \cdots & b_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ b_{n1} & \cdots & b_{ns} & \cdots & b_{nn} \end{vmatrix}$ : $A_1 = \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{r1} & \cdots & a_{rs} & \cdots & a_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix}$ : $A_2 = \begin{vmatrix} a_{11} & \cdots & a_{1s} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a'_{r1} & \cdots & a'_{rs} & \cdots & a'_{rn} \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{ns} & \cdots & a_{nn} \end{vmatrix}$ Then: {{begin-eqn}} {{eqn | l = B | r = \sum_\lambda \left({\operatorname{sgn} \left({\lambda}\right) \prod_{k=1}^n b_{k \lambda \left({k}\right)} }\right) | c = }} {{eqn | r = \sum_\lambda \operatorname{sgn} \left({\lambda}\right) a_{1 \lambda \left({1}\right)} \cdots \left({a_{r \lambda \left({r}\right)} + a'_{r \lambda \left({r}\right)} }\right) \cdots a_{n \lambda \left({n}\right)} | c = }} {{eqn | r = \sum_\lambda \operatorname{sgn} \left({\lambda}\right) a_{1 \lambda \left({1}\right)} \cdots a_{r \lambda \left({r}\right)} \cdots a_{n \lambda \left({n}\right)} + \sum_\lambda \operatorname{sgn} \left({\lambda}\right) a_{1 \lambda \left({1}\right)} \cdots a'_{r \lambda \left({r}\right)} \cdots a_{n \lambda \left({n}\right)} | c = }} {{eqn | r = A_1 + A_2 | c = }} {{end-eqn}} {{qed}} The result for columns follows directly from [[Determinant of Transpose]]. {{qed}} [[Category:Determinants]] 444jdmhgv95gdwkmqke2otcqw0v698t	1
Let $\sequence {x_n} $ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. By [[Norm Sequence of Cauchy Sequence has Limit]], $\sequence {\norm {x_n} }$ is a [[Definition:Convergent Real Sequence|convergent sequence in $\R$]]. By [[Convergent Real Sequence is Bounded]], $\sequence {\norm {x_n} }$ is [[Definition:Bounded Real Sequence|bounded]]. That is: :$\exists M \in \R_{\gt 0}: \forall n \in \N: \norm {x_n} = \size {\norm {x_n} } \le M$ Thus, by definition, $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
As a [[Definition:Diagonal Matrix|diagonal matrix]] is also a [[Definition:Triangular Matrix|triangular matrix]] (both upper and lower), the result follows directly from [[Determinant of Triangular Matrix]]. {{qed}} [[Category:Determinants]] [[Category:Diagonal Matrices]] 46j7gt7zea6di1bocalbw6n08ipn7hx	1
This proof assumes that $R$ is a [[Definition:Field (Abstract Algebra)|field]], which makes the triangulation process slightly quicker. By this assumptions, all [[Definition:Element of Matrix|elements]] of $\mathbf A$ have [[Definition:Inverse Element|multiplicative inverses]]. Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n$]]. We proceed by induction on $n$, the number of [[Definition:Row of Matrix|rows]] of $\mathbf A$. === Basis for the Induction === For $n = 1$, we have a matrix of just one [[Definition:Element of Matrix|element]], which is trivially [[Definition:Diagonal Matrix|diagonal]], hence both upper and lower triangular. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Fix $n \in \N$, and assume all $n \times n$-matrices can be upper triangularised by [[Definition:Elementary Row Operation|elementary row operations]]. If $R$ is a [[Definition:Field (Abstract Algebra)|field]], assume all $n \times n$-matrices can be upper triangularised by elementary row operations of type 2. This forms our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]. === Induction Step === Let $\mathbf A = \left[{a}\right]_{n + 1}$ be a [[Definition:Square Matrix|square matrix]] of order $n + 1$. When the first [[Definition:Column of Matrix|column]] of $\mathbf A$ contains only zeroes, it is upper triangularisable {{iff}} the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ is. Each [[Definition:Elementary Row Operation|elementary row operation]] used in triangularisation process of the submatrix $\mathbf A \left({1; 1}\right)$ will not change the zeros of the first column of $\mathbf A$. So when $\mathbf A \left({1; 1}\right)$ is upper triangularised, then $\mathbf A$ will also be upper triangularised. From the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]], we conclude that $\mathbf A$ can be upper triangularised by elementary row operations. Now suppose that its first column contains a non-zero value. Suppose that $a_{11} = 0$. Let $j$ be the smallest row index such that $a_{j1} \ne 0$, and note that $j$ exists by assumption. Now apply the following operation of type 2: :$r_1 \to r_1 + r_j$ As $a_{j1} \ne 0$, this enforces $a_{11} \ne 0$, and we continue as in the case below. Suppose $a_{11} \ne 0$. We use the following operations of type 2: :$\forall j \in \left\{{2, \ldots, n+1}\right\}: r_j \to r_j - \dfrac {a_{j1}} {a_{11}} r_1$ This will put the first column to zero (except for the first element, $a_{11}$). It follows that $\mathbf A$ can be upper triangularised precisely when the [[Definition:Submatrix|submatrix]] $\mathbf A \left({1; 1}\right)$ can. Again, the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]] renders $\mathbf A$ upper triangularisable by [[Definition:Elementary Row Operation|elementary row operations]]. This completes the case distinction, and hence the result follows by [[Principle of Mathematical Induction|induction]]. To put the matrix $\mathbf A$ into [[Definition:Lower Triangular Matrix|lower triangular form]], just do the same thing, but start with the last column and the last diagonal element $a_{n + 1 \; n + 1}$. {{qed}}	1
Let $L$ be a [[Definition:Integral Lattice|lattice]] in $\R^n$. Let $\left({v_1, \ldots, v_n}\right)$ be an [[Definition:Ordered Basis|ordered basis]] for $L$. Let $v_i = \left({v_{i 1}, \ldots , v_{i n} }\right)$ for $i \in \left\{ {1, \ldots, n}\right\}$. The '''covolume''' of $L$ is the [[Definition:Determinant of Matrix|determinant]] of the [[Definition:Square Matrix|matrix]]: :$\begin{bmatrix} v_{1 1} & v_{1 2} & \cdots & v_{1 n} \\ v_{2 1} & v_{2 2} & \cdots & v_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ v_{n 1} & v_{n 2} & \cdots & v_{n n} \\ \end{bmatrix}$	1
Let $\struct {R, +, \circ}$ be a [[Definition:Division Ring|division ring]] whose [[Definition:Ring Zero|zero]] is denoted $0_R$. {{TFAE}} === [[Definition:Non-Archimedean/Norm (Division Ring)/Definition 1|Definition 1]] === {{:Definition:Non-Archimedean/Norm (Division Ring)/Definition 1}} === [[Definition:Non-Archimedean/Norm (Division Ring)/Definition 2|Definition 2]] === {{:Definition:Non-Archimedean/Norm (Division Ring)/Definition 2}}	1
=== Necessary Condition === Suppose $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ is [[Definition:Linearly Dependent Sequence|linearly dependent]]. [[Definition:By Hypothesis|By hypothesis]], the [[Definition:Set|set]] of all [[Definition:Integer|integers]] $r \in \closedint 1 n$ such that $\sequence {a_k}_{1 \mathop \le k \mathop \le r}$ is [[Definition:Linearly Independent Sequence|linearly dependent]] is not [[Definition:Empty Set|empty]]. Let $p$ be its [[Definition:Smallest Element|smallest element]]. Then from [[Singleton is Linearly Independent]], $p \ge 2$, as $a_1 \ne \mathbf 0$ and hence $\set {a_1}$ is [[Definition:Linearly Independent Sequence|linearly independent]]. Also, there exist [[Definition:Scalar (Vector Space)|scalars]] $\lambda_1, \ldots, \lambda_p$, not all of which are [[Definition:Zero Scalar|zero]], such that $\displaystyle \sum_{k \mathop = 1}^p \lambda_k \circ a_k = \mathbf 0$. Suppose $\lambda_p = 0$. Then not all of $\lambda_1, \ldots, \lambda_{p-1}$ can be [[Definition:Zero Scalar|zero]]. Then $\sequence {a_k}_{1 \mathop \le k \mathop \le p-1}$ is [[Definition:Linearly Independent Sequence|linearly dependent]]. That contradicts the definition of $p$, so $\lambda_p \ne 0$. So, because: :$\displaystyle \lambda_p \circ a_p = - \sum_{k \mathop = 1}^{p - 1} \lambda_k \circ a_k$ we must have: :$\displaystyle a_p = \sum_{k \mathop = 1}^{p - 1} \paren {-{\lambda_p}^{-1} \lambda_k} \circ a_k$ and thus $a_p$ is a [[Definition:Linear Combination|linear combination]] of $\sequence {a_k}_{1 \mathop \le k \mathop \le p - 1}$. {{qed|lemma}} === Sufficient Condition === Now suppose that $a_p$ is a [[Definition:Linear Combination|linear combination]] of $\sequence {a_k}_{1 \mathop \le k \mathop \le p - 1}$. Then: :$\displaystyle a_p = \sum_{k \mathop = 1}^{p - 1} \mu_k \circ a_k$ So we can assign values to $\lambda_k$ as follows: :$\forall k \in \closedint 1 n: \lambda_k = \begin{cases} \mu_k & : k < p \\ -1 & : k = p \\ 0 & : k > p \\ \end{cases}$ Then: :$\displaystyle \sum_{k \mathop = 1}^n \lambda_k \circ a_k = \mathbf 0$ Hence the result. {{qed}}	1
{{proof wanted}} {{AoC|Zorn's Lemma}}	1
$T$ is [[Definition:Metacompact Space|metacompact]] {{iff}} every [[Definition:Open Cover|open cover]] of $S$ has an [[Definition:Open Refinement|open refinement]] which is [[Definition:Point Finite|point finite]]. $T$ is a [[Definition:Lindelöf Space|Lindelöf space]] if every [[Definition:Open Cover|open cover]] of $S$ has a [[Definition:Countable Subcover|countable subcover]]. Having established the definitions, we proceed. Let $\mathcal U$ be an [[Definition:Open Cover|open cover]] of $S$. Let $\mathcal V$ be a [[Definition:Point Finite|point finite]] [[Definition:Open Refinement|open refinement]] of $\mathcal U$. By [[Point Finite Set of Open Sets in Separable Space is Countable]], $\mathcal V$ is [[Definition:Countable Set|countable]]. Define a mapping $H$ on $\mathcal V$ thus: :$\forall V \in \mathcal V: H \left({V}\right) = \left\{{U \in \mathcal U: V \subseteq U}\right\}$ By [[Image of Countable Set under Mapping is Countable]], the [[Definition:Image of Mapping|image]] of $H$ is [[Definition:Countable Set|countable]]. Call this [[Definition:Image of Mapping|image]] $I$. Since $\mathcal V$ is a refinement of $\mathcal U$, $\varnothing \notin I$. By the [[Axiom:Axiom of Countable Choice|Axiom of Countable Choice]], $I$ has a [[Definition:Choice Function|choice function]] $c$. Then $G = c \circ H: \mathcal V \to \mathcal U$ is a mapping such that: :$\forall V \in \mathcal V: V \subseteq G \left({V}\right)$ Then $\mathcal Q = G \left({\mathcal V}\right)$ is [[Definition:Countable Set|countable]] by [[Image of Countable Set under Mapping is Countable]]. Each element of $\mathcal Q$ is an element of $\mathcal U$ by the definition of $G$. Let $x \in S$. Then since $\mathcal V$ is a [[Definition:Cover of Set|cover]] for $S$: : $\exists V \in \mathcal V: x \in V$ Then $x \in V \subseteq G \left({V}\right) \in \mathcal Q$. Thus $\mathcal Q$ is a [[Definition:Countable Subcover|countable subcover]] of $\mathcal U$. Thus each [[Definition:Open Cover|open cover]] of $S$ has a [[Definition:Countable Subcover|countable subcover]], so $T$ is a [[Definition:Lindelöf Space|Lindelöf space]]. {{qed}}	1
We have by definition that a [[Definition:Field (Abstract Algebra)|field]] is a [[Definition:Division Ring|division ring]] which is also [[Definition:Commutative Ring|commutative]]. Hence $F$ is a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. So, from [[Ring of Square Matrices over Commutative Ring with Unity]] we have that $\struct {\map {\MM_F} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]]. From [[Matrix Multiplication is not Commutative]], we have that $\struct {\map {\MM_F} n, +, \times}$ is not a [[Definition:Commutative Ring|commutative ring]]. Hence the result. {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ECO} 3 | t = Interchange [[Definition:Column of Matrix|columns]] $k$ and $l$ | m = \kappa_k \leftrightarrow \kappa_l }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Column Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Column Operation|elementary column operation]]: :$e' := \kappa_k \leftrightarrow \kappa_l$ That is: :$e' = e$	1
We use the [[Two-Step Vector Subspace Test]]. $\left\{{\mathbf 0}\right\}$ is [[Definition:Non-Empty Set|not empty]], because it contains $\mathbf{0}$. $\left\{{\mathbf 0}\right\}$ is [[Definition:Closure (Abstract Algebra)|closed]] under $+$ because: :$\forall \mathbf{x},\mathbf{y} \in \left\{{\mathbf 0}\right\}, \mathbf{x} + \mathbf{y} = \mathbf{0} + \mathbf{0} = \mathbf{0} \in \left\{{\mathbf 0}\right\}$ $\left\{{\mathbf 0}\right\}$ is [[Definition:Closure (Abstract Algebra)|closed]] under multiplication because: :$\forall \lambda \in K, \mathbf{x}\in \left\{{\mathbf 0}\right\}: \lambda\mathbf{x} = \lambda\mathbf{0} = \mathbf{0}\in \left\{{\mathbf 0}\right\}$ Hence the result, from the [[Two-Step Vector Subspace Test]]. {{qed}}	1
Let $K = \displaystyle \bigcup_{n \mathop \in \N} J_n$. Then from [[Increasing Union of Sequence of Ideals is Ideal]], $K$ is an [[Definition:Ideal of Ring|ideal]] of $D$. We have that $D$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. Hence there exists $a \in D$ such that: :$K = \ideal a$ where $\ideal a$ is the [[Definition:Principal Ideal of Ring|principal ideal]] of $D$ generated by $a$. But $a \in J_m$ for some $m \in \N$. Thus $K \subseteq J_m$ Thus it follows that $J_{m + 1} \subseteq J_m$ which [[Definition:Contradiction|contradicts]] our initial assertion that: :$\forall n \in \N: J_n \subsetneq j_{n + 1}$ Hence the result. {{qed}}	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Then: :$(1): \quad \ds \sum_{j \mathop = 1}^n \delta_{i j} a_{j k} = a_{i k}$ :$(2): \quad \ds \sum_{j \mathop = 1}^n \delta_{i j} a_{k j} = a_{k i}$ where: :$\delta_{i j}$ is the [[Definition:Kronecker Delta|Kronecker delta]] :$a_{j k}$ is [[Definition:Element of Matrix|element]] $\tuple {j, k}$ of $\mathbf A$.	1
From [[Empty Set is Open in Normed Vector Space]], $\O$ is [[Definition:Open Set in Normed Vector Space|open]] in $M$. But: :$X = \relcomp X \O$ where $\complement_X$ denotes the [[Definition:Relative Complement|set complement relative to $X$]]. The result follows by definition of [[Definition:Closed Set in Normed Vector Space|closed set]]. {{qed}}	1
Let $A$ be a [[Definition:Noetherian Ring|Noetherian ring]]. Let $n \ge 1$ be an [[Definition:Integer|integer]]. Let $A \sqbrk {x_1, \ldots, x_n}$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms over $A$ in the indeterminates $x_1, \ldots, x_n$]]. Then $A \sqbrk {x_1, \ldots, x_n}$ is also a [[Definition:Noetherian Ring|Noetherian ring]].	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Then $\struct {R, +, \times}$ is a [[Definition:Left Module|left module]] over $\struct {R, +, \times}$.	1
:$\exists n \in \N: 0 < \norm n < 1$.	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf B | r = \sqbrk a_{m n} + \sqbrk b_{m n} | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{eqn | r = \sqbrk {a + b}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {b + a}_{m n} | c = {{Ring-axiom|A2}} }} {{eqn | r = \sqbrk b_{m n} + \sqbrk a_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \mathbf B + \mathbf A | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{end-eqn}} {{qed}}	1
The [[Definition:Gaussian Curvature|Gaussian curvature]] of a [[Definition:Surface|surface]] does not change if one bends the [[Definition:Surface|surface]] without stretching it. That is, [[Definition:Gaussian Curvature|Gaussian curvature]] can be determined entirely by measuring [[Definition:Angle|angles]], [[Definition:Distance (Linear Measure)|distances]] and their [[Definition:Rate of Change|rates]] on the [[Definition:Surface|surface]] itself, without further reference to the particular way in which the [[Definition:Surface|surface]] is embedded in the ambient [[Definition:Dimension (Geometry)|$3$-dimensional]] [[Definition:Euclidean Space|Euclidean space]]. Thus the [[Definition:Gaussian Curvature|Gaussian curvature]] is an intrinsic invariant of a [[Definition:Surface|surface]].	1
Let $\mathbf x = \sqbrk x_{1 n} = \begin {bmatrix} x_1 & x_2 & \cdots & x_n \end {bmatrix}$ be a [[Definition:Row Matrix|row matrix]]. Then $\mathbf x^\intercal$, the [[Definition:Transpose of Matrix|transpose]] of $\mathbf x$, is a [[Definition:Column Matrix|column matrix]]: :$\begin {bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}^\intercal = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$	1
$(1):$ First we show that $x^\wedge \in G^{**}$: {{ProofWanted}} $(2):$ Then we show that $J: G \to G^{**}$ is a [[Definition:Linear Transformation|linear transformation]]: {{ProofWanted}}	1
[[Definition:P-Sequence Space|$P$-Sequence Space]] with [[Definition:P-Norm|$p$-norm]] forms [[Definition:Normed Vector Space|normed vector space]].	1
Let $A$ be a [[Definition:Non-Trivial Ring|non-trivial]] [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Then $A$ has a [[Definition:Prime Ideal of Ring|prime ideal]].	1
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\mathcal M_\R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $\R$]]. Then $\struct {\map {\mathcal M_\R} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]], but is not a [[Definition:Commutative Ring|commutative ring]].	1
Let us take each type of [[Definition:Elementary Column Operation|elementary column operation]] in turn. For each $\map e {\mathbf A}$, we will construct $\map {e'} {\mathbf A'}$ which will transform $\mathbf A'$ into a new [[Definition:Matrix|matrix]] $\mathbf A'' \in \map \MM {m, n}$, which will then be demonstrated to equal $\mathbf A$. In the below, let: :$\kappa_k$ denote [[Definition:Column of Matrix|column]] $k$ of $\mathbf A$ :$\kappa'_k$ denote [[Definition:Column of Matrix|column]] $k$ of $\mathbf A'$ :$\kappa''_k$ denote [[Definition:Column of Matrix|column]] $k$ of $\mathbf A''$ for arbitrary $k$ such that $1 \le k \le m$. By definition of [[Definition:Elementary Column Operation|elementary column operation]]: :only the [[Definition:Column of Matrix|column]] or [[Definition:Column of Matrix|columns]] directly operated on by $e$ is or are different between $\mathbf A$ and $\mathbf A'$ and similarly: :only the [[Definition:Column of Matrix|column]] or [[Definition:Column of Matrix|columns]] directly operated on by $e'$ is or are different between $\mathbf A'$ and $\mathbf A''$. Hence it is understood that in the following, only those [[Definition:Column of Matrix|columns]] directly affected will be under consideration when showing that $\mathbf A = \mathbf A''$. === [[Existence of Inverse Elementary Column Operation/Scalar Product of Column|$\text {ECO} 1$: Scalar Product of Column]] === {{:Existence of Inverse Elementary Column Operation/Scalar Product of Column}}{{qed|lemma}} === [[Existence of Inverse Elementary Column Operation/Add Scalar Product of Column to Another|$\text {ECO} 2$: Add Scalar Product of Column to Another]] === {{:Existence of Inverse Elementary Column Operation/Add Scalar Product of Column to Another}}{{qed|lemma}} === [[Existence of Inverse Elementary Column Operation/Exchange Columns|$\text {ECO} 3$: Exchange Columns]] === {{:Existence of Inverse Elementary Column Operation/Exchange Columns}}{{qed|lemma}} Thus in all cases, for each [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A$ to $\mathbf A'$, we have constructed the only possible [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A'$ to $\mathbf A$. Hence the result. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\struct {R, +, \circ}$ be such that the only [[Definition:Ideal of Ring|ideals]] of $\struct {R, +, \circ}$ are: :$\set {0_R}$ and: $\struct {R, +, \circ}$ itself. That is, such that $\struct {R, +, \circ}$ has no non-[[Definition:Null Ideal|null]] [[Definition:Proper Ideal|proper ideals]]. Then $\struct {R, +, \circ}$ is a [[Definition:Field (Abstract Algebra)|field]].	1
A '''linear equation''' is an equation in the form: :$b = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n$ where all of $a_1, \ldots, a_n, x_1, \ldots x_n, b$ are elements of a given [[Definition:Field (Abstract Algebra)|field]].	1
We need to show that: $\forall x, y, \in G, \forall \lambda, \mu \in R$: :$(1): \quad \lambda \circ \paren {x + y} = \paren {\lambda \circ x} + \paren {\lambda \circ y}$ :$(2): \quad \paren {\lambda +_R \mu} \circ x = \paren {\lambda \circ x} + \paren {\mu \circ x}$ :$(3): \quad \paren {\lambda \times_R \mu} \circ x = \lambda \circ \paren {\mu \circ x}$ Checking the criteria in order: === Criterion 1 === :$(1): \quad \lambda \circ \paren {x + y} = \paren {\lambda \circ x} + \paren {\lambda \circ y}$ Let $x = \tuple {x_1, x_2, \ldots, x_n}, y = \tuple {y_1, y_2, \ldots, y_n} \in G$. {{begin-eqn}} {{eqn | l = \lambda \circ \paren {x + y} | r = \lambda \circ \paren {\tuple {x_1, x_2, \ldots, x_n} + \tuple {y_1, y_2, \ldots, y_n} } | c = }} {{eqn | r = \lambda \circ \tuple {x_1 +_1 y_1, x_2 +_2 y_2, \ldots, x_n +_n y_n} | c = }} {{eqn | r = \tuple {\lambda \circ_1 \paren {x_1 + y_1}, \lambda \circ_2 \paren {x_2 + y_2}, \ldots, \lambda \circ_n \paren {x_n + y_n} } | c = }} {{eqn | r = \tuple {\lambda \circ_1 x_1, \lambda \circ_2 x_2, \ldots, \lambda \circ_n x_n} + \tuple {\lambda \circ_1 y_1, \lambda \circ_2 y_2, \ldots, \lambda \circ_n y_n} | c = }} {{eqn | r = \paren {\lambda \circ \tuple {x_1, x_2, \ldots, x_n} } + \paren {\lambda \circ \tuple {y_1, y_2, \ldots, y_n} } | c = }} {{eqn | r = \paren {\lambda \circ x} + \paren {\lambda \circ y} | c = }} {{end-eqn}} So $(1)$ holds. {{qed|lemma}} === Criterion 2 === :$(2): \quad \paren {\lambda +_R \mu} \circ x = \paren {\lambda \circ x} + \paren {\mu \circ x}$ Let $x = \tuple {x_1, x_2, \ldots, x_n} \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda +_R \mu} \circ x | r = \paren {\lambda +_R \mu} \circ \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda +_R \mu} \circ_1 x_1, \paren {\lambda +_R \mu} \circ_2 x_2, \ldots, \paren {\lambda +_R \mu} \circ_n x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda \circ_1 x_1} +_1 \paren {\mu \circ_1 x_1}, \paren {\lambda \circ_2 x_2} +_2 \paren {\mu \circ_2 x_2}, \ldots, \paren {\lambda \circ_n x_n} +_n \paren {\mu \circ_n x_n} } | c = }} {{eqn | r = \tuple {\lambda \circ_1 x_1, \lambda \circ_2 x_2, \lambda \circ_n x_n} + \tuple {\mu \circ_1 x_1, \mu \circ_2 x_2, \ldots, \mu \circ_n x_n} | c = }} {{eqn | r = \paren {\lambda \circ_1 \tuple {x_1, x_2, \ldots, x_n} } + \paren {\mu \circ_1 \tuple {x_1, x_2, \ldots, x_n} } | c = }} {{eqn | r = \paren {\lambda \circ x} + \paren {\mu \circ x} | c = }} {{end-eqn}} So $(2)$ holds. {{qed|lemma}} === Criterion 3 === :$(3): \quad \paren {\lambda \times_R \mu} \circ x = \lambda \circ \paren {\mu \circ x}$ Let $x = \tuple {x_1, x_2, \ldots, x_n} \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda \times_R \mu} \circ x | r = \paren {\lambda \times_R \mu} \circ \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda \times_R \mu} \circ_1 x_1, \paren {\lambda \times_R \mu} \circ_2 x_2, \ldots, \paren {\lambda \times_R \mu} \circ_n x_n} | c = }} {{eqn | r = \tuple {\lambda \circ_1 \paren {\mu \circ_1 x_1}, \lambda \circ_2 \paren {\mu \circ_2 x_2}, \ldots, \lambda \circ_n \paren {\mu \circ_n x_n} } | c = }} {{eqn | r = \lambda \circ \tuple {\mu \circ_1 x_1, \mu \circ_2 x_2, \ldots, \mu \circ_n x_n} | c = }} {{eqn | r = \lambda \circ \paren {\mu \circ \tuple {x_1, x_2, \ldots, x_n} } | c = }} {{eqn | r = \lambda \circ \paren {\mu \circ x} | c = }} {{end-eqn}} So $(3)$ holds. {{qed|lemma}} Hence the result. {{qed}}	1
By [[Bézout's Lemma]] there exists $a, b \in \Z$ such that $a p + b q = 1$. Then for $s \otimes_\Z t \in \Z / p \Z \otimes \Z / q \Z$: {{begin-eqn}} {{eqn | l = s \otimes t | r = (s \left({a p + b q}\right)) \otimes t | c = $s = s \cdot 1$ }} {{eqn | r = (s a p + s b q) \otimes t | c = By [[Definition:Module|module axiom 2]] }} {{eqn | r = s b q \otimes t + s a p \otimes t | c = By equality in [[Definition:Tensor Product of Modules|tensor product]] }} {{eqn | r = s b \otimes q t + s a p \otimes t | c = By equality in [[Definition:Tensor Product of Modules|tensor product]] }} {{eqn | r = 0 | c = by [[Tensor with Zero Element is Zero in Tensor]] and the fact that $qt = 0$ in $\Z_q$ and $sap=0$ in $\Z_p$ }} {{end-eqn}} {{qed}} {{explain|Notation needs to be tightened up. When the fundamental stuff like this is being addressed, it is important to explain exactly which operation of which component of the module is being used: the scalar product, or the group operation, or whichever of the ring operations. It is inadequate to use mere concatenation, as it is never completely clear which elements of which structure each one is. As for the rest of this proof, it is still impenetrably vague. The reason for the last line is a complete mystery -- the actual definition of $0$ also needs to be stated.}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\mathbf A \in R^{n \times n}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Then $\mathbf A$ is [[Definition:Invertible Matrix|invertible]] {{iff}} its [[Definition:Rank of Matrix|rank]] also equals $n$.	1
Let $\struct {X, \norm { \, \cdot \, } }$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\sequence {x_n}$ be a [[Definition:Convergent Sequence in Normed Vector Space|convergent sequence]] in $R$ to the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $x$. That is, let $\displaystyle \lim_{n \mathop \to \infty} x_n = x$. Then :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n} = \norm x$	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct {S, +, *}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $f : R \to S$ be a [[Definition:Ring Homomorphism|ring homomorphism]]. Let the [[Definition:Image of Set under Mapping|image]] of $f$ be a [[Definition:Subset|subset]] of the [[Definition:Center of Ring|center]] of $S$. Let $\struct {S_R, *}$ be the [[Definition:Algebra Defined by Ring Homomorphism|algebra defined by the ring homomorphism]] $f$. Then $\struct {S_R, *}$ is an [[Definition:Algebra over Ring|algebra over]] $R$.	1
Let $\left({G, \circ}\right)$ be a [[Definition:Finite Group|finite group]]. Let $\left({V, \phi}\right)$ be a [[Definition:G-Module|$G$-module]]. Then $V$ is an [[Definition:Irreducible G-Module|irreducible $G$-module]] {{iff}} $V$ has no [[Definition:Trivial G-Module|non-trivial]] [[Definition:Proper G-Submodule|proper $G$-submodules]].	1
Let $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ be an [[Definition:Absolutely Convergent Real Series|absolutely convergent series in $\R$]]. Then $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Convergent Series|convergent]].	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$. Let $b$ be a [[Definition:Bilinear Form|bilinear form]] on $V$. Let $b$ be [[Definition:Symmetric Bilinear Form|symmetric]]. Then $b$ is [[Definition:Reflexive Bilinear Form|reflexive]].	1
By [[Convergent Sequence in Normed Division Ring is Bounded]], $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. Suppose $\norm {x_n} \le K$ for $n = 1, 2, 3, \ldots$. Then for $n = 1, 2, 3, \ldots$: {{begin-eqn}} {{eqn | l = \norm {x_n y_n - l m} | r = \norm {x_n y_n - x_n m + x_n m - l m} | c = }} {{eqn | o = \le | r = \norm {x_n y_n - x_n m} + \norm {x_n m - l m} | c = [[Definition:Norm on Division Ring|Axiom (N3) of norm]] (Triangle Inequality) }} {{eqn | r = \norm {x_n} \norm {y_n - m} + \norm {x_n - l} \norm m | c = [[Definition:Norm on Division Ring|Axiom (N2) of norm]] (Multiplicativity) }} {{eqn | o = \le | r = K \norm {y_n - m} + \norm m \norm {x_n - l} | c = as $\sequence {x_n}$ is bounded by $K$ }} {{eqn | o = =: | r = z_n | c = }} {{end-eqn}} We note that $\sequence {z_n}$ is a [[Definition:Real Sequence|real sequence]]. But $x_n \to l$ as $n \to \infty$. So from [[Definition:Convergent Sequence in Normed Division Ring]]: :$\norm {x_n - l} \to 0$ as $n \to \infty$ Similarly $\norm {y_n - m} \to 0$ as $n \to \infty$. From the [[Combined Sum Rule for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x'_n + \mu y'_n} = \lambda l' + \mu m'$, $z_n \to 0$ as $n \to \infty$ By applying the [[Squeeze Theorem for Sequences of Complex Numbers]] (which applies as well to real as to complex sequences): :$\sequence {\norm {x_n y_n - l m}}$ [[Definition:Convergent Real Sequence|converges]] to $0$ in $\R$. By definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence in a normed division ring]]: :$\sequence{x_n y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] in $R$ It follows that: :$\displaystyle \lim_{n \mathop \to \infty} \paren {x_n y_n} = l m$ {{qed}}	1
Let $z_1 := r_1 e^{i \theta_1}, z_2 := r_2 e^{i \theta_2} \in \C$ be [[Definition:Exponential Form of Complex Number|complex numbers expressed in exponential form]]. Then: :$z_1 \times z_2 = r_1 r_2 \map \sin {\theta_2 - \theta_1}$ where $z_1 \times z_2$ denotes the [[Definition:Complex Dot Product|dot product]] of $z_1$ and $z_2$.	1
Let $S$ be a [[Definition:Set|set]]. Let $\struct {X, \norm {\, \cdot \,} }$ be a [[Definition:Normed Vector Space|normed vector space]] over $K \in \set {\R, \C}$. Let $\BB$ be the set of [[Definition:Bounded Mapping|bounded mappings]] $S \to X$. Let $\norm {\, \cdot \,}_\infty$ be the [[Definition:Supremum Norm|supremum norm]] on $\BB$. Then $\norm {\, \cdot \,}_\infty$ is a [[Definition:Norm on Vector Space|norm]] on $\BB$. {{MissingLinks|Add a link that establishes that $\BB$ is a vector space}}	1
Let $x, y \in V$. Let $\lambda \in \mathbb K$. Then: {{begin-eqn}} {{eqn | l = 0 | o = \le | r = \innerprod {x - \lambda y} {x - \lambda y} | c = [[Definition:Semi-Inner Product|Property $(4)$ of Semi-Inner Product]] }} {{eqn | r = \innerprod x x + \innerprod x {-\lambda y} + \innerprod {-\lambda y} x + \innerprod {-\lambda y} {-\lambda y} | c = [[Definition:Semi-Inner Product|Property $(3)$ of Semi-Inner Product]] }} {{eqn | r = \innerprod x x - \lambda^* \innerprod x y - \lambda \innerprod x y^* + \lambda \lambda^* \innerprod y y | c = [[Definition:Semi-Inner Product|Property $(1)$ and $(2)$ of Semi-Inner Product]] }} {{end-eqn}} where $\lambda^*$ denotes the [[Definition:Complex Conjugate|complex conjugate]] of $\lambda$. (If $\mathbb K$ is a [[Definition:Field (Abstract Algebra)|subfield]] of $\R$ , then $\lambda^* = \lambda$.) First, suppose $\innerprod y y \ne 0$. Insert $\lambda = \innerprod x y \innerprod y y^{-1}$ in the inequality: {{begin-eqn}} {{eqn | l = 0 | o = \le | r = \innerprod x x - \innerprod x y^* \paren {\innerprod y y^{-1} }^* \innerprod x y - \innerprod x y \innerprod y y^{-1} \innerprod x y^* + \innerprod x y \innerprod y y^{-1} \innerprod x y^* \paren {\innerprod y y^{-1} }^* \innerprod y, y | c = [[Product of Complex Conjugates]] }} {{eqn | r = \innerprod x x - \size {\innerprod x y}^2 \paren {\innerprod y y^{-1} }^* - \size {\innerprod x y}^2 \innerprod y y^{-1} + \size {\innerprod x y}^2 \paren {\innerprod y y^{-1} }^* | c = [[Modulus in Terms of Conjugate]] }} {{eqn | r = \innerprod x x - \size {\innerprod x y}^2 \innerprod y y^{-1} }} {{end-eqn}} Reorder the inequality to get: :$\size {\innerprod x y}^2 \le \innerprod x x \innerprod y y$ {{qed|lemma}} Next, suppose $\innerprod y y = 0$. Let $n \in \N \subset \mathbb K$ be a [[Definition:Natural Number|natural number]]. Insert $\lambda = n \innerprod x y$ in the inequality: {{begin-eqn}} {{eqn | l = 0 | o = \le | r = \innerprod x x - n^* \innerprod x y^* \innerprod x y - n \innerprod x y \innerprod x y^* + 0 | c = [[Product of Complex Conjugates]] }} {{eqn | r = \innerprod x x - n \innerprod x y^* \innerprod x y - n \innerprod x y \innerprod x y^* | c = [[Complex Number equals Conjugate iff Wholly Real]] }} {{eqn | r = \innerprod x x - 2 n \size {\innerprod x y}^2 | c = [[Modulus in Terms of Conjugate]] }} {{end-eqn}} Rearrange the inequality to get: :$\innerprod x x \ge 2 n \size {\innerprod x y}^2$ As this inequality holds for all $n \in \N$, it follows that: :$\size {\innerprod x y}^2 = 0$. Then: :$\size {\innerprod x y}^2 \le 0 = \innerprod x x \innerprod y y$ {{qed}}	1
'''Vector Algebra''' is the branch of [[Definition:Mathematics|mathematics]] which studies the [[Definition:Algebra (Mathematical Branch)|algebra]] of [[Definition:Vector Space|vector spaces]].	1
Let $H$ be a [[Definition:Countably Infinite Set|countably infinite subset]] of $S$. From [[Closure of Infinite Subset of Finite Complement Space]], the [[Definition:Closure (Topology)|closure]] of $H$ is $S$. So by definition $H$ is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Hence the result by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $A = \left({A_F, \oplus}\right)$ be a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]] whose [[Definition:Conjugation (Abstract Algebra)|conjugation]] is denoted $*$. Let $a \in A$. Then the [[Definition:Multiplicative Inverse|multiplicative inverse]] of $a$ is given by: :$a^{-1} = \dfrac {a^*} {\left \Vert {a}\right \Vert^2}$ where: : $a^*$ is the [[Definition:Conjugate (Algebra)|conjugate]] of $a$ : $\left \Vert {a}\right \Vert$ is the [[Definition:Norm on Vector Space|norm]] of $a$.	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]] $0$. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by $\norm {\, \cdot \,}$]]. Let $d$ be [[Definition:Non-Archimedean Metric|non-Archimedean]]. Then: :$\norm {\, \cdot \,}$ is a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]].	1
Let $\mathbf V \left({x_1, x_2, \ldots, x_n}\right)$ be a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$n$ dimensions]]. Let $\left({\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}\right)$ be the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis of $\mathbf V$]]. Let $\mathbf f$ and $\mathbf g: \mathbf V \to \mathbf V$ be [[Definition:Vector-Valued Function|vector-valued functions]] on $\mathbf V$: :$\mathbf f := \left({f_1 \left({\mathbf x}\right), f_2 \left({\mathbf x}\right), \ldots, f_n \left({\mathbf x}\right)}\right)$ :$\mathbf g := \left({g_1 \left({\mathbf x}\right), g_2 \left({\mathbf x}\right), \ldots, g_n \left({\mathbf x}\right)}\right)$ Let $\nabla \cdot \mathbf f$ denote the [[Definition:Divergence Operator|divergence]] of $f$. Then: :$\nabla \cdot \left({\mathbf f + \mathbf g}\right) = \nabla \cdot \mathbf f + \nabla \cdot \mathbf g$	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be an $R$-[[Definition:Module|module]]. Let $(M_i)_{i\in I}$ be a [[Definition:Family|family]] of [[Definition:Submodule|submodules]]. === [[Definition:Internal Direct Sum of Modules/Definition 1|Definition 1]] === {{:Definition:Internal Direct Sum of Modules/Definition 1}} === [[Definition:Internal Direct Sum of Modules/Definition 2|Definition 2]] === {{:Definition:Internal Direct Sum of Modules/Definition 2}} === [[Definition:Internal Direct Sum of Modules/Definition 3|Definition 3]] === {{:Definition:Internal Direct Sum of Modules/Definition 3}}	1
Let $\map {\MM_S} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$ over an [[Definition:Algebraic Structure|algebraic structure]] $\struct {S, \cdot}$. Let $\mathbf A, \mathbf B \in \map {\MM_S} {m, n}$. Let $\mathbf A \circ \mathbf B$ be defined as the [[Definition:Hadamard Product|Hadamard product]] of $\mathbf A$ and $\mathbf B$. The operation $\circ$ of [[Definition:Hadamard Product|Hadamard product]] satisfies the following properties: :$\circ$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\struct {S, \cdot}$ :$\circ$ is [[Definition:Associative|associative]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Associative|associative]] on $\struct {S, \cdot}$ :$\circ$ is [[Definition:Commutative Operation|commutative]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Commutative Operation|commutative]] on $\struct {S, \cdot}$.	1
From [[Inverse of Hilbert Matrix]], $H_n^{-1} = \left[{b}\right]_n$ can be specified as: :$\begin{bmatrix} b_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac {\left({-1}\right)^{i + j} \left({i + n - 1}\right)! \left({j + n - 1}\right)!} {\left({\left({i - 1}\right)!}\right)^2 \left({\left({j - 1}\right)!}\right)^2 \left({n - i}\right)! \left({n - j}\right)! \left({i + j - 1}\right)} \end{bmatrix}$ Thus: {{begin-eqn}} {{eqn | l = b_{i j} | r = \frac {\left({-1}\right)^{i + j} \left({i + n - 1}\right)! \left({j + n - 1}\right)!} {\left({\left({i - 1}\right)!}\right)^2 \left({\left({j - 1}\right)!}\right)^2 \left({n - i}\right)! \left({n - j}\right)! \left({i + j - 1}\right)} | c = }} {{eqn | r = \left({\frac {\left({-1}\right)^{i + j} } {i + j - 1} }\right) \left({\frac {\left({i + n - 1}\right)!} {\left({i - 1}\right)! \, n!} }\right) \left({\frac {\left({j + n - 1}\right)!} {\left({j - 1}\right)! \, n!} }\right) \left({\frac {n! \, i} {i! \, \left({n - i}\right)!} }\right) \left({\frac {n! \, j} {j! \, \left({n - j}\right)! } }\right) | c = }} {{eqn | r = \frac {\left({-1}\right)^{i + j} i j} {i + j - 1} \binom {i + n - 1} n \binom {j + n - 1} n \binom n i \binom n j | c = Definition of [[Definition:Binomial Coefficient|Binomial Coefficient]] }} {{eqn | r = \left({-1}\right)^{i + j} j \binom {i + n - 1} {i - i} \binom {j + n - 1} {n - 1} \binom {i + j - 2} {n - i} \binom n j | c = }} {{end-eqn}} All of the factors of the above expression are [[Definition:Integer|integers]], from [[Binomial Coefficient is Integer]]. {{qed}}	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\left\{ {M_i}\right\}_{i \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Module|$A$-modules]] [[Definition:Indexing Set|indexed]] by $I$. Let $\displaystyle M = \bigoplus_{i \mathop \in I} M_i$ be their [[Definition:Module Direct Sum|direct sum]]. Then $M$ is a [[Definition:Module|module]].	1
=== Necessary Condition === {{begin-eqn}} {{eqn | l = \mathbf v | r = \mathbf 0 | c = [[Definition:By Hypothesis|by hypothesis]] }} {{eqn | ll= \leadsto | l = -1_{\mathbb F} \circ \mathbf v | r = -1_{\mathbb F} \circ \mathbf 0 | c = scaling both sides by the [[Definition:Field Negative|negative]] of the [[Definition:Unity of Field|unity]] of $\mathbb F$ }} {{eqn | ll= \leadsto | l = -\mathbf v | r = \mathbf 0 | c = [[Vector Inverse is Negative Vector]], [[Zero Vector Scaled is Zero Vector]] }} {{eqn | ll= \leadsto | l = \mathbf v | r = -\mathbf v }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === Utilizing the [[Definition:Vector Space Axioms|vector space axioms]]: {{begin-eqn}} {{eqn | l = \mathbf v | r = -\mathbf v | c = [[Definition:By Hypothesis|by hypothesis]] }} {{eqn | ll =\leadsto | l = \mathbf v - \mathbf v | r = -\mathbf v - \mathbf v | c = adding $-\mathbf v$ to both sides }} {{eqn | ll= \leadsto | l = \mathbf 0 | r = \left({-1_{\mathbb F} \circ \mathbf v}\right) + \left({-1_{\mathbb F} \circ \mathbf v}\right) | c = [[Vector Inverse is Negative Vector]] }} {{eqn | ll= \leadsto | l = \mathbf 0 | r = \left({-2 \cdot 1_{\mathbb F} }\right)\circ \mathbf v }} {{end-eqn}} {{qed}} By hypothesis, $\mathbb F$ is infinite. By [[Characteristic of Division Ring is Zero or Prime]]: : $\operatorname{Char} \left({\mathbb F}\right) = 0$ so $-2 \cdot 1_{\mathbb F} \ne 0$. Thus from [[Vector Product is Zero only if Factor is Zero]], $\mathbf v = \mathbf 0$. {{qed}}{{proofread}} [[Category:Vector Algebra]] ik5njc69b6vtfss13careo9tk67dcgu	1
Let $z_n = u_n + i v_n$. We have that: {{begin-eqn}} {{eqn | l = \cmod {z_n} | r = \sqrt { {u_n}^2 + {v_n}^2} | c = }} {{eqn | o = > | r = \sqrt { {u_n}^2} | c = }} {{eqn | o = > | r = \size {u_n} | c = }} {{end-eqn}} and similarly: :$\cmod {z_n} > \size {v_n}$ From the [[Comparison Test]], the [[Definition:Series|series]] $\displaystyle \sum_{n \mathop = 1}^\infty u_n$ and $\displaystyle \sum_{n \mathop = 1}^\infty v_n$ are [[Definition:Absolutely Convergent Real Series|absolutely convergent]]. From [[Absolutely Convergent Series is Convergent/Real Numbers|Absolutely Convergent Series is Convergent: Real Numbers]], $\displaystyle \sum_{n \mathop = 1}^\infty u_n$ and $\displaystyle \sum_{n \mathop = 1}^\infty v_n$ are [[Definition:Convergent Series of Numbers|convergent]]. By [[Convergence of Series of Complex Numbers by Real and Imaginary Part]], it follows that $\displaystyle \sum_{n \mathop = 1}^\infty z_n$ is [[Definition:Convergent Series of Numbers|convergent]]. {{qed}}	1
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]] whose [[Definition:Ring Zero|zero]] is $0_D$ and whose [[Definition:Unity of Ring|unity]] is $1_D$. Let $\sim$ be the [[Definition:Relation|relation]] defined on $D$ as: $\forall x, y \in D: x \sim y$ {{iff}} $x$ is an [[Definition:Associate in Integral Domain|associate]] of $y$ Then $\sim$ is an [[Definition:Equivalence Relation|equivalence relation]].	1
Note that by [[Center of Commutative Ring]], the [[Definition:Image of Set under Mapping|image]] of $f$ is indeed a [[Definition:Subset|subset]] of the [[Definition:Center of Ring|center]] of $S$. By definition, the [[Definition:Multiplication of Algebra|multiplication]] of $(S_R, *)$ is the [[Definition:Ring Product|ring product]] of $S$. Thus it follows immediately from the fact that $S$ is a [[Definition:Ring (Abstract Algebra)|ring]], that $(S_R, *)$ is an [[Definition:Commutative Algebra|commutative algebra]]. {{qed}}	1
Let $\sequence {e_i}_{1 \mathop \le i \mathop \le k}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] that compose $\Gamma_1$. Let $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Column Matrix|elementary column matrices]]. Let $\sequence {f_i}_{1 \mathop \le i \mathop \le l}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] that compose $\Gamma_2$. Let $\sequence {\mathbf F_i}_{1 \mathop \le i \mathop \le l}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Column Matrix|elementary column matrices]]. From [[Column Operation is Equivalent to Post-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf A \mathbf R_1 = \mathbf B$ where $\mathbf R$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$: :$\mathbf R_1 = \mathbf E_1 \mathbf E_2 \dotsb \mathbf E_{k - 1} \mathbf E_k$ Also from [[Column Operation is Equivalent to Post-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf B \mathbf R_2 = \mathbf C$ where $\mathbf R_2$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf F_i}_{1 \mathop \le i \mathop \le l}$: :$\mathbf R_2 = \mathbf F_1 \mathbf F_2 \dotsb \mathbf F_{l - 1} \mathbf F_l$ Hence we have: :$\mathbf A \mathbf R_1 \mathbf R_2 = \mathbf C$ where $\mathbf R := \mathbf R_1 \mathbf R_2$ is the [[Definition:Matrix Product (Conventional)|product]]: :$\mathbf E_1 \mathbf E_2 \dotsb \mathbf E_{k - 1} \mathbf E_k \mathbf F_1 \mathbf F_2 \dotsb \mathbf F_{l - 1} \mathbf F_l$ Let $\Gamma$ be the [[Definition:Column Operation|column operation]] composed of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] $\tuple {e_1, e_2, \ldots, e_{k - 1}, e_k, f_1, f_2, \ldots, f_{l - 1}, f_l}$. Thus $\Gamma$ is a [[Definition:Column Operation|column operation]] which transforms $\mathbf A$ into $\mathbf C$. Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]] whose [[Definition:Dimension of Module|dimension]] is [[Definition:Finite|finite]]. Then the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]] $J: G \to G^{**}$ is an [[Definition:Module Isomorphism|isomorphism]].	1
[[Definition:Supremum Norm|Supremum Norm]] forms a [[Definition:Norm on Vector Space|norm]] on the [[Definition:Vector Space|vector space]] of [[Definition:Real Matrix|real matrices]].	1
From [[Hilbert Matrix is Cauchy Matrix]], $H_n$ is a special case of a [[Definition:Cauchy Matrix|Cauchy matrix]]: :$\begin{bmatrix} c_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac 1 {x_i + y_j} \end{bmatrix}$ where: :$x_i = i$ :$y_j = j - 1$ From [[Inverse of Cauchy Matrix]], the [[Definition:Inverse Matrix|inverse]] of the [[Definition:Square Matrix|square]] [[Definition:Cauchy Matrix|Cauchy matrix]] of [[Definition:Order of Square Matrix|order $n$]] is: :$\begin{bmatrix} b_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac {\displaystyle \prod_{k \mathop = 1}^n \paren {x_j + y_k} \paren {x_k + y_i} } {\displaystyle \paren {x_j + y_i} \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne j} } \paren {x_j - x_k} } \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne i} } \paren {y_i - x_k} } } \end{bmatrix}$ Thus $H_n^{-1}$ can be specified as: :$\begin{bmatrix} b_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac {\displaystyle \prod_{k \mathop = 1}^n \paren {i + k - 1} \paren {j + k - 1} } {\displaystyle \paren {i + j - 1} \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne i} } \paren {i - k} } \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne j} } \paren {j - k} } } \end{bmatrix}$ First, from [[Product of Products]]: :$\displaystyle \prod_{k \mathop = 1}^n \paren {i + k - 1} \paren {j + k - 1} = \prod_{k \mathop = 1}^n \paren {i + k - 1} \prod_{k \mathop = 1}^n \paren {j + k - 1}$ We address in turn the various [[Definition:Factor|factors]] of this [[Definition:Expression|expression]] for $b_{i j}$. {{begin-eqn}} {{eqn | l = \prod_{k \mathop = 1}^n \paren {i + k - 1} | r = \prod_{k \mathop = 0}^{n - 1} \paren {i + k} | c = [[Translation of Index Variable of Product]] }} {{eqn | r = i^{\overline n} | c = Definition of [[Definition:Rising Factorial|Rising Factorial]] }} {{eqn | r = \frac {\paren {i + n - 1}!} {\paren {i - 1}!} | c = [[Rising Factorial as Quotient of Factorials]] }} {{end-eqn}} and similarly: :$\displaystyle \prod_{k \mathop = 1}^n \paren {j + k - 1} = \frac {\paren {j + n - 1}!} {\paren {j - 1}!}$ Then: {{begin-eqn}} {{eqn | l = \prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne i} } \paren {i - k} | r = \paren {\prod_{1 \mathop \le k \mathop < i} \paren {i - k} } \paren {\prod_{i \mathop < k \mathop \le n} \paren {i - k} } | c = }} {{eqn | r = \paren {i - 1}! \paren {\prod_{i \mathop < k \mathop \le n} \paren {i - k} } | c = {{Defof|Factorial}} }} {{eqn | r = \paren {i - 1}! \paren {\prod_{0 \mathop < k \mathop \le n - i} \paren {-k} } | c = [[Translation of Index Variable of Product]] }} {{eqn | r = \paren {i - 1}! \paren {-1}^{n - i} \paren {\prod_{0 \mathop < k \mathop \le n - i} k} | c = }} {{eqn | r = \paren {i - 1}! \paren {-1}^{n - i} \paren {n - i}! | c = {{Defof|Factorial}} }} {{end-eqn}} and similarly: :$\displaystyle \prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne j} } \paren {j - k} = \paren {j - 1}! \paren {-1}^{n - j} \paren {n - j}!$ Thus we can write: {{begin-eqn}} {{eqn | l = \begin{bmatrix} b_{i j} \end{bmatrix} | r = \frac {\paren {\dfrac {\paren {i + n - 1}!} {\paren {i - 1}!} } \paren {\dfrac {\paren {j + n - 1}!} {\paren {j - 1}!} } } {\paren {i + j - 1} \paren {i - 1}! \paren {-1}^{n - i} \paren {n - i}! \paren {j - 1}! \paren {-1}^{n - j} \paren {n - j}!} | c = }} {{eqn | r = \frac {\paren {-1}^{i + j} \paren {i + n - 1}! \paren {j + n - 1}!} {\paren {\paren {i - 1}!}^2 \paren {\paren {j - 1}!}^2 \paren {n - i}! \paren {n - j}! \paren {i + j - 1} } | c = }} {{end-eqn}} {{qed}}	1
In the following, $\mathbf A$, $\mathbf B$ and $\mathbf C$ denote arbitrary [[Definition:Matrix|matrices]] in a given [[Definition:Matrix Space|matrix space]] $\map \MM {m, n}$ for $m, n \in \Z{>0}$. We check in turn each of the conditions for [[Definition:Equivalence Relation|equivalence]]: === Reflexive === Let $\kappa_i$ denote an arbitrary [[Definition:Column of Matrix|column]] of $\mathbf A$. Let $e$ denote the [[Definition:Elementary Column Operation|elementary column operation]] $\kappa_i \to 1 \kappa_i$ applied to $\mathbf A$. Then trivially: :$\map e {\mathbf A} = \mathbf A$ and so $\mathbf A$ is trivially [[Definition:Column Equivalence|column equivalent]] to itself. So [[Definition:Column Equivalence|column equivalence]] has been shown to be [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetric === Let $\mathbf A$ be [[Definition:Column Equivalence|column equivalent]] to $\mathbf B$. Let $\Gamma$ be the [[Definition:Column Operation|column operation]] that transforms $\mathbf A$ into $\mathbf B$. From [[Column Operation has Inverse]] there exists a [[Definition:Column Operation|column operation]] $\Gamma'$ which transforms $\mathbf B$ into $\mathbf A$. Thus $\mathbf B$ is [[Definition:Column Equivalence|column equivalent]] to $\mathbf A$. So [[Definition:Column Equivalence|column equivalence]] has been shown to be [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitive === Let $\mathbf A$ be [[Definition:Column Equivalence|column equivalent]] to $\mathbf B$, and let $\mathbf B$ be [[Definition:Column Equivalence|column equivalent]] to $\mathbf C$. Let $\Gamma_1$ be the [[Definition:Column Operation|column operation]] that transforms $\mathbf A$ into $\mathbf B$. Let $\Gamma_2$ be the [[Definition:Column Operation|column operation]] that transforms $\mathbf B$ into $\mathbf C$. From [[Sequence of Column Operations is Column Operation]], the application of $\mathbf C$ is [[Definition:Column Equivalence|column equivalent]] to $\mathbf A$. So [[Definition:Column Equivalence|column equivalence]] has been shown to be [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} [[Definition:Column Equivalence|Column equivalence]] has been shown to be [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. Hence by definition it is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
Let: :$\mathbf A = \begin{bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 r} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & a_{2 r} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n r} & \cdots & a_{n n} \\ \end{bmatrix}$ :$\mathbf B = \begin{bmatrix} b_{1 1} & b_{1 2} & \cdots & b_{1 r} & \cdots & b_{1 n} \\ b_{2 1} & b_{2 2} & \cdots & b_{2 r} & \cdots & b_{1 n} \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ b_{n 1} & b_{n 2} & \cdots & b_{n r} & \cdots & b_{n n} \\ \end{bmatrix} = \begin{bmatrix} a_{1 1} & a_{1 2} & \cdots & c a_{1 r} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & c a_{2 r} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n r} & \cdots & a_{n n} \\ \end{bmatrix}$ We have that: :$\mathbf A^\intercal = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} & a_{r 2} & \cdots & a_{r n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end {bmatrix}$ where $\mathbf A^\intercal$ denotes the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Similarly, we have that: :$\mathbf B^\intercal = \begin{bmatrix} a_{1 1} & a_{1 2} & \ldots & a_{1 n} \\ a_{2 1} & a_{2 2} & \ldots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ c a_{r 1} & c a_{r 2} & \cdots & c a_{r n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end {bmatrix}$ From [[Determinant with Row Multiplied by Constant]]: :$\map \det {\mathbf B^\intercal} = c \map \det {\mathbf A^\intercal}$ From from [[Determinant of Transpose]]: :$\map \det {\mathbf B^\intercal} = \map \det {\mathbf B}$ :$\map \det {\mathbf A^\intercal} = \map \det {\mathbf A}$ and the result follows. {{qed}}	1
Let $\left({S, \ast_1, \ast_2, \ldots, \ast_n, \circ}\right)_R$ be an [[Definition:R-Algebraic Structure|$R$-algebraic structure]]. Let $\phi: S \to S$ be an [[Definition:R-Algebraic Structure Isomorphism|$R$-algebraic structure isomorphism]] from $S$ to itself. Then $\phi$ is an [[Definition:R-Algebraic Structure Automorphism|$R$-algebraic structure automorphism]]. This definition continues to apply when $S$ is a [[Definition:Module|module]], and also when it is a [[Definition:Vector Space|vector space]].	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$. Let $f$ be a [[Definition:Bilinear Form|bilinear form]] on $V$. Let $f$ be [[Definition:Reflexive Bilinear Form|reflexive]]. Then $f$ is [[Definition:Symmetric Bilinear Form|symmetric]] or [[Definition:Alternating Bilinear Form|alternating]].	1
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N_{> 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$A^n \mathbf v = \lambda^n \mathbf v$ === Basis for the Induction === $P \left({1}\right)$ is true, as this just says: :$A \mathbf v = \lambda \mathbf v$ which follows by definition of [[Definition:Eigenvector|eigenvector]]. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$A^k \mathbf v = \lambda^k \mathbf v$ Then we need to show: :$A^{k+1} \mathbf v = \lambda^{k+1} \mathbf v$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \lambda^{k+1} \mathbf v | r = \lambda \cdot \lambda^k \mathbf v }} {{eqn | r = \lambda A^k \mathbf v | c = [[Eigenvalue of Matrix Powers#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = A^k \lambda \mathbf v }} {{eqn | r = A^k A \mathbf v | c = [[Eigenvalue of Matrix Powers#Basis for the Induction|Basis for the Induction]] }} {{eqn | r = A^{k+1} {\mathbf v} }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k+1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \N_{> 0}: A^n \mathbf v = \lambda^n \mathbf v$ {{Qed}} [[Category:Matrix Algebra]] 888f16pnpzouxje4om8ew0eanyi2lvl	1
Let $S$ be a [[Definition:Simultaneous Linear Equations|system of $m$ simultaneous linear equations in $n$ variables]]: :$\displaystyle \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$ Let $S$ be expressed in [[Definition:Matrix Representation of Simultaneous Linear Equations|matrix form]] as: :$\mathbf A \mathbf x = \mathbf b$ where: :$\mathbf A = \begin {pmatrix} \alpha_{1 1} & \alpha_{1 2} & \cdots & \alpha_{1 n} \\ \alpha_{2 1} & \alpha_{2 2} & \cdots & \alpha_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ \alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m n} \\ \end {pmatrix}$, $\mathbf x = \begin {pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $\mathbf b = \begin {pmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end {pmatrix}$ Then $S$ has exactly one [[Definition:Solution to System of Simultaneous Equations|solution]] {{iff}}: :$\map \rho {\mathbf A} = n$ where $\map \rho {\mathbf A}$ denotes the [[Definition:Rank of Matrix|rank]] of $\mathbf A$.	1
Let $G$ be the [[Definition:Cartesian Product|cartesian product]] of a [[Definition:Sequence|sequence]] $\sequence {G_n}$ of [[Definition:Module|$R$-modules]]. Then for each $j \in \closedint 1 n$, the [[Definition:Canonical Injection (Abstract Algebra)|canonical injection]] $\inj_j$ from $G_j$ into $G$ is a [[Definition:R-Algebraic Structure Monomorphism|monomorphism]].	1
[[Definition:P-Sequence Space|$\ell^1$ space]] is a [[Definition:Separable Space/Normed Vector Space|separable space]].	1
Let $S$ be the [[Definition:Set|set]] of all [[Definition:Vector|vectors]] in a [[Definition:Vector Space|vector space]] of [[Definition:Dimension (Linear Algebra)|dimension]] $3$. Let $\times$ denote the [[Definition:Vector Cross Product|cross product operation]]. Then the [[Definition:Algebraic Structure|algebraic structure]] $\struct {S, +, \times}$ is not a [[Definition:Ring (Abstract Algebra)|ring]].	1
Let $x, y \in \R$ be [[Definition:Real Number|real numbers]]. Let $\size x$ denote the [[Definition:Absolute Value|absolute value]] of $x$. Then: :$\size {x + y} \le \size x + \size y$	1
The [[Definition:Sufficient Condition|sufficient condition]] is proved in [[Vector Scaled by Zero is Zero Vector]], and in [[Zero Vector Scaled is Zero Vector]]. The [[Definition:Necessary Condition|necessary condition]] is proved in [[Vector Product is Zero only if Factor is Zero]]. {{qed}}	1
'''Tensor theory''' is the branch of [[Definition:Linear Algebra|linear algebra]] which studies [[Definition:Tensor|tensors]].	1
Let $M$ be a [[Definition:Topological Space|topological space]]. Let $d$ and $k$ be [[Definition:Natural Number|natural numbers]]. Let $\mathcal A$ denote the [[Definition:Set|set]] of all $d$-[[Definition:Dimension of Atlas|dimensional]] [[Definition:Atlas|atlases]] of [[Definition:Class of Atlas|class]] $\mathcal C^k$ on $M$. Define a [[Definition:Relation|relation]] $\sim$ on $\mathcal A$ by putting, for any two [[Definition:Atlas|$\mathcal C^k$-atlases]] $\mathcal F$ and $\mathcal G$: :$\mathcal F \sim \mathcal G$ {{iff}} $\mathcal F$ and $\mathcal G$ are $C^k$-[[Definition:Compatible Atlases|compatible]]. Then $\sim$ is an [[Definition:Equivalence Relation|equivalence relation]] on $\mathcal A$.	1
Let $L: \alpha_1 x + \alpha_2 y = \beta$ be a [[Equation of Straight Line in Plane|straight line in $\R^2$]]. Then the straight line $L'$ is [[Definition:Parallel Lines|parallel]] to $L$ iff there is a $\beta' \in \R^2$ such that: :$L' = \set {\tuple {x, y} \in \R^2: \alpha_1 x + \alpha_2 y = \beta'}$	1
$T$ is [[Definition:Metacompact Space|metacompact]] {{iff}} every [[Definition:Open Cover|open cover]] of $S$ has an [[Definition:Open Refinement|open refinement]] which is [[Definition:Point Finite|point finite]]. $T$ is a [[Definition:Lindelöf Space|Lindelöf space]] if every [[Definition:Open Cover|open cover]] of $S$ has a [[Definition:Countable Subcover|countable subcover]]. Having established the definitions, we proceed. Let $\mathcal U$ be an [[Definition:Open Cover|open cover]] of $S$. Let $\mathcal V$ be a [[Definition:Point Finite|point finite]] [[Definition:Open Refinement|open refinement]] of $\mathcal U$. By [[Point Finite Set of Open Sets in Separable Space is Countable]], $\mathcal V$ is [[Definition:Countable Set|countable]]. Define a mapping $H$ on $\mathcal V$ thus: :$\forall V \in \mathcal V: H \left({V}\right) = \left\{{U \in \mathcal U: V \subseteq U}\right\}$ By [[Image of Countable Set under Mapping is Countable]], the [[Definition:Image of Mapping|image]] of $H$ is [[Definition:Countable Set|countable]]. Call this [[Definition:Image of Mapping|image]] $I$. Since $\mathcal V$ is a refinement of $\mathcal U$, $\varnothing \notin I$. By the [[Axiom:Axiom of Countable Choice|Axiom of Countable Choice]], $I$ has a [[Definition:Choice Function|choice function]] $c$. Then $G = c \circ H: \mathcal V \to \mathcal U$ is a mapping such that: :$\forall V \in \mathcal V: V \subseteq G \left({V}\right)$ Then $\mathcal Q = G \left({\mathcal V}\right)$ is [[Definition:Countable Set|countable]] by [[Image of Countable Set under Mapping is Countable]]. Each element of $\mathcal Q$ is an element of $\mathcal U$ by the definition of $G$. Let $x \in S$. Then since $\mathcal V$ is a [[Definition:Cover of Set|cover]] for $S$: : $\exists V \in \mathcal V: x \in V$ Then $x \in V \subseteq G \left({V}\right) \in \mathcal Q$. Thus $\mathcal Q$ is a [[Definition:Countable Subcover|countable subcover]] of $\mathcal U$. Thus each [[Definition:Open Cover|open cover]] of $S$ has a [[Definition:Countable Subcover|countable subcover]], so $T$ is a [[Definition:Lindelöf Space|Lindelöf space]]. {{qed}}	1
Let $\C$ be the [[Definition:Field of Complex Numbers|field of complex numbers]]. Let $\F$ be a [[Definition:Subfield|subfield]] of $\C$. Let $V$ be a [[Definition:Vector Space|vector space]] over $\F$ A '''semi-inner product''' is a [[Definition:Mapping|mapping]] $\innerprod \cdot \cdot: V \times V \to \mathbb F$ that satisfies the following properties: {{begin-axiom}} {{axiom | n = 1 | lc= [[Definition:Conjugate Symmetric Mapping|Conjugate Symmetry]] | q = \forall x, y \in V | m = \quad \innerprod x y = \overline {\innerprod y x} }} {{axiom | n = 2 | lc= [[Definition:Sesquilinear Form|Sesquilinearity]] | q = \forall x, y, z \in V, \forall a \in \mathbb F | m = \quad \innerprod {a x + y} z = a \innerprod x z + \innerprod y z }} {{axiom | n = 3 | lc= [[Definition:Non-Negative Definite Mapping|Non-Negative Definiteness]] | q = \forall x \in V | m = \quad \innerprod x x \in \R_{\ge 0} }} {{end-axiom}} {{refactor|This section merits a separate page: "Semi-Inner Product in Real Number Field", perhaps.}} If $\mathbb F$ is a [[Definition:Subfield|subfield]] of the [[Definition:Field of Real Numbers|field of real numbers]] $\R$, it follows from [[Complex Number equals Conjugate iff Wholly Real]] that $\overline {\innerprod y x} = \innerprod y x$ for all $x, y \in V$. Then $(1)$ above may be replaced by: {{begin-axiom}} {{axiom | n = 1^\prime | lc= [[Definition:Symmetric Mapping (Linear Algebra)|Symmetry]] | q = \forall x, y \in V | m = \innerprod x y = \innerprod y x }} {{end-axiom}} === [[Definition:Semi-Inner Product Space|Semi-Inner Product Space]] === {{:Definition:Semi-Inner Product Space}}	1
=== Euclidean 2-space === Define the [[Definition:Ordered Tuple|ordered 2-tuples]]: {{begin-eqn}} {{eqn | l = \mathbf i | r = \left\langle{1, 0}\right\rangle }} {{eqn | l = \mathbf j | r = \left\langle{0, 1}\right\rangle }} {{end-eqn}} From [[Standard Ordered Basis is Basis]], we have that any [[Definition:Vector (Euclidean Space)|vector in $\R^2$]] can be represented by: :$c_1 \mathbf i + c_2 \mathbf j$ where $c_1, c_2 \in \R$. This way of presenting vectors is called '''engineering notation'''. === Euclidean 3-space === Define the [[Definition:Ordered Tuple|ordered 3-tuples]]: {{begin-eqn}} {{eqn | l = \mathbf i | r = \left\langle{1, 0, 0}\right\rangle }} {{eqn | l = \mathbf j | r = \left\langle{0, 1, 0}\right\rangle }} {{eqn | l = \mathbf k | r = \left\langle{0, 0, 1}\right\rangle }} {{end-eqn}} By the same logic as the above definition, we can write any [[Definition:Vector (Euclidean Space)|vector in $\R^3$]] as: :$c_1 \mathbf i + c_2 \mathbf j + c_3 \mathbf k$ where $c_1, c_2, c_3 \in \R$. Note that $\mathbf i$ and $\mathbf j$ take on a different meaning in $3$-space than in $2$-space. === Euclidean $n$-space === In higher [[Definition:Dimension|dimensions]], rather than writing $\mathbf l, \mathbf m, \mathbf n$, and so on, the convention is to use: {{begin-eqn}} {{eqn | l = \mathbf e_1 | r = \left\langle{1, 0, 0, \ldots, 0, 0}\right\rangle }} {{eqn | l = \mathbf e_2 | r = \left\langle{0, 1, 0, \ldots, 0, 0}\right\rangle }} {{eqn | o = \vdots }} {{eqn | l = \mathbf e_n | r = \left\langle{0, 0, 0, \ldots, 0, 1}\right\rangle }} {{end-eqn}} Then any vector in $\R^n$ can be expressed as: :$c_1 \mathbf e_1 + c_2 \mathbf e_2 + \cdots + c_n \mathbf e_n$ where $c_1, c_2, \cdots, c_n \in \R$. This convention is also frequently seen for $2$-space and $3$-space.	1
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]] whose [[Definition:Ring Zero|zero]] is $0_D$. === $(1)$ implies $(2)$ === Let $x$ be an [[Definition:Irreducible Element of Ring/Definition 1|irreducible element of $\struct {D, +, \circ}$ by definition 1]]. By definition: :$x$ has no [[Definition:Non-Trivial Factorization|non-trivial factorization]] in $D$. Let $x = y \circ z$ for some $y, z \in D$. By definition, it cannot be the case that neither $y$ nor $z$ are [[Definition:Unit of Ring|units]] of $D$. So either $y$ or $z$ is a [[Definition:Unit of Ring|unit]] of $D$. {{WLOG}}, suppose $y$ is a [[Definition:Unit of Ring|unit]] of $D$. Then by definition $z$ is an [[Definition:Associate in Integral Domain|associate]] of $x$. Contrariwise, suppose $z$ is a [[Definition:Unit of Ring|unit]] of $D$. Then by definition $y$ is an [[Definition:Associate in Integral Domain|associate]] of $x$. Thus both $y$ and $z$ are either a [[Definition:Unit of Ring|unit]] of $D$ or an [[Definition:Associate in Integral Domain|associate]] of $x$. $x = y \circ z$ is an arbitrary [[Definition:Factorization|factorization]] of $x$ in $D$. It follows that the only [[Definition:Divisor of Ring Element|divisors]] of $x$ are its [[Definition:Associate in Integral Domain|associates]] and the [[Definition:Unit of Ring|units]] of $D$. Thus $x$ is an [[Definition:Irreducible Element of Ring/Definition 2|irreducible element of $\struct {D, +, \circ}$ by definition 2]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Let $x$ be an [[Definition:Irreducible Element of Ring/Definition 2|irreducible element of $\struct {D, +, \circ}$ by definition 2]]. Then by definition: :the only [[Definition:Divisor of Ring Element|divisors]] of $x$ are its [[Definition:Associate in Integral Domain|associates]] and the [[Definition:Unit of Ring|units]] of $D$. Let $x = y \circ z$. Then either: :$y$ is an [[Definition:Associate in Integral Domain|associate]] of $x$ and $z$ is a [[Definition:Unit of Ring|unit]] of $D$ or: :$z$ is an [[Definition:Associate in Integral Domain|associate]] of $x$ and $y$ is a [[Definition:Unit of Ring|unit]] of $D$. {{explain|Have we ruled out the possibility of $x {{=}} y \circ z$ where both $y$ and $z$ are [[Definition:Associate in Integral Domain|associates]] of $x$ but neither are [[Definition:Unit of Ring|units]]?}} In either case, $y \circ z$ is a [[Definition:Trivial Factorization|trivial factorization]] of $x$. Thus $x$ is an [[Definition:Irreducible Element of Ring/Definition 1|irreducible element of $\struct {D, +, \circ}$ by definition 1]]. {{qed}} [[Category:Irreducible Elements of Rings]] qei69jydym7qnq0d0a8fox7am92d2z3	1
=== Existence === The existence of $C$ follows immediately from the definition of a [[Definition:Basis (Linear Algebra)|basis]]. {{qed|lemma}} === Uniqueness === Let $C, D$ be [[Definition:Finite Set|finite]] [[Definition:Subset|subsets]] of $R \times B$ satisfying the requirements. Let $Q = \left\{{v: \exists r \in R: \left({r, v}\right) \in C \cup D}\right\}$. Let $V' = \operatorname{span} \left({Q}\right)$. Then $V'$ is a [[Definition:Finite Dimensional Vector Space|finite-dimensional vector space]] with [[Definition:Basis (Linear Algebra)|basis]] $Q$ and $x \in V'$. Thus the theorem follows from [[Expression of Vector as Linear Combination from Basis is Unique]]. {{explain}} {{qed}} [[Category:Vector Spaces]] [[Category:Linear Algebra]] 0cxx8banz3dgsvjrbnh5fx843yhiku0	1
The [[Definition:Inverse Matrix|inverse]] of the [[Definition:Unit Matrix|unit matrix]] $\mathbf I_n$ of [[Definition:Order of Square Matrix|order]] $n$ is $\mathbf I_n$. That is, a [[Definition:Unit Matrix|unit matrix]] it its own [[Definition:Inverse Matrix|inverse]].	1
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by $\norm {\,\cdot\,}$]]. Then $d$ is a [[Definition:Non-Archimedean Metric|non-Archimedean metric]].	1
By [[Definition:Definition|definition]], a [[Definition:Subset|subset]] $S \subseteq X$ is [[Definition:Open Set in Normed Vector Space|open]] if: :$\forall x \in X : \exists \epsilon \in \R_{>0} : \map {B_\epsilon} x \subseteq S$ Let $S = X$. {{AimForCont}} $X$ is [[Definition:Negation|not]] [[Definition:Open Set in Normed Vector Space|open]]. By [[De Morgan's Laws (Predicate Logic)|De Morgan's laws]]: :$\exists x \in X : \forall \epsilon \in \R_{>0} : \map {B_\epsilon} x \cap \paren {X \setminus X} \ne \O$ Note that: :$X \setminus X = \O$. By [[Intersection with Empty Set]]: :$\map {B_\epsilon} x \cap \O = \O$ Hence: :$\exists x \in X : \forall \epsilon \in \R_{>0} : \O \ne \O$. Since [[Empty Set is Unique]], we have a [[Definition:Contradiction|contradiction]]. {{qed}}	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\,\cdot\,}$ to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$, then: :$\forall \epsilon \in \R_{\gt 0}: \exists N \in \N : \forall n \ge N: \norm {x_n - l} < \epsilon$ Let $n_1$ satisfy: :$\forall n \ge n_1: \norm {x_n - l} < 1$ Then $\forall n \ge n_1$: {{begin-eqn}} {{eqn | l = \norm {x_n} | r = \norm {x_n - l + l} }} {{eqn | o = \le | r = \norm {x_n - l} + \norm l | c = [[Definition:Norm Axioms|Norm axiom (N3)]] (Triangle Inequality) }} {{eqn | o = \le | r = 1 + \norm l | c = as $n \ge n_1$ }} {{end-eqn}} Let $K = \max \set {\norm {x_1}, \norm {x_2}, \dots, \norm {x_{n_1 - 1} }, 1 + \norm l}$. Then: :$\forall n < n_1: \norm {x_n} \le K$ :$\forall n \ge n_1: \norm {x_n} \le 1 + \norm l \le K$ Thus, by definition, $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
From [[Ring is Commutative iff Opposite Ring is Itself]], $\struct {R, +_R, \times_R}$ is its own [[Definition:Opposite Ring|opposite ring]]. From [[Left Module over Ring Induces Right Module over Opposite Ring]], $\struct{G, +_G, \circ’}$ is a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. {{qed}}	1
Let $M$ be a [[Definition:Topological Space|topological space]]. Let $A$ be a $d$-[[Definition:Dimension of Atlas|dimensional]] [[Definition:Atlas|atlas]] of [[Definition:Class of Atlas|class]] $C^k$. Then $A$ is contained in a [[Definition:Unique|unique]] [[Definition:Maximal Atlas|maximal atlas]] of class $C^k$.	1
Let $\struct{R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $a \in R$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map {{B_\epsilon}^-} {a; \norm {\,\cdot\,} }$ denote the [[Definition:Closed Ball of Normed Division Ring|$\epsilon$-closed ball of $a$]] in $\struct {R, \norm {\,\cdot\,} }$. Let $\map {B_\epsilon} {a; \norm {\,\cdot\,} }$ denote the [[Definition:Open Ball of Normed Division Ring|$\epsilon$-open ball of $a$]] in $\struct {R, \norm {\,\cdot\,} }$. Let $\map {S_\epsilon} {a; \norm {\,\cdot\,} }$ denote the [[Definition:Sphere in Normed Division Ring|$\epsilon$-sphere of $a$]] in $\struct {R, \norm {\,\cdot\,} }$. Then: :$\map {S_\epsilon} {a; \norm {\,\cdot\,} } = \map { {B_\epsilon}^-} {a; \norm {\,\cdot\,} } \setminus \map {B_\epsilon} {a; \norm {\,\cdot\,} }$	1
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]] with [[Definition:Ring Zero|zero]] $0_R$ and [[Definition:Unity of Ring|unity]] $1_R$. Let $\OO$ be the [[Definition:Valuation Ring Induced by Non-Archimedean Norm|valuation ring induced]] by the [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$, that is: :$\OO = \set{x \in R : \norm x \le 1}$ Let $\PP$ be the [[Definition:Valuation Ideal Induced by Non-Archimedean Norm|valuation ideal induced]] by the [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$, that is: :$\PP = \set{x \in R : \norm x < 1}$ Then $\PP$ is an [[Definition:Ideal of Ring|ideal]] of $\OO$: :$(a):\quad \PP$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] :$(b):\quad \PP$ is a [[Definition:Maximal Right Ideal of Ring|maximal right ideal]] :$(c):\quad$ the [[Definition:Quotient Ring|quotient ring]] $\OO / \PP$ is a [[Definition:Division Ring|division ring]].	1
Let $A = \tuple {x_1, y_1}, B = \tuple {x_2, y_2}, C = \tuple {x_3, y_3}$ be [[Definition:Point|points]] in the [[Definition:Cartesian Plane|Cartesian plane]]. The [[Definition:Area|area]] $\mathcal A$ of the [[Definition:Triangle (Geometry)|triangle]] whose [[Definition:Vertex of Polygon|vertices]] are at $A$, $B$ and $C$ is given by: :$\mathcal A = \dfrac 1 2 \size {\paren {\begin{vmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \\ \end{vmatrix} } }$	1
Let $\mathbf A = \left[{a}\right]_{m n}$ be an [[Definition:Matrix|$m \times n$ matrix]]. Let $\mathbf B = \left[{b}\right]_{n p}$ be an [[Definition:Matrix|$n \times p$ matrix]]. Let the [[Definition:Row Sum|row sum]] of $\mathbf A$ and $\mathbf B$ be equal to $1$. Then the [[Definition:Row Sum|row sum]] of their [[Definition:Matrix Product (Conventional)|(conventional) product]] is also $1$.	1
Let $z = x + i y$. Let $a = l - i m$. Then: {{begin-eqn}} {{eqn | l = \map \Re {a z} | r = 1 | c = }} {{eqn | ll= \leadsto | l = \dfrac {\paren {a z + \overline {a z} } } 2 | r = 1 | c = [[Sum of Complex Number with Conjugate]] }} {{eqn | ll= \leadsto | l = a z + \overline a \cdot \overline z | r = 2 | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | ll= \leadsto | l = \paren {l - i m} \paren {x + i y} + \paren {l + i m} \paren {x - i y} | r = 2 | c = {{Defof|Complex Conjugate}} }} {{eqn | ll= \leadsto | l = \paren {\paren {l x + m y} + i \paren {l y - m x} } + \paren {\paren {l x + m y} - i \paren {l y - m x} } | r = 2 | c = {{Defof|Complex Multiplication}} }} {{eqn | ll= \leadsto | l = l x + m y | r = 1 | c = simplifying }} {{end-eqn}} {{qed}}	1
=== Definition 1 implies Definition 2 === Let $\mathbf a \cdot \mathbf b$ be a [[Definition:Dot Product/Definition 1|dot product by definition 1]]. From [[Cosine Formula for Dot Product]]: :$\mathbf a \cdot \mathbf b = \norm {\mathbf a} \norm {\mathbf b} \cos \theta$ where $\theta$ is the [[Definition:Angle Between Vectors|angle between $\mathbf v$ and $\mathbf w$]]. Thus $\cdot$ is a [[Definition:Dot Product/Definition 2|dot product by definition 2]]. {{qed|lemma}} === Definition 2 implies Definition 1 === Let $\mathbf a \cdot \mathbf b$ be a [[Definition:Dot Product/Definition 2|dot product by definition 2]]. Let $\mathbf a$ and $\mathbf b$ be expressed in the form: {{begin-eqn}} {{eqn | l = \mathbf a | r = a_1 \mathbf e_1 + a_2 \mathbf e_2 + \cdots + a_n \mathbf e_n }} {{eqn | r = \sum_{i \mathop = 1}^n a_i \mathbf e_i }} {{eqn | l = \mathbf b | r = b_1 \mathbf e_1 + b_2 \mathbf e_2 + \cdots + b_n \mathbf e_n }} {{eqn | r = \sum_{j \mathop = 1}^n b_j \mathbf e_j }} {{end-eqn}} Then we have: {{begin-eqn}} {{eqn | l = \mathbf a \cdot \mathbf b | r = \paren {\sum_{i \mathop = 1}^n a_i \mathbf e_i} \cdot \paren {\sum_{j \mathop = 1}^n b_j \mathbf e_j} | c = }} {{eqn | r = \sum_{i, j \mathop = 1}^n a_i b_j \mathbf e_i \mathbf e_j | c = [[Dot Product Distributes over Addition]] }} {{eqn-intertext | noting at this point that [[Dot Product Distributes over Addition]] has been derived from {{Defof|Dot Product|index = 2}} }} {{eqn | r = \sum_{i, j \mathop = 1}^n a_i b_j \delta_{i j} | c = [[Dot Product of Orthogonal Basis Vectors]] }} {{eqn-intertext | noting at this point that [[Dot Product of Orthogonal Basis Vectors]] has also ultimately been derived from {{Defof|Dot Product|index = 2}} }} {{eqn | r = \sum_{i \mathop = 1}^n a_i b_i | c = simplifying }} {{end-eqn}} Thus $\cdot$ is a [[Definition:Dot Product/Definition 1|dot product by definition 1]]. {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M, N$ be [[Definition:Free Module|free $R$-modules]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $m, n > 0$ respectively. Let $\mathcal A, \mathcal B$ be [[Definition:Ordered Basis (Linear Algebra)|ordered bases]] of $M$ and $N$ respectively. Let $f: M \to N$ be a [[Definition:Linear Transformation|linear transformation]]. Let $\mathbf M_{f, \mathcal B, \mathcal A}$ be its [[Definition:Relative Matrix of Linear Transformation|matrix relative to]] $\mathcal A$ and $\mathcal B$. Then for all $m \in M$: :$\left[{f \left({m}\right)}\right]_{\mathcal B} = \mathbf M_{f, \mathcal B, \mathcal A} \cdot \left[{m}\right]_{\mathcal A}$ where $\left[{\, \cdot \,}\right]_{-}$ denotes the [[Definition:Coordinate Vector|coordinate vector]] with respect to a [[Definition:Basis (Linear Algebra)|basis]].	1
=== Necessary Condition === {{ProofWanted}} === Sufficient Condition === Let $P' \ne P$ be a plane given by the equation: :$\alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma'$ Suppose we have a point $\mathbf x = \left({x_1, x_2, x_3}\right) \in P \cap P'$. Then, as $\mathbf x \in P$, it also satisfies: :$\alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma$ It follows that $\gamma = \gamma'$, so $P = P'$. This contradiction shows that $P \cap P' = \varnothing$, i.e., $P$ and $P'$ are [[Definition:Parallel Planes|parallel]]. The remaining case is when $P' = P$. By definition, $P$ is parallel to itself. The result follows. {{qed}}	1
By [[Definition:Equivalent Division Ring Norms by Cauchy Sequence|Cauchy Sequence Equivalence]], for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm{\,\cdot\,}_1$ {{iff}} $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm{\,\cdot\,}_2$. By [[Definition:Equivalent Division Ring Norms by Convergence|Convergent Equivalence]], for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] in $\norm{\,\cdot\,}_1$ {{iff}} $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] in $\norm{\,\cdot\,}_2$. Hence: :every [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm{\,\cdot\,}_1$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] in $\norm{\,\cdot\,}_1$ {{iff}} every [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm{\,\cdot\,}_2$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] in $\norm{\,\cdot\,}_2$. The result follows. {{qed}} [[Category:Normed Division Rings]] [[Category:Complete Metric Spaces]] ekkypuxvfwisc4mndifp6qmzk0hcwwq	1
Let: :$V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 2} & x_1^{n - 1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 2} & x_2^{n - 1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 2} & x_n^{n - 1} \end{vmatrix}$ Let $\map f x$ be '''any''' [[Definition:Monic Polynomial|monic polynomial]] of [[Definition:Degree of Polynomial|degree]] $n - 1$: :$\ds \map f x = x^{n - 1} + \sum_{i \mathop = 0}^{n - 2} a_i x^i$ Apply [[Effect of Elementary Row Operations on Determinant|elementary column operations]] to $V_n$ repeatedly to show: :$V_n = W$ where: :$ W = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 2} & \map f {x_1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 2} & \map f {x_2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 2} & \map f {x_n} \end{vmatrix}$ Select a specific degree $n - 1$ monic polynomial: :$\ds \map f x = \prod_{k \mathop = 1}^{n - 1} \paren {x - x_k}$ The selected polynomial is zero at all values $x_1, \ldots, x_{n - 1}$. Then the last column of $W$ is all zeros except the entry $\map f {x_n}$. Expand $\map \det W$ by cofactors along the last column to prove: {{begin-eqn}} {{eqn | n = 1 | l = V_n | r = \map f {x_n} V_{n - 1} | c = [[Expansion Theorem for Determinants]] for columns }} {{eqn | r = V_{n - 1} \prod_{k \mathop = 1}^{n - 1} \paren {x_n - x_k} }} {{end-eqn}} For $n \ge 2$, let $\map P n$ be the statement: :$\ds V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {x_j - x_i}$ [[Definition:Mathematical Induction|Mathematical induction]] will be applied. === Basis for the Induction === By definition, determinant $V_1 = 1$. To prove $\map P 2$ is true, use equation $(1)$ with $n = 2$: :$V_2 = \paren {x_2 - x_1} V_1$ This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Step === This is the [[Definition:Induction Step|induction step]]: Let $\map P n$ is be assumed true. We are to prove that $\map P {n + 1}$ is true. As follows: {{begin-eqn}} {{eqn | l = V_{n + 1} | r = V_n \prod_{k \mathop = 1}^n \paren {x_{n + 1} - x_k} | c = from $(1)$, setting $n \to n + 1$ }} {{eqn | r = \prod_{1 \mathop \le m \mathop < k \mathop \le n} \paren {x_k - x_m} \prod_{k \mathop = 1}^n \paren {x_{n + 1} - x_k} | c = [[Vandermonde Determinant/Proof 4#Induction Hypothesis|Induction hypothesis]] with new indexing symbols: $i \to m, j \to k$ }} {{eqn | r = \prod_{1 \mathop \le i \mathop < j \mathop \le n + 1} \paren {x_j - x_i} | c = simplifying }} {{end-eqn}} Thus $\map P {n + 1}$ has been shown to be true. The induction is complete. {{qed}}	1
Let $\mathbf u, \mathbf v, \mathbf w$ be [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Vector Space|vector space]] $\R^n$. Let $c$ be a [[Definition:Real Number|real]] [[Definition:Scalar (Vector Space)|scalar]]. The [[Definition:Dot Product|dot product]] has the following properties:	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be an [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Column Operation|inverse]] of $e$. Then $e'$ is an [[Definition:Elementary Column Operation|elementary column operation]] which always exists and is [[Definition:Unique|unique]].	1
=== Construction of Algebra === From [[Real Numbers form Field]], $\left({\R, +, \times}\right)$ is a [[Definition:Field (Abstract Algebra)|field]]. Let this be expressed as $\left({\R, +_\R, \times_\R}\right)$ in order to call attention to the precise scope of the operators. From [[Real Numbers form Vector Space]], we have that $\left({\R^1, +, \cdot}\right)_\R$ is a [[Definition:Vector Space|vector space]], where: :the [[Definition:Field (Abstract Algebra)|field]] is $\left({\R, +_\R, \times_\R}\right)$ :the [[Definition:Abelian Group|abelian group]] is $\left({\R, +_G}\right)$ where $+_G$ is [[Definition:Real Addition|real addition]]. In [[Real Numbers form Vector Space]], it is established that elements of $\left({\R^1, +, \cdot}\right)_\R$ are in fact just [[Definition:Real Number|real numbers]]. So, let $\times$ be the [[Definition:Binary Operation|binary operation]] on $\left({\R^1, +, \cdot}\right)_\R$ defined as: :$\forall x, y \in \left({\R^1, +, \cdot}\right)_\R: x \times y = x \times_\R y$ where $\times_\R$ is [[Definition:Real Multiplication|real multiplication]]. === Proof of an Algebra === We need to show that $\times$ as defined on $\left({\R^1, +, \cdot}\right)_\R$ as: :$\forall x, y \in \left({\R^1, +, \cdot}\right)_\R = x \times_\R y$ is [[Definition:Bilinear Mapping|bilinear]]. That is: $\forall a, b \in \R, x, y \in \R^1$: : $\left({\left({a \cdot x}\right) + \left({b \cdot y}\right)}\right) \times z = \left({a \cdot \left({x \times z}\right)}\right) + \left({b \cdot \left({y \times z}\right)}\right)$ : $z \times \left({\left({a \cdot x}\right) + \left({b \cdot y}\right)}\right) = \left({a \cdot \left({z \times x}\right)}\right) + \left({b \cdot \left({z \times y}\right)}\right)$ So: {{begin-eqn}} {{eqn | l = \left({\left({a \cdot x}\right) + \left({b \cdot y}\right)}\right) \times z | r = \left({\left({a \times_\R x}\right) + \left({b \times_\R y}\right)}\right) \times_\R z | c = $\cdot$ and $\times$ are [[Definition:Real Multiplication|Real Multiplication]] }} {{eqn | r = \left({\left({a \times_\R x}\right) \times_\R z}\right) + \left({\left({b \times_\R y}\right) \times_\R z}\right) | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \left({a \times_\R \left({x \times_\R z}\right)}\right) + \left({b \times_\R \left({y \times_\R z}\right)}\right) | c = [[Real Multiplication is Associative]] }} {{eqn | r = \left({a \cdot \left({x \times z}\right)}\right) + \left({b \cdot \left({y \times z}\right)}\right) | c = $\cdot$ and $\times$ are [[Definition:Real Multiplication|Real Multiplication]] }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = z \times \left({\left({a \cdot x}\right) + \left({b \cdot y}\right)}\right) | r = z \times_\R \left({\left({a \times_\R x}\right) + \left({b \times_\R y}\right)}\right) | c = $\cdot$ and $\times$ are [[Definition:Real Multiplication|Real Multiplication]] }} {{eqn | r = \left({z \times_\R \left({a \times_\R x}\right)}\right) + \left({z \times_\R \left({b \times_\R y}\right)}\right) | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \left({\left({z \times_\R a}\right) \times_\R x}\right) + \left({\left({z \times_\R b}\right) \times_\R y}\right) | c = [[Real Multiplication is Associative]] }} {{eqn | r = \left({\left({a \times_\R z}\right) \times_\R x}\right) + \left({\left({b \times_\R z}\right) \times_\R y}\right) | c = [[Real Multiplication is Commutative]] }} {{eqn | r = \left({a \times_\R \left({z \times_\R x}\right)}\right) + \left({b \times_\R \left({z \times_\R y}\right)}\right) | c = [[Real Multiplication is Associative]] }} {{eqn | r = \left({a \cdot \left({z \times x}\right)}\right) + \left({b \cdot \left({z \times y}\right)}\right) | c = $\cdot$ and $\times$ are [[Definition:Real Multiplication|Real Multiplication]] }} {{end-eqn}} So the [[Definition:Real Number|set of real numbers]] forms an [[Definition:Algebra over Field|algebra]] $\left({\R, \times}\right)$. {{qed|lemma}} === Proof of Associativity === Elements of $\left({\R, \times}\right)$ are merely [[Definition:Real Number|real numbers]], and $\times$ is just [[Definition:Real Multiplication|real multiplication]]. Associativity of $\times$ therefore follows directly from [[Real Multiplication is Associative]]. {{qed|lemma}} === Proof of Commutativity === Elements of $\left({\R, \times}\right)$ are merely [[Definition:Real Number|real numbers]], and $\times$ is just [[Definition:Real Multiplication|real multiplication]]. Associativity of $\times$ therefore follows directly from [[Real Multiplication is Commutative]]. {{qed|lemma}} === Proof of Normed Division Algebra === Elements of $\left({\R, \times}\right)$ are merely [[Definition:Real Number|real numbers]], and $\times$ is just [[Definition:Real Multiplication|real multiplication]]. So from [[Real Multiplication Identity is One]], $\left({\R, \times}\right)$ has a [[Definition:Unit of Algebra|unit]], which is $1$. So $\left({\R, \times}\right)$ is a [[Definition:Unitary Algebra|unitary algebra]]. From [[Inverses for Real Multiplication]], every element of $\left({\R, \times}\right)$ except $0$ has a [[Definition:Multiplicative Inverse|multiplicative inverse]]. So $\left({\R, \times}\right)$ is a [[Definition:Division Algebra|division algebra]] and hence a [[Definition:Unitary Division Algebra|unitary division algebra]]. We define a [[Definition:Norm on Vector Space|norm]] on $\left({\R, \times}\right)$ by: :$\forall a \in \R: \left \Vert {a} \right \Vert = \left \vert {a} \right \vert = \sqrt {a^2}$ That is, by the [[Definition:Absolute Value|absolute value]] of $a$. This is a [[Definition:Norm on Vector Space|norm]] because: : $(1): \quad \left \Vert x \right \Vert = 0 \iff x = \mathbf 0$ : $(2): \quad \left \Vert \lambda x \right \Vert = \left \vert \lambda \right \vert \left \vert x \right \vert = \left \vert \lambda \right \vert \left \Vert x \right \Vert$ : $(3): \quad \left \Vert x - y \right \Vert \le \left \Vert x - z \right \Vert + \left \Vert z - y \right \Vert$ which follows from [[Real Number Line is Metric Space]]. It also follows that: : $\left \Vert x \times y \right \Vert = \left \vert x \times y \right \vert = \left \vert x \right \vert \times \left \vert y \right \vert = \left \Vert x \right \Vert \times \left \Vert y \right \Vert$ and so $\left({\R, \times}\right)$ is a [[Definition:Normed Division Algebra|normed division algebra]]. {{qed|lemma}} === Proof of Nicely Normed $*$-Algebra === We define the [[Definition:Conjugation (Abstract Algebra)|conjugation]] $*$ by making it the [[Definition:Identity Mapping|identity mapping]] on $\R$. That is: :$\forall a \in \R: a^* = a$ We have that: {{begin-eqn}} {{eqn | n = 1 | l = \left({a^*}\right)^* | r = a^* | c = Definition of $*$ }} {{eqn | r = a | c = Definition of $*$ }} {{end-eqn}} {{begin-eqn}} {{eqn | n = 2 | l = \left({a + b}\right)^* | r = a + b | c = Definition of $*$ }} {{eqn | r = b + a | c = [[Real Addition is Commutative]] }} {{eqn | r = b^* + a^* | c = }} {{end-eqn}} demonstrating that $*$ is indeed a [[Definition:Conjugation (Abstract Algebra)|conjugation]]. Then we have that: {{begin-eqn}} {{eqn | n = 3 | l = a + a^* | r = a + a | c = Definition of $*$ }} {{eqn | o = \in | r = \R | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = 4 | l = a \times a^* | r = a \times a | c = Definition of $*$ }} {{eqn | r = a^2 | c = }} {{eqn | o = > | r = 0 | c = [[Square of Real Number is Non-Negative]] }} {{end-eqn}} Similarly for $a^* + a$. Trivially, $a^* + a$ and $a \times a^*$ are both [[Definition:Real Number|real]]. So $\left({\R, \times}\right)$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. {{qed|lemma}} === Proof of Real $*$-Algebra === By definition of $*$: :$\forall a \in \R: a^* = a$ Hence, trivially: :$\forall a \in \R: a^* \in \R$ That is, $\left({\R, \times}\right)$ is a [[Definition:Real Star-Algebra|real $*$-algebra]]. {{qed}} [[Category:Real Numbers]] [[Category:Algebras]] g52sstetggd37595ci3qfc76fbeeapi	1
:$\norm {x - y} \le \norm x + \norm y$	1
{{AimForCont}} this supposition is false. Let $m$ be the smallest [[Definition:Integer|integer]] which can not be expressed as the [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. As a [[Definition:Prime Number|prime number]] is trivially a [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]], $m$ can not itself be [[Definition:Prime Number|prime]]. Hence: :$\exists r, s \in \Z: 1 < r < m, 1 < s < m: m = r s$ As $m$ is our [[Principle of Least Counterexample|least counterexample]], both $r$ and $s$ can be expressed as the [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. Say $r = p_1 p_2 \cdots p_k$ and $s = q_1 q_2 \cdots q_l$, where all of $p_1, \ldots, p_k, q_1, \ldots, q_l$ are [[Definition:Prime Number|prime]]. Hence $m = r s = p_1 p_2 \cdots p_k q_1 q_2 \cdots q_l$, which is a [[Definition:Integer Multiplication|product]] of [[Definition:Prime Number|primes]]. Hence there is no such counterexample. {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $A \in B \left({H, K}\right)$ be a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. Then $A$ is an [[Definition:Isometry (Hilbert Spaces)|isometry]] iff: :$A^*A = I_H$ where $A^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$, and $I_H$ the [[Definition:Identity Operator|identity operator]] on $H$.	1
{{AimForCont}} $C > 1$. By [[Characterizing Property of Supremum of Subset of Real Numbers]]: :$\exists m \in \N_{> 0}: \norm {m \cdot 1_R} > 1$ Let :$x = m \cdot 1_R$ :$y = x^{-1}$ By [[Properties of Norm on Division Ring/Norm of Inverse|Norm of Inverse]]: :$\norm y < 1$ By [[Sequence of Powers of Number less than One]]: :$\displaystyle \lim_{n \mathop \to \infty} \norm y^n = 0$ By [[Reciprocal of Null Sequence]] then: :$\displaystyle \lim_{n \mathop \to \infty} \frac 1 {\norm y^n} = +\infty$ For all $n \in \N_{> 0}$: {{begin-eqn}} {{eqn | l = \dfrac 1 {\norm y^n} | r = \norm {y^{-1} }^n | c = [[Properties of Norm on Division Ring/Norm of Inverse|Norm of Inverse]] }} {{eqn | r = \norm x^n | c = Definition of $y$ }} {{eqn | r = \norm {x^n} | c = {{NormAxiom|2}} }} {{eqn | r = \norm {\paren {m \cdot 1_R}^n} | c = Definition of $x$ }} {{eqn | r = \norm {m^n \cdot 1_R} }} {{end-eqn}} So: :$\displaystyle \lim_{n \mathop \to \infty} \norm {m^n \cdot 1_R} = +\infty$ Hence: :$\sup \set {\norm{n \cdot 1_R}: n \in \N_{> 0} } = +\infty$ This [[Definition:Contradiction|contradicts]] the assumption that $C < +\infty$. {{qed|lemma}} It follows that $C \le 1$. Then: :$\forall n \in \N_{>0}: \norm{n \cdot 1_R} \le 1$ By [[Characterisation of Non-Archimedean Division Ring Norms]], $\norm{\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. By [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 1|Corollary 1]]: :$\sup \set {\norm {n \cdot 1_R}: n \in \N_{> 0} } = 1$ So $C = 1$. {{qed}}	1
The [[Definition:Complex Plane|complex plane]] $\C$ is a [[Definition:Vector Space|vector space]] over $\C$. That the [[Definition:Norm Axioms (Vector Space)|norm axioms]] are satisfied is proven in [[Complex Modulus is Norm]]. Then we have [[Complex Plane is Complete Metric Space]]. Hence the result. {{qed}}	1
By definition, a [[Definition:Field (Abstract Algebra)|field]] is a [[Definition:Division Ring|division ring]]. From [[Null Ring is Ideal]] and [[Ring is Ideal of Itself]], it is always the case that $\set {0_F}$ and $\struct {F, +, \circ}$ are [[Definition:Ideal of Ring|ideals]] of $\struct {F, +, \circ}$. From [[Ideals of Division Ring]], it follows that the only [[Definition:Ideal of Ring|ideals]] of $\struct {F, +, \circ}$ are $\struct {F, +, \circ}$ and $\set {0_F}$. {{qed}}	1
:[[File:AreaOfTriangleComplex.png|400px]] Let $A$, $B$ and $C$ be defined as [[Definition:Complex Number|complex numbers]] in the [[Definition:Complex Plane|complex plane]]. The [[Definition:Complex Number as Vector|vectors]] from $C$ to $A$ and from $C$ to $B$ are given by: :$z_1 = \paren {x_1 - x_3} + i \paren {y_1 - y_3}$ :$z_2 = \paren {x_2 - x_3} + i \paren {y_2 - y_3}$ From [[Area of Triangle in Terms of Side and Altitude]], $\mathcal A$ is half that of a [[Definition:Parallelogram|parallelogram]] contained by $z_1$ and $z_2$. Thus: {{begin-eqn}} {{eqn | l = \mathcal A | r = \frac 1 2 z_1 \times z_2 | c = [[Area of Parallelogram in Complex Plane]] }} {{eqn | r = \frac 1 2 \size {\paren {\map \Im {\paren {x_1 - x_3} - i \paren {y_1 - y_3} } \paren {\paren {x_2 - x_3} - i \paren {y_2 - y_3} } } } | c = {{Defof|Vector Cross Product|subdef = Complex|index = 3}} }} {{eqn | r = \frac 1 2 \size {\paren {x_1 - x_3} \paren {y_2 - y_3} - \paren {y_1 - y_3} \paren {x_2 - x_3} } | c = {{Defof|Complex Multiplication}} }} {{eqn | r = \frac 1 2 \size {x_1 y_2 - y_1 x_2 + x_2 y_3 - y_2 x_3 + x_3 y_1 - y_3 x_1} | c = multiplying out }} {{eqn | r = \frac 1 2 \size {\paren {\begin{vmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \\ \end{vmatrix} } } | c = {{Defof|Determinant of Order 3}} }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A: H \to H$ be a [[Definition:Diagonalizable Operator|diagonalizable operator]]. Let $\left({\alpha_e}\right)_{e \in E}$ be the [[Definition:Diagonalizable Operator#Value Set|value set]] of $A$, with respect to a suitable [[Definition:Basis (Hilbert Space)|basis]] $E$ for $H$. Then $A$ is [[Definition:Bounded Linear Operator|bounded]] iff $\left({\alpha_e}\right)_{e \in E}$ is, i.e., iff: :$\exists M \in \R: \forall e \in E: \left\vert{\alpha_e}\right\vert \le M$	1
By [[Rational Numbers are Countably Infinite]], the [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] is [[Definition:Countably Infinite Set|countably infinite]]. By [[P-adic Numbers are Uncountable|P-adic Numbers are Uncountable]], the [[Definition:Set|set]] of [[Definition:P-adic Number|$p$-adic numbers $\Q_p$]] is [[Definition:Uncountable Set|uncountably infinite]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|commutative ring of Cauchy sequences]] over $\struct {\Q, \norm {\,\cdot\,}_p}$. Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence|null sequences]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. The [[Definition:P-adic Number/Quotient of Cauchy Sequences in P-adic Norm|$p$-adic numbers]] $\Q_p$ is the [[Definition:Quotient Ring|quotient ring]] $\CC \, \big / \NN$ by definition. By [[Embedding Division Ring into Quotient Ring of Cauchy Sequences]], the mapping $\phi: \Q \to \Q_p$ defined by: :$\map \phi r = \sequence {r, r, r, \dotsc} + \NN$ where $\sequence {r, r, r, \dotsc} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {r, r, r, \dotsc}$, is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|monomorphism]]. By [[Surjection from Natural Numbers iff Countable/Corollary 2|Corollary to Surjection from Natural Numbers iff Countable]] then $\phi$ is not a [[Definition:Surjection|surjection]]. Hence: :$\exists \sequence {x_n} \in \CC: \sequence {x_n} + \NN \not \in \map \phi \Q$ By [[Cauchy Sequence Converges Iff Equivalent to Constant Sequence]] then $\sequence {x_n}$ is not [[Definition:Convergent Sequence in Normed Division Ring|convergent]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. The result follows. {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be an [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$. Then $e'$ is an [[Definition:Elementary Row Operation|elementary row operation]] which always exists and is [[Definition:Unique|unique]].	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $F \subseteq X$ be [[Definition:Finite Set|finite]]. Then $F$ is [[Definition:Closed Set in Normed Vector Space|closed]] in $M$.	1
[[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] is an [[Definition:Equivalence Relation|equivalence relation]].	1
Taking the [[Definition:Group Axioms|group axioms]] in turn: === $G \, 0$: Closure === Let $\mathbf A, \mathbf B \in S$. By definition of [[Definition:Matrix Product (Conventional)|matrix product]], $\mathbf {A B}$ is a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. From [[Determinant of Matrix Product]]: :$\det \mathbf A \det \mathbf B = \map \det {\mathbf {A B} }$ Hence the [[Definition:Determinant of Matrix|determinant]] of $\mathbf {A B}$ is either $1$ or $-1$. Thus $\mathbf {A B} \in S$ and so $\struct {S, \times}$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed|lemma}} === $G \, 1$: Associativity === We have that [[Matrix Multiplication is Associative]]. Thus $\times$ is [[Definition:Associative|associative]] on $\struct {S, \times}$. {{qed|lemma}} === $G \, 2$: Identity === From [[Unit Matrix is Unity of Ring of Square Matrices]], the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$ serves as the [[Definition:Identity Element|identity element]] of $\struct {S, \times}$. {{qed|lemma}} === $G \, 3$: Inverses === Because the [[Definition:Determinant of Matrix|determinants]] of the [[Definition:Element|elements]] of $S$ are $1$ or $-1$, they are by definition [[Definition:Invertible Matrix|invertible]]. We have that $\mathbf I$ is the [[Definition:Identity Element|identity element]] of $\struct {\R, \circ}$. From the definition of [[Definition:Invertible Matrix|invertible matrix]], the [[Definition:Inverse Element|inverse]] of any [[Definition:Invertible Matrix|invertible matrix]] $\mathbf A$ is $\mathbf A^{-1}$. {{qed|lemma}} All the [[Definition:Group Axioms|group axioms]] are thus seen to be fulfilled, and so $\struct {S, \times}$ is a [[Definition:Group|group]]. {{qed}}	1
If $L \equiv 0$ identically, then $L h = 0 = \innerprod h 0$, and the theorem holds. Otherwise, set: :$M = \map \ker L = \map {L^{-1} } {\set 0}$ Then $M$ is a subspace. Because $L$ is bounded, [[Continuity of Linear Functionals|it is continuous]]. Because $\set 0$ is closed, [[Continuity Defined from Closed Sets|the continuity of $L$ implies that $M$ is closed]]. Then we can [[Direct Sum of Subspace and Orthocomplement|decompose $H$ as a direct sum]]: :$H \cong M \oplus M^\perp$ As $L \not \equiv 0$: :$M^\perp \ne \set 0$ Choose a $z \in M^\perp$ with norm $1$. By linearity of $L$, for any $h \in H$: {{begin-eqn}} {{eqn | l = L \paren {z L h - h L z} | r = L z L h - L h L z }} {{eqn | r = 0 }} {{end-eqn}} So: :$z L h - h L z \in \ker L = M$ Then: {{begin-eqn}} {{eqn | l = L h | r = L h \innerprod z z | c = as $\norm z = 1$ }} {{eqn | r = \innerprod {z L h} z | c = linearity in the first argument }} {{eqn | r = \innerprod {z L h - h L z + h L z} z | c = adding and subtracting $h L z$ in the first argument }} {{eqn | r = \innerprod {z L h - h L z} z + \innerprod {h L z} z | c = linearity in the first argument }} {{eqn | r = \innerprod {h L z} z | c = $z L h - h L z \in M, z \in M^\perp$ }} {{eqn | r = \innerprod h {z \paren {L z}^*} | c = conjugate symmetry }} {{end-eqn}} Thus $L h = \innerprod h {h_0}$ for $h_0 = z (Lz)^*$. To show uniqueness, assume $h_0$ and $h_1$ both satisfy the above equation for all $h \in H$: {{begin-eqn}} {{eqn | l = \innerprod h {h_0} | r = \innerprod h {h_1} }} {{eqn | ll= \leadsto | l = \innerprod h {h_0} - \innerprod h {h_1} | r = 0 }} {{eqn | r = \innerprod h {h_0 - h_1} | c = additivity in the second argument }} {{end-eqn}} The result follows from Setting $h = h_0 - h_1$ and invoking the positive definiteness of the inner product. {{qed}} {{Namedfor|Frigyes Riesz|cat = Riesz F}}	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,}}$. Thus, by definition, $\sequence {x_n} $ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, d}$. By [[Convergent Sequence in Metric Space is Cauchy Sequence|Convergent Sequence is Cauchy Sequence in metric space]], $\sequence {x_n} $ is a [[Definition:Cauchy Sequence in Metric Space|Cauchy sequence]] in $\struct {R, d}$. Thus, by definition, $\sequence {x_n} $ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {R, \norm {\,\cdot\,}}$. {{qed}}	1
Let $\phi: \R^\R \to \R^\R, y \mapsto \phi\left({y}\right)$ be a [[Definition:Linear Operator|linear operator]] on the space of functions from $\R\to\R$. Let $y$ be a [[Definition:Real Function|real function]] such that $\forall x \in \R$, $y\left({x}\right) > \mathbf{0}\left({x}\right) = 0$. Let $\log_a y$ be the [[Definition:General Logarithm|logarithm of $y$ to base $a$]]. Then: :$\phi \left({\log_a y}\right) = \dfrac 1 {\ln a} \left({\phi \left({\ln y}\right)}\right)$ where $\ln$ is the [[Definition:Natural Logarithm|natural logarithm]].	1
Let $\left({G, +_G, \circ}\right)_R$ be an [[Definition:Module|$R$-module]]. Let $\left({H, +_H, \circ}\right)_R$ be an [[Definition:R-Algebraic Structure|$R$-algebraic structure]]. Let $\phi: G \to H$ be an [[Definition:R-Algebraic Structure Epimorphism|epimorphism]]. Then $H$ is an [[Definition:Module|$R$-module]].	1
By [[Elementary Matrix corresponding to Elementary Column Operation/Scale Column|Elementary Matrix corresponding to Elementary Column Operation: Scale Column]], the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to $e_1$ is of the form: :$E_{a b} = \begin {cases} \delta_{a b} & : a \ne k \\ \lambda \cdot \delta_{a b} & : a = k \end{cases}$ where: :$E_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf E_1$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$ :$\delta_{a b}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: ::$\delta_{a b} = \begin {cases} 1 & : \text {if $a = b$} \\ 0 & : \text {if $a \ne b$} \end {cases}$ Thus when $a \ne b$, $E_{a b} = 0$. This means that $\mathbf E_1$ is a [[Definition:Diagonal Matrix|diagonal matrix]]. {{begin-eqn}} {{eqn | l = \displaystyle \map \det {\mathbf E_1} | r = \prod_i E_{i i} | c = [[Determinant of Diagonal Matrix]] | cc= where the [[Definition:Index Variable of Indexed Product|index variable]] $i$ ranges over the [[Definition:Order of Square Matrix|order]] of $\mathbf E_1$ }} {{eqn | r = \prod_i \paren {\begin {cases} 1 & : i \ne k \\ \lambda & : a = k \end{cases} } | c = }} {{eqn | r = \prod_{i \mathop \ne k} 1 \times \prod_{i \mathop = k} \lambda | c = }} {{eqn | r = 1 \times \lambda | c = }} {{eqn | r = \lambda | c = }} {{end-eqn}} {{qed}}	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $p$ be an [[Definition:Irreducible Element of Ring|irreducible element]] of $D$. Let $\ideal p$ be the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $p$]]. Then $\ideal p$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$.	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Then the [[Definition:Empty Set|empty set]] $\O$ is an [[Definition:Open Set in Normed Vector Space|open set]] of $M$.	1
A [[Definition:Plane|plane]] $P$ is the [[Definition:Set|set]] of all $\tuple {x, y, z} \in \R^3$, where: :$\alpha_1 x + \alpha_2 y + \alpha_3 z = \gamma$ where $\alpha_1, \alpha_2, \alpha_3, \gamma \in \R$ are given, and not all of $\alpha_1, \alpha_2, \alpha_3$ are [[Definition:Zero (Number)|zero]].	1
It will be shown that the two [[Definition:Matrix|matrices]] defined are equal [[Definition:Column of Matrix|column-wise]]. Let $\displaystyle b_i = \sum_{j \mathop = 1}^n c_{i j} a_j$ for $i$ ranging from $1$ to $n$, where $c_{i j}$'s are [[Definition:Scalar (Module)|scalars]]. The [[Definition:Unique|uniqueness]] of the above expression is justified by [[Expression of Vector as Linear Combination from Basis is Unique]]. Then by definition of [[Definition:Coordinate Vector|coordinate vector]], the $i$th [[Definition:Column of Matrix|column of the matrix]] defined in [[Definition:Change of Basis Matrix/Definition 1|definition 1]] is: :$\begin {bmatrix} c_{i 1} & c_{i 2} & \ldots & c_{in} \end {bmatrix}^T$ We also have: :$\displaystyle \map {I_G} {b_i} = b_i = \sum_{j \mathop = 1}^n c_{i j} a_j$ So by definition of [[Definition:Relative Matrix|relative matrix]], the $i$th [[Definition:Column of Matrix|column of the matrix]] defined in [[Definition:Change of Basis Matrix/Definition 2|definition 2]] is: :$\begin {bmatrix} c_{i 1} & c_{i 2} & \ldots & c_{i n} \end {bmatrix}^T$ The two [[Definition:Matrix|matrices]] are equal, so the two definitions are [[Definition:Logical Equivalence|equivalent]]. {{qed}} [[Category:Linear Algebra]] [[Category:Matrix Theory]] 18lwx8an6rap6pmfy0ne495dys719ed	1
Let $\struct{R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $a \in R$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map { {B_\epsilon}^-} a$ be the [[Definition:Closed Ball of Normed Division Ring|closed $\epsilon$-ball of $a$]] in $\struct{R, \norm {\,\cdot\,} }$. Then: :$a \in \map { {B_\epsilon}^-} a$	1
From [[Inclusion Mapping on Subring is Monomorphism]], $i$ is a [[Definition:Ring Monomorphism|ring monomorphism]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\, \cdot \,}$. Let $d_S$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\, \cdot \,}_S$. Thus: {{begin-eqn}} {{eqn | lo = \forall x, y \in S : | l = \map d {\map i x, \map i y} | r = \map d {x, y} | c = {{Defof|Inclusion Mapping}} }} {{eqn | r = \norm {x - y} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm {x - y}_S | c = {{Defof|Normed Division Subring|Norm on Subring}} }} {{eqn | r = \map {d_S} {x, y} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{end-eqn}} It follows that $i$ is [[Definition:Distance-Preserving Mapping|distance-preserving]] by definition. {{qed}} [[Category:Completion of Normed Division Ring]] ijjb8zqa2nqgvagkiab34103jzk9rrw	1
This is a special case of a [[Definition:Direct Product of Vector Spaces|direct product of vector spaces]] where each of the $G_k$ is the [[Definition:Vector Space|$K$-vector space]] $K$. {{qed}}	1
Let $\left({D, +, \circ}\right)$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $\left({p}\right)$ be the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $p$]]. Let $\left({p}\right)$ be a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$. Then $p$ is [[Definition:Irreducible Element of Ring|irreducible]].	1
=== Necessary Condition === Let the [[Definition:Binary Operation|operation]] $\cdot$ be [[Definition:Closure (Abstract Algebra)|closed]] on $\struct {S, \cdot}$. Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of $\map {\MM_S} {m, n}$. Let $\sqbrk c_{m n} = \sqbrk a_{m n} + \sqbrk b_{m n}$. Then: :$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} = a_{i j} \circ b_{i j}$ Thus: :$\struct {S, \cdot}$ is [[Definition:Closed Algebraic Structure|closed]] {{iff}} $c_{i j} \in S$. From the definition of [[Definition:Hadamard Product|Hadamard product]], $\sqbrk c_{m n}$ has the same [[Definition:Order of Matrix|order]] as both $\sqbrk a_{m n}$ and $\sqbrk b_{m n}$. Thus it follows that: :$\sqbrk c_{m n} \in \map {\MM_S} {m, n}$ Thus $\struct {\map {\MM_S} {m, n}, +}$, as it is defined, is [[Definition:Closure (Abstract Algebra)|closed]]. {{qed|lemma}} === Sufficient Condition === Suppose $\struct {S, \cdot}$ is such that $\cdot$ is not [[Definition:Closure (Abstract Algebra)|closed]] on $\struct {S, \cdot}$. Then there exists $a$ and $b$ such that: :$a \cdot b \notin S$ Let $\mathbf A$ and $\mathbf B$ be [[Definition:Element|elements]] of $\map {\MM_S} {m, n}$ such that: :$a_{i j} = a$, $b_{i j} = b$ where: :$a_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf A$ :$b_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf B$ Then: :$a_{i j} \cdot b_{i j} \notin S$ That is: :$\mathbf A \circ \mathbf B \notin \map {\MM_S} {m, n}$ because (at least) one [[Definition:Element of Matrix|element]] of $\mathbf A \circ \mathbf B$ is not an element of $S$. That is, $\circ$ is not [[Definition:Closure (Abstract Algebra)|closed]] on $\map {\MM_S} {m, n}$. {{qed}}	1
Let $\map {\MM_S} {m, n}$ be the [[Definition:Matrix Space|matrix space]] over a [[Definition:Semigroup|semigroup]] $\struct {S, \cdot}$. Then the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\map {\MM_S} {m, n}, \circ}$, where $\circ$ is the [[Definition:Hadamard Product|Hadamard product]], is also a [[Definition:Semigroup|semigroup]]. If $\struct {S, \cdot}$ is a [[Definition:Commutative Semigroup|commutative semigroup]] then so is $\struct {\map {\MM_S} {m, n}, \circ}$. If $\struct {S, \cdot}$ is a [[Definition:Monoid|monoid]] then so is $\struct {\map {\MM_S} {m, n}, \circ}$.	1
Let $d_p$ be the [[Definition:P-adic Metric on P-adic Numbers|$p$-adic metric]] on $\Q_p$: :$\forall x, y \in \Q_p: \map {d_p} {x, y} = \norm {x - y}_p$ From [[P-adic Metric on P-adic Numbers is Non-Archimedean Metric]], $d_p$ is a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]]. By definition of a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]]: :$\forall x, y, z \in R: \norm {x - y}_p \le \max \set {\norm {x - z}_p, \norm {y - z}_p}$ {{qed}} [[Category:Normed Division Rings]] [[Category:Non-Archimedean Norms]] 1gv6hs4a6f9mz3ak4qmxkpzjaafetxk	1
Let $C_n$ be the [[Definition:Combinatorial Matrix|combinatorial matrix]] of [[Definition:Order of Square Matrix|order $n$]] given by: :$C_n = \begin{bmatrix} x + y & y & \cdots & y \\ y & x + y & \cdots & y \\ \vdots & \vdots & \ddots & \vdots \\ y & y & \cdots & x + y \end{bmatrix}$ Then the [[Definition:Determinant of Matrix|determinant]] of $C_n$ is given by: :$\det \left({C_n}\right) = x^{n-1} \left ({x + n y}\right)$	1
=== Necessary Condition === Assume that $V$ is an [[Definition:Irreducible G-Module|irreducible]] [[Definition:G-Module|$G$-module]], but it has a [[Definition:Trivial G-Module|non-trivial]] [[Definition:Proper G-Submodule|proper $G$-submodule]]. By the definition [[Definition:Irreducible Linear Representation|irreducible]], its associated [[Definition:Linear Representation|representation]] is [[Definition:Irreducible Linear Representation|irreducible]]. Let this [[Definition:Linear Representation|representation]] be denoted $\tilde \phi = \rho: G \to \operatorname{GL} \left({V}\right)$). In [[Correspondence between Linear Group Actions and Linear Representations]] it is defined as: :$\rho \left({g}\right) \left({v}\right) = \phi \left({g, v}\right)$ where $g \in G$ and $v \in V$. Since $V$ has a [[Definition:Proper Subset|proper]] [[Definition:G-Submodule|$G$-submodule]], there exists $W$ a non-trivial [[Definition:Proper Subset|proper]] [[Definition:Vector Subspace|vector subspace]] which $\phi \left({G, W}\right) \subseteq W$ and so $\rho \left({G}\right) W \subseteq W$. Hence $W$ is [[Definition:Invariant Subspace|invariant]] by every [[Definition:Linear Transformation|linear operators]] in $\left\{ {\rho \left({g}\right): g \in G}\right\}$. By definition, $\rho$ cannot be [[Definition:Irreducible Linear Representation|irreducible]]. Thus we have reached a contradiction, and $V$ has then no non-trivial [[Definition:Proper G-Submodule| proper $G$-submodules]]. {{qed|lemma}} === Sufficient Condition === Assume now that $V$ has no [[Definition:Proper Subset|proper]] [[Definition:G-Submodule|$G$-submodules]], but it is a [[Definition:Reducible Linear Representation|reducible $G$-module]]. By the definition of [[Definition:Reducible G-Module|reducible $G$-module]], it follow that its associated [[Definition:Linear Representation|representation]] is [[Definition:Reducible Linear Representation|reducible]]. Let this [[Definition:Linear Representation|representation]] be denoted $\tilde \phi = \rho: G \to \operatorname{GL} \left({V}\right)$). From the definition of [[Definition:Reducible Linear Representation|reducible representation]], it follows that there exists a [[Definition:Vector Subspace|vector space]] $W$ of $V$. This is [[Definition:Invariant Subspace|invariant]] under all the [[Definition:Linear Transformation|linear operators]] in $\left\{ {\rho \left({g}\right): g \in G}\right\}$. {{explain|Why is it invariant?}} Then: :$\phi \left({G, W}\right) = \rho \left({G}\right) W \subseteq W$ which is the definition of a [[Definition:G-Submodule|$G$-submodule]] of $V$. By our assumption, $V$ has no non-trivial [[Definition:Proper G-Submodule|proper $G$-submodules]]. Thus we have reached a contradiction and $V$ must be then an [[Definition:Irreducible G-Module|irreducible $G$-module]]. {{qed}} [[Category:Representation Theory]] 73n1827vgl6vqae7ngeyjo0kv1fpmmu	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\operatorname{Re} \left({z}\right)$ denote the [[Definition:Real Part|real part]] of $z$, and $\operatorname{Im} \left({z}\right) $ the [[Definition:Imaginary Part|imaginary part]] of $z$. Then:	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct{G, +_G, \circ}$ be a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. Let $\circ' : R \times G \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: \lambda \circ' x = x \circ \lambda$ Then $\struct{G, +_G, \circ'}$ is not necessarily a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$	1
:$\forall \sequence {x_n}, \sequence {y_n} \in \NN: \sequence {x_n} + \paren {-\sequence {y_n} } \in \NN$	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :$\exists \alpha \in \R_{> 0}: \forall x \in R: \norm x_1 = \norm x_2^\alpha$ Then for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$ {{iff}} $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Row of Matrix|row]] $i$ by $\lambda$ | m = r_i \to \lambda r_i }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r_i \to \dfrac 1 \lambda r_i$	1
Suppose $\sequence {y_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0$. Then: :$\exists K \in \N: \forall n > K : y_n \ne 0$ and the [[Definition:Sequence|sequences]]: :$\sequence { {x_{K + n} } \paren {y_{K + n} }^{-1} }_{n \mathop \in \N}$ and $\sequence {\paren {y_{K + n} }^{-1} {x_{K + n} } }_{n \mathop \in \N}$ are well-defined and [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequences]].	1
From [[Ring of Integers Modulo Prime is Field]] we have that $\struct {\Z_p, +, \times}$ is a [[Definition:Field (Abstract Algebra)|field]]. So [[Characteristic of Finite Ring with No Zero Divisors]] applies, and so the [[Definition:Characteristic of Ring|characteristic]] of $\struct {\Z_p, +, \times}$ is [[Definition:Prime Number|prime]]. The result follows. {{qed}}	1
Let $\left({R, +, \circ}\right)$ be a [[Definition:Ring with Unity|ring with unity]]. Let $G$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$ Let $H$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $m>0$ Let $\left \langle {a_n} \right \rangle$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Let $\left \langle {b_m} \right \rangle$ be an [[Definition:Ordered Basis|ordered basis]] of $H$. Let $u : G \to H$ be a [[Definition:Linear Transformation|linear transformation]]. The '''matrix of $u$ relative to $\left \langle {a_n} \right \rangle$ and $\left \langle {b_m} \right \rangle$''' is the [[Definition:Matrix|$m \times n$ matrix]] $\left[{\alpha}\right]_{m n}$ where: :$\displaystyle \forall \left({i, j}\right) \in \left[{1 \,.\,.\, m}\right] \times \left[{1 \,.\,.\, n}\right]:u \left({a_j}\right) = \sum_{i \mathop = 1}^m \alpha_{i j} \circ b_i$ That is, the matrix whose [[Definition:Column of Matrix|columns]] are the [[Definition:Coordinate Vector|coordinate vectors]] of the image of the basis elements of $\mathcal A$ relative to the basis $\mathcal B$. The matrix of such a [[Definition:Linear Transformation|linear transformation]] $u$ relative to the ordered bases $\left \langle {a_n} \right \rangle$ and $\left \langle {b_m} \right \rangle$ is denoted: : $\left[{u; \left \langle {b_m} \right \rangle, \left \langle {a_n} \right \rangle}\right]$ If $u$ is an [[Definition:Module Automorphism|automorphism]] on an [[Definition:Dimension of Module|$n$-dimensional]] module $G$, we can write $\left[{u; \left \langle {a_n} \right \rangle, \left \langle {a_n} \right \rangle}\right]$ as $\left[{u; \left \langle {a_n} \right \rangle}\right]$.	1
From [[Unit Matrix is Unity of Ring of Square Matrices]], $I$ can be identified as the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $2$]]. Then: :$A^2 = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I$ :$A B = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = C$ :$A C = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = B$ :$B^2 = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I$ :$B A = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = C$ :$B C = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = A$ :$C^2 = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I$ :$C A = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = B$ :$C B = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = A$ Putting this together into a [[Definition:Cayley Table|Cayley table]]: :$\begin{array}{c|cccccc} & I & A & B & C \\ \hline I & I & A & B & C \\ A & A & I & C & B \\ B & B & C & I & A \\ C & C & B & A & I \\ \end{array}$ it is verified by inspection that this is an instance of the [[Definition:Klein Four-Group|Klein four-group]]. {{qed}}	1
By [[Quotient Ring of Cauchy Sequences is Normed Division Ring]] then $\CC \,\big / \NN$ is a [[Definition:Normed Division Ring|normed division ring]]. By [[Quotient Ring of Cauchy Sequences is Division Ring/Corollary 1|Corollary to Quotient Ring of Cauchy Sequences is Normed Division Ring]] then $\CC \,\big / \NN$ is a [[Definition:Field (Abstract Algebra)|field]]. The result follows. {{qed}}	1
Let $P \left({\R}\right)$ be the [[Definition:Vector Space|vector space]] of all [[Definition:Real Polynomial Function|polynomial functions]] on the [[Definition:Real Number Line|real number line]] $\R$. Let $S$ be the [[Definition:Mapping|mapping]] defined as: :$\displaystyle \forall p \in P \left({\R}\right): \forall x \in \R: S \left({p \left({x}\right)}\right) = \int_0^x p \left({t}\right) \mathrm d t$ Then $S$ is a [[Definition:Linear Operator|linear operator]] on $P \left({\R}\right)$.	1
Let $K \in \C^d$, where $d \in \N_{> 0}$. === Norm Axiom $(\text N 1)$ === Suppose $\sequence {x_n}_{n \mathop \in {\N}, \, n \mathop \le d} \in K$. By definition of [[Definition:P-Norm|$p$-norm]]: :$\displaystyle \norm {\mathbf x}_p = \paren {\sum_{n \mathop = 0}^d \size {x_n}^p}^{1/p}$ The [[Definition:Complex Modulus|complex modulus]] of $x_n$ is [[Definition:Real Number|real]] and [[Complex Modulus is Non-Negative|non-negative]]. We have the results: :[[Sum of Non-Negative Reals is Non-Negative]] :[[Power of Positive Real Number is Positive]] :[[Definition:Power of Zero|Zero Raised to Positive Power is Zero]] Hence, $\norm {\mathbf x}_p \ge 0$. Suppose, $\norm {\mathbf x}_p = 0$. Then: {{begin-eqn}} {{eqn | l = \norm {\mathbf x}_p | r = 0 }} {{eqn | ll= \leadsto | l = \sum_{n \mathop = 0}^d \size {x_n}^p | r = 0 | c = raising to power $p > 0$ }} {{eqn | ll= \leadsto | l = \size {x_n} | r = 0 | c = [[Sum of Non-Negatives vanishes iff Summands vanish]] }} {{eqn | ll= \leadsto | l = x_n | r = 0 | c = [[Complex Modulus equals Zero iff Zero]] }} {{eqn | ll= \leadsto | l = \bf x | r = \sequence 0_{n \mathop \in \N, \, n \mathop \le d} }} {{end-eqn}} Thus [[Definition:Norm Axioms|norm axiom $(\text N 1)$]] is satisfied. {{qed|lemma}} === Norm Axiom $(\text N 2)$ === Suppose, $\lambda \in K$. {{begin-eqn}} {{eqn | l = \norm {\lambda \mathbf x}_p | r = \paren {\sum_{n \mathop = 0}^d \size {\lambda x_n}^p}^{1/p} }} {{eqn | r = \paren {\size{\lambda}^p \sum_{n \mathop = 0}^d \size {x_n}^p}^{1/p} }} {{eqn | r = \size {\lambda} \paren {\sum_{n \mathop = 0}^d \size {x_n}^p}^{1/p} }} {{eqn | r = \size {\lambda} \norm {\mathbf x}_p }} {{end-eqn}} Thus [[Definition:Norm Axioms|norm axiom $(\text N 2)$]] is satisfied. {{qed|lemma}} === Norm Axiom $(\text N 3)$ === If $\mathbf x = \sequence 0$ and $\mathbf y = \sequence 0$, then by $\paren {\text N 1}$ we have [[Definition:Equality|equality]]. If $\mathbf x + \mathbf y = \sequence 0$ and both $\bf x$ and $\bf y$ nonvanishing, then by $\paren {\text N 1}$ we get a [[Definition:Strict Ordering|strict]] [[Definition:Inequality|inequality]]. If $\mathbf x + \mathbf y \ne \sequence 0$, then consider [[Definition:P-Norm|p-norm]] [[Definition:Real Power|raised to the power]] of $p$: {{begin-eqn}} {{eqn | l = \norm {\bf x + \bf y}_p^p | r = \sum_{n \mathop = 0}^d \size {x_n + y_n} \size {x_n + y_n}^{p \mathop - 1} }} {{eqn | o = \le | r = \sum_{n \mathop = 0}^d \size {x_n} \size {x_n + y_n}^{p \mathop - 1} + \sum_{n \mathop = 0}^d \size {y_n} \size {x_n + y_n}^{p \mathop - 1} | c = [[Triangle Inequality for Complex Numbers]] }} {{eqn | o = \le | r = \sum_{n \mathop = 0}^d \size {x_n \paren{x_n + y_n}^{p \mathop - 1} } + \sum_{n \mathop = 0}^d \size {y_n \paren{ x_n + y_n}^{p \mathop - 1} } | c = [[Modulus of Product]] }} {{eqn | o = \le | r = \norm {\bf x}_p \norm {\paren{\mathbf x + \mathbf y}^{p \mathop - 1} }_q + \norm {\mathbf y}_p \norm {\paren{\mathbf x + \mathbf y}^{p \mathop - 1} }_q | c = [[Hölder's Inequality for Sums]]: $\dfrac 1 p + \dfrac 1 q = 1$ }} {{eqn | o = \le | r = \norm {\bf x}_p \norm {\mathbf x + \mathbf y}_p^{p \mathop - 1} + \norm {\mathbf y}_p \norm {\mathbf x + \mathbf y}_p^{p \mathop - 1} | c = [[Transformation of P-Norm|Transformation of $p$-Norm]]: $q \paren {p - 1} = p$ }} {{eqn | ll= \leadsto | l = \norm {\mathbf x + \mathbf y}_p \norm {\mathbf x + \mathbf y}_p^{p \mathop - 1} | o = \le | r = \norm {\mathbf x}_p \norm {\mathbf x + \mathbf y}_p^{p \mathop - 1} + \norm {\bf y}_p \norm {\bf x + \bf y}_p^{p \mathop - 1} }} {{eqn | ll= \leadsto | l = \norm {\bf x + \bf y}_p | o = \le | r = \norm {\bf x}_p + \norm {\bf y}_p | c = [[Definition:Real Division|Division]] by $\norm {\bf x + \bf y}_p^{p \mathop - 1}$ }} {{end-eqn}} Thus [[Definition:Norm Axioms|norm axiom $(\text N 3)$]] is satisfied. {{qed|lemma}} All [[Definition:Norm Axioms|norm axioms]] are seen to be satisfied. Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\mathbf A \in R^{n \times n}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Let $\adj {\mathbf A}$ be its [[Definition:Adjugate Matrix|adjugate matrix]]. Then: {{begin-eqn}} {{eqn | l = \mathbf A \cdot \adj {\mathbf A} | r = \map \det {\mathbf A} \cdot \mathbf I_n }} {{eqn | l = \adj {\mathbf A} \cdot \mathbf A | r = \map \det {\mathbf A} \cdot \mathbf I_n }} {{end-eqn}} where $\map \det {\mathbf A}$ is the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$.	1
Let $I = \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $\map \CC I$ be the [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|space of real-valued functions, continuous on $I$]]. Let $\norm {\,\cdot\,}_\infty$ be the [[Definition:Supremum Norm/Continuous on Closed Interval Real-Valued Function|supremum norm on real-valued functions, continuous on]] $I$. Then $\struct {\map \CC I, \norm {\,\cdot\,}_\infty}$ is a [[Definition:Banach Space|Banach space]].	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\Gamma$ be a [[Definition:Column Operation|column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$. Then there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf K$ of [[Definition:Order of Square Matrix|order $n$]] such that: :$\mathbf A \mathbf K = \mathbf B$ where $\mathbf K$ is the [[Definition:Matrix Product (Conventional)|product]] of a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Matrix|elementary column matrices]].	1
Let $\cmod z$ denote the [[Definition:Complex Modulus|modulus]] of a [[Definition:Complex Number|complex number]] $z$. Let $e^z$ be the [[Definition:Complex Exponential Function|complex exponential]] of $z$. Let $x$ be [[Definition:Wholly Real|wholly real]]. Then: :$\cmod {e^{i x} } = 1$	1
Apply [[Vandermonde Matrix Identity for Cauchy Matrix]] and [[Hilbert Matrix is Cauchy Matrix]]. Matrices $V_x$ and $V_y$ are invertible by [[Inverse of Vandermonde Matrix]]. Matrices $P$ and $Q$ are invertible because all diagonal elements are nonzero. {{qed}}	1
Let $m \in \Z: m \ge 2$. Let $\struct {\Z_m, +, \times}$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. Then $\struct {\Z_m, +, \times}$ cannot be an [[Definition:Ordered Integral Domain|ordered integral domain]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $R$. Let $\sequence {x_{m_n} }$ be a [[Definition:Subsequence|subsequence]] of $\sequence {x_n}$. Then: :$\displaystyle \lim_{n \mathop \to \infty} {x_n - x_{m_n} } = 0$	1
Let $\mathbf x, \mathbf y$ be [[Definition:Vector (Euclidean Space)|vectors in $\R^n$]]. Let $\norm {\, \cdot \,}$ denote [[Definition:Vector Length|vector length]]. Then: :$\norm {\mathbf x + \mathbf y} \le \norm {\mathbf x} + \norm {\mathbf y}$ If the two vectors are [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] where said scalar is non-negative, an equality holds: :$\exists \lambda \in \R, \lambda \ge 0: \mathbf x = \lambda \mathbf y \iff \norm {\mathbf x + \mathbf y} = \norm {\mathbf x} + \norm {\mathbf y}$	1
Let $\struct {K, +, \times}$ be a [[Definition:Division Ring|division ring]]. Let $\struct {S, +, \times}$ be the [[Definition:Prime Subfield|prime subfield]] of $K$ Then $\struct {K, +, \times_S}_S$ is an [[Definition:Vector Space|$S$-vector space]], where $\times_S$ is the [[Definition:Restriction of Operation|restriction]] of $\times$ to $S \times K$.	1
=== Necessary condition === Let $r = \cmod z$. If $z = 0$, we have: :$z = 0e^{i \theta} = re^{i \theta}$ Suppose $z \ne 0$ and $\theta = \map \arg z$. By definition of [[Definition:Argument of Complex Number|argument]], the following two equations hold: :$(1): \quad \dfrac {\map \Re z} r = \cos \theta$ :$(2): \quad \dfrac {\map \Im z} r = \sin \theta$ where: :$\map \Re z$ denotes the [[Definition:Real Part|real part]] of $z$ :$\map \Im z$ denotes the [[Definition:Imaginary Part|imaginary part]] of $z$. Then: {{begin-eqn}} {{eqn | l = z | r = \map \Re z + i \map \Im z | c = {{Defof|Complex Number}} }} {{eqn | r = r \cos \theta + i r \sin \theta | c = from $(1)$ and $(2)$ }} {{eqn | r = r \paren {\cos \theta + i \sin \theta} }} {{eqn | r = re^{i \theta} | c = [[Euler's Formula]] }} {{end-eqn}} {{qed|lemma}} === Sufficient condition === Let $z = re^{i \theta}$. From the equations above, we find: :$\map \Re {re^{i \theta} } = r \cos \theta$ :$\map \Im {re^{i \theta} } = r \sin \theta$ Then: {{begin-eqn}} {{eqn | l = \cmod {r e^{i \theta} } | r = \sqrt {\paren {r \cos \theta}^2 + \paren {r \sin \theta}^2} | c = {{Defof|Modulus of Complex Number}} }} {{eqn | r = r \sqrt {\cos^2 \theta + \sin^2 \theta} | c = as $r \ge 0$ }} {{eqn | r = r | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} If $r \ne 0$, we find $\map \arg {r e^{i \theta} }$ by solving the two equations by definition of [[Definition:Argument of Complex Number|argument]]: :$(1): \quad \dfrac {r \cos \theta} r = \map \cos {\map \arg {r e^{i \theta} } }$ :$(2): \quad \dfrac {r \sin \theta} r = \map \sin {\map \arg {r e^{i \theta} } }$ We find: :$\map \arg {r e^{i \theta} } = \theta$ {{qed}}	1
From the definition of [[Definition:Matrix Entrywise Addition|matrix addition]], we have: :$\forall i, j \in \left[{1 .. n}\right]: c_{ij} = a_{ij} + b_{ij}$ If $\mathbf A$ and $\mathbf B$ are [[Definition:Upper Triangular Matrix|upper triangular matrices]], we have: :$\forall i > j: a_{ij} = b_{ij} = 0$ Hence: :$\forall i > j: c_{ij} = a_{ij} + b_{ij} = 0 + 0 = 0$ and so $\mathbf C$ is itself [[Definition:Upper Triangular Matrix|upper triangular]]. Similarly, if $\mathbf A$ and $\mathbf B$ are [[Definition:Lower Triangular Matrix|lower triangular matrices]], we have: :$\forall i < j: a_{ij} = b_{ij} = 0$ Hence: :$\forall i < j: c_{ij} = a_{ij} + b_{ij} = 0 + 0 = 0$ and so $\mathbf C$ is itself [[Definition:Lower Triangular Matrix|lower triangular]]. {{Qed}} [[Category:Triangular Matrices]] [[Category:Matrix Entrywise Addition]] h6dwze7055xyqeo5fwlnt4ak9rikwy0	1
Let $z_1$ and $z_2$ be represented by the [[Definition:Point|points]] $A$ and $B$ respectively in the [[Definition:Complex Plane|complex plane]]. From [[Geometrical Interpretation of Complex Addition]], we can construct the [[Definition:Parallelogram|parallelogram]] $OACB$ where: :$OA$ and $OB$ represent $z_1$ and $z_2$ respectively :$OC$ represents $z_1 + z_2$. :[[File:Triangle-Inequality-Complex.png|400px]] As $OACB$ is a [[Definition:Parallelogram|parallelogram]], we have that $OB = AC$. The [[Definition:Length of Line|lengths]] of $OA$, $AC$ and $OC$ are: {{begin-eqn}} {{eqn | l = OA | r = \cmod {z_1} }} {{eqn | l = AC | r = \cmod {z_2} }} {{eqn | l = OC | r = \cmod {z_1 + z_2} }} {{end-eqn}} But $OA$, $OB$ and $OC$ form the [[Definition:Side of Polygon|sides]] of a [[Definition:Triangle (Geometry)|triangle]]. The result then follows directly from [[Sum of Two Sides of Triangle Greater than Third Side]]. {{qed}}	1
Let $M = \struct {I^\omega, d_2}$ be the [[Definition:Hilbert Cube|Hilbert cube]]. Then $M$ is a [[Definition:Completely Normal Space|completely normal space]].	1
{{AimForCont}} $n_0$ is a [[Definition:Composite Number|composite number]]. Let $n_1, n_2 \in \N$ such that $n_1, n_2 < n_0$ and $n_0 = n_1 n_2$. By the definition of $n_0$ then: :$\norm {n_1} = 1$ :$\norm {n_2} = 1$ By {{NormAxiom|2}}: :$\norm {n_0} = \norm {n_1 n_2} = \norm {n_1} \norm {n_2} = 1$ This [[Definition:Contradiction|contradicts]] the assumption that $\norm {n_0} < 1$. So $n_0$ must be a [[Definition:Prime Number|prime number]]. {{qed}}	1
Suppose $\alpha \le 1$. It is shown that $\norm{\,\cdot\,}$ satisfies the [[Definition:Norm/Division Ring|norm axioms (N1)-(N3)]]. ==== (N1) Positive Definiteness ==== Let $x \in \Q$. {{begin-eqn}} {{eqn|l= \norm {x} = 0 |o= \iff |r= \size {x}^\alpha = 0 |c = Definition of $\norm{\,\cdot\,}$ }} {{eqn|o= \iff |r= \size {x} = 0 |c= Definition of $a$ to the [[Definition:Power (Algebra)|power]] of $r$ for $a \in \R_{\ge 0}$ and $r \in \R_{\gt 0}$ }} {{eqn|o= \iff |r= x = 0 |c= [[Absolute Value is Norm]] and [[Definition:Norm/Division Ring|norm axiom (N1) (Positive Definiteness)]] }} {{end-eqn}} {{qed|lemma}} ==== (N2) Multiplicativity ==== Let $x, y \in \Q$. Then: {{begin-eqn}} {{eqn|l= \norm {x y} |r= \size {x y}^\alpha |c = Definition of $\norm{\,\cdot\,}$ }} {{eqn|r= \paren {\size {x} \size {y} }^\alpha |c= [[Absolute Value is Norm]] and [[Definition:Norm/Division Ring|norm axiom (N2) (Multiplicativity)]] }} {{eqn|r= \size {x}^\alpha \size {y}^\alpha |c= [[Exponent Combination Laws/Power of Product|Power of product]] }} {{eqn|r= \norm {x} \norm {y} |c = Definition of $\norm{\,\cdot\,}$ }} {{end-eqn}} {{qed|lemma}} ==== (N3) Triangle Inequality ==== Let $x, y \in \Q$. {{WLOG}} let $\norm y \lt \norm x$. If $\norm x = 0$ then $\norm y = 0$. By (N1) above, $x = y = 0$. Hence: {{begin-eqn}} {{eqn|l= \norm{x + y} |r= \norm 0 }} {{eqn|r= 0 }} {{eqn|r= 0 + 0 }} {{eqn|r= \norm x + \norm y }} {{end-eqn}} If $\norm x \gt 0$ then: :$\norm x \gt 0 \iff \size {x}^\alpha \gt 0 \iff \size x \gt 0$ Hence: {{begin-eqn}} {{eqn|l= \norm{x + y} |r= \size{x + y}^\alpha }} {{eqn|o= \le |r= \paren{\size x + \size y}^\alpha |c= [[Definition:Norm/Division Ring|Norm axiom (N3) (Triangle Inequality)]] }} {{eqn|r= \size {x}^\alpha \paren{1 + \dfrac {\size y} {\size x} }^\alpha }} {{eqn|o= \le |r= \size {x}^\alpha \paren{1 + \dfrac {\size y} {\size x} } |c= [[Power Function on Base Greater than One is Strictly Increasing/Real Number|Real Power Function on base Greater than One is Strictly Increasing]] }} {{eqn|o= \le |r= \size {x}^\alpha \paren{1 + \dfrac {\size {y}^\alpha} {\size {x}^\alpha} } |c= [[Power Function on Base between Zero and One is Strictly Decreasing/Real Number|Real Power Function on base between Zero and One is Strictly Decreasing]] }} {{eqn|o= \le |r= \size {x}^\alpha + \size {y}^\alpha }} {{eqn|r= \norm {x} + \norm {y} }} {{end-eqn}}	1
Let $\mathbf A = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} & a_{r 2} & \cdots & a_{r n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{s 1} & a_{s 2} & \cdots & a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end {bmatrix}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf A}$ denote the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf B = \begin{bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} + k a_{s 1} & a_{r 2} + k a_{s 2} & \cdots & a_{r n} + k a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{s 1} & a_{s 2} & \cdots & a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end{bmatrix}$. Then $\map \det {\mathbf B} = \map \det {\mathbf A}$. That is, the value of a [[Definition:Determinant of Matrix|determinant]] remains unchanged if a [[Definition:Constant|constant]] multiple of any [[Definition:Row of Matrix|row]] is added to any other [[Definition:Row of Matrix|row]].	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\struct {G, +_G, \circ}_R$ be a [[Definition:Unitary Module|unitary $R$-module]] such that $\map \dim G = n$. Let $\struct {H, +_H, \circ}_R$ be a [[Definition:Unitary Module|unitary $R$-module]] such that $\map \dim H = m$. Let $\map {\LL_R} {G, H}$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Then: :$\map \dim {\map {\LL_R} {G, H} } = n m$ Let $\sequence {a_n}$ be an [[Definition:Ordered Basis|ordered basis]] for $G$. Let $\sequence {b_m}$ be an [[Definition:Ordered Basis|ordered basis]] for $H$. Let $\phi_{i j}: G \to H$ be the unique [[Definition:Linear Transformation|linear transformation]] defined for each $i \in \closedint 1 n, j \in \closedint 1 m$ which satisfies: :$\forall k \in \closedint 1 n: \map {\phi_{i j} } {a_k} = \delta_{i k} b_j$ where $\delta$ is the [[Definition:Kronecker Delta|Kronecker delta]]. Then: :$\set {\phi_{i j}: i \in \closedint 1 n, j \in \closedint 1 m}$ is a [[Definition:Basis (Linear Algebra)|basis]] for $\map \dim {\map {\LL_R} {G, H} }$.	1
Let $F$ be a [[Definition:Closed Set in Normed Vector Space|closed set]] in $X$. Suppose $S \subseteq F$. === $S^-$ is contained in $F$=== Let $L$ be a [[Definition:Limit Point/Normed Vector Space|limit point]] of $S$. Then there exists a [[Definition:Sequence|sequence]] $\sequence {x_n}_{n \mathop \in \N}$ in $S \setminus \set L$ which [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $L$. In other words: :$\forall n \in \N : x_n \in S \setminus \set L$. Furthermore: :$S \setminus \set L \subseteq S \subseteq F$ Since $F$ is [[Definition:Closed Set in Normed Vector Space|closed]], $L \in F$. So all [[Definition:Limit Point/Normed Vector Space|limit points]] of $S$ belong to $F$. Hence, $S^- \subseteq F$. {{qed|lemma}} === $S^-$ is closed === Let $\sequence {x_n}_{n \mathop \in \N}$ be a [[Definition:Sequence|sequence]] in $S^-$. Let $\sequence {x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converge]] to $L$: :$\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall n \in \N: n > N \implies \norm {x_n - L} < \epsilon$ We can have either $L \in S$ or $L \notin S$. Suppose $L \in S$. :By [[Definition:Closure/Normed Vector Space|definition]], $L \in S^-$. Suppose $L \notin S$. Define a new [[Definition:Sequence|sequence]] $x_n'$ using $x_n$ as follows: :$(1): \quad$ if $x_n \in S$, then $x_n' := x_n$; :$(2): \quad$ if $x_n \notin S$, then $x_n$ is a [[Definition:Limit Point/Normed Vector Space/Set|limit point]] of $S$. ::Then there is [[Definition:Open Ball in Normed Vector Space|open ball]] $\ds \map {B_{\frac 1 n}} {x_n}$ which has an element of $S$. ::Define $x_n'$ such that $x_n' \in S$ and $x_n' \in \map {B_{\frac 1 n}} {x_n}$ Suppose $x_n \in S$. Then: :$\norm {x_n' - L} = \norm {x_n - L}$ Suppose $x_n \notin S$. Then: {{begin-eqn}} {{eqn | l = \norm {x_n' - L} | r = \norm {x_n' - x_n + x_n - L} }} {{eqn | o = \le | r = \norm {x_n' - x_n} + \norm {x_n - L} | c = [[Definition:Norm|Norm axiom]]: triangle inequality }} {{eqn | o = < | r = \frac 1 n + \norm {x_n - L} }} {{eqn | o = < | r = \frac 1 n + \epsilon }} {{end-eqn}} Thus, $\sequence {x_n'}_{n \mathop \in \N}$ is a sequence in $S \setminus \set L$ which [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $L$. So $L$ is a [[Definition:Limit Point/Normed Vector Space|limit point]] of $S$: :$L \in S^-$. By [[Definition:Closed Set/Normed Vector Space/Definition 2|definition]], $S^-$ is [[Definition:Closed Set in Normed Vector Space|closed]]. {{qed|lemma}} === $S^-$ is the smallest closed set containing $S$ === {{AimForCont}} there exists a [[Definition:Closed Set in Normed Vector Space|closed set]] $Q$ [[Definition:Smaller Set|smaller]] than $S^-$ which [[Definition:Contain|contains]] $S$. $S^-$ differs from $S$ only by [[Definition:Limit Point/Normed Vector Space|limit points]] of $S$. If $Q$ is [[Definition:Smaller Set|smaller]] than $S^-$, it has to [[Definition:Contain|contain]] fewer [[Definition:Limit Point/Normed Vector Space|limit points]] of $S$ than $S^-$. Hence, $Q$ would not [[Definition:Contain|contain]] all its [[Definition:Limit Point/Normed Vector Space|limit points]]. By definition, $Q$ would not be [[Definition:Closed Set/Normed Vector Space/Definition 2|closed]]. This is a [[Definition:Contradiction|contradiction]]. {{qed}}	1
By [[Null Sequences form Maximal Left and Right Ideal/Lemma 1|Lemma 1 of Null Sequences form Maximal Left and Right Ideal]]: :$\NN$ is an [[Definition:Ideal of Ring|ideal]] of $\CC$. Hence $\NN$ is a [[Definition:Left Ideal of Ring|left ideal]] of $\CC$. It remains to show that $\NN$ is [[Definition:Maximal Left Ideal of Ring|maximal]]. === [[Null Sequences form Maximal Left and Right Ideal/Lemma 2/Lemma 2.1|Lemma 2.1]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 2/Lemma 2.1}}{{qed|lemma}} === [[Null Sequences form Maximal Left and Right Ideal/Lemma 2/Lemma 2.2|Lemma 2.2]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 2/Lemma 2.2}}{{qed|lemma}} The result follows by definition of [[Definition:Maximal Left Ideal of Ring|maximal left ideal]]. {{qed}}	1
Let $\left({v, w}\right) \in V \times V$ with $b \left({v, w}\right) = 0$. Because $b$ is [[Definition:Symmetric Bilinear Form|symmetric]], $b \left({w, v}\right) = 0$. Because $\left({v, w}\right)$ was arbitrary, $b$ is [[Definition:Reflexive Bilinear Form|reflexive]]. {{qed}}	1
Let $\mathbb K \in \left\{ {\C, \R}\right\}$. Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $\mathfrak{so}_n \left({\mathbb K}\right)$ be the [[Definition:Lie Algebra|Lie algebra]] of the [[Definition:Special Orthogonal Group|special orthogonal group]] $\operatorname{SO}_n \left({\mathbb K}\right)$. Then its [[Definition:Killing Form|Killing form]] is $B: \left({X, Y}\right) \mapsto \left({n - 2}\right) \operatorname {tr} \left({X Y}\right)$.	1
Let $\mathbf I$ denote the [[Definition:Unit Matrix|unit matrix]] of arbitrary [[Definition:Order of Square Matrix|order]] $n$. By [[Determinant of Unit Matrix]]: :$\map \det {\mathbf I} = 1$ Let $\rho$ be the [[Definition:Permutation on n Letters|permutation]] on $\tuple {1, 2, \ldots, n}$ which [[Definition:Transposition|transposes]] $i$ and $j$. From [[Parity of K-Cycle]], $\map \sgn \rho = -1$. By definition we have that $\mathbf E_3$ is $\mathbf I$ with [[Definition:Row of Matrix|rows]] $i$ and $j$ [[Definition:Transposition|transposed]]. By the definition of a [[Definition:Determinant of Matrix|determinant]]: :$\displaystyle \map \det {\mathbf I} = \sum_{\lambda} \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{k \map \lambda k} }$ By [[Permutation of Determinant Indices]]: :$\displaystyle \map \det {\mathbf E_3} = \sum_\lambda \paren {\map \sgn \rho \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \rho k \map \lambda k} }$ We can take $\map \sgn \rho = -1$ outside the summation because it is constant, and so we get: {{begin-eqn}} {{eqn | l = \map \det {\mathbf E_3} | r = \map \sgn \rho \sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \rho k \map \lambda k} } | c = }} {{eqn | r = -\sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{k \map \lambda k} } | c = }} {{eqn | r = -\map \det {\mathbf I} | c = }} {{end-eqn}} Hence the result. {{qed}}	1
Let $\phi$ be a [[Definition:Linear Operator|linear operator]] on $\R^2$. Let $\alpha_{11}, \alpha_{12}, \alpha_{21}, \alpha_{22} \in \R$ be the [[Definition:Real Number|real numbers]] which satisfy the equations: {{begin-eqn}} {{eqn | l = \phi \left({e_1}\right) | r = \alpha_{11} e_1 + \alpha_{21} e_2 | c = }} {{eqn | l = \phi \left({e_2}\right) | r = \alpha_{12} e_1 + \alpha_{22} e_2 | c = }} {{end-eqn}} where $\left({e_1, e_2}\right)$ is the [[Definition:Standard Ordered Basis|standard ordered basis]] of $\R^2$. Then, by linearity: {{begin-eqn}} {{eqn | l = \phi \left({\lambda_1, \lambda_2}\right) | r = \phi \left({\lambda_1 e_1 + \lambda_2 e_2}\right) | c = }} {{eqn | r = \lambda_1 \phi \left({e_1}\right) + \lambda_2 \phi \left({e_2}\right) | c = }} {{eqn | r = \left({\lambda_1 \alpha_{11} + \lambda_2 \alpha_{12} }\right) e_1 + \left({\lambda_1 \alpha_{21} + \lambda_2 \alpha_{22} }\right) e_2 | c = }} {{eqn | r = \left({\lambda_1 \alpha_{11} + \lambda_2 \alpha_{12}, \lambda_1 \alpha_{21} + \lambda_2 \alpha_{22} }\right) | c = }} {{end-eqn}} Conversely, if $\alpha_{11}, \alpha_{12}, \alpha_{21}, \alpha_{22} \in \R$ are ''any'' real numbers, then we can define the mapping $\phi$ as: : $\phi \left({\lambda_1, \lambda_2}\right) = \left({\lambda_1 \alpha_{11} + \lambda_2 \alpha_{12}, \lambda_1 \alpha_{21} + \lambda_2 \alpha_{22}}\right)$ which is easily verified as being a [[Definition:Linear Operator|linear operator]] on $\R^2$: {{begin-eqn}} {{eqn | l = b \cdot \phi \left({\lambda_1, \lambda_2}\right) + c \cdot \phi \left({\lambda_3, \lambda_4}\right) | r = b \left({\lambda_1 \alpha_{11} + \lambda_2 \alpha_{12}, \lambda_1 \alpha_{21} + \lambda_2 \alpha_{22} }\right) + c \left({\lambda_3 \alpha_{11} + \lambda_4 \alpha_{12}, \lambda_3 \alpha_{21} + \lambda_4 \alpha_{22} }\right) | c = }} {{eqn | r = \left({b \lambda_1 \alpha_{11} + b \lambda_2 \alpha_{12}, b \lambda_1 \alpha_{21} + b \lambda_2 \alpha_{22} }\right) + \left({c \lambda_3 \alpha_{11} + c\lambda_4 \alpha_{12}, c\lambda_3 \alpha_{21} + c\lambda_4 \alpha_{22} }\right) | c = }} {{eqn | r = \left({b \lambda_1 \alpha_{11} + b \lambda_2 \alpha_{12} + c \lambda_3 \alpha_{11} + c \lambda_4 \alpha_{12}, b \lambda_1 \alpha_{21} + b \lambda_2 \alpha_{22} + c \lambda_3 \alpha_{21} + c \lambda_4 \alpha_{22} }\right) | c = }} {{eqn | r = \left({\left({b \lambda_1 + c \lambda_3}\right) \alpha_{11} + \left({b \lambda_2 + c \lambda_4}\right) \alpha_{12}, \left({b \lambda_1 + c \lambda_3}\right) \alpha_{21} + \left({c \lambda_2 + c \lambda_4}\right) \alpha_{22} }\right) | c = }} {{eqn | r = \left({b \lambda_1 + c \lambda_3, b \lambda_2 + c \lambda_4}\right) | c = }} {{end-eqn}} Thus, by [[Condition for Linear Transformation]], $\phi$ is a linear operator on $\R^2$. Thus each linear operator on $\R^2$ is completely determined by the [[Definition:Ordered Tuple|ordered tuple]]: :$\left({\alpha_{11}, \alpha_{12}, \alpha_{21}, \alpha_{22}}\right)$ of [[Definition:Real Number|real numbers]]. {{Qed}}	1
Let $X$ denote either $\R$ or $\C$ as appropriate. From [[Real Number Line is Metric Space]] and [[Complex Plane is Metric Space]] the [[Definition:Distance Function|distance function]] $d: X \times X \to \R$ can be defined as: :$\map d {x, y} = \size {x - y}$ From the [[Reverse Triangle Inequality]] as applied to [[Definition:Metric Space|metric spaces]]: :$(1): \quad \forall x, y, z \in X: \size {\map d {x, z} - \map d {y, z} } \le \map d {x, y}$ Let $z = 0$. Then $(1)$ translates to: :$\forall x, y, z \in X: \size {\size {x - 0} - \size {y - 0} } \le \size {x - y}$ Hence the result. {{qed}}	1
:[[File:Straight-line-normal-form.png|400px]] Let $A$ be the [[Definition:X-Intercept|$x$-intercept]] of $\mathcal L$. Let $B$ be the [[Definition:Y-Intercept|$y$-intercept]] of $\mathcal L$. Let $A = \tuple {a, 0}$ and $B = \tuple {0, b}$. From the [[Equation of Straight Line in Plane/Two-Intercept Form|Equation of Straight Line in Plane: Two-Intercept Form]], $\mathcal L$ can be expressed in the form: :$(1): \quad \dfrac x a + \dfrac y a = 1$ Then: {{begin-eqn}} {{eqn | l = p | r = a \cos \alpha | c = {{Defof|Cosine of Angle}} }} {{eqn | ll= \leadsto | l = a | r = \dfrac p {\cos \alpha} | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = p | r = b \sin \alpha | c = {{Defof|Sine of Angle}} }} {{eqn | ll= \leadsto | l = b | r = \dfrac p {\sin \alpha} | c = }} {{end-eqn}} Substituting for $a$ and $b$ in $(1)$: {{begin-eqn}} {{eqn | l = \dfrac x a + \dfrac y a | r = 1 | c = }} {{eqn | ll= \leadsto | l = \dfrac {x \cos \alpha} p + \dfrac {y \sin \alpha} p | r = 1 | c = }} {{eqn | ll= \leadsto | l = x \cos \alpha + y \sin \alpha | r = p | c = }} {{end-eqn}} {{Qed}}	1
From [[Limit Points of Either-Or Topology]], the only [[Definition:Limit Point of Set|limit point]] of any set of $S$ is $0$. So the only set whose [[Definition:Closure (Topology)|closure]] is $S$ are $S \setminus \set 0$ and $S$ itself. So these two are the only [[Definition:Subset|subsets]] of $S$ which are [[Definition:Everywhere Dense|everywhere dense]] in $S$. Both of these are [[Definition:Uncountable Set|uncountable]]. Hence the result, by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $\R^3$ be a [[Definition:Real Cartesian Space|real cartesian space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]]. Consider the [[Definition:Set|set]] $S$ of [[Definition:Directed Line Segment|directed line segments]] in $\R^3$. Let the [[Definition:Equivalence Relation|equivalence relation]] $\sim$ be applied to $\R^3$ such that: :$\forall L_1, L_2 \in \R^3: L_1 \sim L_2$ {{iff}} there exists a [[Definition:Translation|translation]] $T$ such that $T \left({L_1}\right) = L_2$ Let $\mathbb V$ denote the [[Definition:Set|set]] of [[Definition:Equivalence Class|equivalence classes]] of $\sim$ on $S$. The [[Definition:Element|elements]] of $\mathbb V$ form a [[Definition:Vector Space|vector space]] where: :$\forall \mathbf v_1, \mathbf v_2 \in \mathbb V: \mathbf v_1 + \mathbf v_2$ denotes the [[Definition:Element|element]] of $\mathbb V$ exemplified by the result of the operation of joining of a representative element of [[Definition:Element|element]] of $\mathbf v_2$ to the end of a similarly representative element of [[Definition:Element|element]] of $\mathbf v_1$. :$\forall \mathbf v \in \mathbb V, \lambda \in \R: \lambda \mathbf v$ denotes the [[Definition:Element|element]] of $\mathbb V$ exemplified by the result of the operation of mulitplying a representative element of [[Definition:Element|element]] of $\mathbf v$ by the scale factor $\lambda$.	1
Let $\mathbf x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \R^m$. We have that: {{begin-eqn}} {{eqn | l = \mathbf x | o = \in | r = \map {\mathrm N} {\mathbf A} }} {{eqn | ll= \leadstoandfrom | l = \mathbf A \mathbf x_{n \times 1} | r = \mathbf 0_{m \times 1} }} {{eqn | ll= \leadstoandfrom | l = \begin{bmatrix} \mathbf a_1 & \mathbf a_2 & \cdots & \mathbf a_n \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} | r = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} }} {{eqn | ll= \leadstoandfrom | l = \sum_{k \mathop = 1}^n x_k \mathbf a_k | r = \mathbf 0 }} {{end-eqn}} === Sufficient Condition === Let $\set {\mathbf a_1, \mathbf a_2, \cdots, \mathbf a_n}$ be [[Definition:Linearly Independent Set|linearly independent]]. Then by definition: :$\forall k: 1 \le k \le n: x_k = 0 \iff \mathbf x = \mathbf 0_{n \times 1}$ By the definition of [[Definition:Null Space|null space]]: :$\map {\mathrm N} {\mathbf A} = \set {\mathbf 0_{n \times 1} }$ {{qed|lemma}} === Necessary Condition === Let $\map {\mathrm N} {\mathbf A} = \set {\mathbf 0_{n \times 1} }$. Then by the definition of [[Definition:Null Space|null space]]: :$\mathbf x = \mathbf 0_{n \times 1}$ This means that: :$\forall k: 1 \le k \le n: x_k = 0$ from which it follows that $\set {\mathbf a_1, \mathbf a_2, \cdots, \mathbf a_n}$ is [[Definition:Linearly Independent Set|linearly independent]]. {{qed}}	1
By [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 1|Corollary 1 of Characterisation of Non-Archimedean Division Ring Norms]] then: :$\sup \set {\norm{n \cdot 1_R}: n \in \N_{> 0}} = 1$ By [[Definition:Norm on Division Ring|Norm Axiom $(\text N 1)$ (Positive Definiteness)]] then: :$\norm {0 \cdot 1_R} = 0 \le 1$ Let $n < 0$ then: {{begin-eqn}} {{eqn | l = \norm {n \cdot 1_R} | r = \norm {-\underbrace {\paren {1_R + 1_R + \dots + 1_R} }_{\text {$-n$ times} } } }} {{eqn | r = \norm {\underbrace {1_R + 1_R + \dots + 1_R}_{\text {$-n$ times} } } | c = [[Norm of Ring Negative]] }} {{eqn | r = \norm {\paren {-n} \cdot 1_R} | c = }} {{eqn | o = \le | r = 1 | c = [[Characterisation of Non-Archimedean Division Ring Norms]] }} {{end-eqn}} The result follows. {{qed}}	1
Let $\mathbf A$ be a [[Definition:Hermitian Matrix|Hermitian matrix]]. Then, by definition, $\mathbf A = \mathbf A^\dagger$, where $^\dagger$ designates the [[Definition:Conjugate Transpose of Matrix|conjugate transpose]]. Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $\mathbf A$. Let $\mathbf v$ be an [[Definition:Eigenvector|eigenvector]] corresponding to the [[Definition:Eigenvalue|eigenvalue]] $\lambda$ of $\mathbf A$. Denote with $\left\langle{\cdot, \cdot}\right\rangle$ the [[Definition:Inner Product|inner product]] on $\C$. {{begin-eqn}} {{eqn | l = \lambda * \left\langle v, v\right\rangle | r = \left\langle \lambda*v, v\right\rangle | c = [[Properties of Complex Inner Product]] }} {{eqn | r = \left\langle \mathbf A*v, v\right\rangle | c = {{Defof|Eigenvector}}: $\lambda*v = \mathbf A*v$ }} {{eqn | r = \left\langle v, \mathbf A^\dagger * v\right\rangle | c = [[Properties of Adjugate]] }} {{eqn | r = \left\langle v, \mathbf A*v\right\rangle | c = $\mathbf A$ is [[Definition:Hermitian Matrix|Hermitian]], so $\mathbf A^\dagger = \mathbf A$ }} {{eqn | r = \left\langle v, \lambda*v\right\rangle | c = {{Defof|Eigenvector}}: $\lambda*v = \mathbf A*v$ }} {{eqn | r = \overline{\lambda}*\left\langle v, v\right\rangle | c = [[Properties of Complex Inner Product]] }} {{end-eqn}} We have that $v \ne 0$, and because of the [[Definition:Positive Definite|positive definiteness]], it must be that: : $\left\langle v, v\right\rangle \ne 0$ {{explain|A link is needed to whatever it is ($\left\langle v, v\right\rangle$, presumably) is [[Definition:Positive Definite|positive definite]].}} Thus: : $\left\langle v, v\right\rangle \ne 0$ So we can divide both sides by $\left\langle v, v\right\rangle$. Thus $\lambda = \overline{\lambda}$. By [[Complex Number equals Conjugate iff Wholly Real]], $\lambda$ is a [[Definition:Real Number|real number]]. $\lambda$ was arbitrary, so it follows that every [[Definition:Eigenvalue|eigenvalue]] is a [[Definition:Real Number|real number]]. Hence the result. {{qed}}	1
Follows directly from [[Standard Ordered Basis is Basis]]. {{qed}} [[Category:Module on Cartesian Product]] 4e2u0vlwjpxp8kx7zj4kipknso5ie65	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $G$ be a [[Definition:Free Module|free $R$-module]]. An '''ordered basis''' of $G$ is a [[Definition:Sequence|sequence]] $\left \langle {a_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ of [[Definition:Element|elements]] of $G$ such that $\left\{{a_1, \ldots, a_n}\right\}$ is a [[Definition:Basis of Module|basis]] of $G$.	1
We have that $L$ is a By definition, $L / K$ is a [[Definition:Field Extension|field extension]] over $K$. Thus, by definition, $K$ is a [[Definition:Subfield|subfield]] of $L$. Thus, also by definition, $K$ is a [[Definition:Division Subring|division subring]] of $L$. The result follows by [[Vector Space over Division Subring is Vector Space]]. {{qed}} [[Category:Examples of Vector Spaces]] 1xsw69v077em48g99y2v1buy7i14d4k	1
[[Proof by Counterexample]]: Let $\struct {S, +, \times}$ be a [[Definition:Ring with Unity|ring with unity]] Let $\struct {\map {\mathcal M_S} 2, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $2$ over $S$]]. From [[Ring of Square Matrices over Ring is Ring]], $\struct {\map {\mathcal M_S} 2, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. Let $G = \set {\begin{bmatrix} x & 0 \\ y & 0 \end{bmatrix} : x, y \in S }$ === [[Left Module Does Not Necessarily Induce Right Module over Ring/Lemma|Lemma]] === {{:Left Module Does Not Necessarily Induce Right Module over Ring/Lemma}}{{qed|lemma}} From [[Left Ideal is Left Module over Ring]], $\struct {G, +, \times}$ is a [[Definition:Left Module|left module]] over $\struct {\map {\mathcal M_S} 2, +, \times}$. Let $R = \map {\mathcal M_S} 2$ Let $\mathbf A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \mathbf B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}, \mathbf C = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}$ Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A \times \mathbf B} \times \mathbf C | r = \paren { \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} } \times \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \paren {\mathbf B \times \mathbf A} \times \mathbf C | r = \paren { \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} } \times \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} }} {{end-eqn}} Hence for $\mathbf A, \mathbf B \in R, \mathbf C \in G$: :$ \paren {\mathbf A \times \mathbf B} \times \mathbf C \neq \paren {\mathbf B \times \mathbf A} \times \mathbf C$ Let $\circ : G \times R \to R$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \mathbf \Lambda \in R: \forall \mathbf X \in G: \mathbf X \circ \mathbf \Lambda = \mathbf \Lambda \times \mathbf X$ From [[Left Module induces Right Module over same Ring iff Actions are Commutative]], $\struct {G, +, \circ}$ is not a [[Definition:Right Module|right module]] over $\struct {R, +, \times}$ {{qed}}	1
=== [[Characterisation of Non-Archimedean Division Ring Norms/Necessary Condition|Necessary Condition]] === {{:Characterisation of Non-Archimedean Division Ring Norms/Necessary Condition}}{{qed|lemma}} === [[Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition|Sufficient Condition]] === {{:Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition}}{{qed}}	1
Let $\mathbf x, \mathbf y \in \R^n$. We have: {{begin-eqn}} {{eqn | l = \norm {\mathbf x + \mathbf y}^2 | r = \paren {\mathbf x + \mathbf y} \cdot \paren {\mathbf x + \mathbf y} | c = [[Dot Product of Vector with Itself]] }} {{eqn | r = \mathbf x \cdot \mathbf x + \mathbf x \cdot \mathbf y + \mathbf y \cdot \mathbf x + \mathbf y \cdot \mathbf y | c = [[Dot Product Distributes over Addition]] }} {{eqn | r = \mathbf x \cdot \mathbf x + 2 \paren {\mathbf x \cdot \mathbf y} + \mathbf y \cdot \mathbf y | c = [[Dot Product Operator is Commutative]] }} {{eqn | r = \norm {\mathbf x}^2 + 2 \paren {\mathbf x \cdot \mathbf y} + \norm {\mathbf y}^2 | c = [[Dot Product of Vector with Itself]] }} {{end-eqn}} From the [[Cauchy-Bunyakovsky-Schwarz Inequality]]: {{begin-eqn}} {{eqn | l = \size {\mathbf x \cdot \mathbf y} | o = \le | r = \norm {\mathbf x} \norm {\mathbf y} }} {{eqn | ll= \leadsto | l = \mathbf x \cdot \mathbf y | o = \le | r = \norm {\mathbf x} \norm {\mathbf y} | c = [[Negative of Absolute Value]] }} {{eqn | l = \norm {\mathbf x}^2 + 2 \paren {\mathbf x \cdot \mathbf y} + \norm {\mathbf y}^2 | o = \le | r = \norm {\mathbf x}^2 + 2 \paren {\norm {\mathbf x} \norm {\mathbf y} } + \norm {\mathbf y}^2 }} {{eqn | r = \paren {\norm {\mathbf x} + \norm {\mathbf y} }^2 }} {{eqn | ll= \leadsto | l = \norm {\mathbf x + \mathbf y}^2 | o = \le | r = \paren {\norm {\mathbf x} + \norm {\mathbf y} }^2 }} {{eqn | ll= \leadsto | l = \norm {\mathbf x + \mathbf y} | o = \le | r = \norm {\mathbf x} + \norm {\mathbf y} | c = taking the [[Definition:Square Root|square root]] of both sides }} {{end-eqn}} {{qed}} To prove that the equality holds if the [[Definition:Vector (Euclidean Space)|vectors]] are [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of each other, assume: :$\exists \lambda \in \R, \lambda \ge 0: \mathbf v = \lambda \mathbf w$ === Sufficient Condition === {{begin-eqn}} {{eqn | l = \norm {\mathbf v + \mathbf w} | r = \norm {\lambda \mathbf w + \mathbf w} }} {{eqn | r = \norm {\paren {\lambda + 1} \mathbf w} }} {{eqn | r = \paren {\lambda + 1} \norm {\mathbf w} }} {{eqn | r = \lambda \norm {\mathbf w} + 1 \norm {\mathbf w} }} {{eqn | r = \norm {\lambda \mathbf w} + \norm {1 \mathbf w} }} {{eqn | r = \norm {\mathbf v} + \norm {\mathbf w} }} {{end-eqn}} {{qed|lemma}} === Necessary Condition === {{proof wanted}}	1
The [[Definition:Unit Matrix|unit matrix]] $\mathbf I_n$ of [[Definition:Order of Square Matrix|order]] $n$ is [[Definition:Orthogonal Matrix|orthogonal]].	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $T \in B_0 \left({H, K}\right)$ be a [[Definition:Compact Linear Transformation|compact linear transformation]]. Let $A \in B \left({H}\right), B \in B \left({K}\right)$ be [[Definition:Bounded Linear Operator|bounded linear operators]]. Then the [[Definition:Composition of Mappings|compositions]] $TA$ and $BT$ are also [[Definition:Compact Linear Transformation|compact linear transformations]].	1
Because $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $l$: :$\exists \epsilon \in \R_{>0}: \forall n \in \N, \exists m \ge n: \norm {x_m - l} \ge \epsilon$ Because $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]: :$\exists K \in \N: \forall n, m \ge K: \norm {x_n - x_m} < \dfrac \epsilon 2$ Let $M >= K: \norm {x_M - l} >= \epsilon$ Then $\forall n > K$: {{begin-eqn}} {{eqn | l = \epsilon | o = \le | r = \norm {x_M - l} | c = }} {{eqn | r = \norm {x_M - x_n + x_n - l} | c = }} {{eqn | o = \le | r = \norm {x_M - x_n} + \norm {x_n - l} | c = {{NormAxiom|3}} }} {{eqn | o = < | r = \dfrac \epsilon 2 + \norm {x_n - l} | c = because $n, M \ge K$ }} {{eqn | ll= \leadsto | l = \dfrac \epsilon 2 | o = < | r = \norm {x_n - l} | c = Subtracting $\dfrac \epsilon 2$ from both sides of the equation. }} {{end-eqn}} Let $C = \dfrac \epsilon 2$ and the result follows. {{qed}} [[Category:Cauchy Sequence in Normed Division Ring is Bounded]] p5b1gjcn4xk1dx8b3mtgnv2vjlyqab6	1
=== Necessary Condition === When $L' = L$, the claim is trivial. Let $L' \ne L$ be [[Equation of Straight Line in Plane|described by]] the equation: :$\alpha'_1 x + \alpha'_2 y = \beta'$ Without loss of generality, let $\alpha'_1 \ne 0$ (the case $\alpha'_2 \ne 0$ is similar). Then for $\tuple {x, y} \in L'$ to hold, one needs: {{begin-eqn}} {{eqn | l = \alpha'_1 x + \alpha'_2 y | r = \beta' }} {{eqn | ll= \leadstoandfrom | l = x | r = \frac {-\alpha'_2} {\alpha'_1} y + \frac {\beta'} {\alpha'_1} }} {{end-eqn}} For $L'$ to be [[Definition:Parallel Lines|parallel]] to $L$, it is required that then $\tuple {x, y} \notin L$, that is: {{begin-eqn}} {{eqn | l = \alpha_1 x + \alpha_2 y | o = \ne | r = \beta }} {{eqn | ll= \leadstoandfrom | l = \alpha_1 \paren {\frac {- \alpha'_2} {\alpha'_1} y + \frac {\beta'} {\alpha'_1} } + \alpha_2 y | o = \ne | r = \beta }} {{eqn | ll= \leadstoandfrom | l = \paren {\alpha_2 - \alpha_1 \frac {\alpha'_2} {\alpha'_1} } y + \alpha_1 \frac {\beta'} {\alpha'_1} | o = \ne | r = \beta }} {{eqn | ll= \leadstoandfrom | l = \paren {\alpha_2 - \alpha_1 \frac {\alpha'_2} {\alpha'_1} } y | o = \ne | r = \beta - \alpha_1 \frac {\beta'} {\alpha'_1} }} {{end-eqn}} It follows that necessarily $\beta - \alpha_1 \frac {\beta'} {\alpha'_1} \ne 0$, or taking $y = 0$ would yield equality. The only remaining way to obtain the desired inequality for all $y$ is that: :$\alpha_2 - \alpha_1 \dfrac {\alpha'_2} {\alpha'_1} = 0$ One observes that now $\alpha_1 = 0 \implies \alpha_2 = 0$. However, as $L: \alpha_1 x + \alpha_2 y = \beta$ is a [[Equation of Straight Line in Plane|straight line in $\R^2$]], it cannot be that $\alpha_1 = \alpha_2 = 0$. So $\alpha_1 \ne 0$, and one finds: :$\alpha'_2 = \dfrac {\alpha'_1} {\alpha_1} \alpha_2$ Hence obtain: {{begin-eqn}} {{eqn | l = \alpha'_1 x + \alpha'_2 y | r = \beta' }} {{eqn | ll= \leadstoandfrom | l = \frac {\alpha'_1} {\alpha_1} \paren {\alpha_1 x + \alpha_2 y} | r = \beta' }} {{eqn | ll= \leadstoandfrom | l = \alpha_1 x + \alpha_2 y | r = \beta' \frac {\alpha_1} {\alpha'_1} }} {{end-eqn}} That is, $L'$ is described by an equation of the required form. {{qed|lemma}} {{proofread}} === Sufficient Condition === Let $L' \ne L$ be a straight line given by the equation: :$\alpha_1 x + \alpha_2 y = \beta'$ Suppose we have a point $\mathbf x = \tuple {x, y} \in L \cap L'$. Then, as $\mathbf x \in L$, it also satisfies: :$\alpha_1 x + \alpha_2 y = \beta$ It follows that $\beta = \beta'$, so $L = L'$. This contradiction shows that $L \cap L' = \O$, that is, $L$ and $L'$ are [[Definition:Parallel Lines|parallel]]. The remaining case is when $L' = L$. By definition, $L$ is parallel to itself. The result follows. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf B | r = \sqbrk a_{m n} + \sqbrk b_{m n} | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{eqn | r = \sqbrk {a + b}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {b + a}_{m n} | c = {{Ring-axiom|A2}} }} {{eqn | r = \sqbrk b_{m n} + \sqbrk a_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \mathbf B + \mathbf A | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M, N, P$ be [[Definition:Free Module|free $R$-modules]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $m, n, p > 0$ respectively. Let $\mathcal A,\mathcal B,\mathcal C$ be [[Definition:Ordered Basis (Linear Algebra)|ordered bases]] of $M, N, P$. Let $f: M \to N$ and $g : N \to P$ be [[Definition:Linear Transformation|linear transformations]], and $g \circ f$ be their [[Definition:Composition of Mappings|composition]]. Let $\mathbf M_{f, \mathcal B, \mathcal A}$ and $\mathbf M_{g, \mathcal C, \mathcal B}$ be their [[Definition:Relative Matrix|matrices relative]] to $\mathcal A, \mathcal B$ and $\mathcal B, \mathcal C$ respectively. Then the [[Definition:Matrix|matrix]] of $g \circ f$ [[Definition:Relative Matrix|relative to]] $\mathcal A$ and $\mathcal C$ is: :$\mathbf M_{g \mathop \circ f, \mathcal C, \mathcal A} = \mathbf M_{g, \mathcal C, \mathcal B}\cdot \mathbf M_{f, \mathcal B, \mathcal A}$	1
:$\ds \lambda \circ \paren {\sum_{k \mathop = 1}^m x_k} = \sum_{k \mathop = 1}^m \paren {\lambda \circ x_k}$	1
Let $A = \struct {A_R, \oplus}$ be an [[Definition:Algebra over Ring|algebra over the ring]] $R$ such that $A$ is ''not'' a [[Definition:Boolean Algebra|boolean algebra]]. Then $A$ is [[Definition:Alternative Algebra|alternative]] {{iff}}: :$\forall a, b \in A: \paren {a \oplus a} \oplus b = a \oplus \paren {a \oplus b}$ :$\forall a, b \in A: \paren {b \oplus a} \oplus a = b \oplus \paren {a \oplus a}$	1
Let $\mathbf v \in \map {\mathrm N} {\mathbf A}$, $\lambda \in \R$. By the definition of [[Definition:Null Space|null space]]: {{begin-eqn}} {{eqn | l = \mathbf {A v} | r = \mathbf {0} }} {{end-eqn}} Observe that: {{begin-eqn}} {{eqn | l = \mathbf A \paren {\lambda \mathbf v} | r = \lambda \paren {\mathbf {A v} } | c = [[Matrix Multiplication is Homogeneous of Degree 1|Matrix Multiplication is Homogeneous of Degree $1$]] }} {{eqn | r = \lambda \mathbf 0 | c = }} {{eqn | r = \mathbf 0 }} {{end-eqn}} Hence the result, by the definition of [[Definition:Null Space|null space]]. {{qed}}	1
Let $D$ be an [[Definition:Everywhere Dense|everywhere dense]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Countable Set|countable]], as is guaranteed as $T$ is [[Definition:Separable Space|separable]]. Consider the [[Definition:Mapping|mapping]] $\Phi: S \to 2^{\powerset D}$ defined as: :$\forall x \in S: \map {\map \Phi x} A = 1 \iff A = D \cap U_x$ for some [[Definition:Neighborhood of Point|neighborhood]] $U_x$ of $x$ {{explain|It is not clear in Steen & Seeabch what is meant by $\Phi: S \to 2^{\powerset D}$ -- presumably $2^{\powerset D}$ is the ordinal which is the power set of the power set of $D$. It is also not clear what the notation $\map {\map \Phi x} A$ means -- in fact is may be the case that a transcription error has been committed. Hence the proof cannot be attempted until these points have been cleared up.}} It is seen that if $T$ is a [[Definition:Hausdorff Space|Hausdorff space]], then $\Phi$ is an [[Definition:Injection|injection]]. It follows that: :$\card S \le \card {2^{\powerset D} } = 2^{2^{\aleph_0} }$ {{explain|the chain of reasoning leading to the above}}	1
Let $U$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$. By the definition of the [[Definition:Fortissimo Space|fortissimo space]], $U$ is [[Definition:Closed Set (Topology)|closed]]. From [[Closed Set Equals its Closure]], $U^- = U \ne S$. Thus, by definition, $U$ is not [[Definition:Everywhere Dense|everywhere dense in $T$]]. Thus, there exists no [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense in $T$]]. So, by definition, $T$ is not a [[Definition:Separable Space|separable space]]. {{qed}}	1
:$\norm {\, \cdot \,}_1$ is the [[Definition:Trivial Norm on Division Ring|trivial norm]].	1
The [[Cauchy-Binet Formula]] gives: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m}}\right) \det \left({\mathbf B_{j_1 j_2 \ldots j_m}}\right)$ where: :$\mathbf A$ is an [[Definition:Matrix|$m \times n$ matrix]] :$\mathbf B$ is an [[Definition:Matrix|$n \times m$ matrix]]. :For $1 \le j_1, j_2, \ldots, j_m \le n$: ::$\mathbf A_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. ::$\mathbf B_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. When $m = n$, the only set $j_1, j_2, \ldots, j_m$ that fulfils $1 \le j_1 < j_2 < \cdots < j_m \le n$ is $\left\{ {1, 2, \ldots, n}\right\}$. Hence the result. {{qed}}	1
Let $\Omega$ denote the first [[Definition:Uncountable Ordinal|uncountable ordinal]]. Let $\closedint 0 \Omega$ denote the [[Definition:Uncountable Closed Ordinal Space|closed ordinal space]] on $\Omega$. Then $\closedint 0 \Omega$ is not a [[Definition:Separable Space|separable space]].	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\exp z$ denote the [[Definition:Complex Exponential Function|complex exponential function]]. Let $\cmod {\, \cdot \,}$ denote the [[Definition:Complex Modulus|complex modulus]] Then: :$\cmod {\exp z} = \map \exp {\Re z}$ where $\Re z$ denotes the [[Definition:Real Part|real part]] of $z$.	1
Let $\R^n$ be an [[Definition:Dimension of Vector Space|$n$-dimensional]] [[Definition:Real Vector Space|real vector space]]. Let the [[Definition:Euclidean Metric/Real Vector Space|Euclidean metric]] $d$ be applied to $\R^n$. Then $\struct {\R^n, d}$ is a '''Euclidean $n$-space'''.	1
[[Sum of Elements of Invertible Matrix]] implies the sum of elements in $V_n^{-1}$ is: :$ 1 - \det \paren { V_n^{-1} } \det \paren { V_n - J_n } $. The plan is to expand $\det \paren { V_n - J_n } $ and simplify. The method is efficiently communicated in the $3\times 3$ case: {{begin-eqn}} {{eqn | l = \det \paren { V_3 - J_3 } = | r = \det \paren { \begin{smallmatrix} x_1 - 1 & x_2 - 1 & x_3 - 1 \\ x_1^2 - 1 & x_2^2 - 1 & x_3^2 - 1 \\ x_1^3 - 1 & x_2^3 - 1 & x_3^3 - 1 \\ \end{smallmatrix} } | c = $J_3$ is the $3\times 3$ ones matrix. }} {{eqn | l = | r = \paren {x_1 - 1} \paren {x_2 - 1} \paren {x_3 - 1} \det \paren { \begin{smallmatrix} 1 & 1 & 1 \\ x_1+1 & x_2+1 & x_3+1 \\ x_1^2+x+1+1 & x_2^2+x_2+1 & x_3^2+x_3+1 \\ \end{smallmatrix} } | c = Use $a^n - b^n$ factorization. [[Effect of Elementary Row Operations on Determinant]] collects common column factors outside the determinant. }} {{eqn | l = | r =\paren {x_1 - 1} \paren {x_2 - 1} \paren {x_3 - 1} \det \paren { \begin{smallmatrix} 1 & 1 & 1 \\ x_1 & x_2 & x_3 \\ x_1^2 & x_2^2 & x_3^2 \\ \end{smallmatrix} } | c = [[Effect of Elementary Row Operations on Determinant]] simplifies the determinant. }} {{end-eqn}} The proof is completed by '''induction''' using the lemmas below. === Lemma 1 === Let: :$\displaystyle A_n = \det \paren { \begin{smallmatrix} 1 & \cdots & 1 \\ x_1 & \cdots & x_n \\ \vdots & \cdots & \vdots \\ x_1^{n-1} & \cdots & x_n^{n-1} \\ \end{smallmatrix} }$ Then: :$\displaystyle \det \paren { V_n } = \paren { \prod_{k \mathop = 1}^n x_k } \det \paren { A_n }$ [[Effect of Elementary Row Operations on Determinant]] === Lemma 2 === :$\displaystyle \det(V_n - J_n) = \prod_{k \mathop = 1}^n \paren { x_k - 1 } \det \paren { A_n }$ === Lemma 3 === :$\displaystyle \det \paren { V_n^{-1} } \det(V_n - J_n) = \dfrac { \prod_{k \mathop = 1}^n \paren { x_k - 1 } } { \prod_{k \mathop = 1}^n { x_k } }$ by Lemmas 1 and 2. {{qed}}	1
The proof proceeds by [[Principle of Mathematical Induction|induction]] over $n$, the [[Definition:Order of Square Matrix|order of the square matrix]]. === Basis for the Induction === Let $n = 2$, which is the smallest [[Definition:Natural Number|natural number]] for which a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]] can have two identical [[Definition:Row of Matrix|rows]]. Let $\mathbf A = \left[{a}\right]_2$ be a [[Definition:Square Matrix|square matrix]] over $R$ with two identical [[Definition:Row of Matrix|rows]]. Then, by definition of [[Definition:Determinant of Matrix|determinant]]: :$\det \left({\mathbf A}\right) = a_{11}a_{22} - a_{12}a_{21} = a_{11}a_{22}-a_{22}a_{11} = 0$ {{qed|lemma}} This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Assume for $n \in \N_{\ge 2}$ that any [[Definition:Square Matrix|square matrices of order $n$]] over $R$ with two identical [[Definition:Row of Matrix|rows]] has [[Definition:Determinant of Matrix|determinant]] equal to $0$. === Induction Step === Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n+1$]] over $R$ with two identical [[Definition:Row of Matrix|rows]]. Let $i_1, i_2 \in \left\{ {1, \ldots, n+1}\right\}$ be the indices of the identical rows, and let $i_1 < i_2$. Let $i \in \left\{ {1, \ldots, n+1}\right\}$. Let $\mathbf A \left({i ; 1}\right)$ denote the [[Definition:Submatrix|submatrix]] obtained from $\mathbf A$ by removing row $i$ and [[Definition:Column of Matrix|column]] $1$. If $i \ne i_1$ and $i \ne i_2$, then $\mathbf A \left({i ; 1}\right)$ still contains two identical rows. By the [[Square Matrix with Duplicate Rows has Zero Determinant/Proof 1#Induction Hypothesis|induction hypothesis]]: : $\det \left({\mathbf A \left({i ; 1}\right) }\right) = 0$ Now consider the [[Definition:Matrix|matrices]] $\mathbf A \left({i_1 ; 1}\right)$ and $\mathbf A \left({i_2 ; 1}\right)$. Let $r_j$ denote row $j$ of $\mathbf A \left({i_1 ; 1}\right)$. If we perform the following [[Definition:Sequence|sequence]] of $i_2 - i_1 -1$ [[Definition:Elementary Row Operation|elementary row operations]] on $\mathbf A \left({i_1 ; 1}\right)$: :$r_{i_1} \leftrightarrow r_{i_1 +1} \ ; \ r_{i_1 + 1} \leftrightarrow r_{i_1 +2} \ ; \ \ldots \ ; \ r_{i_2 - 1} \leftrightarrow r_{i_2}$ we will transform $\mathbf A \left({i_1 ; 1}\right)$ into $\mathbf A \left({i_2 ; 1}\right)$. From [[Determinant with Rows Transposed]], it follows that $\det \left({\mathbf A \left({i_1 ; 1}\right) }\right) = \left({-1}\right)^{i_2 - i_1 - 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right)$ Then: {{begin-eqn}} {{eqn | l = \det \left({\mathbf A}\right) | r = \sum_{k \mathop = 1 }^{n + 1} a_{k1} \left({-1}\right)^{k + 1} \det \left({\mathbf A \left({k ; 1}\right) }\right) | c = [[Expansion Theorem for Determinants|expanding]] the determinant along column $1$ }} {{eqn | r = a_{i_1 1}\left({-1}\right)^{i_1 + 1} \det \left({\mathbf A \left({i_1 ; 1}\right) }\right) + a_{i_2 1} \left({-1}\right)^{i_2 + 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right) }} {{eqn | r = a_{i_2 1} \left({-1}\right)^{i_1 + 1 + i_2 - i_1 - 1} \det \left({\mathbf A \left({i_2; 1}\right) }\right) + a_{i_2 1} \left({-1}\right)^{i_2 + 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right) }} {{eqn | r = 0 }} {{end-eqn}} {{qed}}	1
From [[Elementary Row Matrix for Inverse of Elementary Row Operation is Inverse]] it is demonstrated that: :if $\mathbf E$ is the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to an [[Definition:Elementary Row Operation|elementary row operation]] $e$ then: :the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$ corresponds to an [[Definition:Elementary Row Matrix|elementary row matrix]] which is the [[Definition:Inverse Matrix|inverse]] of $\mathbf E$. So as $\mathbf E$ has an [[Definition:Inverse Matrix|inverse]], [[Definition:A Priori|a priori]] it is [[Definition:Invertible Matrix|invertible]]. {{qed}}	1
Let $\mathbf a, \mathbf b$ be a [[Definition:Vector Quantity|vector quantities]]. Let $m$ be a [[Definition:Scalar Quantity|scalar quantity]]. Then: :$m \paren {\mathbf a + \mathbf b} = m \mathbf a + m \mathbf b$	1
From [[Complex Modulus of Product of Complex Numbers]]: : $\left\vert{z_1 z_2}\right\vert = \left\vert{z_1}\right\vert \cdot \left\vert{z_2}\right\vert$ for $z_1, z_2 \in \C$. Set $z = z_1 = z_2$ and the result follows. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,}} $ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Let $\sequence {x_n} $ be a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$ Then: :$\sequence {x_n} $ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in the [[Definition:Normed Division Ring|normed division ring]] $\struct {R, \norm {\,\cdot\,} }$	1
By the [[Combination Theorem for Cauchy Sequences/Inverse Rule|Inverse Rule for Normed Division Ring]]: :$\exists K \in \N : \forall n > K : y_n \ne 0$. and the [[Definition:Sequence|sequence]]: :$\sequence {\paren {x_{K + n} }^{-1} }_{n \mathop \in \N}$ is well-defined and a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. By [[Subsequence of Cauchy Sequence in Normed Division Ring is Cauchy Sequence]], $\sequence {x_{K + n} }_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. By [[Combination Theorem for Cauchy Sequences/Product Rule|Product Rule for Normed Division Ring Sequences]]: :the [[Definition:Sequence|sequences]] $\sequence { {x_{K + n} } \paren {y_{K + n} }^{-1} }_{n \mathop \in \N}$ and $\sequence {\paren {y_{K + n} }^{-1} {x_{K + n} } }_{n \mathop \in \N}$ are [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequences]]. {{qed}}	1
Note that: $\left({R, +, \circ}\right)$ is a [[Definition:Ring (Abstract Algebra)|ring]] by assumption. $\left({R, +}\right)$ is an [[Definition:Abelian Group|abelian group]] by the definition of a [[Definition:Ring (Abstract Algebra)|ring]]. Let us verify the [[Definition:Module Axioms|module axioms]]: {{begin-axiom}} {{axiom | n = 1 | q = \forall x, y, z \in R | m = x \circ \left({y + z}\right) = \left({x \circ y}\right) + \left({x \circ z}\right) }} {{axiom | n = 2 | q = \forall x, y, z \in R | m = \left({x + y}\right) \circ z = \left({x \circ z}\right) + \left({y \circ z}\right) }} {{axiom | n = 3 | q = \forall x, y, z \in R | m = \left({x \circ y}\right) \circ z = x \circ \left({y \circ z}\right) }} {{end-axiom}} Axiom $(1)$ and $(2)$ follow from [[Definition:Ring (Abstract Algebra)|distributivity]] of $\circ$. Axiom $(3)$ follows from [[Definition:Ring (Abstract Algebra)|associativity]] of $\circ$. {{qed|lemma}} Assume now that $\left({R, +, \circ}\right)$ has a [[Definition:Unity of Ring|unity]], $1_R$. For $\left({R, +, \circ}\right)_R$ to be [[Definition:Unitary Module|unitary]], it must satisfy the additional axiom: {{begin-axiom}} {{axiom | n = 4 | q = \forall x \in R | m = 1_R \circ x = x }} {{end-axiom}} The axiom follows from the definition of a [[Definition:Unity of Ring|unity]]. {{qed}}	1
Let $\struct {V, \norm {\,\cdot\,} }$ be a [[Definition:Normed Vector Space|normed]] [[Definition:Vector Space|vector space]]. Then the [[Definition:Mapping|mapping]] $x \mapsto \norm x$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]]. Here, the [[Definition:Metric|metric]] used is the metric $d$ [[Definition:Metric Induced by Norm|induced]] by $\norm {\,\cdot\,}$.	1
Let $H$ be a [[Definition:Finite-Dimensional Hilbert Space|finite-dimensional]] [[Definition:Real Hilbert Space|real]] or [[Definition:Complex Hilbert Space|complex]] [[Definition:Hilbert Space|Hilbert space]] (that is, [[Definition:Inner Product Space|inner product space]]). Let $t: H \to H$ be a [[Definition:Normal Operator|normal operator]] on $H$. Let $t$ stabilize a subspace $V$. Then $t$ also stabilizes its [[Definition:Orthogonal (Hilbert Space)#Orthogonal Complement|orthogonal complement]] $V^\perp$.	1
Let $V$ be a [[Definition:Vector Space|vector space]] of real or complex-valued functions on a set $J$. Let $f_1, \ldots, f_n$ be functions in $V$. Let '''samples''' $x_1, \ldots, x_n$ from $J$ be given. Define '''sample matrix''' :$\displaystyle S = \begin{bmatrix} f_1(x_1) & \cdots & f_n(x_1) \\ \vdots & \ddots & \vdots \\ f_1(x_n) & \cdots & f_n(x_n) \\ \end{bmatrix}$ Let $S$ be [[Definition:Invertible Matrix|invertible]]. Then $f_1, \ldots, f_n$ are [[Definition:Linearly Independent|linearly independent]] in $V$.	1
Follows directly from [[Dimension of Proper Subspace is Less Than its Superspace]]. {{qed}}	1
:$\left \langle {A \mathbf u,\mathbf v} \right \rangle = \left \langle {\mathbf u, A^T\mathbf v} \right \rangle$ where $\mathbf u$ and $\mathbf v$ are both $1 \times n$ column vectors.	1
Let $R$ be a [[Definition:Unique Factorization Domain|unique factorization domain]]. Then the [[Definition:Ring of Polynomials|ring of polynomials]] $R \sqbrk X$ is also a [[Definition:Unique Factorization Domain|unique factorization domain]].	1
=== Positive definiteness === Let $x \in \CC \closedint a b$. Then $\forall t \in \closedint 0 1 : \size {\map x t} \ge 0 $. Hence: :$\displaystyle \int_a^b \size {\map x t} \rd t = \norm x_1 \ge 0$. Suppose $\forall t \in \closedint a b : \map x t = 0$. Then $\norm x_1 = 0$. Therefore: :$\paren {x = 0} \implies \paren {\norm x_1 = 0}$ Let $x \in \CC \closedint a b : \norm x_1 = 0$. Suppose: :$\forall t \in \openint a b : \map x t = 0$. By [[Definition:Assumption|assumption]] of [[Definition:Continuous Real Function on Closed Interval|continuity]] of $x$: :$\forall t \in \closedint a b : \map x t = 0$. {{AimForCont}}: :$\exists t_0 \in \openint a b : \map x {t_0} \ne 0$. By [[Definition:Assumption|assumption]], $x$ is [[Definition:Continuous Real Function on Closed Interval|continuous]] at $t_0$. :$\forall \epsilon \in \R_{> 0} : \exists \delta \in \R_{> 0} : \size {t - t_0} < \delta \implies \size {\map x t - \map x {t_0} } < \epsilon$ Furthermore: :$\exists \delta \in \R_{> 0} : \paren{ a < t_0 - \delta} \land \paren {t_0 + \delta < b}$ Let $\displaystyle \epsilon = \frac {\size {\map x {t_0}}} 2$. We have that: {{begin-eqn}} {{eqn | l = \size {\map x t} | r = \size {\map x t + \map x {t_0} - \map x {t_0} } }} {{eqn | o = \ge | r = \size {\map x {t_0} } - \size {\map x t - \map x {t_0} } | c = [[Reverse Triangle Inequality/Normed Vector Space|Reverse triangle inequality]] }} {{eqn | o = > | r = \size {\map x {t_0} } - \frac {\size {\map x {t_0} } } 2 }} {{eqn | r = \frac {\size {\map x {t_0} } } 2 }} {{eqn | o = > | r = 0 | c = $\map x {t_0} \ne 0$ }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = 0 | r = \norm x_1 }} {{eqn | r = \int_a^b \size {\map x t} \rd t }} {{eqn | o = \ge | r = \int_{t_0 - \delta}^{t_0 + \delta} \size {\map x t} \rd t | c = $t_0 + \delta < b$, $a < t_0 - \delta$ }} {{eqn | o = \ge | r = \int_{t_0 - \delta}^{t_0 + \delta} \frac {\size {\map x {t_0} } } 2 \rd t }} {{eqn | r = 2 \delta \frac {\size {\map x {t_0} } } 2 }} {{eqn | r = \delta \size {\map x {t_0} } }} {{eqn | o = > | r = 0 }} {{end-eqn}} Hence, we reached a [[Definition:Contradiction|contradiction]]. Therefore: :$\paren {\norm x_1 = 0} \implies \paren {x = 0}$ === Positive homogeneity === Let $x \in \CC \closedint a b$, $\alpha \in \R$. Then: {{begin-eqn}} {{eqn | l = \size {\alpha x}_1 | r = \int_a^b \size {\alpha \map x t} \rd t }} {{eqn | r =\size \alpha \int_a^b \size {\map x t} \rd t }} {{eqn | r =\size \alpha \norm x_1 }} {{end-eqn}} === Triangle inequality === Let $x, y \in \CC \closedint a b$ {{begin-eqn}} {{eqn | l = \norm {x + y}_1 | r = \int_a^b \size {\map {\paren {x + y} } t} \rd t }} {{eqn | r = \int_a^b \size {\map x t + \map y t} \rd t | c = {{defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | o = \le | r = \int_a^b \paren {\size x + \size y} \rd t | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | r = \int_a^b \size x \rd t + \int_a^b \size y \rd t }} {{eqn | r = \norm x_1 + \norm y_1 }} {{end-eqn}} {{qed}}	1
Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ERO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Row of Matrix|row]] $j$ to [[Definition:Row of Matrix|row]] $i$ | m = r_i \to r_i + \lambda r_j }} {{end-axiom}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be an [[Definition:Module|$R$-module]]. Then for every $m \in M$ there exists a unique $R$-module morphism: :$\psi: R \to M$ that sends $1$ to $m$.	1
=== Necessary Condition === When $\theta$ denotes the [[Definition:Angle Between Vectors|angle]] between $\mathbf u$ and $\mathbf v$ measured in [[Definition:Radian|radians]], we have: {{begin-eqn}} {{eqn | l = 0 | r = \mathbf u \cdot \mathbf v | c = {{Defof|Orthogonal Vectors}} }} {{eqn | r = \left\Vert{\mathbf u}\right\Vert \left\Vert{\mathbf v}\right\Vert \cos \theta | c = [[Cosine Formula for Dot Product]] }} {{end-eqn}} where $\left\Vert{\cdot}\right\Vert$ denotes the [[Definition:Euclidean Norm|Euclidean norm]]. As $\mathbf u, \mathbf v$ are non-[[Definition:Zero Vector|zero vectors]], it follows from the [[Definition:Norm on Vector Space|norm axiom of positive definiteness]] that $\left\Vert{\mathbf u}\right\Vert \ne 0$, and $\left\Vert{\mathbf v}\right\Vert \ne 0$. Then $\cos \theta = 0$. It follows from [[Zeroes of Sine and Cosine]] that $\theta = \dfrac \pi 2$, given that $\theta \in \left[{0\,.\,.\,\pi}\right]$ by the convention of the [[Definition:Angle Between Vectors|definition of angle between vectors]]. Then $\mathbf u$ and $\mathbf v$ are [[Definition:Perpendicular (Linear Algebra)|perpendicular]] by the [[Definition:Right Angle|measurement of a right angle]]. {{qed|lemma}} === Sufficient Condition === If $\mathbf u$ and $\mathbf v$ are [[Definition:Perpendicular (Linear Algebra)|perpendicular]], then the [[Definition:Angle Between Vectors|angle]] between them measures $\dfrac \pi 2$ in [[Definition:Radian|radians]]. Then $\cos \theta = 0$, and the calculations above show that $\mathbf u \cdot \mathbf v = 0$. Hence, $\mathbf u$ and $\mathbf v$ are [[Definition:Orthogonal (Linear Algebra)|orthogonal]]. {{qed}} [[Category:Linear Algebra]] [[Category:Analytic Geometry]] ojyl8da3qmaktopioezmvwcsytw6juf	1
Let $x, y \in L, t \in \closedint 0 1$ be arbitrary. Then, as $L$ is by definition closed under addition and scalar multiplication, it follows immediately that :$t x + \paren {1 - t} y \in L$ Hence $L$ is [[Definition:Convex Set (Vector Space)|convex]]. {{qed}}	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,}}$. By [[Convergent Sequence is Cauchy Sequence/Normed Division Ring|Convergent Sequence is Cauchy Sequence in Normed Division Ring]], $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {R, \norm {\,\cdot\,}}$. By [[Cauchy Sequence is Bounded/Normed Division Ring|Cauchy Sequence in Normed Division Ring is Bounded]], $\sequence {x_n}$ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in $\struct {R, \norm {\,\cdot\,}}$. {{qed}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct {A_R, *}$ be an [[Definition:Unital Algebra|unital algebra]] over $R$ whose [[Definition:Unit of Algebra|unit]] is $1_A$. Let $\struct {B_R, *}$ be a [[Definition:Subalgebra|subalgebra]] of $A_R$. {{TFAE|def = Unital Subalgebra}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\adj {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Then: :$\paren {\adj {\mathbf A} } \mathbf A = \map \det {\mathbf A} \mathbf I = \mathbf A \paren {\adj {\mathbf A} }$ where $\mathbf I$ denotes the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]].	1
We have that: : [[Rationals are Dense in Compact Complement Topology]] : [[Rational Numbers are Countably Infinite]] Hence the result by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
It is first shown that $\sequence {\norm {x_n} }$ is a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] in $\R$. Let $\epsilon \in \R_{>0}$ be given. By the definition of [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]] then: :$\exists N \in \N: \forall n, m > N, \norm {x_n - x_m} < \epsilon$ By the [[Reverse Triangle Inequality/Normed Division Ring|reverse triangle inequality]], then: :$\forall n, m > N: \cmod {\norm {x_n} - \norm {x_m} } \le \norm {x_n - x_m} < \epsilon$ From the definition of a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] it follows that $\sequence {\norm {x_n} }$ is a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] in $\R$. By [[Real Sequence is Cauchy iff Convergent]] the sequence $\sequence {\norm {x_n} }$ has a [[Definition:Limit of Real Sequence|limit]] in $\R$. {{qed}}	1
{{ProofWanted|tedious}}	1
:[[File:OrderedBasisForPlane.png|500px|right|thumb]] Let $P$ be any [[Definition:Point|point]] in the [[Definition:Plane|plane]] for which we want to provide a linear combination of $a_1$ and $a_2$. Let the [[Definition:Length (Linear Measure)|distance]] from $O$ to the point determined by $a_1$ be defined as being $1$ unit of length on the [[Definition:Line|line]] $L_1$. Let the [[Definition:Length (Linear Measure)|distance]] from $O$ to the point determined by $a_2$ be defined as being $1$ unit of length on the [[Definition:Line|line]] $L_2$. Draw [[Definition:Parallel Lines|lines parallel]] to $L_1$ and $L_2$ through $P$. Then the [[Definition:Coordinate (Coordinate System)|coordinates]] $\lambda_1$ and $\lambda_2$ of $P$ are given by: :$P = \lambda_1 a_1 + \lambda_2 a_2$ by the [[Parallelogram Law]]. {{ProofWanted}}	1
Let $\mathbf V$ be a [[Definition:Vector Space|vector space]], with [[Definition:Zero Vector|zero]] $\mathbf 0$. Likewise let $\mathbf V\,'$ be another vector space, with [[Definition:Zero Vector|zero]] $\mathbf 0'$. Let $T: \mathbf V \to \mathbf V\,'$ be a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Then: :$T: \mathbf 0 \mapsto \mathbf 0'$	1
Define a [[Definition:Relation|relation]] $\sim$ on the [[Definition:Cartesian Product|Cartesian product]] $A \times S$ by: :$\tuple {a, s} \sim \tuple {b, t} \iff \exists u \in S: a t u = b s u$ === [[Localization of Ring Exists/Lemma 1|Lemma 1]] === {{:Localization of Ring Exists/Lemma 1}}{{qed|lemma}} Let $A_S$ be used to denote the $\paren {A \times S} / \sim$. Let $a / s$ be the [[Definition:Equivalence Class|equivalence class]] of $\tuple {a, s}$ in $\paren {A \times S} / \sim$. For $\dfrac a s, \dfrac b t \in A_S$, let the following be defined: :$\dfrac a s + \dfrac b t = \dfrac {a t + b s} {s t}$ :$\dfrac a s \cdot \dfrac b t = \dfrac {a b} {s t}$ === [[Localization of Ring Exists/Lemma 2|Lemma 2]] === {{:Localization of Ring Exists/Lemma 2}}{{qed|lemma}} Now define $\iota: A \to A_S$ by: :$\map \iota a := \dfrac a 1$ It is to be shown that $\struct {A_S, \iota}$ satisfy the universal property for localization. {{MissingLinks|universal property for localization}} Let $B$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $g: A \to B$ be a [[Definition:Mapping|mapping]] such that: :$\map g S \subseteq B^\times$ {{explain|$B^\times$}} Suppose that $h: A_S \to B$ is a [[Definition:Ring Homomorphism|ring homomorphism]] with $h \circ \iota = g$. Then we must have: :$\map h {\dfrac a 1} = \map g a$ and: :$\map h {\dfrac 1 s} \cdot \map h {\dfrac s 1} = 1$ Therefore: :$\map h {\dfrac 1 s} \map g s = 1$ so: :$\map h {\dfrac 1 s} = \map g s^{-1}$ Therefore: :$\map h {\dfrac a s} = \map h {\dfrac a 1} \cdot \map h {\dfrac 1 s} = \map g a \map g s^{-1}$. So if such $h$ exists it must equal $\map g a \map g s^{-1}$, so is unique. {{MissingLinks|Links to results demonstrating the validity of the steps in the above.}} Therefore, to conclude the proof we pull out: === [[Localization of Ring Exists/Lemma 3|Lemma 3]] === {{:Localization of Ring Exists/Lemma 3}}{{qed|lemma}} Hence the result. {{qed}}	1
Let $a \in D$ be a [[Definition:Proper Element of Ring|proper element]] of $D$. Because $\struct {D, +, \circ}$ is not a [[Definition:Field (Abstract Algebra)|field]], such an [[Definition:Element|element]] is known to exist. From [[Principal Ideal in Integral Domain generated by Power Plus One is Subset of Principal Ideal generated by Power]]: :$\forall n \in \Z_{\ge 0}: \ideal {a^{n + 1} } \subsetneq \ideal {a_n}$ where $\ideal x$ denotes the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $x$]]. Hence the [[Definition:Set|set]]: :$S = \set {\ideal {1_D}, \ideal a, \ideal {a^2}, \ideal {a^3}, \dotsc}$ is [[Definition:Infinite Set|infinite]]. {{qed}}	1
From [[Finite Set of Elements in Principal Ideal Domain has GCD]] we have that at least one such [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] exists. So, let $y$ be a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] of $S$. Let $J$ be the [[Definition:Set|set]] of all [[Definition:Linear Combination|linear combinations]] in $D$ of $\set {a_1, a_2, \dotsc, a_n}$. From [[Set of Linear Combinations of Finite Set of Elements of Principal Ideal Domain is Principal Ideal]]: :$J = \ideal x$ for some $x \in D$, where $\ideal x$ denotes the [[Definition:Principal Ideal of Ring|principal ideal]] [[Definition:Generator of Ideal|generated]] by $x$. From [[Finite Set of Elements in Principal Ideal Domain has GCD]], $x$ is a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] of $S$. From [[Greatest Common Divisors in Principal Ideal Domain are Associates]], $y$ is an [[Definition:Associate in Integral Domain|associate]] of $x$. By definition of [[Definition:Associate in Integral Domain|associate]]: :$\ideal y = \ideal x$ Therefore: :$y \in J$ and so by definition, $y$ is expressible in the form: :$y = d_1 a_1 + d_2 a_2 + \dotsb + d_n a_n$ where $d_1, d_2, \dotsc, d_n \in D$. {{qed}}	1
Let $X$ be a [[Definition:Vector Space|vector space]]. Let $\norm {\, \cdot \,}_a$ and $\norm {\, \cdot \,}_b$ be [[Definition:Equivalence of Norms|equivalent norms]] on $X$. Denote this [[Definition:Relation|relation]] by $\sim$: :$\norm {\, \cdot \,}_a \sim \norm {\, \cdot \,}_b$. Then $\sim$ is an [[Definition:Equivalence Relation|equivalence relation]].	1
Let $\mathbf a$ be a [[Definition:Vector Quantity|vector quantity]]. Let $m, n$ be [[Definition:Scalar Quantity|scalar quantities]]. Then: :$m \paren {n \mathbf a} = \paren {m n} \mathbf a = n \paren {m \mathbf a}$	1
=== [[Sequence is Bounded in Norm iff Bounded in Metric/Necessary Condition|Necessary Condition]] === {{:Sequence is Bounded in Norm iff Bounded in Metric/Necessary Condition}}{{qed|lemma}} === [[Sequence is Bounded in Norm iff Bounded in Metric/Sufficient Condition|Sufficent Condition]] === {{:Sequence is Bounded in Norm iff Bounded in Metric/Sufficient Condition}}{{qed}}	1
By definition of [[Definition:Kronecker Delta|Kronecker delta]]: :$\delta_{i j} = \begin {cases} 1 & : i = j \\ 0 & : i \ne j \end {cases}$ Thus: :$\delta_{i j} a_{j k} = \begin {cases} a_{i k} & : i = j \\ 0 & : i \ne j \end {cases}$ and: :$\delta_{i j} a_{k j} = \begin {cases} a_{k i} & : i = j \\ 0 & : i \ne j \end {cases}$ from which the result follows. {{qed}}	1
Self-evident. {{Qed}}	1
Let $z_1 = x_1 + i y_1$ and $z_2 = x_2 + i y_2$, where $x_1, y_1, x_2, y_2 \in \R$. {{begin-eqn}} {{eqn | l = \cmod {z_1 z_2} | r = \sqrt {\paren {x_1 x_2 - y_1 y_2}^2 + \paren {x_1 y_2 + x_2 y_1}^2} | c = {{Defof|Complex Modulus}}, {{Defof|Complex Multiplication}} }} {{eqn | r = \sqrt {\paren {x_1^2 x_2^2 + y_1^2 y_2^2 - 2 x_1 x_2 y_1 y_2} + \paren {x_1^2 y_2^2 + x_2^2 y_1^2 + 2 x_1 x_2 y_1 y_2} } | c = }} {{eqn | r = \sqrt {x_1^2 x_2^2 + y_1^2 y_2^2 + x_1^2 y_2^2 + x_2^2 y_1^2} | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \cmod {z_1} \cdot \cmod {z_2} | r = \sqrt {x_1^2 + y_1^2} \sqrt {x_2^2 + y_2^2} | c = }} {{eqn | r = \sqrt {\paren {x_1^2 + y_1^2} \paren {x_2^2 + y_2^2} } | c = }} {{eqn | r = \sqrt {x_1^2 x_2^2 + y_1^2 y_2^2 + x_1^2 y_2^2 + x_2^2 y_1^2} | c = }} {{end-eqn}} {{qed}}	1
From [[Elementary Column Operations as Matrix Multiplications]], an [[Definition:Elementary Column Operation|elementary column operation]] on $\mathbf A$ is equivalent to [[Definition:Matrix Product (Conventional)|matrix multiplication]] by the [[Definition:Elementary Column Matrix|elementary column matrices]] corresponding to the [[Definition:Elementary Column Operation|elementary column operations]]. From [[Determinant of Elementary Column Matrix]], the [[Definition:Determinant of Matrix|determinants]] of those [[Definition:Elementary Column Matrix|elementary column matrices]] are as follows: === [[Determinant of Elementary Column Matrix/Scale Column|Scale Column]] === {{:Determinant of Elementary Column Matrix/Scale Column}} === [[Determinant of Elementary Column Matrix/Scale Column and Add|Add Scalar Product of Column to Another]] === {{:Determinant of Elementary Column Matrix/Scale Column and Add}} === [[Determinant of Elementary Column Matrix/Exchange Columns|Exchange Columns]] === {{:Determinant of Elementary Column Matrix/Exchange Columns}} Hence the result. {{qed}}	1
Let $A$ be a [[Definition:Division Algebra|division algebra]], in the sense that: :$\forall a, b \in A_F, b \ne \mathbf 0_A: \exists_1 x \in A_F, y \in A_F: a = b \oplus x, a = y \oplus b$ Suppose that $\exists a, b \in A_F \setminus \left\{{\mathbf 0_A}\right\}: \mathbf 0_A = b \oplus a$. Then by definition of the [[Definition:Zero Vector|zero vector]] we also have that $\mathbf 0_A = b \oplus \mathbf 0_A$. So there are two elements $x$ of $A_F$ such that $\mathbf 0_A = b \oplus x$, that is, $a$ and $\mathbf 0_A$. Similarly, suppose that $\exists a, b \in A_F \setminus \left\{{\mathbf 0_A}\right\}: \mathbf 0_A = a \oplus b$. Then by definition of the [[Definition:Zero Vector|zero vector]] we also have that $\mathbf 0_A = \mathbf 0_A \oplus b$. So there are two elements $y$ of $A_F$ such that $\mathbf 0_A = y \oplus b$, that is, $a$ and $\mathbf 0_A$. So $A$ can not be a [[Definition:Division Algebra|division algebra]]. So it follows that if: :$\forall a, b \in A_F, b \ne \mathbf 0_A: \exists_1 x \in A_F, y \in A_F: a = b \oplus x, a = y \oplus b$ then: :$\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$ {{qed|lemma}} Now suppose that: :$\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$ Suppose $\exists x_1, x_2 \in A_F, x_1 \ne x_2$ such that: :$a = b \oplus x_1$ :$a = b \oplus x_2$ We have that $x_1 \ne x_2$ and so $x_1 - x_2 = z \ne \mathbf 0_A$. Thus $b \oplus x_1 - b \oplus x_2 = \mathbf 0_A$. As $\oplus$ is a [[Definition:Bilinear Mapping|bilinear mapping]], it follows that $b \oplus \left({x_1 - x_2}\right) = \mathbf 0_A$ and so $b \oplus z = \mathbf 0_A$. But $z \ne \mathbf 0_A$ and so it is not the case that $\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$. Similarly it can be shown that if $\exists y_1, y_2 \in A_F, y_1 \ne y_2$ such that: :$a = y_1 \oplus b$ :$a = y_2 \oplus b$ then it is not the case that $\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$. Now suppose that: :$\exists a, b \in A_F, b \ne \mathbf 0_A: \not \exists_1 x \in A_F: a = b \oplus x$ or :$\exists a, b \in A_F, b \ne \mathbf 0_A: \not \exists_1 y \in A_F: a = y \oplus b$ {{finish|Haven't found an approach yet ...}} Thus it is not the case that $\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$. So it follows that if: :$\forall a, b \in A_F: a \oplus b = \mathbf 0_A \implies a = \mathbf 0_A \lor b = \mathbf 0_A$ then: :$\forall a, b \in A_F, b \ne \mathbf 0_A: \exists_1 x \in A_F, y \in A_F: a = b \oplus x, a = y \oplus b$ {{qed}} [[Category:Algebras]] ezmzalib7kvhrt517srmx91mhyslu5d	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Let $\mathbf 0_R = \sqbrk {0_R}_{m n}$ be the [[Definition:Zero Matrix over Ring|zero matrix]] of $\map {\MM_R} {m, n}$. Then $\mathbf 0_R$ is the [[Definition:Identity Element|identity element]] for [[Definition:Matrix Entrywise Addition over Ring|matrix entrywise addition]].	1
Let $A_\mathcal B$ and $A_\mathcal C$ be the [[Definition:Matrix|matrices]] of $A$ [[Definition:Relative Matrix|relative]] to $\mathcal B$ and $\mathcal C$ respectively. Let $\det$ also denote the [[Definition:Determinant of Matrix|determinant]] of a [[Definition:Matrix|matrix]]. We are required to show that $\det A_\mathcal B = \det A_\mathcal C$. Let $P$ be the [[Definition:Change of Basis Matrix|change of basis]] matrix from $\mathcal B$ to $\mathcal C$. By [[Change of Coordinate Vectors Under Linear Mapping]] and since $A_\mathcal B$ and $A_\mathcal C$ represent the same [[Definition:Linear Operator|linear operator]] with respect to different bases, the following diagram commutes: :[[File:Determinant Independent of Basis.png|250px]] where $u \in V$, and $\left[{u}\right]_\mathcal B$ indicates the [[Definition:Coordinate Vector|coordinate vector]] of $u$ with repect to $\mathcal B$, and similarly for $\left[{u}\right]_\mathcal C$. That is, $P A_\mathcal B = A_\mathcal C P$. Therefore, because a [[Change of Basis is Invertible]] we have: : $A_\mathcal B = P^{-1} A_\mathcal C P$ So: {{begin-eqn}} {{eqn | l = \det \left({A_\mathcal B}\right) | r = \det \left({P^{-1} A_\mathcal C P}\right) }} {{eqn | r = \det \left({P^{-1} }\right) \det \left({A_\mathcal C}\right) \det \left({P}\right) | c = [[Determinant of Matrix Product]] }} {{eqn | r = \det \left({P}\right)^{-1} \det \left({A_\mathcal C}\right) \det \left({P}\right) | c = [[Determinant of Inverse Matrix]] }} {{eqn | r = \det \left({A_\mathcal C}\right) }} {{end-eqn}} Hence the result. {{qed}} [[Category:Linear Operators]] [[Category:Determinants]] k7z62jj2gyon3luhlhzgyqlbikf8elv	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $A, B, C$ be [[Definition:Matrix|matrices]] over $R$ such that the [[Definition:Matrix Product|matrix product]] $ABC$ is defined. Then $\map {\operatorname {vec} }{ABC} = \paren {C^\intercal \otimes A} \cdot \map {\operatorname {vec} } B$ where: :$\operatorname {vec}$ denotes [[Definition:Vectorization of Matrix|vectorization]] :$C^\intercal$ is the [[Definition:Transpose of Matrix|transpose]] of $C$ :$\otimes$ denotes [[Definition:Kronecker Product of Matrices|Kronecker product]] :$\cdot$ denotes [[Definition:Matrix Product (Conventional)|matrix product]]	1
From [[Ring is Ideal of Itself]], $R$ is a [[Definition:Left Ideal|left ideal]]. From [[Left Ideal is Left Module over Ring]], $\struct {R, +, \times}$ is a [[Definition:Left Module|left module]] over $\struct {R, +, \times}$. {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. [[Definition:Matrix Product (Conventional)|Matrix multiplication (conventional)]] is [[Definition:Associative Operation|associative]].	1
Let $M_a = \struct {X, \norm {\, \cdot \, }_a}$ and $M_b = \struct {X, \norm {\, \cdot \,}_b}$ be [[Definition:Normed Vector Space|normed vector spaces]]. Let $\sequence {x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $M_a$. Suppose, $\norm {\, \cdot \, }_a$ and $\norm {\, \cdot \,}_b$ are [[Definition:Equivalence of Norms|equivalent norms]], i.e. $\norm {\, \cdot \, }_a \sim \norm {\, \cdot \,}_b$. Then $\sequence {x_n}_{n \mathop \in \N}$ is also a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $M_b$.	1
Let $x > 0$ and $x y > 0$. Suppose $y \le 0$. Then from [[Multiplicative Ordering on Integers]] and [[Ring Product with Zero]]: :$x y \le x \, 0 = 0$ From this contradiction we deduce that $y > 0$. Now, if we have $x > 0$ and $x y = 1$, then $y > 0$ and hence $y \in \N$ by [[Natural Numbers are Non-Negative Integers]]. Hence by [[Invertible Elements under Natural Number Multiplication]] it follows that $x = 1$. Thus $1$ is the only element of $\N$ that is [[Definition:Invertible Element|invertible]] for [[Definition:Natural Number Multiplication|multiplication]]. Therefore by [[Natural Numbers are Non-Negative Integers]] and [[Product with Ring Negative]], the result follows. {{qed}}	1
Let the [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator on $A$ be $*$. Let $\left({a, b}\right), \left({c, d}\right) \in A'$. In order to streamline notation, let $\oplus$ and $\oplus'$ both be denoted by [[Definition:Multiplicative Notation|product notation]]: :$a \oplus b =: a b$ :$x \oplus' y =: x y$ The context will make it clear which is meant. Let $A$ be a [[Definition:Nicely Normed Star-Algebra|nicely normed]] [[Definition:Associative Algebra|associative algebra]]. Then: {{begin-eqn}} {{eqn | l = \left({\left({a, b}\right) \left({a, b}\right)}\right) \left({c, d}\right) | r = \left({a a - b b^*, a^* b + a b}\right) \left({c, d}\right) | c = }} {{eqn | r = \left({\left({a a - b b^*}\right) c - d \left({a^* b + a b}\right)^*, \left({a a - b b^*}\right)^* d + c \left({a^* b + a b}\right)}\right) | c = }} {{eqn | r = \left({\left({a a - b b^*}\right) c - d \left({b^* a + b^* a^*}\right), \left({a^* a^* - b b^*}\right) d + c \left({a^* b + a b}\right)}\right) | c = from definition of [[Definition:Conjugation (Abstract Algebra)|conjugation]] }} {{eqn | r = \left({a a c - b b^* c - d b^* a - d b^* a^*, a^* a^* d - b b^* d + c a^* b + c a b}\right) | c = $A$ is [[Definition:Associative Algebra|associative]] }} {{eqn | r = \left({a a c - b b^* c - d b^* \left({a + a^*}\right), a^* a^* d - b b^* d + c \left({a^* + a}\right) b}\right) | c = }} {{eqn | r = \left({a a c - \left \Vert {b}\right \Vert^2 c - d b^* \left({2 \Re \left({a}\right)}\right), a^* a^* d - \left \Vert {b}\right \Vert^2 d + c \left({2 \Re \left({a}\right)}\right) b}\right) | c = $A$ is [[Definition:Nicely Normed Star-Algebra|nicely normed]] }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \left({a, b}\right) \left({\left({a, b}\right) \left({c, d}\right)}\right) | r = \left({a, b}\right) \left({a c - d b^*, a^* d + c b}\right) | c = }} {{eqn | r = \left({a \left({a c - d b^*}\right) - \left({a^* d + c b}\right) b^*, a^* \left({a^* d - c b}\right) + \left({a c + d b^*}\right) b}\right) | c = }} {{eqn | r = \left({a a c - a d b^* - a^* d b^* - c b b^*, a^* a^* d + a^* c b + a c b - d b^* b}\right) | c = $A$ is [[Definition:Associative Algebra|associative]] }} {{eqn | r = \left({a a c - \left({a + a^*}\right) d b^* - c \left \Vert {b}\right \Vert^2, a^* a^* d + \left({a^* + a}\right) c b - d b^* b}\right) | c = }} {{eqn | r = \left({a a c - \left({2 \Re \left({a}\right)}\right) d b^* - c \left \Vert {b}\right \Vert^2, a^* a^* d + \left({2 \Re \left({a}\right)}\right) c b - d \left \Vert {b}\right \Vert^2}\right) | c = $A$ is [[Definition:Nicely Normed Star-Algebra|nicely normed]] }} {{end-eqn}} Thus it can be seen that: : $\left({\left({a, b}\right) \left({a, b}\right)}\right) \left({c, d}\right) = \left({a, b}\right) \left({\left({a, b}\right) \left({c, d}\right)}\right)$ Similarly it can be shown that: : $\left({\left({c, d}\right) \left({a, b}\right)}\right) \left({a, b}\right) = \left({c, d}\right) \left({\left({a, b}\right) \left({a, b}\right)}\right)$ and so $A'$ is seen to be an [[Definition:Alternative Algebra|alternative algebra]]. It follows from reversing the chain of equalities that if $A'$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed]] and [[Definition:Alternative Algebra|alternative algebra]] then $A$ has to be a [[Definition:Nicely Normed Star-Algebra|nicely normed]] [[Definition:Associative Algebra|associative algebra]]. {{qed|lemma}} Then from [[Cayley-Dickson Construction from Nicely Normed Algebra is Nicely Normed]], we have that $A'$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]] {{iff}} $A$ is also a [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]]. Hence the result. {{qed}}	1
Let $A$ be a [[Definition:Nonassociative Algebra|nonassociative]] [[Definition:Division Algebra|division algebra]] with [[Definition:Real Number|real]] [[Definition:Scalar (Vector Space)|scalars]]. Then the [[Definition:Dimension (Linear Algebra)|dimension]] of $A$ is a [[Definition:Power|power]] of $2$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $P, Q$ be [[Definition:Projection (Hilbert Spaces)|projections]]. Then the following are equivalent: :$(1): \quad PQ$ is a [[Definition:Projection (Hilbert Spaces)|projection]] :$(2): \quad PQ = QP$ :$(3): \quad P + Q - PQ$ is a [[Definition:Projection (Hilbert Spaces)|projection]] {{MissingLinks|Provide proper linking to the def of addition and multiplication of operators}}	1
If two [[Definition:Row of Matrix|rows]] of a [[Definition:Square Matrix|square matrix]] over a [[Definition:Commutative Ring|commutative ring]] $\struct {R, +, \circ}$ are the same, then its [[Definition:Determinant of Matrix|determinant]] is [[Definition:Zero (Number)|zero]].	1
From the [[Equation of Straight Line in Plane/Slope-Intercept Form|slope-intercept form]] of the equation of the [[Definition:Straight Line|straight line]]: :$(1): \quad y = m x + c$ which is to be satisfied by both $\tuple {x_1, y_1}$ and $\tuple {x_2, y_2}$. We express $m$ and $c$ in terms of $\paren {x_1, y_1}$ and $\paren {x_2, y_2}$: {{begin-eqn}} {{eqn | l = y_1 | r = m x_1 + c | c = }} {{eqn | l = y_2 | r = m x_2 + c | c = }} {{eqn | ll= \leadsto | l = c | r = y_1 - m x_1 | c = }} {{eqn | ll= \leadsto | l = y_2 | r = m x_2 + y_1 - m x_1 | c = }} {{eqn | n = 2 | ll= \leadsto | l = m | r = \dfrac {y_2 - y_1} {x_2 - x_1} | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = y_1 | r = m x_1 + c | c = }} {{eqn | l = y_2 | r = m x_2 + c | c = }} {{eqn | ll= \leadsto | l = m | r = \dfrac {y_2 - c} {x_2} | c = }} {{eqn | ll= \leadsto | l = y_1 | r = \dfrac {y_2 - c} {x_2} x_1 + c | c = }} {{eqn | ll= \leadsto | l = y_1 x_2 | r = x_1 y_2 + c \paren {x_2 - x_1} | c = }} {{eqn | n = 3 | ll= \leadsto | l = c | r = \dfrac {y_1 x_2 - x_1 y_2} {x_2 - x_1} | c = }} {{end-eqn}} Substituting for $m$ and $c$ in $(1)$: {{begin-eqn}} {{eqn | l = y | r = m x + c | c = which is $(1)$ }} {{eqn | ll= \leadsto | l = y | r = \dfrac {y_2 - y_1} {x_2 - x_1} x + \dfrac {y_1 x_2 - x_1 y_2} {x_2 - x_1} | c = from $(2)$ and $(3)$ }} {{eqn | ll= \leadsto | l = y \paren {x_2 - x_1} + x_1 y_2 | r = x \paren {y_2 - y_1} + y_1 x_2 | c = }} {{eqn | ll= \leadsto | l = y \paren {x_2 - x_1} + x_1 y_2 - y_1 x_1 | r = x \paren {y_2 - y_1} + y_1 x_2 - x_1 y_1 | c = adding $y_1 x_1 = x_1 y_1$ to both sides }} {{eqn | ll= \leadsto | l = y \paren {x_2 - x_1} - y_1 \paren {x_2 - x_1} | r = x \paren {y_2 - y_1} - x_1 \paren {y_2 - y_1} | c = }} {{eqn | ll= \leadsto | l = \paren {y - y_1} \paren {x_2 - x_1} | r = \paren {x - x_1} \paren {y_2 - y_1} | c = }} {{eqn | ll= \leadsto | l = \dfrac {y - y_1} {x - x_1} | r = \dfrac {y_2 - y_1} {x_2 - x_1} | c = }} {{eqn | ll= \leadsto | l = \dfrac {x - x_1} {x_2 - x_1} | r = \dfrac {y - y_1} {y_2 - y_1} | c = }} {{end-eqn}} {{qed}}	1
If $\norm {\, \cdot \,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] then: :$\sup \set {\norm {n \cdot 1_R}: n \in \Z} = 1$ where $n \cdot 1_R = \begin{cases} \underbrace {1_R + 1_R + \dots + 1_R}_{\text {$n$ times} } & : n > 0 \\ 0 & : n = 0 \\ \\ -\underbrace {\paren {1_R + 1_R + \dots + 1_R} }_{\text {$-n$ times} } & : n < 0 \\ \end{cases}$	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector Quantity|vector quantities]]. Consider a [[Definition:Parallelogram|parallelogram]], two of whose adjacent sides represent $\mathbf a$ and $\mathbf b$ (in [[Definition:Magnitude|magnitude]] and [[Definition:Direction|direction]]). :[[File:ParallelogramLaw.png|300px]] Then the [[Definition:Diagonal of Quadrilateral|diagonal]] of the [[Definition:Parallelogram|parallelogram]] through that common point represents the [[Definition:Magnitude|magnitude]] and [[Definition:Direction|direction]] of $\mathbf a + \mathbf b$, the [[Definition:Vector Sum|sum]] of $\mathbf a$ and $\mathbf b$.	1
The elements of the [[Definition:Scalar Ring|scalar ring]] $\struct {R, +_R, \times_R}$ are called '''scalars'''.	1
Let $\struct {F, +, \times}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Unity of Field|unity]] is $1_F$. Let $F \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] over $F$. Then $F \sqbrk X$ is an [[Definition:Vector Space|vector space over $F$]].	1
Let $G, H$ be [[Definition:Abelian Group|abelian groups]]. Let $f : G \to H$ be a [[Definition:mapping|mapping]]. {{TFAE}} # $f$ is a [[Definition:Group Homomorphism|group homomorphism]]. # $f$ is a $\Z$-[[Definition:Module Homomorphism|module homomorphism]] between the $\Z$-[[Definition:Z-Module Associated with Abelian Group|modules associated with]] $G$ and $H$.	1
Let $\mathbf A$ be a [[Definition:Block Diagonal Matrix|block diagonal matrix]] of order $n$. Let $\mathbf A_1,\ldots,\mathbf{A}_k$ be the [[Definition:Square Matrix|square matrices]] on the diagonal, i.e.: :$\displaystyle \mathbf A = \begin{bmatrix} \mathbf A_1 & 0 & \cdots & 0 \\ 0 & \mathbf A_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \mathbf A_k \end{bmatrix}$ Then the [[Definition:Determinant of Matrix|determinant]] $\det \left({\mathbf A}\right)$ of $\mathbf A$ satisfies: :$\displaystyle \det \left({\mathbf A}\right) = \prod_{i \mathop = 1}^k \det \left({\mathbf A_i}\right)$	1
Let $T$ be a [[Definition:Topological Space|topological space]]. Let $H \subseteq T$. Let $H^-$ denote the [[Definition:Closure (Topology)|closure]] of $H$ in $T$. Then $H^-$ is the [[Definition:Smallest Set by Set Inclusion|smallest]] [[Definition:Superset|superset]] of $H$ that is [[Definition:Closed Set (Topology)|closed]] in $T$.	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $M$ be a [[Definition:Vector Subspace|subspace]] of the [[Definition:Dimension of Vector Space|$n$-dimensional]] [[Definition:Vector Space|vector space $K^n$]]. The following statements are equivalent: :$(1): \quad \map \dim M = n - 1$ :$(2): \quad M$ is the [[Definition:Kernel of Linear Transformation|kernel]] of a nonzero [[Definition:Linear Form|linear form]] :$(3): \quad$ There exists a [[Definition:Sequence|sequence]] $\sequence {\alpha_n} $ of [[Definition:Scalar (Vector Space)|scalars]], not all of which are zero, such that: :::$M = \set {\tuple {\lambda_1, \ldots, \lambda_n} \in K^n: \alpha_1 \lambda_1 + \cdots + \alpha_n \lambda_n = 0}$ Also, suppose the above hold. Let $\sequence {\beta_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Scalar (Vector Space)|scalars]] such that: :$M = \set {\tuple {\lambda_1, \ldots, \lambda_n} \in K^n: \beta_1 \lambda_1 + \cdots + \beta_n \lambda_n = 0}$ Then there is a non-zero [[Definition:Scalar (Vector Space)|scalar]] $\gamma$ such that: :$\forall k \in \closedint 1 n: \beta_k = \gamma \alpha_k$	1
'''Linear algebra''' is the branch of [[Definition:Algebra (Mathematical Branch)|algebra]] which studies [[Definition:Vector Space|vector spaces]] and [[Definition:Linear Transformation|linear transformations]] between them.	1
Let $\struct {V, +, \circ }$ and $\struct {W, +', \circ'}$ be [[Definition:Vector Space|$K$-vector spaces]]. Then $\phi: V \to W$ is a '''vector space isomorphism''' {{iff}}: : $(1): \quad \phi$ is a [[Definition:Bijection|bijection]] : $(2): \quad \forall \mathbf x, \mathbf y \in V: \map \phi {\mathbf x + \mathbf y} = \map \phi {\mathbf x} +' \map \phi {\mathbf y}$ : $(3): \quad \forall \mathbf x \in V: \forall \lambda \in K: \map \phi {\lambda \mathbf x} = \lambda \map \phi {\mathbf x}$	1
Checking the [[Definition:Module Axioms|module axioms]] in turn: :$(\text M 1): \quad \lambda \circ \paren {x +_G y} = e_G = e_G +_G e_G = \paren {\lambda \circ x} +_G \paren {\lambda \circ y}$ :$(\text M 2): \quad \paren {\lambda +_R \mu} \circ x = e_G = e_G +_G e_G = \paren {\lambda \circ x} +_G \paren {\mu \circ x}$ :$(\text M 3): \quad \paren {\lambda \times_R \mu} \circ x = e_G = \lambda \circ e_G = \lambda \circ \paren {\mu \circ x}$ Thus the [[Definition:Trivial Module|trivial module]] is indeed a [[Definition:Module|module]]. {{qed}}	1
let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $a \in A$. Let $B$ be an [[Definition:Integral Ring Extension|integral ring extension]] of $A$. {{TFAE}} :$(1): \quad a$ is a [[Definition:Unit of Ring|unit]] of $A$ :$(2): \quad a$ is a [[Definition:Unit of Ring|unit]] of $B$	1
By [[Complex Modulus is Norm]] then the [[Definition:Complex Modulus|complex modulus]] satisfies the [[Definition:Norm on Division Ring|norm axioms]] on the [[Definition:Complex Number|set of complex numbers]] $\C$. Since the [[Definition:Real Number|real numbers]] $\R$ is a [[Definition:Subset|subset]] of the [[Definition:Complex Number|complex numbers]] $\C$ then the [[Definition:Complex Modulus|complex modulus]] satisfies the [[Definition:Norm on Division Ring|norm axioms]] on the [[Definition:Real Number|real numbers]] $\R$. By [[Complex Modulus of Real Number equals Absolute Value]] then the [[Definition:Absolute Value|absolute value]] satisfies the [[Definition:Norm on Division Ring|norm axioms]] on [[Definition:Real Number|set of real numbers]] $\R$. {{qed}}	1
Let $\struct {\Z_m, +_m, \times_m}$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. Then $\eqclass k m \in \Z_m$ has an [[Definition:Multiplicative Inverse|inverse]] in $\struct {\Z_m, \times_m}$ {{iff}} $k \perp m$.	1
=== Necessary Condition === Suppose $V = \left\{{\mathbf 0}\right\}$. We have that $V$ has no $\mathbf v \in V, \mathbf v \ne \mathbf 0$. Thus there exists no $\left\{{\mathbf v}\right\} \subseteq V$ such that $\mathbf v \ne \mathbf 0$. Such a $\left\{{\mathbf v}\right\}$ would be a [[Definition:Linearly Independent Set|linearly independent set]]. Hence $\varnothing$ is the only possible [[Definition:Basis (Linear Algebra)|basis]] for $V$. Hence $\dim \left({V}\right) = \left|{\varnothing}\right| = 0$. {{qed|lemma}} === Sufficient Condition === Suppose $\dim \left({V}\right) = 0$. Then by definition of [[Definition:Dimension (Linear Algebra)|dimension]], $0 = \dim \left({V}\right) = \left\vert|{B}\right\vert|$, where $B$ is a [[Definition:Basis (Linear Algebra)|basis]] for $V$. Thus $B = \varnothing$. Suppose there are sets $\left\{{\mathbf v}\right\}$ such that $\mathbf v \in V, \mathbf v \ne \mathbf 0$. By [[Singleton is Linearly Independent]], any such $\left\{{\mathbf v}\right\}$ would be a [[Definition:Linearly Independent Set|linearly independent set]]. So $V$ has no such $\mathbf v \ne \mathbf 0$. Thus $V = \left\{{\mathbf 0}\right\}$. {{qed}} [[Category:Linear Algebra|{{SUBPAGENAME}}]] [[Category:Vector Spaces|{{SUBPAGENAME}}]] jvrj05y0ei1p0owjuzmpa1n0j08l0h8	1
Let $x, y = 0_R$. Then: :$\norm x, \norm y = 0$ Therefore: :$\max \set {\norm x, \norm y} = 0$. Hence: {{begin-eqn}} {{eqn | l = \norm {x + y} | r = \norm {0_R + 0_R} | c = }} {{eqn | r = \norm {0_R} | c = }} {{eqn | r = 0 | c = }} {{eqn | r = \max \set {\norm x, \norm y} | c = }} {{end-eqn}} Let $x \ne 0_R$ or $y \ne 0_R$. Then: :$\norm x = 1$ or $\norm y = 1$ Therefore: :$\max \set {\norm x, \norm y} = 1$. Hence: {{begin-eqn}} {{eqn | l = \norm {x + y} | o = \le | r = 1 | c = {{Defof|Trivial Norm on Division Ring}} }} {{eqn | r = \max \set {\norm x, \norm y} | c = }} {{end-eqn}} {{qed}} [[Category:Trivial Norms]] s5has14axjeaicsn7dkar1jr2vq0ibz	1
Let $E / F$ be an [[Definition:Algebraic Field Extension|algebraic field extension]]. Let $A \subseteq E$ be a [[Definition:Unital Subalgebra|unital subalgebra]] over $F$. Then $A$ is a [[Definition:Field (Abstract Algebra)|field]].	1
From [[Ring of Square Matrices over Ring is Ring]] we have that $\struct {\map {\MM_R} n, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. As $R$ has a [[Definition:Unity of Ring|unity]], the [[Definition:Unit Matrix|unit matrix]] can be formed. The [[Definition:Unity of Ring|unity]] of $\struct {\map {\MM_R} n, +, \times}$ is this [[Definition:Unit Matrix|unit matrix]]. {{qed}}	1
By definition of [[Definition:Matrix Product (Conventional)|(conventional) matrix product]], $\mathbf C$ is of [[Definition:Order of Matrix|order]] $1 \times n$. By definition of [[Definition:Matrix Scalar Product|matrix scalar product]], $\mathbf D$ is also of [[Definition:Order of Matrix|order]] $1 \times n$. Consider arbitrary [[Definition:Element of Matrix|elements]] $c_i \in \mathbf C$ and $d_i \in \mathbf D$ for some [[Definition:Index of Matrix Element|index]] $i$ where $1 \le i \le n$. We have: {{begin-eqn}} {{eqn | l = c_i | r = \sum_{j \mathop = 1}^i a_{j j} b_j | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | r = a b_j | c = Definition of $\mathbf A$ }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = d_i | r = a b_j | c = {{Defof|Matrix Scalar Product}} }} {{eqn | r = c_i | c = from above }} {{end-eqn}} {{qed}}	1
Let $\struct {V, +, \circ}_K$ be a [[Definition:Vector Space|$K$-vector space]]. Let $\family {M_i}_{i \mathop \in I}$ be an [[Definition:Indexed Family|$I$-indexed family]] of [[Definition:Vector Subspace|subspaces]] of $V$. Then $M := \ds \bigcap_{i \mathop \in I} M_i$ is also a [[Definition:Vector Subspace|subspace]] of $V$.	1
Let $\struct {X, \Sigma, \mu}$ be a [[Definition:Measure Space|measure space]]. Let $f: X \to \overline \R$ be a [[Definition:Integrable Function on Measure Space|$\mu$-integrable function]]. Then: :$\displaystyle \size {\int_X f \rd \mu} \le \int_X \size f \rd \mu$	1
By definition: :$\forall a, b \in A_F: \norm {a \oplus b} = \norm a \norm b$ So: :$\norm {1_A} = \norm {1_A \oplus 1_A} = \norm {1_A} \norm {1_A}$ So $\norm {1_A} \in \R$ is [[Definition:Idempotent Element|idempotent]] under [[Definition:Real Multiplication|real multiplication]]. From [[Idempotent Elements of Ring with No Proper Zero Divisors]], only $0 \in \R$ and $1 \in \R$ fit that bill. But $\norm {1_A}$ can not be $0$ as that would make: :$\forall a \in A_F: \norm a = \norm {1_A \oplus a} = \norm {1_A} \norm a = 0 \norm a = 0$ which contradicts the definition of the [[Definition:Norm on Vector Space|norm]]. Hence the result. {{Qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Unity of Ring|unity]] is $1_R$. {{TFAE|def = Unit of Ring}}	1
The [[Definition:Contrapositive|contrapositive]] is proved. Let there exist $x \in R$ such that $\norm x_1 < 1$ and $\norm x_2 \ge 1$. Let $\sequence {x_n}$ be the [[Definition:Sequence|sequence]] defined by: $\forall n: x_n = x^n$. By [[Sequence of Powers of Number less than One/Normed Division Ring|Sequence of Powers of Number less than One in Normed Division Ring]] then $\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_1$. By [[Convergent Sequence is Cauchy Sequence/Normed Division Ring|convergent sequence in normed division ring is a Cauchy sequence]] then $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$. Let $0_R$ be the [[Definition:Ring Zero|zero]] of $R$ and $1_R$ be the [[Definition:Unit of Ring|unit]] of $R$. By [[Norm of Unity of Division Ring]] and the assumption $\norm x_1 < 1$: :$x \ne 1_R$ Hence: :$x - 1_R \ne 0_R$ By [[Definition:Norm on Division Ring|norm axiom $(\text N 1)$: Positive Definiteness]]: :$\norm {x - 1_R}_2 > 0$ Let $\epsilon = \dfrac {\norm {x - 1_R}_2} 2$. Then $\norm {x - 1_R}_2 > \epsilon$. We have that $\norm x_2 \ge 1$. Hence for all $n \in \N$: {{begin-eqn}} {{eqn | l = \norm {x_n}_2 | r = \norm {x^n}_2 | c = Definition of $x_n$ }} {{eqn | r = \norm x_2^n | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = \ge | r = 1 | c = }} {{end-eqn}} For all $n \in \N$: {{begin-eqn}} {{eqn | l = \norm {x_{n + 1} - x_n}_2 | r = \norm {x^{n + 1} - x^n}_2 }} {{eqn | r = \norm {x^n x - x^n}_2 }} {{eqn | r = \norm {x^n \paren {x - 1_R} }_2 }} {{eqn | r = \norm {x^n}_2 \norm {x - 1_R}_2 | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = > | r = \epsilon }} {{end-eqn}} So $\sequence {x_n}$ is not a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$. The theorem now follows by the [[Rule of Transposition]]. {{qed}}	1
By definition, $H$ is [[Definition:Separable Space|separable]] {{iff}} there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Let $V \subseteq H$ where $V$ is [[Definition:Countable Set|countable]]. $V$ is not [[Definition:Open Set (Topology)|open]] in $T$ as it does not contain $p$. From [[Subset of Particular Point Space is either Open or Closed]] it follows that $V$ is [[Definition:Closed Set (Topology)|closed]]. From [[Closed Set Equals its Closure]], $V^- = V$. But $V^- \ne H$ as $V$ is [[Definition:Countable Set|countable]] and $H$ is [[Definition:Uncountable Set|uncountable]]. So whatever $V$ is, if it is [[Definition:Countable Set|countable]] it is not [[Definition:Everywhere Dense|everywhere dense]]. The result follows from definition of [[Definition:Separable Space|separable]]. {{qed}}	1
Follows from: :[[Real Number Line is Second-Countable]] :[[Second-Countable Space is Separable]] {{qed}}	1
By definition of [[Definition:Lie Algebra|Lie algebra]], it suffices to prove two properties: :$(1): \forall a \in S: a \times a = 0$ :$(2): \forall a, b, c \in S: a \times \paren {b \times c} + b \times \paren {c \times a} + c \times \paren {a \times b} = 0$ === Proof of $(1)$ === [[Cross Product of Vector with Itself is Zero]] {{Qed|lemma}} === Proof of $(2)$ === [[Vector Cross Product satisfies Jacobi Identity]] {{Qed|lemma}} Both properties hold, and the result follows. {{qed}}	1
A [[Definition:Subset|subset]] of a [[Definition:Linearly Independent Set|linearly independent set]] is also [[Definition:Linearly Independent Set|linearly independent]].	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector Quantity|vector quantities]]. Let $\mathbf c = \mathbf a + \mathbf b$ and $\mathbf d = \mathbf a - \mathbf b$ be given. Then: {{begin-eqn}} {{eqn | l = \mathbf a | r = \dfrac 1 2 \paren {\mathbf c + \mathbf d} }} {{eqn | l = \mathbf b | r = \dfrac 1 2 \paren {\mathbf c - \mathbf d} }} {{end-eqn}}	1
:$\iota : \struct {R^* ,d^*} \to \struct {R, d} : \map \iota x = x^{-1}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]].	1
We can construct a [[Definition:Parallelogram|parallelogram]] as follows: :[[File:Vector-difference-parallelogram.png|400px]] and the construction is apparent.	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Let $\mathbf v \in \mathbf V, \lambda \in F$. Then: :$\lambda \circ \mathbf v = \bszero \iff \paren {\lambda = 0_F \lor x = \bszero}$	1
{{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \lambda_k e_k | r = \lambda_1 \tuple {1_R, 0_R, 0_R, \ldots, 0_R} | c = }} {{eqn | o = + | r = \lambda_2 \tuple {0_R, 1_R, 0_R, \ldots, 0_R} | c = }} {{eqn | o = + | r = \ldots | c = }} {{eqn | o = + | r = \lambda_n \tuple {0_R, 0_R, 0_R, \ldots, 1_R} | c = }} {{eqn | r = \tuple {\lambda_1, \lambda_2, \lambda_3, \ldots, \lambda_n} | c = }} {{end-eqn}} {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf 0 | r = \sqbrk a_{m n} + \sqbrk 0_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {a + 0}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = [[Identity Element of Addition on Numbers]] }} {{eqn | ll= \leadsto | l = \mathbf A + \mathbf 0 | r = \mathbf A | c = {{Defof|Zero Matrix}} }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \mathbf 0 + \mathbf A | r = \sqbrk 0_{m n} + \sqbrk a_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {0 + a}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = [[Identity Element of Addition on Numbers]] }} {{eqn | ll= \leadsto | l = \mathbf 0 + \mathbf A | r = \mathbf A | c = {{Defof|Zero Matrix}} }} {{end-eqn}} {{qed}}	1
Let $M$ and $N$ be [[Definition:Metric Space|metric spaces]]. Let $M$ be [[Definition:Complete Metric Space|complete]]. Let $f : M \times N \to M$ be a [[Definition:Uniform Contraction Mapping|uniform contraction]]. Then for all $t\in N$ there exists a [[Definition:Unique|unique]] $g(t) \in M$ such that $f(g(t), t) = g(t)$, and if $f$ is [[Definition:Lipschitz Continuous at Point|Lipschitz continuous]] at a point $(g(t),t)$, then $g$ is [[Definition:Lipschitz Continuous at Point|Lipschitz continuous]] at $t$.	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf v | r = \norm {\mathbf u} \norm {\mathbf v} \cos \angle \mathbf u, \mathbf v | c = {{Defof|Dot Product}} }} {{eqn | r = \norm {\mathbf v} \norm {\mathbf u} \cos \angle \mathbf u, \mathbf v | c = [[Real Multiplication is Commutative]] }} {{eqn | r = \norm {\mathbf v} \norm {\mathbf u} \cos \angle \mathbf v, \mathbf u | c = [[Cosine Function is Even]] }} {{eqn | r = \mathbf v \cdot \mathbf u | c = {{Defof|Dot Product}} }} {{end-eqn}} {{qed}}	1
For $\left\langle{r_i}\right\rangle_{i \mathop \in I} \in R^{\left({I}\right)}$ we have: :$\Psi \left({\left\langle{r_i}\right\rangle_{i \mathop \in I}}\right) = \displaystyle \sum_{i \mathop \in I} m_i r_i$ Thus $\Psi$ is [[Definition:Surjection|surjective]] {{iff}} every [[Definition:Element|element]] of $M$ is a linear combination of $S$. {{qed}} [[Category:Module Theory]] pgaxjw8u1428mvui4ro0gn8dhdxm9jr	1
From [[Symmetric Difference on Power Set forms Abelian Group]], $\struct {\powerset S, *}$ is an [[Definition:Abelian Group|abelian group]], where $\O$ is the [[Definition:Identity Element|identity]] and each [[Definition:Element|element]] is [[Definition:Self-Inverse Element|self-inverse]]. From [[Power Set with Intersection is Monoid]], $\struct {\powerset S, \cap}$ is a [[Definition:Commutative Monoid|commutative monoid]] whose [[Definition:Identity Element|identity]] is $S$. Also [[Intersection Distributes over Symmetric Difference]]. Thus $\struct {\powerset S, \cap}$ is a [[Definition:Commutative and Unitary Ring|commutative ring with a unity]] which is $S$. From [[Intersection with Empty Set]]: :$\forall A \in \powerset S: A \cap \O = \O = \O \cap A$ Thus $\O$ is indeed the [[Definition:Ring Zero|zero]]. However, from [[Set Intersection Not Cancellable]], it follows that $\struct {\powerset S, *, \cap}$ is not an [[Definition:Integral Domain|integral domain]]. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Matrix|matrices]] whose [[Definition:Order of Matrix|order]] is $m \times n$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf B | r = \sqbrk a_{m n} + \sqbrk b_{m n} | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{eqn | r = \sqbrk {a + b}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {b + a}_{m n} | c = [[Commutative Law of Addition]] }} {{eqn | r = \sqbrk b_{m n} + \sqbrk a_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \mathbf B + \mathbf A | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{end-eqn}} {{qed}}	1
:$\mathcal N$ is an [[Definition:Ideal of Ring|ideal]] of $\mathcal C$.	1
=== Non-Vertical Tangent Lines === From [[Equation of Circle]], $\mathcal C$ can be described on the [[Definition:Cartesian Plane|$x y$-plane]] in the form: :$\paren {x - a}^2 + \paren {y - b}^2 = r^2$ where $P = \tuple {a, b}$ is the [[Definition:Center of Circle|center of the circle]] and $r$ is the [[Definition:Radius of Circle|radius]]. We use the definition of the [[Definition:Derivative|derivative]] as the [[Definition:Gradient|gradient]] of the [[Definition:Tangent to Curve|tangent line]] $\mathcal T$. Taking the [[Definition:Derivative|derivative]] {{WRT|Differentiation}} $x$ of both sides of the equation we get: {{begin-eqn}} {{eqn | l = 2 \paren {x - a} + 2 \paren {y - b} \frac {\d y} {\d x} | r = 0 | c = [[Derivative of Constant]], [[Chain Rule for Derivatives]], [[Power Rule for Derivatives]] }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \frac {a - x} {y - b} | c = }} {{end-eqn}} This is the [[Definition:Gradient|slope]] at any [[Definition:Point|point]] on $\mathcal C$. From the [[Equation of Straight Line in Plane/Slope-Intercept Form|slope-intercept form]] of a [[Definition:Straight Line|line]], the equation of such a line is: :$y - y_n = m \paren {x - x_n}$ given any point $\paren {x_n, y_n}$ and the [[Definition:Gradient|gradient]] $m$. For $\mathcal T$: :$m = \left.{\dfrac {\d y} {\d x} }\right\vert^{x = x_n}_{y = y_n}$ Thus the equation of $\mathcal T$ is: :$y - y_n = \dfrac {a - x_n} {y_n - b} \paren {x - x_n}$ {{qed|lemma}} === Vertical Tangent Lines === {{ProofWanted|by the [[Infinite Limit Theorem]]}}	1
:$\NN$ is a [[Definition:Maximal Right Ideal of Ring|maximal right ideal]].	1
Let $H_n$ be the [[Definition:Hilbert Matrix|Hilbert matrix]] of [[Definition:Order of Square Matrix|order $n$]]: :$\begin{bmatrix} a_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac 1 {i + j - 1} \end{bmatrix}$ Consider its [[Definition:Inverse Matrix|inverse]] $H_n^{-1}$. All the [[Definition:Element of Matrix|elements]] of $H_n^{-1}$ are [[Definition:Integer|integers]]. The sum of all the [[Definition:Element of Matrix|elements]] $b_{i j}$ of $H_n^{-1}$ is: :$\displaystyle \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} = n^2$	1
Let $\left({H_n}\right)_{n \in \N}$ be a [[Definition:Sequence|sequence]] of [[Definition:Hilbert Space|Hilbert spaces]]. Denote by $H = \displaystyle \bigoplus_{n \mathop = 1}^\infty H_n$ their [[Definition:Hilbert Space Direct Sum|Hilbert space direct sum]]. For each $n \in \N$, let $T_n \in B \left({H_n}\right)$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Suppose that one has $\displaystyle \sup_{n \mathop \in \N} \, \left\Vert{T_n}\right\Vert < \infty$, where $\left\Vert{\cdot}\right\Vert$ signifies the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]]. Define $T: H \to H$ by: :$\forall h = \left({h_n}\right)_{n \in \N}: T h = \left({T_n h_n}\right)_{n \in \N} \in H$ Then $T \in B \left({H}\right)$ is a [[Definition:Bounded Linear Operator|bounded linear operator]].	1
Let $a, b \in A$. Then all of $a, b, a^*, b^*$ can be generated by $\Im \left({a}\right)$ and $\Im \left({b}\right)$. {{explain|Prove the above statement.}} So as $A$ is an [[Definition:Alternative Algebra|alternative algebra]], it follows that $\oplus$ is [[Definition:Associative|associative]] for $a, b, a^*, b^*$. So: {{begin-eqn}} {{eqn | l = \left \Vert{a b}\right \Vert^2 | r = \left({a \oplus b}\right) \oplus \left({a \oplus b}\right)^* | c = Definition of [[Definition:Norm on Vector Space|norm]] in [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]] }} {{eqn | r = a \oplus b \oplus \left({b^* \oplus a^*}\right) | c = Definition of [[Definition:Conjugate (Algebra)|conjugate]] }} {{eqn | r = a \oplus \left({b \oplus b^*}\right) \oplus a^* | c = [[Definition:Associative|Associativity]] of $\oplus$ (from above) }} {{eqn | r = \left \Vert{a}\right \Vert^2 \left \Vert{b}\right \Vert^2 | c = }} {{end-eqn}} {{qed}}	1
Follows directly from: : [[Characterisation of Linearly Independent Set through Free Module Indexed by Set]] : [[Characterisation of Spanning Set through Free Module Indexed by Set]]. {{qed}} [[Category:Module Theory]] d3onr5592y2nzbayp16sgunznhp5emy	1
Let: :$ \mathbf A_{m \times n} = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}$, $\mathbf x_{n \times 1} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$, $\mathbf 0_{m \times 1} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ be [[Definition:Matrix|matrices]] where each [[Definition:Column of Matrix|column]] is a member of a [[Definition:Real Vector Space|real vector space]]. The [[Definition:Set|set]] of all solutions to $\mathbf A \mathbf x = \mathbf 0$: :$\map {\operatorname N} {\mathbf A} = \set {\mathbf x \in \R^n : \mathbf {A x} = \mathbf 0}$ is called the '''null space''' of $\mathbf A$.	1
For any $x, y \in S$, we have $x = y = v$. It follows that: :$\forall t \in \left[{0 \,.\,.\, 1}\right]: t x + \left({1 - t}\right)y = v \in S$ Hence $S$ is a [[Definition:Convex Set (Vector Space)|convex set]]. {{qed}}	1
Then: {{begin-eqn}} {{eqn | l = \norm {x - y} | r = \norm {x + \paren {-y} } }} {{eqn | o = \le | r = \norm x + \norm {-y} | c = [[Definition:Norm Axioms|Norm axiom $(\text N 3)$: Triangle Inequality]] }} {{eqn | r = \norm x + \norm y | c = [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] }} {{end-eqn}} as desired. {{qed}}	1
The [[Definition:Complex Number|set of complex numbers]] $\C$, with the operations of [[Definition:Complex Addition|addition]] and [[Definition:Complex Multiplication|multiplication]], forms a [[Definition:Vector Space|vector space]].	1
Let $\eqclass {x_n} {}, \eqclass {y_n} {} \in \CC \,\big / \NN$ {{begin-eqn}} {{eqn | l = \norm {\eqclass {x_n} {} \eqclass {y_n} {} } _1 | r = \norm {\eqclass {x_n y_n} {} }_1 | c = Multiplication on quotient ring }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n y_n} | c = Definition of $\norm {\,\cdot\,}_1$ }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n} \norm {y_n} | c = {{NormAxiom|2}} }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n} \times \lim_{n \to \infty} \norm {y_n } | c = [[Product Rule for Real Sequences]] }} {{eqn | r = \norm {\eqclass {x_n} {} }_1 \times \norm {\eqclass {y_n} {} } _1 | c = Definition of $\norm {\,\cdot\,}_1$ }} {{end-eqn}} {{qed}}	1
[[Proof by Counterexample]]: Let $\struct {S, +, \times}$ be a [[Definition:Ring with Unity|ring with unity]] Let $\struct {\map {\mathcal M_S} 2, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $2$ over $S$]]. From [[Ring of Square Matrices over Ring is Ring]], $\struct {\map {\mathcal M_S} 2, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. Let $G = \set {\begin{bmatrix} x & y \\ 0 & 0 \end{bmatrix} : x, y \in S }$ === [[Right Module Does Not Necessarily Induce Left Module over Ring/Lemma|Lemma]] === {{:Right Module Does Not Necessarily Induce Left Module over Ring/Lemma}}{{qed|lemma}} From [[Right Ideal is Right Module over Ring]], $\struct {G, +, \times}$ is a [[Definition:Right Module|right module]] over $\struct {\map {\mathcal M_S} 2, +, \times}$. Let $R = \map {\mathcal M_S} 2$. Let $\mathbf A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}, \mathbf B = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \mathbf C = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$ Then: {{begin-eqn}} {{eqn| l = \mathbf A \times \paren {\mathbf B \times \mathbf C} | r = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \times \paren {\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} } }} {{eqn | r = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} }} {{end-eqn}} And: {{begin-eqn}} {{eqn| l = \mathbf A \times \paren {\mathbf C \times \mathbf B} | r = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \times \paren {\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} } }} {{eqn | r = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \times \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} }} {{end-eqn}} Hence for $\mathbf A, \mathbf B \in R, \mathbf C \in G$: :$ \mathbf A \times \paren{\mathbf B \times \mathbf C} \neq \mathbf A \times \paren {\mathbf C \times \mathbf B}$ Let $\circ : G \times R \to R$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \mathbf \Lambda \in R: \forall \mathbf X \in G: \mathbf \Lambda \circ \mathbf X = \mathbf X \times \mathbf \Lambda$ From [[Right Module induces Left Module over same Ring iff Actions are Commutative]], $\struct {G, +, \circ}$ is not a [[Definition:Left Module|left module]] over $\struct {R, +, \times}$ {{qed}}	1
This is a reformulation of [[Ring with Unity has Prime Ideal]]. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{m n}$ and $\mathbf C = \sqbrk c_{m n}$ be [[Definition:Matrix|matrices]] whose [[Definition:Order of Matrix|order]] is $m \times n$. Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A + \mathbf B} + \mathbf C | r = \paren {\sqbrk a_{m n} + \sqbrk b_{m n} } + \sqbrk c_{m n} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{eqn | r = \sqbrk {a + b}_{m n} + \sqbrk c_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {\paren {a + b} + c}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {a + \paren {b + c} }_{m n} | c = [[Associative Law of Addition]] }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {b + c}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} + \paren {\sqbrk b_{m n} + \sqbrk c_{m n} } | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \mathbf A + \paren {\mathbf B + \mathbf C} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{end-eqn}} {{qed}}	1
Let: :$\forall n \in \N_{>0}: \norm {n \cdot 1_R} \le 1$ Let $x, y \in R$. Let $y = 0_R$ where $0_R$ is the [[Definition:Ring Zero|zero]] of $R$. Then $\norm {x + y} = \norm x = \max \set {\norm x, 0} = \max \set {\norm x, \norm y}$ ==== [[Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 1|Lemma 1]] ==== {{:Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 1}} {{qed|lemma}} Hence to complete the proof it is sufficient to prove: :$\forall x \in R: \norm {x + 1_R} \le \max \set {\norm x, 1}$ For $n \in \N$: {{begin-eqn}} {{eqn | l = \norm {x + 1_R}^n | r = \norm {\sum_{i \mathop = 0}^n \binom n i \cdot x^i} | c = [[Binomial Theorem]] }} {{eqn | o = \le | r = \sum_{i \mathop = 0}^n \norm {\binom n i \cdot x^i} | c = {{NormAxiom|3}} }} {{eqn | r = \sum_{i \mathop = 0}^n \norm {\binom n i \cdot 1_R} \norm x^i | c = {{NormAxiom|2}} }} {{eqn | o = \le | r = \sum_{i \mathop = 0}^n \norm x^i | c = $\forall n \in \N_{>0}: \norm {n \cdot 1_R} \le 1$ }} {{end-eqn}} ==== [[Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 2|Lemma 2]] ==== {{:Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 2}} {{qed|lemma}} Hence {{begin-eqn}} {{eqn | l = \norm {x + 1_R}^n | o = \le | r = \sum_{i \mathop = 0}^n \norm x^i | c = continuing from above }} {{eqn | o = \le | r = \sum_{i \mathop = 0}^n \max \set {\norm x^n , 1} | c = [[Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 2|Lemma 2]] }} {{eqn | r = \paren {n + 1} \max \set {\norm x^n , 1} }} {{end-eqn}} Taking $n$th roots yields: :$\norm {x + 1_R} \le \paren {n + 1}^{1/n} \max \set {\norm x, 1}$ ==== [[Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 3|Lemma 3]] ==== {{:Characterisation of Non-Archimedean Division Ring Norms/Sufficient Condition/Lemma 3}}{{qed|lemma}} By the [[Multiple Rule for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {n + 1}^{1/n} \max \set {\norm x, 1} = \max \set {\norm x, 1}$ By [[Inequality Rule for Real Sequences]]: :$\norm {x + 1_R} \le \max \set {\norm x, 1}$ The result follows.	1
{{ProofWanted|First a homomorphism is established between $\struct {\Z_m \times \Z_n}$ and $\struct {\Z_m \times \Z_n}$. Then it is demonstrated that $\phi: \struct {\Z_m \times \Z_n} \to \Z_{m n}$ is a bijection. Some of this has already been done, I'm fairly sure.}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $G$ and $H$ be [[Definition:Free Module|free $R$-modules]] of [[Definition:Dimension (Linear Algebra)|finite dimensions]] $n,m>0$ respectively. Let $\left \langle {a_n} \right \rangle$ and $\left \langle {{a_n}'} \right \rangle$ be [[Definition:Ordered Basis|ordered bases]] of $G$. Let $\left \langle {b_m} \right \rangle$ and $\left \langle {{b_m}'} \right \rangle$ be [[Definition:Ordered Basis|ordered bases]] of $H$. Let $u: G \to H$ be a [[Definition:Linear Transformation|linear transformation]], and let $\left[{u; \left \langle {b_m} \right \rangle, \left \langle {a_n} \right \rangle}\right]$ be the [[Definition:Relative Matrix|matrix of $u$ relative to $\left \langle {a_n} \right \rangle$ and $\left \langle {b_m} \right \rangle$]]. Let: : $\mathbf A = \left[{u; \left \langle {b_m} \right \rangle, \left \langle {a_n} \right \rangle}\right]$ : $\mathbf B = \left[{u; \left \langle {{b_m}'} \right \rangle, \left \langle {{a_n}'} \right \rangle}\right]$ Then: :$\mathbf B = \mathbf Q^{-1} \mathbf A \mathbf P$ where: : $\mathbf P$ is the [[Definition:Change of Basis Matrix|matrix corresponding to the change of basis from $\left \langle {a_n} \right \rangle$ to $\left \langle {{a_n}'} \right \rangle$]] : $\mathbf Q$ is the [[Definition:Change of Basis Matrix|matrix corresponding to the change of basis from $\left \langle {b_m} \right \rangle$ to $\left \langle {{b_m}'} \right \rangle$]].	1
By the definition of a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] then: :there exists a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphisms]] $\phi_1: R \to S_1$ :$R_1 = \map {\phi_1} R$ is a [[Definition:Dense|dense]] [[Definition:Subring|subring]] of $S_1$ :$S_1$ is a [[Definition:Complete Metric Space|complete metric space]] :there exists a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphisms]] $\phi_2: R \to S_2$ :$R_2 = \map {\phi_2} R$ is a [[Definition:Dense|dense]] [[Definition:Subring|subring]] of $S_2$ :$S_2$ is a [[Definition:Complete Metric Space|complete metric space]] === [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 1|Lemma 1]] === {{:Normed Division Ring Completions are Isometric and Isomorphic/Lemma 1}} === [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 2|Lemma 2]] === {{:Normed Division Ring Completions are Isometric and Isomorphic/Lemma 2}} === [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 3|Lemma 3]] === {{:Normed Division Ring Completions are Isometric and Isomorphic/Lemma 3}} === [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 4|Lemma 4]] === {{:Normed Division Ring Completions are Isometric and Isomorphic/Lemma 4}} === [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 5|Lemma 5]] === {{:Normed Division Ring Completions are Isometric and Isomorphic/Lemma 5}}{{qed}} [[Category:Normed Division Rings]] [[Category:Complete Metric Spaces]] [[Category:Completion of Normed Division Ring]] pmio9r7oj1x0usm2h8v3hcam5m5lnvb	1
Let $\mathbf A = \sqbrk a_n$ and $\mathbf B = \sqbrk b_n$ be [[Definition:Square Matrix|order $n$ square matrices]] over $R$. By definition of [[Definition:Matrix Product (Conventional)|matrix multiplication]], $\mathbf A \mathbf B = \mathbf C = \sqbrk c_n$ where: :$\displaystyle \forall i \in \closedint 1 n, j \in \closedint 1 n: c_{i j} = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j}$ But by definition of the [[Definition:Trivial Ring|trivial ring]]: :$\forall a, b \in R: a \circ b = 0_R$ where $0_R$ is the [[Definition:Ring Zero|zero]] of $R$. Thus $\mathbf A \mathbf B$ is the [[Definition:Zero Matrix|zero $n \times n$ matrix]]. The same applies to $\mathbf B \mathbf A$, which is also the [[Definition:Zero Matrix|zero $n \times n$ matrix]]. That is: :$\mathbf A \mathbf B = \mathbf B \mathbf A = \bszero_n$ and the result follows by definition of [[Definition:Commutative Operation|commutative operation]]. {{qed}} [[Category:Conventional Matrix Multiplication]] [[Category:Trivial Rings]] 1t7trbnhmhdi2g676rctto7cmkgtb9k	1
It is noted that: :$\sup \set {\size n: n \in \Z} = +\infty$ By a [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 3|corollary of Characterisation of Non-Archimedean Division Ring Norms]] then $\size {\,\cdot\,}$ is [[Definition:Archimedean Division Ring Norm|Archimedean]]. By [[P-adic Norm is Non-Archimedean Norm]] then $\norm {\,\cdot\,}_p$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. By [[Equivalent Norms are both Non-Archimedean or both Archimedean]], $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. {{qed}}	1
Let $\map S t = \map \Phi {t + T} {\map \Phi T}^{-1}$. Then: {{begin-eqn}} {{eqn | l = \map {\frac \d {\d t} } {\map S t} | r = \map {\Phi'} {t + T} {\map \Phi T}^{-1} | c = }} {{eqn | r = \map {\mathbf A} {t + T} \map \Phi {t + T} {\map \Phi T}^{-1} | c = }} {{eqn | r = \map {\mathbf A} t \map S t | c = }} {{end-eqn}} So $\map S t$ is a [[Definition:Fundamental Matrix|fundamental matrix]] and: :$\map S 0 = Id$ Then: :$\map S t = \map \Phi t$ which means that: :$\map \Phi {t + T} = \map \Phi t \map \Phi T$ Hence by the [[Existence of Matrix Logarithm|existence of the matrix logarithm]], there exists a [[Definition:Matrix|matrix]] $\mathbf B$ such that: :$\map \Phi T = e^{\mathbf B T}$ Defining $\map {\mathbf P} t = \map \Phi t e^{-\mathbf B t}$, it follows that: {{begin-eqn}} {{eqn | l = \map {\mathbf P} {t + T} | r = \map \Phi {t + T} e^{-\mathbf B t - \mathbf B T} | c = }} {{eqn | r = \map \Phi t \map \Phi T e^{-\mathbf B T} e^{-\mathbf B t} | c = }} {{eqn | r = \map \Phi t e^{-\mathbf B t} | c = }} {{eqn | r = \map {\mathbf P} t | c = }} {{end-eqn}} and hence $\map {\mathbf P} t$ is a [[Definition:Periodic Function|periodic function]] with period $T$. As $\map \Phi t = \map {\mathbf P} t e^{\mathbf B t}$, the second implication also holds. {{qed}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Then: :$\map \det {\mathbf A} = \map \det {\mathbf A^\intercal}$	1
The [[Definition:P-adic Norm|$p$-adic norm]] forms a [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Number|rational numbers]] $\Q$.	1
The [[Definition:Unit Matrix|unit matrix]] $\mathbf I_n$ of [[Definition:Order of Square Matrix|order]] $n$ is [[Definition:Proper Orthogonal Matrix|proper orthogonal]].	1
Let $\Z \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] in $X$ over $\Z$. Then $\Z \sqbrk X$ is not a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $r \in R$ be a [[Definition:Unit of Ring|unit]] in $R$. Let $r \, \mathbf I_n$ be the $n \times n$ [[Definition:Rescaling Matrix|rescaling matrix]] of $r$. Then $\left({r \, \mathbf I_n}\right)^{-1} = r^{-1} \, \mathbf I_n$.	1
If $\norm x > 1$ then for all $i$, $0 \le i \le n$: :$\norm x^i \le \norm {x}^n \le \max \set {\norm x^n, 1}$ If $\norm x \le 1$ then for all $i$, $0 \le i \le n$: :$\norm x^i \le 1 \le \max \set {\norm x^n, 1}$ In either case for all $i$, $0 \le i \le n$: :$\norm x^i \le \max \set {\norm x^n , 1}$ {{qed}}	1
Consider the [[Definition:Cartesian 3-Space|Cartesian $3$-space]] $C$. Let $\mathbf i$, $\mathbf j$ and $\mathbf k$ denote the [[Definition:Unit Vector|unit vectors]] in the [[Definition:Positive Direction|positive directions]] of the [[Definition:X-Axis|$x$-axis]], [[Definition:Y-Axis|$y$-axis]] and [[Definition:Z-Axis|$z$-axis]] respectively. Then $\set {\mathbf i, \mathbf j, \mathbf k}$ forms a [[Definition:Basis of Vector Space|basis]] for $C$.	1
First, a lemma: === [[Transitivity of Integrality/Lemma|Lemma]] === {{:Transitivity of Integrality/Lemma}}{{qed|lemma}} Now let $x \in C$. Thus $x$ is supposed [[Definition:Integral Element of Ring Extension|integral]] over $B$. That is, we can find an expression: :$(1): \quad x^n + b_{n - 1} x^{n - 1} + \dotsb + b_1 x + b_0 = 0, \quad b_i \in B, \ i = 0, \dotsc, n - 1$ Let $D$ be the [[Definition:Subring|subring]] of $C$ generated by $A \cup \set {b_0, \dotsc, b_{n - 1} }$. By the lemma, $D$ is finitely generated over $A$. Moreover, $D \sqbrk x$ is finitely generated over $D$ because of the equation $(1)$. Therefore by [[Transitivity of Finite Generation]]: :$D \sqbrk x$ is a finitely generated $A$-module. Finally by [[Equivalent Definitions of Integral Dependence]], $x$ is [[Definition:Integral Element of Ring Extension|integral]] over $A$. {{qed}}	1
Let $\sequence {H_n}_{n \mathop \in \N}$ be a [[Definition:Sequence|sequence]] of [[Definition:Hilbert Space|Hilbert spaces]]. Denote by $H = \displaystyle \bigoplus_{n \mathop = 1}^\infty H_n$ their [[Definition:Hilbert Space Direct Sum|Hilbert space direct sum]]. For each $n \in \N$, let $T_n \in \map B {H_n}$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Suppose that one has $\displaystyle \sup_{n \mathop \in \N} \norm {T_n} < \infty$, where $\norm {\, \cdot \, }$ signifies the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]]. Define $T \in \map B H$ by: :$\forall h = \sequence {h_n}_{n \mathop \in \N}: T h = \sequence {T_n h_n}_{n \mathop \in \N} \in H$ (That $T$ is indeed [[Definition:Bounded Linear Operator|bounded]] follows from [[Bounded Linear Operator on Hilbert Space Direct Sum]].) Then $T$ is [[Definition:Compact Linear Operator|compact]] {{iff}} the following conditions hold: : For each $n \in \N$, $T_n$ is [[Definition:Compact Linear Operator|compact]] : $\displaystyle \lim_{n \mathop \to \infty} \norm {T_n} = 0$	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Matrix|matrices]] whose [[Definition:Order of Matrix|order]] is $m \times n$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf B | r = \sqbrk a_{m n} + \sqbrk b_{m n} | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{eqn | r = \sqbrk {a + b}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk {b + a}_{m n} | c = [[Commutative Law of Addition]] }} {{eqn | r = \sqbrk b_{m n} + \sqbrk a_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \mathbf B + \mathbf A | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{end-eqn}} {{qed}}	1
Let $K$ be a [[Definition:Division Subring|division subring]] of the [[Definition:Division Ring|division ring]] $\struct {L, +_L, \times_L}$. Let $\struct {G, +_G, \circ}_L$ be a [[Definition:Vector Space|$L$-vector space]]. Then $\struct {G, +_G, \circ_K}_K$ is a [[Definition:Vector Space|$K$-vector space]], where $\circ_K$ is the [[Definition:Restriction of Operation|restriction]] of $\circ$ to $K \times G$. The [[Definition:Vector Space|$K$-vector space]] $\struct {G, +_G, \circ_K}_K$ is called the '''$K$-vector space obtained from $\struct {L, +_L, \times_L}$ by restricting scalar multiplication'''.	1
{{ProofWanted|Need to consider which definition you start from}}	1
By [[Unique Representation by Ordered Basis]], $\psi$ is a [[Definition:Bijection|bijection]]. We have: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \lambda_k a_k + \sum_{k \mathop = 1}^n \mu_k a_k | r = \sum_{k \mathop = 1}^n \paren {\lambda_k a_k + \mu_k a_k} | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {\lambda_k + \mu_k} a_k | c = }} {{end-eqn}} and we have: {{begin-eqn}} {{eqn | l = \beta \sum_{k \mathop = 1}^n \lambda_k a_k | r = \sum_{k \mathop = 1}^n \beta \paren {\lambda_k a_k} | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {\beta \lambda_k} a_k | c = }} {{end-eqn}} thus proving that $\psi$ is also a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]]. {{Qed}}	1
Let $0$ be the [[Definition:Ring Zero|zero]] of $\struct {R, \norm {\,\cdot\,} }$. Let $d$ denote the [[Definition:Metric Induced by Norm|metric induced by $\norm {\, \cdot \,}$]], that is: :$\map d {x, y} = \norm {x - y}$ From [[Metric Induced by Norm on Normed Division Ring is Metric]] we have that $d$ is indeed a [[Definition:Metric|metric]]. Then, from the [[Reverse Triangle Inequality]] as applied to [[Definition:Metric Space|metric spaces]]: :$\forall x, y, z \in R: \bigsize {\norm {x - z} - \norm {y - z} } \le \norm {x - y}$ Then: :$\forall x, y \in R: \bigsize {\norm x - \norm y} = \bigsize {\norm{x - 0} - \norm{y - 0} } \le \norm {x - y}$ {{qed}} [[Category:Triangle Inequality]] [[Category:Normed Division Rings]] s4owpzx2q00sisb93nn4yho7dlsvgj2	1
Let $\mathbf A$ be an [[Definition:Invertible Matrix|invertible matrix]]. Then: :$\paren {\mathbf A^{-1} }^{-1} = \mathbf A$ That is, an [[Definition:Invertible Matrix|invertible matrix]] equals the [[Definition:Inverse Matrix|inverse]] of its [[Definition:Inverse Matrix|inverse]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]] such that $n \ne 1$. Let $\map {\MM_R} n$ denote the [[Definition:Matrix Space|$n \times n$ matrix space]] over $R$. Then [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]] over $\map {\MM_R} n$ is not [[Definition:Commutative Operation|commutative]]: :$\exists \mathbf A, \mathbf B \in \map {\MM_R} n: \mathbf {A B} \ne \mathbf {B A}$ If $R$ is specifically not [[Definition:Commutative Ring|commutative]], then the result holds when $n = 1$ as well.	1
Let $T = \struct {S, \tau}$ be a [[Definition:Separable Space|separable space]]. Then $T$ satisfies the [[Definition:Countable Chain Condition|countable chain condition]].	1
Taking $\phi = N$ in the proof of [[Cayley-Hamilton Theorem for Finitely Generated Modules]] we see that $N$ satisfies: :$p_N \left({x}\right) = \det \left({x \cdot I_n - N}\right) = 0$ Take $\mathfrak a$ to be the ideal generated by the entries of $N$. {{qed}} {{explain}} [[Category:Linear Algebra]] 12a2gfrxh7ezx78uzydbd8w2iqbcxjm	1
A [[Definition:Vector Space|vector space]] is by definition a [[Definition:Unitary Module|unitary module]] over a [[Definition:Division Ring|division ring]]. $S$ is a [[Definition:Division Ring|division ring]] by assumption. $\struct {R, +, \circ_S}_S$ is a [[Definition:Unitary Module|unitary module]] by [[Subring Module/Special Case]]. {{qed}}	1
Follows immediately from [[Countable Space is Separable]]. {{qed}}	1
Let $\JJ$ be a [[Definition:Left Ideal of Ring|left ideal]] of $\CC$ such that $\NN \subsetneq \JJ \subseteq \\CC$. It will be shown that $\JJ$ = $\CC$, from which the result will follow. Let $\sequence {x_n} \in \JJ \setminus \NN$ By [[Combination Theorem for Cauchy Sequences/Inverse Rule|Inverse Rule for Cauchy sequences]] then :$\exists K \in \N: \sequence { \paren {x_{K + n} }^{-1} }_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. Let $\sequence {y_n}$ be the [[Definition:Sequence|sequence]] defined by: :$y_n = \begin {cases} 0 & : n \le K \\ \paren {x_n}^{-1} & : n > K \end {cases}$ By [[Cauchy Sequence with Finite Elements Prepended is Cauchy Sequence]] then $\sequence {y_n} \in \CC$ By the definition of a [[Definition:Left Ideal of Ring|left ideal]] the product $\sequence {y_n} \sequence {x_n} = \sequence {y_n x_n} \in \JJ$ By the definition of $\sequence {y_n}$ then: :$y_n x_n = \begin {cases} 0 & : n \le K \\ 1 & : n > K \end {cases}$ Let $\mathcal {1} = \tuple {1, 1, 1, \dotsc}$ be the [[Definition:Unity of Ring|unity]] of $\CC$ Then $\mathcal {1} - \sequence {y_n} \sequence {x_n}$ is the [[Definition:Sequence|sequence]] $\sequence {w_n}$ defined by :$w_n = \begin{cases} 1 & : n \le K \\ 0 & : n > K \end {cases}$ By [[Convergent Sequence with Finite Elements Prepended is Convergent Sequence]] then $\sequence {w_n}$ is convergent to 0. So $\sequence {w_n} \in \NN \subsetneq \JJ$ Since $\sequence {y_n} \sequence {x_n}, \sequence {w_n} \in \JJ$ by the definition of a [[Definition:Ideal of Ring|ring ideal]] then: :$\sequence {w_n} + \sequence {y_n} \sequence {x_n} = \mathcal {1} \in \JJ$ By the definition of a [[Definition:Left Ideal of Ring|left ideal]] then: :$\forall \sequence {a_n} \in \CC, \sequence {a_n} \circ \mathcal {1} = \sequence {a_n} \in \JJ$ Hence $\JJ = \CC$ {{qed}}	1
The elements of the [[Definition:Scalar Field|scalar field]] $\struct {K, +_K, \times_K}$ are called '''scalars'''.	1
Let $\mathbf J_n$ be the $n \times n$ [[Definition:Square Ones Matrix|square ones matrix]]. Let $\mathbf B$ be an $n\times n$ [[Definition:Invertible Matrix|invertible matrix]] with entries $b_{i j}$, $1 \le i, j \le n$. Then: :$\displaystyle \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n b_{i j} = 1 - \map \det {\mathbf B} \map \det {\mathbf B^{-1} - \mathbf J_n}$	1
Let $p \ge 1$ be a [[Definition:Real Number|real number]]. Let $\ell^p$ denote the [[Definition:P-Sequence Space|$p$-sequence space]]. Let $\mathbf x = \sequence {x_n} \in \ell^p$. Let $\norm {\mathbf x}_p$ be a [[Definition:P-Norm|p-norm]]. Suppose, $\norm {\mathbf x}_p \ne 0$. Then: :$\displaystyle \dfrac \d {\d p} \norm {\mathbf x}_p = \frac {\norm {\mathbf x}_p} p \paren { \frac {\sum_{n \mathop = 0}^\infty \size {x_n}^p \map \ln {\size {x_n} } } {\norm {\bf x}_p^p} - \map \ln {\norm {\bf x}_p} }$	1
Let $\struct {S, \cdot}$ be an [[Definition:Algebraic Structure|algebraic structure]]. Let $\map {\MM_S} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. For $\mathbf A, \mathbf B \in \map {\MM_S} {m, n}$, let $\mathbf A \circ \mathbf B$ be defined as the [[Definition:Hadamard Product|Hadamard product]] of $\mathbf A$ and $\mathbf B$. The operation $\circ$ is [[Definition:Associative Operation|associative]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Associative Operation|associative]] on $\struct {S, \cdot}$.	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $M$ be an $A$-[[Definition:Module|module]]. {{TFAE|def = Noetherian Module}}	1
There exists at least one example of a [[Definition:Separable Space|separable topological space]] which is not also a [[Definition:Second-Countable Space|second-countable space]].	1
By the definition of [[Definition:Unique Factorization Domain|unique factorization domain]], we need to show that: For all $x \in D$ such that $x$ is non-[[Definition:Ring Zero|zero]] and not a [[Definition:Unit of Ring|unit]] of $D$: :$(1): \quad x$ has a [[Definition:Complete Factorization|complete factorization]] in $D$ :$(2): \quad$ Any two [[Definition:Complete Factorization|complete factorizations]] of $x$ in $D$ are [[Definition:Equivalent Factorizations|equivalent]]. === Proof of Existence === [[Proof by Complete Induction]]: Let $\nu$ be the [[Definition:Euclidean Domain|Euclidean valuation function]] on $D$. For all $n \in \Z_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :every $x \in D$ such that $\map \nu x = n$ is either a [[Definition:Unit of Ring|unit of $D$]] or has a [[Definition:Complete Factorization|complete factorization]] in $D$. That is, it can be written as the [[Definition:Ring Product|product]] of a [[Definition:Finite Set|finite]] number of [[Definition:Irreducible Element of Ring|irreducible elements]]. ==== Basis for the Induction ==== If $\map \nu x = \map \nu 1$ then $x$ is a [[Definition:Unit of Ring|unit of $D$]]. So $\map P n$ is true for $n = \map \nu 1$. This is our [[Definition:Basis for the Induction|basis for the induction]]. ==== Induction Hypothesis ==== Now we need to show that, if $\map P k$ is true for all values of $k < \map \nu x$, then it logically follows that $\map P n$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :Every $x \in D$ such that $\map \nu x < n$ is either a [[Definition:Unit of Ring|unit of $D$]] or can be written as the [[Definition:Ring Product|product]] of a [[Definition:Finite Set|finite]] number of [[Definition:Irreducible Element of Ring|irreducible elements]]. Then we need to show: :Every $x \in D$ such that $\map \nu x = n$ is either a [[Definition:Unit of Ring|unit of $D$]] or can be written as the [[Definition:Ring Product|product]] of a [[Definition:Finite Set|finite]] number of [[Definition:Irreducible Element of Ring|irreducible elements]]. ==== Induction Step ==== This is our [[Definition:Induction Step|induction step]]: Suppose $x \in D$ such that $\map \nu x = n > \map \nu 1$. If $x$ is [[Definition:Irreducible Element of Ring|irreducible]], then it is the [[Definition:Ring Product|product]] of a [[Definition:Finite Set|finite]] number (that is, $1$) of [[Definition:Irreducible Element of Ring|irreducible elements]]. If not, then $x = y z$, where neither $b$ nor $c$ are either [[Definition:Unit of Ring|unit of $D$]] or [[Definition:Irreducible Element of Ring|irreducible]]. By [[Euclidean Valuation of Non-Unit is less than that of Product]] we have that: :$\map \nu y < \map \nu x$ and $\map \nu z < \map \nu x$ By the [[Unique Factorization Theorem#Induction Hypothesis|induction hypothesis]], we assume the truth of $\map P n$ for all values of $n < \map \nu x$. Hence we know that both $y$ and $z$ can be written as the [[Definition:Ring Product|product]] of a [[Definition:Finite Set|finite]] number of [[Definition:Irreducible Element of Ring|irreducible elements]]. Thus we may deduce the same about $x$. So $\map P n$ has been shown to be true by the [[Second Principle of Mathematical Induction]]. {{qed|lemma}} === Proof of Equivalence === Now we need to show that any two [[Definition:Complete Factorization|complete factorizations]] of $x$ in $D$ are [[Definition:Equivalent Factorizations|equivalent]]. Suppose $p_1 p_2 \ldots p_r = q_1 q_2 \ldots q_s$ where all the $p$s and $q$s are [[Definition:Irreducible Element of Ring|irreducible elements]] of $D$. Then: :$p_1 \divides q_1 q_2 \ldots q_s$ and by [[Euclid's Lemma for Irreducible Elements]]: :$p_1 \divides q_i$ for some $i$. Since each of these is [[Definition:Irreducible Element of Ring|irreducible]], they must by definition either be [[Definition:Associate in Integral Domain|associates]] or that $p_1 = q_i$. Thus we can write $q_i = u_i p_1$ where $u_i$ is a [[Definition:Unit of Ring|unit]] of $D$. Cancelling $p_1$ from both sides, we continue similarly with $p_2$, and so on. After a [[Definition:Finite Set|finite number]] of steps we determine that $r = s$ and that $q_1, q_2, \ldots, q_s$ are just [[Definition:Associate in Integral Domain|associates]] of $p_1, p_2, \ldots p_r$ perhaps in a different order. It follows directly, by definition, that $p_1 p_2 \ldots p_r$ and $q_1 q_2 \ldots q_s$ are [[Definition:Equivalent Factorizations|equivalent factorizations]]. {{qed}}	1
The [[Definition:Complex Modulus|complex modulus]] is a [[Definition:Norm on Division Ring|norm]] on the [[Definition:Complex Number|set of complex numbers]] $\C$.	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :$\exists \alpha \in \R_{\gt 0}: \forall x \in R: \norm x_1 = \norm x_2^\alpha$ Then $d_1$ and $d_2$ are [[Definition:Topologically Equivalent Metrics|topologically equivalent metrics]].	1
Let $h \in H$ such that $\norm h_H \le 1$. Then: {{begin-eqn}} {{eqn | l = \norm {A h}_K^2 | r = \innerprod {A h} {A h}_K | c = {{Defof|Inner Product Norm}} }} {{eqn | r = \innerprod {A^* A h} h_H | c = {{Defof|Adjoint Linear Transformation}} }} {{eqn | o = \le | r = \norm {A^*A h}_H \norm h_H | c = [[Cauchy-Bunyakovsky-Schwarz Inequality]] }} {{eqn | o = \le | r = \norm {A^* A} \norm h_H^2 | c = [[Submultiplicativity of Operator Norm]] }} {{eqn | o = \le | r = \norm {A^* A} | c = Assumption on $h$ }} {{eqn | o = \le | r = \norm {A^*} \norm A | c = [[Submultiplicativity of Operator Norm]] }} {{end-eqn}} By [[Definition:Norm on Bounded Linear Transformation|definition $(1)$]] for $\norm A$, it follows that: :$\norm A^2 \le \norm {A^* A} \le \norm {A^*} \norm A$ That is: :$\norm A \le \norm {A^*}$. By substituting $A^*$ for $A$, and using $A^{**} = A$ from [[Double Adjoint is Itself]], the reverse inequality is obtained. Hence $\norm A^2 = \norm {A^* A} = \norm {A^*}^2$. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $S$ be a [[Definition:Division Subring|division subring]] of $R$, such that $1_R \in S$. The [[Definition:Vector Space over Division Subring|vector space $\struct {R, +, \circ_S}_S$ over $\circ_S$]] is a [[Definition:Vector Space|$S$-vector space]].	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,}}$. By the definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence in a normed division ring]], $\sequence {x_n} $ is [[Definition:Convergent Sequence in Metric Space|convergent]] to the [[Definition:Limit of Sequence (Metric Space)|limit]] $l$ in $\struct {R, d}$. By [[Convergent Sequence in Metric Space is Bounded]], $\sequence {x_n} $ is a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in $\struct {R, d}$. By [[Sequence is Bounded in Norm iff Bounded in Metric]], $\sequence {x_n} $ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in $\struct {R, \norm {\,\cdot\,} }$. {{qed}}	1
As $D$ is a [[Definition:Principal Ideal Domain|principal ideal domain]], every [[Definition:Ideal of Ring|ideal]] of $D$ is a [[Definition:Principal Ideal of Ring|principal ideal]] $\ideal r$ generated by some $r \in D$. So, let $\ideal p$ be an arbitrary [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]] of $D$ [[Definition:Principal Ideal of Ring|generated by $p$]], where $p \ne 0_R$. As $\ideal p$ is [[Definition:Prime Ideal of Commutative and Unitary Ring|prime]], $p$ is [[Definition:Irreducible Element of Ring|irreducible]]. {{explain|Prove the above}} The result follows from [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]]. {{qed}}	1
Let $v_p$ be the [[Definition:P-adic Valuation|$p$-adic valuation]] on the [[Definition:Rational Number|rational numbers]]. Recall that the [[Definition:P-adic Norm|$p$-adic norm]] is defined as: :$\forall q \in \Q: \norm q_p := \begin{cases} 0 & : q = 0 \\ p^{- \map {\nu_p} q} & : q \ne 0 \end{cases}$ We must show the following hold for all $x$, $y \in \Q$: {{begin-axiom}} {{axiom | n = \text N 1 | q = \forall x \in \Q | ml= \norm x_p = 0 | mo= \iff | mr= x = 0 }} {{axiom | n = \text N 2 | q = \forall x, y \in \Q | ml= \norm {x y} | mo= = | mr= \norm x_p \times \norm y_p }} {{axiom | n = \text N 3 | q = \forall x, y \in \Q | ml= \norm {x + y}_p | mo= \le | mr= \norm x_p + \norm y_p }} {{end-axiom}} === Norm Axiom $(\text N 1)$ === By [[Power of Positive Real Number is Positive/Real Number|Power of Positive Real Number is Positive]]: :$\displaystyle \forall s \in \R: \frac 1 {p^s} > 0$ By definition of the [[Definition:P-adic Norm|$p$-adic norm]] it follows that: :$\forall x \in \Q: \norm x_p = 0 \iff x = 0$ Thus the [[Definition:P-adic Norm|$p$-adic norm]] fulfils [[Definition:Norm Axioms|axiom $(\text N 1)$]]. {{qed|lemma}} === Norm Axiom $(\text N 2)$ === Let $x = 0$ or $y = 0$. Then $\norm x_p = 0$ or $\norm y_p = 0$ from [[Definition:Norm Axioms|axiom $(\text N 1)$]], and: {{begin-eqn}} {{eqn | l = x y | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm {x y}_p | r = 0 | c = [[Definition:Norm Axioms|Norm Axioms: Axiom $(\text N 1)$]] }} {{eqn | r = \norm x_p \times \norm y_p | c = }} {{end-eqn}} Let $x, y\in \Q_{\ne 0}$. Then: {{begin-eqn}} {{eqn | l = \norm {x y}_p | r = \frac 1 {p^{\map {\nu_p} {x y} } } | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | r = \frac 1 {p^{\map {\nu_p} x + \map {\nu_p} y} } | c = [[P-adic Valuation is Valuation|$p$-adic Valuation is Valuation]] }} {{eqn | r = \frac 1 {p^{\map {\nu_p} x} p^{\map {\nu_p} y} } | c = [[Exponent Combination Laws/Product of Powers|Exponent Combination Laws: Product of Powers]] }} {{eqn | r = \norm x_p \times \norm y_p | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{end-eqn}} Thus the [[Definition:P-adic Norm|$p$-adic norm]] fulfils [[Definition:Norm Axioms|axiom $(\text N 2)$]]. {{qed|lemma}} === Norm Axiom $(\text N 3)$ === Let $x = 0$ or $y = 0$, or $x + y = 0$, the result is trivial. Let $x = 0$. Then: {{begin-eqn}} {{eqn | l = x | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm x_p | r = 0 | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | ll= \leadsto | l = \norm x_p + \norm y_p | r = \norm y_p | c = }} {{eqn | r = \norm {x + y}_p | c = }} {{end-eqn}} and so $\norm {x + y}_p \le \norm x_p + \norm y_p$ The same argument holds for $y = 0$. Let $x + y = 0$. {{begin-eqn}} {{eqn | l = x + y | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm {x + y}_p | r = 0 | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | o = \le | r = \norm x_p + \norm y_p | c = as $\norm x_p \ge 0$ and $\norm y_p \ge 0$ from [[Definition:Norm Axioms|Norm Axioms: Axiom $(N1)$]] }} {{end-eqn}} Let $x, y, x + y \in \Q_{\ne 0}$. From [[P-adic Valuation is Valuation|$p$-adic Valuation is Valuation]]: :$\map {\nu_p} {x + y} \ge \min \set {\map {\nu_p} x, \map {\nu_p} y}$ Then: {{begin-eqn}} {{eqn | l = \norm {x + y}_p | r = p^{-\map {\nu_p} {x + y} } | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | o = \le | r = \max \set {p^{-\map {\nu_p} x}, p^{-\map {\nu_p} y} } }} {{eqn | r = \max \set {\norm x_p, \norm y_p} | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | o = \le | r = \norm x_p + \norm y_p }} {{end-eqn}} Thus the [[Definition:P-adic Norm|$p$-adic norm]] fulfils [[Definition:Norm Axioms|axiom $(\text N 3)$]]. {{qed|lemma}} All [[Definition:Norm Axioms|norm axioms]] are seen to be satisfied. Hence the result. {{qed}}	1
Let $\mathbf 1, \mathbf i, \mathbf j, \mathbf k$ denote the following four [[Definition:Element|elements]] of the [[Definition:Matrix Space|matrix space]] $\map {\mathcal M_\C} 2$: :$\mathbf 1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \qquad \mathbf i = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} \qquad \mathbf j = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \qquad \mathbf k = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} $ where $\C$ is the set of [[Definition:Complex Number|complex numbers]]. Then $\mathbf 1, \mathbf i, \mathbf j, \mathbf k$ are related to each other in the following way: {{begin-eqn}} {{eqn | l = \mathbf i \mathbf j = - \mathbf j \mathbf i | r = \mathbf k }} {{eqn | l = \mathbf j \mathbf k = - \mathbf k \mathbf j | r = \mathbf i }} {{eqn | l = \mathbf k \mathbf i = - \mathbf i \mathbf k | r = \mathbf j }} {{eqn | l = \mathbf i^2 = \mathbf j^2 = \mathbf k^2 = \mathbf i \mathbf j \mathbf k | r = -\mathbf 1 }} {{end-eqn}}	1
Let $x = x + 0 i \in \R$. Then: {{begin-eqn}} {{eqn | l = \cmod {x + 0 i} | r = \sqrt {x^2 + 0^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \sqrt {x^2} | c = }} {{eqn | r = \size x | c = {{Defof|Absolute Value|index = 2}} }} {{end-eqn}} {{qed}}	1
Let $\C$ be the [[Definition:Field of Complex Numbers|field of complex numbers]]. Let $\F$ be a [[Definition:Subfield|subfield]] of $\C$. Let $V$ be a [[Definition:Vector Space|vector space]] over $\F$. An '''inner product''' is a [[Definition:Mapping|mapping]] $\innerprod \cdot \cdot: V \times V \to \mathbb F$ that satisfies the following properties: {{begin-axiom}} {{axiom | n = 1 | lc= [[Definition:Conjugate Symmetric Mapping|Conjugate Symmetry]] | q = \forall x, y \in V | m = \quad \innerprod x y = \overline {\innerprod y x} }} {{axiom | n = 2 | lc= [[Definition:Bilinear Mapping|Bilinearity]] | q = \forall x, y \in V, \forall a \in \mathbb F | m = \quad \innerprod {a x + y} z = a \innerprod x z + \innerprod y z }} {{axiom | n = 3 | lc= [[Definition:Non-Negative Definite Mapping|Non-Negative Definiteness]] | q = \forall x \in V | m = \quad \innerprod x x \in \R_{\ge 0} }} {{axiom | n = 4 | lc= [[Definition:Positiveness|Positiveness]] | q = \forall x \in V | m = \quad \innerprod x x = 0 \implies x = \mathbf 0_V }} {{end-axiom}} That is, an inner product is a [[Definition:Semi-Inner Product|semi-inner product]] with the additional condition $(4)$. If $\mathbb F$ is a [[Definition:Subfield|subfield]] of the [[Definition:Field of Real Numbers|field of real numbers]] $\R$, it follows from [[Complex Number equals Conjugate iff Wholly Real]] that $\overline {\innerprod y x} = \innerprod y x$ for all $x, y \in V$. Then $(1)$ above may be replaced by: {{begin-axiom}} {{axiom | n = 1' | lc= [[Definition:Symmetric Mapping (Linear Algebra)|Symmetry]] | q = \forall x, y \in V | m = \innerprod x y = \innerprod y x }} {{end-axiom}} === [[Definition:Inner Product Space|Inner Product Space]] === {{:Definition:Inner Product Space}}	1
Let $\Omega$ denote the first [[Definition:Uncountable Ordinal|uncountable ordinal]]. Let $\hointr 0 \Omega$ denote the [[Definition:Uncountable Open Ordinal Space|open ordinal space]] on $\Omega$. Then $\hointr 0 \Omega$ is not a [[Definition:Separable Space|separable space]].	1
Let $\mathbf I$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order]] $m$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $e$ be an [[Definition:Elementary Row Operation|elementary row operation]] on $\mathbf I$. Let $\mathbf E$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] of [[Definition:Order of Square Matrix|order]] $m$ [[Definition:Unique|uniquely]] defined as: :$\mathbf E = e \paren {\mathbf I}$ where $\mathbf I$ is the [[Definition:Unit Matrix|unit matrix]]. Let $r_k$ denote the $k$th [[Definition:Row of Matrix|row]] of $\mathbf I$ for $1 \le k \le m$.	1
We have that: :[[Real Vector Space is Vector Space]] :By [[Euclidean Space is Normed Space]], $\norm {\, \cdot \,}_2$ is a [[Definition:Norm on Vector Space|norm]] on $\R^n$ By [[Definition:Definition|definition]], $\struct {\R^n, \norm {\, \cdot \,}_2}$ is a [[Definition:Normed Vector Space|normed vector space]]. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $P, Q$ be [[Definition:Projection (Hilbert Spaces)|projections]]. Then the following are equivalent: :$(1): \quad P - Q$ is a [[Definition:Projection (Hilbert Spaces)|projection]] :$(2): \quad P Q = Q$ :$(3): \quad Q P = Q$ {{MissingLinks|Provide proper linking to the def of addition of operators}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\Bbb F \in \set {\R, \C}$. Let $T \in \map {B_0} H$ be a [[Definition:Compact Operator|compact operator]]. Let $\lambda \in \Bbb F, \lambda \ne 0$ be a nonzero [[Definition:Scalar (Vector Space)|scalar]]. Suppose that the following holds: :$\inf \set {\norm {\paren {T - \lambda I} h}_H: \norm h_H = 1} = 0$ Then $\lambda \in \map {\sigma_p} T$, that is, $\lambda$ is an [[Definition:Eigenvalue|eigenvalue]] for $T$.	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $M$ be a [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Module|$A$-module]]. Let $\mathfrak a$ be an [[Definition:Ideal of Ring|ideal]] of $A$. Let $\phi$ be an [[Definition:Endomorphism|endomorphism]] of $M$ such that $\phi \left({M}\right) \subseteq \mathfrak a M$. Then $\phi$ satisfies an equation of the form: :$\phi^n + a_{n-1} \phi^{n-1} + \cdots + a_1 \phi + a_0 = 0$ with the $a_i \in \mathfrak a$.	1
:$\quad \forall \sequence {x_n} \in \NN, \sequence {y_n} \in \CC: \sequence {x_n} \sequence {y_n} \in \NN, \sequence {y_n} \sequence {x_n} \in \NN$	1
For $\struct {S, +, \times}$ to be a [[Definition:Ring (Abstract Algebra)|ring]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is a [[Definition:Semigroup|semigroup]]. For $\struct {S, \times}$ to be a [[Definition:Semigroup|semigroup]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is [[Definition:Closed Algebraic Structure|closed]]. That is: :$\forall x, y \in S: x \times y \in S$ Let $x = \begin {pmatrix} 0 & 1 \\ 1 & 0 \end {pmatrix}$ and $y = \begin {pmatrix} 0 & 2 \\ 2 & 0 \end {pmatrix}$. Both $x$ and $y$ are in $S$, as both are [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $2$]] whose [[Definition:Diagonal Element|diagonal elements]] are [[Definition:Zero (Number)|zero]]. But then: :$x y = \begin {pmatrix} 0 & 1 \\ 1 & 0 \end {pmatrix} \begin {pmatrix} 0 & 2 \\ 2 & 0 \end {pmatrix} = \begin {pmatrix} 2 & 0 \\ 0 & 2 \end {pmatrix}$ which is a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $2$]] whose [[Definition:Diagonal Element|diagonal elements]] are ''not'' [[Definition:Zero (Number)|zero]]. Hence $x y \notin S$ and so $\struct {S, \times}$ is not [[Definition:Closed Algebraic Structure|closed]]. The result follows. {{qed}}	1
The [[Definition:Real Number|set of real numbers]] $\R$ forms an [[Definition:Algebra over Field|algebra]] over the [[Definition:Field of Real Numbers|field of real numbers]]. This [[Definition:Algebra over Field|algebra]] is: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]]. :$(2): \quad$ A [[Definition:Commutative Algebra|commutative algebra]]. :$(3): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]]. :$(4): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]] whose $*$ operator is the [[Definition:Identity Mapping|identity mapping]]. :$(5): \quad$ A [[Definition:Real Star-Algebra|real $*$-algebra]].	1
Let $p \in \Z_{>0}$ and $q \in \Z_{>0}$ be [[Definition:Strictly Positive Integer|positive]] [[Definition:Coprime Integers|coprime integers]]. Let [[Definition:Ring of Integers Modulo m|$\Z / p \Z$]] and $\Z / q \Z$ be $\Z$-[[Definition:Module|modules]]. {{explain|It is not a good idea to use the same notation for both a ring and a module. Either $\Z / p \Z$ is a ring or it is a module. Please consider taking the advice in the explain template at the bottom of this page.}} Then: :$\Z / p \Z \otimes_\Z \Z / q\Z = 0$ where $\otimes_\Z$ denotes [[Definition:Tensor Product of Modules|tensor product]] over integers.	1
=== Definition 2 implies Definition 1 === Let $\lambda \in \R$ and $f, g \in C^\infty \left({V, \R}\right)$. {{begin-eqn}} {{eqn | l = X_m \left({f + \lambda g} \right) | r = \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} \, \left({f + \lambda g}\right) \circ \gamma \left({\tau}\right) | c = [[Definition:Tangent Vector/Definition 2|Definition 2]] }} {{eqn | r = \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} \, \left({f \circ \gamma \left({\tau}\right)}\right) + \lambda \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} \, \left({g \circ \gamma} \left({\tau}\right) \right) }} {{eqn | r = X_m \left({f}\right) + \lambda \, X_m \left({g}\right) }} {{end-eqn}} Thus $X_m$ is [[Definition:Linear Mapping|linear]]. {{begin-eqn}} {{eqn | l = X_m \left({f g} \right) | r = \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} \left({f g}\right) \circ \gamma \left({\tau}\right) | c = Definition 2 }} {{eqn | r = \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} f \circ \gamma \left({\tau}\right) \, g \circ \gamma \left({0}\right) + f \circ \gamma \left({0}\right) \, \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} g \circ \gamma \left({\tau}\right) | c = [[Product Rule]] }} {{eqn | r = X_m \left({f}\right) \, g \left({m}\right) + f \left({m}\right) \, X_m \left({g}\right) | c = Definition 2, $\gamma \left({0}\right) = m$ }} {{end-eqn}} Hence $X_m$ satisfies the ''Leibniz law''. Thus $X_m$ satisfies Definition 1. {{qed|lemma}} === Definition 1 implies Definition 2 === === Lemma 1 === Let $X_m$ be a [[Definition:Tangent Vector|tangent vector]] at $m \in M$ according to Definition 1. Let $V$ be an [[Definition:Open Neighborhood|open neighborhood]] of $M$. Let $f \in C^\infty \left({V, \R}\right)$ be [[Definition:Constant Mapping|constant]]. Then $X_m \left({f}\right) = 0$. === Proof of Lemma 1 === Let $f \left({m}\right) = 0$. Then, by [[Definition:Constant_Mapping|constancy]], $f = 0$ on $V$. Hence, by [[Definition:Linear Mapping|linearity]], $X_m \left({0}\right) = 0$. Let $f \left({m}\right) \ne 0$. {{begin-eqn}} {{eqn | l = X_m \left({f}\right) | r = X_m \left({1 \, f}\right) }} {{eqn | r = X_m \left({1}\right) \, f \left({m}\right) + X_m \left({f}\right) | c = Leibniz law }} {{eqn | l = \iff | o = }} {{eqn | l = X_m \left({1}\right) | r = 0 | c = $f \left({m}\right) \ne 0$ }} {{end-eqn}} $f$ is [[Definition:Constant Mapping|constant]], {{iff}} $\exists \lambda \in \R : f \left({V}\right) = \left\{ {\lambda} \right\}$ {{iff}} $f = \lambda$. {{explain|$\exists$ in the above? Should it not be $\forall$?}} {{begin-eqn}} {{eqn | l = \lambda X_m \left({1}\right) | r = X_m \left({\lambda}\right) | c = Definition of [[Definition:Linear Mapping|Linear Mapping]] }} {{eqn | r = X_m \left({f}\right) | c = as $f = \lambda$ }} {{eqn | r = 0 | c = as $X_m \left({1}\right) = 0$ }} {{end-eqn}} {{qed|lemma}} Let $X_m$ be a [[Definition:Tangent Vector|tangent vector]] at $m \in M$ according to Definition 1. Denote $n := \dim M$. Let $f \in C^\infty \left({V, \R}\right)$. Let $(U, \kappa)$ be a [[Definition:Chart|chart]] with $\kappa \left( {m} \right) = 0$. Let $\kappa^i$ be the $i$th [[Definition:Coordinate Function|coordinate function]] of the chart $(U, \kappa)$. By, [[Taylor's Theorem/n Variables]] : {{begin-eqn}} {{eqn | l = f | r = f \circ \kappa^{-1} \circ \kappa | c = }} {{eqn | r = \left({f \circ \kappa^{-1} }\right) \left({\kappa}\right) | c = }} {{eqn | r = \left({f \circ \kappa^{-1} }\right) \left({0}\right) + \sum_{i \mathop = 1}^n \frac {\partial} {\left({f \circ \kappa^{-1} }\right)} {\partial{\kappa^i} } \left({0}\right) \, \kappa^i + \mathcal O_2 \left({\kappa}\right) | c = }} {{end-eqn}} Observe that $\displaystyle \left({f \circ \kappa^{-1} }\right) \left({0}\right) = \left({f \circ \kappa^{-1} }\right) \left( {\kappa \left({m}\right)}\right) = f \left({m}\right) $ is a [[Definition:Constant Mapping|constant mapping]] on $V$. {{explain|The above needs more than observation.}} Define $X^i := X_m \left({\kappa ^i}\right)$. Then by [[Definition:Linear Mapping|linearity]]: :$\displaystyle X_m \left({f}\right) = X_m \left({f \left({m}\right)}\right) + \sum_{i \mathop = 1}^n \frac {\partial \left({f \circ \kappa^{-1} }\right)} {\partial \kappa^i} \left({0}\right) \ X^i + X_m \left({\mathcal O_2 \left({\kappa}\right) }\right)$ === Lemma 2 === :$X_m \left({\mathcal O_2 \left({\kappa}\right)}\right) = 0$ === Proof of Lemma 2 === By [[Taylor's Theorem/n Variables]], for each [[Definition:Summand|summand]] of $\mathcal O _2 \left({\kappa}\right)$ there exists $i \in \left\{ {1, \ldots, n} \right\}$ and an $h \in C^\infty \left({V, \R}\right)$ with $h \left({m}\right) = 0$ such that the summand is $\kappa^i h$. {{begin-eqn}} {{eqn | l = X_m \left({\kappa^i h}\right) | r = X_m \left({\kappa^i}\right) h \left({m}\right) + \kappa^i \left({m}\right) X_m \left({h}\right) | c = Definition 1 (Leibniz law) }} {{eqn | r = X_m \left({\kappa^i}\right) 0 + 0 X_m \left({h}\right) | c = as $h \left({m}\right) = 0$, $\kappa^i \left({m}\right) = 0$ }} {{eqn | r = 0 }} {{end-eqn}} Thus the sum $\mathcal O_2 \left({\kappa}\right)$ vanishes. {{qed|lemma}} By Lemma 1 : :$\displaystyle X_m \left({f \left({m}\right)} \right) = 0$ By Lemma 2 : :$\displaystyle X_m \left({\mathcal O_2 \left({\kappa }\right)}\right) = 0$ Hence: :$\displaystyle X_m \left({f}\right) = \sum_{i \mathop = 1}^n \frac {\partial \left({f \circ \kappa^{-1}}\right)} {\partial \kappa^i} \left({0}\right) \ X^i$ Let $\left\{{e_i}\right\}$ be a [[Definition:Basis|basis]] of $\R^n$ such that: :$\displaystyle \kappa = \sum_{i \mathop = 1}^n \kappa^i e_i$ {{Disambiguate|Definition:Basis}} Choose a [[Definition:Smooth Curve|smooth curve]] $\gamma: I \to M$ with $0 \in I \subseteq \R$ such that $\gamma \left({0}\right) = m$ and: :$\dfrac {\mathrm d \kappa^i \circ \gamma} {\mathrm d \tau} \left({0}\right) := X^i$ Then: {{begin-eqn}} {{eqn | l = X_m \left({f}\right) | r = \sum_{i \mathop = 1}^n \frac{\partial \left({f \circ \kappa^{-1} }\right)} {\partial \kappa^i} \left({\kappa \left({m}\right)}\right) \frac {\mathrm d \kappa^i \circ \gamma} {\mathrm d \tau} \left({0}\right) | c = as $\kappa \left({m}\right) = 0$ }} {{eqn | r = \sum_{i \mathop = 1}^n \frac {\partial \left({f \circ \kappa^{-1} }\right)} {\partial \kappa^i} \left({\kappa \circ \gamma \left({0}\right)}\right) \frac {\mathrm d \kappa^i \circ \gamma} {\mathrm d \tau} \left({0}\right) | c = as $m = \gamma \left({0}\right)$ }} {{eqn | r = \sum_{i \mathop = 1}^n \left. {\frac {\partial \left({f \circ \kappa^{-1} }\right)} {\partial \kappa^i} \left({\kappa \circ \gamma \left({\tau}\right)}\right) \frac {\mathrm d \kappa^i \circ \gamma} {\mathrm d \tau} \left({\tau}\right)} \right \rvert_{\tau \mathop = 0} }} {{eqn | r = \intlimits {\frac {\mathrm d \left({f \circ \kappa^{-1} \circ \kappa \circ \gamma}\right)} {\mathrm d \tau} \left({\tau}\right)} {\tau \mathop = 0} {} | c = [[Chain Rule for Derivatives]] }} {{eqn | r = \intlimits {\frac {\mathrm d \left({f \circ \gamma} \right)} {\mathrm d \tau} \left({\tau}\right)} {\tau \mathop = 0} {} | c = as $f \circ \kappa^{-1} \circ \kappa = f$ }} {{eqn | r = \frac {\mathrm d} {\mathrm d \tau} {\restriction_0} \, f \circ \gamma \left({\tau}\right) }} {{end-eqn}} Hence $X_m$ is a [[Definition:Tangent Vector|tangent vector]] according to Definition 2. This proves the assertion. {{qed}}	1
As $U$ is an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]], it is necessarily [[Definition:Surjection|surjective]]. Suppose now that $g, h \in H$ are such that $Ug = Uh$. Then as $U$ is a [[Definition:Linear Mapping|linear map]], it follows that $U \left({g - h}\right) = \mathbf{0}_K$, the [[Definition:Zero Vector|zero vector]] of $K$. From property $(3)$ of an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]], conclude that: :$0 = \left\langle{\mathbf{0}_K, \mathbf{0}_K}\right\rangle_K = \left\langle{U \left({g - h}\right), U \left({g - h}\right)}\right\rangle_K = \left\langle{g - h, g - h}\right\rangle_H$ Property $(5)$ of an [[Definition:Inner Product|inner product]] ensures us that $g - h = \mathbf{0}_H$, i.e. $g = h$. Hence $U$ is [[Definition:Injection|injective]]. As $U$ is both [[Definition:Injection|injective]] and [[Definition:Surjection|surjective]], it is a [[Definition:Bijection|bijection]]. {{qed}} [[Category:Hilbert Spaces]] avhhbg8lwg75uz4yiuep5vqe4ari45v	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. From [[Open Ball in Normed Division Ring is Open Ball in Induced Metric]], $\map {B_\epsilon} a$ is the [[Definition:Open Ball|open $\epsilon$-ball of $a$]] in the [[Definition:Metric Space|metric space]] $\struct{R,d}$. From [[Center is Element of Open Ball]]: :$a \in \map {B_\epsilon} a$ {{qed}} [[Category:Normed Division Rings]] [[Category:Center is Element of Open Ball]] hbfz7z7mx8al9c4afrud8yyvslxss3i	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {R, +_R, *_R}$ be the [[Definition:Opposite Ring|opposite ring]] of $\struct {R, +_R, \times_R}$. Let $\struct{G, +_G, \circ}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Let $\circ’ : G \times R \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: x \circ’ \lambda = \lambda \circ x$ Then $\struct{G, +_G, \circ’}$ is a [[Definition:Right Module|right module]] over $\struct {R, +_R, *_R}$.	1
=== $(1) \implies (2)$ === By hypothesis, there exist $r_0, \ldots, r_{n-1} \in R$ such that: :$x^n + r_{n-1} x^{n-1} + \cdots + r_1 x + r_0 = 0$ So the powers $x^k$, $k \ge n$ can be written as an $R$-[[Definition:Linear Combination|linear combination]] of: :$\left\{ {1, \ldots, x^{n-1} }\right\}$ Therefore this [[Definition:Set|set]] [[Definition:Generator|generates]] $R \left[{x}\right]$. {{qed|lemma}} === $(2) \implies (3)$ === $B = R \left[{x}\right]$ trivially satisfies the required conditions. {{qed|lemma}} === $(3) \implies (4)$ === By $(3)$ we have an $R$-module $B$ such that $R \subseteq B$, $B$ is finitely generated over $R$. Also, $x \in B$, so $x B \subseteq B$ as required. {{qed|lemma}} === $(4) \implies (5)$ === By $(4)$ we have an $R \left[{x}\right]$-module $B$ that is finitely generated over $R$. Let $y$ lie in the [[Definition:Annihilator|annihilator]] $\operatorname{Ann}_{R \left[{x}\right]} \left({B}\right)$ We have that $1 \in B$. Then in particular $y \cdot 1 = 0$, and $y = 0$. Therefore, $B$ is [[Definition:Faithful Module|faithful]] over $R \left[{x}\right]$. {{qed|lemma}} === $(5) \implies (1)$ === Let $B$ be as in $(5)$, say generated by $m_1, \ldots, m_n \in B$. Then there are $r_{i j} \in R$, $i, j = 1,\ldots, n$ such that: :$\displaystyle x \cdot m_i = \sum_{j \mathop = 1}^n r_{i j} m_j$ Let $b_{i j} = x \delta_{i j} - r_{i j}$ where $\delta_{i j}$ is the [[Definition:Kronecker Delta|Kronecker delta]]. Then: :$\displaystyle \sum_{j \mathop = 1}^n b_{i j} m_j = 0, \quad i = 1, \ldots, n$ So, let $M = \left({b_{i j} }\right)_{1 \le i, j \le n}$. Then by [[Cramer's Rule]]: :$\left({\det M}\right) m_i = 0$, $i = 1, \ldots, n$ Since $\det M \in R \left[{x}\right]$, also $\det M \in \operatorname{Ann}_{R \left[{x}\right]} \left({B}\right)$. So $\det M = 0$ by hypothesis. But $\det M = 0$ is a [[Definition:Monic Polynomial|monic polynomial]] in $x$ with coefficients in $R$. Thus $x$ is integral over $R$. {{qed}} [[Category:Algebraic Number Theory]] [[Category:Commutative Algebra]] 4kaiy1bq732m22rz278dj5jzit8839p	1
:$\mathbf u \cdot \mathbf u \ge 0$	1
Let $x \in R$ and $\epsilon \in \R_{\gt 0}$ Then for $y \in R$: {{begin-eqn}} {{eqn | l = \norm {y - x}_1 < \epsilon | o = \leadstoandfrom | r = \norm {y - x}_2^\alpha < \epsilon }} {{eqn | o = \leadstoandfrom | r = \norm {y - x}_2 < \epsilon^{1 / \alpha} }} {{end-eqn}} Hence: :$\map {B^1_\epsilon} x = \map {B^2_{\epsilon^{1 / \alpha} } } x$ where: :$\map {B^1_\epsilon} x$ is the [[Definition:Open Ball|open ball]] in $d_1$ [[Definition:Center of Open Ball|centered]] on $x$ with [[Definition:Radius of Open Ball|radius]] $\epsilon$ :$\map {B^2_{\epsilon^{1 / \alpha} } } x$ is the [[Definition:Open Ball|open ball]] in $d_2$ [[Definition:Center of Open Ball|centered]] on $x$ with [[Definition:Radius of Open Ball|radius]] $\epsilon^{1 / \alpha}$ Since $x$ and $\epsilon$ were arbitrary then: :every [[Definition:Open Ball|$d_1$-open ball]] is a [[Definition:Open Ball|$d_2$-open ball]]. Similarly, for $y \in R$: {{begin-eqn}} {{eqn | l = \norm {y - x}_2 < \epsilon | o = \leadstoandfrom | r = \norm {y - x}_2^\alpha < \epsilon^\alpha }} {{eqn | o = \leadstoandfrom | r = \norm {y - x}_1 < \epsilon^\alpha }} {{end-eqn}} So: :every [[Definition:Open Ball|$d_2$-open ball]] is a [[Definition:Open Ball|$d_1$-open ball]]. By the definition of an [[Definition:Open Set of Metric Space|open set of a metric space]] it follows that $d_1$ and $d_2$ are [[Definition:Topologically Equivalent Metrics|topologically equivalent metrics]], {{qed}}	1
Let $\mathcal C$ be a smooth curve given by the vector function $\mathbf r \left({t}\right)$ for $a \le t \le b$. Let $f$ be a differentiable function of two or three variables whose gradient vector $\nabla f$ is continuous on $\mathcal C$. Then: :$\displaystyle \int_\mathcal C \nabla f \cdot d \mathbf r = f \left({\mathbf r \left({b}\right)}\right) - f \left({\mathbf r \left({a}\right)}\right)$	1
The [[Definition:P-Norm|$p$-norm]] on the [[Definition:Real Number|real numbers]] is a [[Definition:Norm on Vector Space|norm]].	1
If $\mathbf A \sim \mathbf B$ then $\mathbf B = \mathbf P^{-1} \mathbf A \mathbf P$. Let $\mathbf Q = \mathbf P$. Then $\mathbf A$ are [[Definition:Matrix Equivalence|equivalent]] to $\mathbf B$, as: :$\mathbf B = \mathbf Q^{-1} \mathbf A \mathbf P$ {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. For $\mathbf A, \mathbf B \in \map {\MM_R} {m, n}$, let $\mathbf A + \mathbf B$ be defined as the [[Definition:Matrix Entrywise Addition over Ring|matrix entrywise sum]] of $\mathbf A$ and $\mathbf B$. The operation $+$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\map {\MM_R} {m, n}$.	1
Let $z_3 := z_1 + z_2$. Then: {{begin-eqn}} {{eqn | l = \cmod {z_3} + \cmod {\paren {-z_2} } | o = \ge | r = \cmod {z_3 + \paren {-z_2} } | c = [[Triangle Inequality for Complex Numbers]] }} {{eqn | ll= \leadsto | l = \cmod {z_3} + \cmod {z_2} | o = \ge | r = \cmod {z_3 - z_2} | c = [[Complex Modulus of Additive Inverse]] }} {{eqn | ll= \leadsto | l = \cmod {z_1 + z_2} + \cmod {z_2} | o = \ge | r = \cmod {z_1} | c = substituting $z_3 = z_1 + z_2$ }} {{eqn | ll= \leadsto | l = \cmod {z_1 + z_2} | o = \ge | r = \cmod {z_1} - \cmod {z_2} | c = }} {{end-eqn}} {{qed}}	1
Both sides are linear in $m$ and they coincide on the elements of $\mathcal A$ by definition of $\mathbf M_{f, \mathcal B, \mathcal A}$. So they are equal for all $m\in M$. {{explain|this has to be fleshed out}} {{qed}}	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\struct {R, \norm {\, \cdot \,} }$. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]]. Let $\NN = \set {\sequence {x_n}: \displaystyle \lim_{n \mathop \to \infty} x_n = 0_R}$. Let $\norm {\, \cdot \,}: \CC \, \big / \NN \to \R_{\ge 0}$ be the [[Quotient Ring of Cauchy Sequences is Normed Division Ring|norm on the quotient ring $\CC \, \big / \NN$]] defined by: :$\displaystyle \forall \sequence {x_n} + \NN: \norm {\sequence {x_n} + \NN } = \lim_{n \mathop \to \infty} \norm{x_n}$ Let $d'$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\struct {\CC \, \big / \NN, \norm {\, \cdot \,} }$. By [[Quotient Ring of Cauchy Sequences is Normed Division Ring]], $\struct {\CC \, \big / \NN, \norm {\, \cdot \,} }$ is a [[Definition:Normed Division Ring|normed division ring]]. By [[Quotient of Cauchy Sequences is Metric Completion]], $\struct {\CC \, \big / \NN, d' }$ is the [[Definition:Completion (Metric Space)|metric completion]] of $\struct {R, d}$. Let $\phi: R \to \CC \, \big / \NN$ be the [[Definition:Mapping|mapping]] from $R$ to the [[Definition:Quotient Ring|quotient ring]] $\CC \,\big / \NN$ defined by: :$\quad \quad \quad \forall a \in R: \map \phi a = \tuple {a, a, a, \ldots} + \NN$ where $\tuple {a, a, a, \ldots} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\tuple {a, a, a, \ldots} $. By [[Quotient of Cauchy Sequences is Metric Completion]], $\map \phi R$ is a [[Definition:Everywhere Dense|dense subset]] of $\struct {\CC \, \big / \NN, d' }$. By [[Embedding Division Ring into Quotient Ring of Cauchy Sequences]], $\phi$ is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]]. By the definition of a [[Definition:Completion (Normed Division Ring)|normed division ring completion]], $\struct {\CC \, \big / \NN, \norm {\, \cdot \,} }$ is a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] of $\struct {R, \norm {\, \cdot \,} }$. {{qed}}	1
{{proof wanted}} [[Category:Examples of Isomorphisms of Categories]] [[Category:Category of Modules]] [[Category:Category of Abelian Groups]] tvzmplj60h6ilgkmjxv76mf8rkg4nml	1
The [[Definition:Euclidean Norm|Euclidean norm]] is a [[Definition:Norm on Vector Space|norm]] on the [[Definition:Euclidean Space|Euclidean space]] $\R^n$.	1
=== Sufficient Condition === Suppose that $\dim \left({V}\right) = 1$. That $\rho$ is [[Definition:Irreducible Linear Representation|irreducible]] is shown on [[Representation of Degree One is Irreducible]]. {{qed|lemma}} === Necessary Condition === Suppose that $\rho$ is an [[Definition:Irreducible Linear Representation|irreducible linear representation]]. Let $g \in G$ be arbitrary. Now, for all $h \in G$, have: {{begin-eqn}} {{eqn|l = \rho \left({g}\right) \rho \left({h}\right) |r = \rho \left({g h}\right) |c = $\rho$ is a [[Definition:Group Homomorphism|group homomorphism]] }} {{eqn|r = \rho \left({h g}\right) |c = $G$ is an [[Definition:Abelian Group|abelian group]] }} {{eqn|r = \rho \left({h}\right) \rho \left({g}\right) |c = $\rho$ is a [[Definition:Group Homomorphism|group homomorphism]] }} {{end-eqn}} Now, combining [[Commutative Linear Transformation is G-Module Homomorphism]] and [[Schur's Lemma (Representation Theory)/Corollary|Corollary to Schur's Lemma (Representation Theory)]] yields that: :$\exists \lambda_g \in k: \rho \left({g}\right) = \lambda_g \operatorname{Id}_V$ That is, there is a $\lambda_g \in k$ such that $\rho \left({g}\right)$ is the [[Definition:Linear Transformation|linear mapping]] of multiplying by $\lambda_g$. Hence, for all $v \in V$, $\rho \left({g}\right) \left({v}\right) = \lambda_g v$. It follows that any [[Definition:Vector Subspace|vector subspace]] of $V$ of [[Definition:Dimension (Linear Algebra)|dimension]] $1$ is [[Definition:Invariant Subspace|invariant]]. So, had $V$ any [[Definition:Proper Vector Subspace|proper vector subspace]] of [[Definition:Dimension (Linear Algebra)|dimension]] $1$, $\rho$ would not be [[Definition:Irreducible Linear Representation|irreducible]]. Since $V$ is non-[[Definition:Null Module|null]], it follows from [[Trivial Vector Space iff Zero Dimension]] that $\dim \left({V}\right) > 0$. Hence necessarily $\dim \left({V}\right) = 1$. {{qed}} [[Category:Representation Theory]] fogs6da21i3iult165g0jn3efol0b3p	1
Let $G$ be a [[Definition:Vector Space|vector space]] whose [[Definition:Dimension of Vector Space|dimension]] is $n$. Let $H$ be a [[Definition:Vector Subspace|subspace]] of $G$. Then $H$ is [[Definition:Finite Dimensional Vector Space|finite dimensional]], and $\map \dim H \le \map \dim G$. If $H$ is a [[Definition:Proper Vector Subspace|proper subspace]] of $G$, then $\map \dim H < \map \dim G$.	1
This follows directly from [[Finite Direct Product of Modules is Module]] and the definition of [[Definition:Vector Space|vector space]].	1
Since $\map \beta {\mathbf x + \mathbf y} = \beta \mathbf x + \beta \mathbf y$ and $\map \beta {\lambda \mathbf x} = \map \lambda {\beta \mathbf x}$, the fact of $s_\beta$ being a linear operator is immediately apparent. We have: :$\map {\paren {s_{\beta^{-1} } \circ s_\beta} } {\mathbf x} = \map {\beta^{-1} } {\beta \mathbf x} = \mathbf x = \map \beta {\beta^{-1} \mathbf x} = \map {\paren {s_\beta \circ s_{\beta^{-1} } } } {\mathbf x}$ which proves the second bit.	1
This is a direct application of [[Linearity of Expectation Function]]. {{qed}}	1
Let $\mathbf A = \begin {bmatrix} a_{1 1} & \cdots & a_{1 r} & \cdots & a_{1 s} & \cdots & a_{1 n} \\ a_{2 1} & \cdots & a_{2 r} & \cdots & a_{2 s} & \cdots & a_{2 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n r} & \cdots & a_{n s} & \cdots & a_{n n} \\ \end {bmatrix}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf A}$ denote the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf B = \begin{bmatrix} a_{1 1} & \cdots & a_{1 r} + \lambda a_{1 s} & \cdots & a_{1 s} & \cdots & a_{1 n} \\ a_{2 1} & \cdots & a_{2 r} + \lambda a_{2 s} & \cdots & a_{2 s} & \cdots & a_{2 n} \\ \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\ a_{n 1} & \cdots & a_{n r} + \lambda a_{n s} & \cdots & a_{n s} & \cdots & a_{n n} \\ \end{bmatrix}$. Then $\map \det {\mathbf B} = \map \det {\mathbf A}$. That is, the value of a [[Definition:Determinant of Matrix|determinant]] remains unchanged if a [[Definition:Constant|constant]] multiple of any [[Definition:Column of Matrix|column]] is added to any other [[Definition:Column of Matrix|column]].	1
By assumption: :$\forall y \in R:\norm y_1 \ge 1 \iff \norm y_2 \ge 1$ By [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 1|Lemma 1]]: :$\forall y \in R:\norm y_1 \le 1 \iff \norm y_2 \le 1$ Hence $\forall y \in R$: {{begin-eqn}} {{eqn | l = \norm y_1 = 1 | o = \leadstoandfrom | r = \norm y_1 \le 1, \norm y_1 \ge 1 }} {{eqn | o = \leadstoandfrom | r = \norm y_2 \le 1, \norm y_2 \ge 1 | c = [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 1|Lemma 1]] and by assumption }} {{eqn | o = \leadstoandfrom | r = \norm y_2 = 1 }} {{end-eqn}} {{qed}} [[Category:Equivalence of Definitions of Equivalent Division Ring Norms]] i3p50dq69v4rnr033jf1x68v04xmtjz	1
Since $l \ne 0$, by [[Definition:Norm on Division Ring|norm axiom (N1)]]: $\norm l > 0$. Let us choose $N$ such that: :$\forall n > N: \norm {x_n - l} < \dfrac {\norm l} 2$ Then: {{begin-eqn}} {{eqn | l = \norm {x_n - l} | o = < | r = \frac {\norm l} 2 | c = }} {{eqn | ll= \leadsto | l = \norm l - \norm {x_n} | o = \le | r = \norm {x_n - l} | c = [[Reverse Triangle Inequality]] }} {{eqn | o = < | r = \frac {\norm l} 2 | c = }} {{eqn | ll= \leadsto | l = \norm {x_n} | o = > | r = \norm l - \frac {\norm l} 2 | c = }} {{eqn | r = \frac {\norm l} 2 | c = }} {{end-eqn}} {{qed}} [[Category:Sequences]] [[Category:Limits of Sequences]] [[Category:Normed Division Rings]] t6eaiqi7jbcka4w01d7fpn9fkegn4oz	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf B$ be the [[Definition:Square Matrix|matrix]] resulting from one [[Definition:Column of Matrix|column]] of $\mathbf A$ having been multiplied by a [[Definition:Constant|constant]] $c$. Then: :$\map \det {\mathbf B} = c \map \det {\mathbf A}$ That is, multiplying one [[Definition:Column of Matrix|column]] of a [[Definition:Square Matrix|square matrix]] by a [[Definition:Constant|constant]] multiplies its [[Definition:Determinant of Matrix|determinant]] by that [[Definition:Constant|constant]].	1
Let $\mathbb F$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb F$. Let $S \subset V$ be a [[Definition:Subset|subset]] of $V$. Then $S$ is an '''orthonormal basis''' {{iff}}: :$(1): \quad S$ is a [[Definition:Basis (Linear Algebra)|basis]]. :$(2): \quad S$ is [[Definition:Orthonormal Subset|orthonormal]]. [[Category:Definitions/Linear Algebra]] rn1yswtz371sj368r4xkcfz5q37cxc5	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $a_1, a_2, \dotsc, a_n$ be non-[[Definition:Ring Zero|zero]] [[Definition:Element|elements]] of $D$. Then $a_1, a_2, \dotsc, a_n$ all have a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]].	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $J \subseteq R$ be a [[Definition:Left Ideal|left ideal]] of $R$. Let $\circ : R \times J \to J$ be the [[Definition:Restriction of Mapping|restriction]] of $\times$ to $R \times J$. Then $\struct {J, +, \circ}$ is a [[Definition:Left Module|left module]] over $\struct {R, +, \times}$.	1
Let $T = \struct {\R, \tau}$ be the [[Definition:Compact Complement Topology|compact complement topology]] on $\R$. Then $T$ is a [[Definition:Separable Space|separable space]].	1
From [[Unit Matrix is Orthogonal]], the [[Definition:Unit Matrix|unit matrix $\mathbf I_n$]] is [[Definition:Orthogonal Matrix|orthogonal]]. Let $\mathbf A, \mathbf B \in \operatorname O \left({n, k}\right)$. Then, by definition, $\mathbf A$ and $\mathbf B$ are [[Definition:Orthogonal Matrix|orthogonal]]. Then by [[Inverse of Orthogonal Matrix is Orthogonal]]: :$\mathbf B^{-1}$ is a [[Definition:Orthogonal Matrix|orthogonal matrix]]. By [[Product of Orthogonal Matrices is Orthogonal Matrix]]: :$\mathbf A \mathbf B^{-1}$ is a [[Definition:Orthogonal Matrix|orthogonal matrix]]. Thus by definition of [[Definition:Orthogonal Group|orthogonal group]]: :$\mathbf A \mathbf B^{-1} \in \operatorname O \left({n, k}\right)$ Hence the result by [[One-Step Subgroup Test]]. {{qed}} [[Category:Orthogonal Groups]] [[Category:General Linear Group]] m22ama3xu94uxysoaj0ku2jn1iz1ck8	1
From [[Rational Numbers are Countably Infinite]], $\Q$ is itself [[Definition:Countable Set|countable]]. The result follows by [[Countable Space is Separable]]. {{qed}}	1
Let $\mathbf u$, $\mathbf v$ be [[Definition:Vector (Linear Algebra)|vectors in $\R^n$]]. Then $\mathbf u$ and $\mathbf v$ are said to be '''orthogonal''' {{iff}} their [[Definition:Dot Product|dot product]] is zero: :$\mathbf u \cdot \mathbf v = 0$ As [[Dot Product is Inner Product]], this is a special case of the [[Definition:Orthogonal (Linear Algebra)|definition of orthogonal vectors]].	1
[http://www.math.lsa.umich.edu/~ablass/bases-AC.pdf Blass, 1984] {{proof wanted}}	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$. Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then: :$\forall \mathbf v \in \mathbf V: 0_F \circ \mathbf v = \bszero$	1
By [[Complex Argument of Additive Inverse]], $\theta_2 + \pi$ is an [[Definition:Argument of Complex Number|argument]] of $-z_2$. We have: {{begin-eqn}} {{eqn | l = \cmod {z_1 - z_2}^2 | r = \cmod {z_1}^2 + \cmod {-z_2}^2 + 2 \cmod {z_1} \cmod {-z_2} \, \map \cos {\theta_1 - \theta_2 - \pi} | c = [[Complex Modulus of Sum of Complex Numbers]] }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 - 2 \cmod {z_1} \cmod {z_2} \, \map \cos {\theta_1 - \theta_2} | c = [[Complex Modulus of Additive Inverse]] }} {{end-eqn}} {{qed}}	1
'''Preliminaries''': [[Vandermonde Matrix Identity for Cauchy Matrix]] supplies matrix equation :$\displaystyle (1)\quad - C = PV_x^{-1} V_y Q^{-1}$ :Definitions of symbols: ::$\displaystyle V_x=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ x_1 & x_2 & \cdots & x_n \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1} \\ \end{smallmatrix} },\quad V_y=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ y_1 & y_2 & \cdots & y_n \\ \vdots & \vdots & \ddots & \vdots \\ y_1^{n-1} & y_2^{n-1} & \cdots & y_n^{n-1} \\ \end{smallmatrix} }$ [[Definition:Vandermonde Matrix|Vandermonde matrices]] ::$\displaystyle P= \paren {\begin{smallmatrix} p_1(x_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p_n(x_n) \\ \end{smallmatrix} }, \quad Q= \paren {\begin{smallmatrix} p(y_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p(y_n) \\ \end{smallmatrix} }$ [[Definition:Diagonal Matrix|Diagonal matrices]] ::$\displaystyle p(x) = \prod_{i \mathop = 1}^n \paren {x - x_i}, \quad \displaystyle p_k(x) = \prod_{i \mathop = 1,i \mathop \ne k}^n \, \paren {x - x_i}, \quad 1 \mathop \le k \mathop \le n$ [[Definition:Polynomial/Complex Numbers|Polynomials]] '''Compute the inverse''' $C^{-1}$ for: {{begin-eqn}} {{eqn | l = C | r = \begin{bmatrix} \dfrac 1 {x_1 - y_1} & \dfrac 1 {x_1 - y_2} & \cdots & \dfrac 1 {x_1 - y_n} \\ \dfrac 1 {x_2 - y_1} & \dfrac 1 {x_2 - y_2} & \cdots & \dfrac 1 {x_2 - y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n - y_1} & \dfrac 1 {x_n - y_2} & \cdots & \dfrac 1 {x_n - y_n} \\ \end{bmatrix} | c = Assume $\set {x_1,\ldots,x_n,y_1,\ldots,y_n}$ has distinct elements. }} {{end-eqn}} Replacement $y_k \to -y_k$ then gives the inverse $C_n^{-1}$ in the theorem. [[Inverse of Matrix Product]] applied to equation (1) gives: :$ C^{-1} = -Q V_y^{-1} V_x P^{-1}$ Let ${\vec K}_1,\ldots,{\vec K}_n$ denote the columns of the $n\times n$ identity matrix. Then $n\times n$ matrix $B = C^{-1}$ has entries $b_{ij} = {\vec K}_i^T C^{-1} {\vec K}_j$. Hold fixed until the end of the proof the row and column index symbols $i$ and $j$. Define column vectors $\vec A$, $\vec B$ so that $b_{ij} = {\vec A}^T \vec B$: :$\displaystyle \vec A = \paren { {\vec K}_i^T \, Q \, V_y^{-1} }^T, \quad \vec B = -V_x \, P^{-1} \, {\vec K}_j $ Define $u = x_j$ and simplify: {{begin-eqn}} {{eqn | l = \vec A | r = {\map p {y_i} } \, \paren { V_y^{-1} }^T \, {\vec K}_i | c = [[Transpose of Matrix Product]] }} {{eqn | r = \dfrac{ \map p {y_i} }{\det \paren {V_y} } \paren { \adj {V_y} }^T {\vec K}_i | c = [[Matrix Product with Adjugate Matrix]] }} {{eqn | r = \dfrac{ \map p {y_i} }{\det \paren {V_y} } \begin{bmatrix} {\mathbf {Cofactor} } \paren {V_y,1,i } \\ \vdots \\ {\mathbf {Cofactor} } \paren {V_y,n,i } \\ \end{bmatrix} | c = ${\mathbf {Cofactor} } \paren {M,r,s}$ denotes [[Definition:Cofactor|cofactor]] $M_{rs}$ }} {{eqn | l = \vec B | r = -\dfrac{1}{\map {p_j} {x_j} } \, V_x \, {\vec K}_j }} {{eqn | r = -\dfrac{1}{\map {p_j} {x_j} } \, \begin{bmatrix} 1 \\ u \\ \vdots \\ u^{n-1} \\ \end{bmatrix} | c = Symbol $u = x_j$. }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = A^T\, B | r = \dfrac { -\map p {y_i} } {\map {p_j} {x_j} \det \paren {V_y} } \sum_{k \mathop = 1}^n {\mathbf {Cofactor} } \paren {V_y,k,i } \, u^{k-1} | c = A [[Expansion Theorem for Determinants|Cofactor expansion]] along column $i$. }} {{eqn | r = \dfrac { -\map p {y_i} } {\map {p_j} {x_j} \det \paren {V_y} } \det \paren { \begin{matrix} 1 & \cdots & 1 & 1 & 1 & \cdots & 1 \\ y_1 & \cdots & y_{i-1} & u & y_{i+1} & \cdots & y_n \\ \vdots & \cdots & \vdots &\vdots & \vdots & \cdots & \vdots \\ y_1^{n-1} & \cdots & y_{i-1}^{n-1} & u^{n-1} & y_{i+1}^{n-1} & \cdots & y_n^{n-1} \\ \end{matrix} } }} {{eqn | r = \dfrac { -\map p {y_i} } {\map {p_j} {x_j} } \,\, \dfrac { \left. \paren { \det \paren {V_y} } \right \vert_{y_i \mathop = u} } { \det \paren {V_y} } }} {{end-eqn}} Simplify the fraction on the right: {{begin-eqn}} {{eqn | l = (2)\quad \displaystyle \dfrac { \displaystyle \left. \paren { \det \paren {V_y} } \right\vert_{y_i \mathop = u} } { \displaystyle \det \paren {V_y} } | r = \displaystyle \dfrac {\displaystyle \left. \paren { \prod_{1 \mathop \le m \mathop \lt k \mathop \le n} \paren { y_k - y_m } } \right\vert_{ y_i \mathop = u } } {\displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n} \paren { y_k - y_m } } | c = [[Vandermonde Determinant]] }} {{end-eqn}} Define sets $D,A,B,C$ with disjoint decomposition $D = A \cup B \cup C$: {{begin-eqn}} {{eqn | l = D | r = \set { \paren { m,k } : 1 \le m \lt k \le n } }} {{eqn | l = A | r = \set { \paren { m,k } \in D : m \neq i \mbox{ and } k \neq i } }} {{eqn | l = B | r = \set { \paren { m,k } \in D : m = i } }} {{eqn | l = C | r = \set { \paren { m,k } \in D : k = i } }} {{end-eqn}} Use $\prod_{D} = \prod_{A} \prod_{B} \prod_{C}$ to convert the numerator and denominator in the right side of (2). Then: {{begin-eqn}} {{eqn | l = \displaystyle \dfrac { \displaystyle \left. \paren { \det \paren {V_y} } \right\vert_{y_i \mathop = u} } { \displaystyle \det \paren {V_y} } | r = \displaystyle \dfrac {\displaystyle \prod_{k \mathop = 1,\, k \mathop \neq i}^n \paren { u - y_k } } {\displaystyle \prod_{k \mathop = 1,\, k \mathop \neq i}^n \paren { y_i - y_k } } | c = Common factors canceled in (2). }} {{end-eqn}} Replacement of ${ \map p {y_i} }$ and ${ \map {p_j} {x_j} }$ by products gives: {{begin-eqn}} {{eqn | l = (3)\quad \displaystyle b_{ij} | r = \paren {-1} \, \dfrac { \displaystyle \prod_{k \mathop = 1}^n \paren {y_i - x_k } } { \displaystyle \prod_{k \mathop = 1,\, k \mathop \neq j}^n \paren { x_j - x_k } } \,\, \prod_{k \mathop = 1,\, k \mathop \neq i}^n \paren { \dfrac { x_j - y_k }{ y_i - y_k } } | c = Replaced $u = x_j$. }} {{end-eqn}} After replacement $y_k \to -y_k$ and canceling a common factor, ''Knuth's original identity'' (1997) becomes: {{begin-eqn}} {{eqn | l = \displaystyle (4)\quad b_{ij} | r = \displaystyle \dfrac { \displaystyle \prod_{k \mathop = 1}^n \paren { x_k - y_i } } { \displaystyle \prod_{k \mathop = 1,\, k \mathop \neq j}^n \paren { x_j - x_k } } \dfrac { \displaystyle \prod_{k \mathop = 1,\, k \mathop \neq i}^n \paren { x_j - y_k } } { \displaystyle \prod_{k \mathop = 1,\, k \mathop \neq i}^n \paren { - y_i + y_k } } }} {{end-eqn}} In (3), factor $(-1)^{n+1}$ from the numerator and $(-1)^{n-1}$ from the denominator. Then $(-1)^{n+1} = (-1)^{n-1}$ verifies that (3) matches (4).{{qed}}	1
By definition: :$\mathbf I_n := \sqbrk a_n: a_{i j} = \delta_{i j}$ That is: each of the elements on the [[Definition:Main Diagonal|main diagonal]] is equal to $1$. There are $n$ such elements. Hence the result. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H}\right)$ be an [[Definition:Idempotent Operator|idempotent operator]]. Then the following are equivalent: :$(1): \qquad A$ is a [[Definition:Projection (Hilbert Spaces)|projection]] :$(2): \qquad A$ is the [[Definition:Orthogonal Projection|orthogonal projection]] onto $\operatorname{ran} A$ :$(3): \qquad \left\Vert{A}\right\Vert = 1$, where $\left\Vert{\cdot}\right\Vert$ is the [[Definition:Norm on Bounded Linear Transformation|norm on bounded linear operators]]. :$(4): \qquad A$ is [[Definition:Self-Adjoint Operator|self-adjoint]] :$(5): \qquad A$ is [[Definition:Normal Operator|normal]] :$(6): \qquad \forall h \in H: \left\langle{Ah, h}\right\rangle_H \ge 0$	1
From [[Left Module over Commutative Ring induces Right Module]], $\struct{G, +_G, \circ’}$ is a [[Definition:Right Module|right module]]. Let $\lambda, \mu \in R$ and $x \in G$. Then: {{begin-eqn}} {{eqn | l = \lambda \circ \paren{x \circ’ \mu} | r = \lambda \circ \paren{\mu \circ x} | c = Definition of $\circ’$ }} {{eqn | r = \paren {\lambda \circ \mu} \circ x | c = [[Definition:Left Module Axioms|Left module axiom $(M \,3)$ : Associativity of Scalar Multiplication]] }} {{eqn | r = \paren {\mu \circ \lambda} \circ x | c = [[Definition:Ring Product|Ring product]] $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | r = \mu \circ \paren{\lambda \circ x} | c = [[Definition:Left Module Axioms|Left module axiom $(M \,3)$ : Associativity of Scalar Multiplication]] }} {{eqn | r = \paren{\lambda \circ x} \circ’ \mu | c = Definition of $\circ’$ }} {{end-eqn}} Hence $\struct{G, +_G, \circ, \circ’}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +_R, \times_R}$ by definition. {{qed}}	1
Let $S$ be a [[Definition:Simultaneous Equations|system of simultaneous equations]]. Then it is possible that $S$ may have a [[Definition:Solution Set to System of Simultaneous Equations|solution set]] which is a [[Definition:Singleton|singleton]].	1
Let $T = \left({S, \tau}\right)$ be a [[Definition:Countable Discrete Topology|countable discrete topological space]]. From [[Countable Discrete Space is Second-Countable]]: :$T$ is [[Definition:Second-Countable Space|second-countable]]. From [[Second-Countable Space is Separable]]: :$T$ is [[Definition:Separable Space|separable]]. {{qed}}	1
Let $\sup \set {\norm {n \cdot 1_R}: n \in \N_{> 0} } = C < +\infty$.	1
Let $\left({R, +_R, \times_R}\right)$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\left({A_1, +_1, \circ_1}\right)_R, \left({A_2, +_2, \circ_2}\right)_R, \ldots, \left({A_n, +_n, \circ_n}\right)_R, \left({A_{n+1}, +_{n+1}, \circ_{n+1}}\right)_R$ be [[Definition:Module|$R$-modules]]. Let $\oplus: A_1 \times A_2 \times \cdots \times A_n \to A_{n+1}$ be a [[Definition:Multiary Operator|multiary operator]] with the property that: $\forall \left({a_1, a_2, \ldots, a_n}\right) \in A_1 \times A_2 \times \cdots \times A_n$: * $a_1 \mapsto a_1 \oplus a_2 \oplus \cdots \oplus a_n$ is a [[Definition:Linear Transformation|linear transformation]] from $A_1$ to $A_{n+1}$ * $a_2 \mapsto a_1 \oplus a_2 \oplus \cdots \oplus a_n$ is a [[Definition:Linear Transformation|linear transformation]] from $A_2$ to $A_{n+1}$ * $\vdots$ * $a_n \mapsto a_1 \oplus a_2 \oplus \cdots \oplus a_n$ is a [[Definition:Linear Transformation|linear transformation]] from $A_n$ to $A_{n+1}$ Then $\oplus$ is a '''multilinear mapping'''.	1
Let $\mathbf A = \sqbrk a_{m n}, \mathbf B = \sqbrk b_{n p}, \mathbf C = \sqbrk c_{n p}$ be [[Definition:Matrix|matrices]] over a [[Definition:Ring (Abstract Algebra)|ring]] $\struct {R, +, \circ}$. Consider $\mathbf A \paren {\mathbf B + \mathbf C}$. Let $\mathbf R = \sqbrk r_{n p} = \mathbf B + \mathbf C, \mathbf S = \sqbrk s_{m p} = \mathbf A \paren {\mathbf B + \mathbf C}$. Let $\mathbf G = \sqbrk g_{m p} = \mathbf A \mathbf B, \mathbf H = \sqbrk h_{m p} = \mathbf A \mathbf C$. Then: {{begin-eqn}} {{eqn | l = s_{i j} | r = \sum_{k \mathop = 1}^n a_{i k} \circ r_{k j} | c = }} {{eqn | l = r_{k j} | r = b_{k j} + c_{k j} | c = }} {{eqn | ll= \leadsto | l = s_{i j} | r = \sum_{k \mathop = 1}^n a_{i k} \circ \paren {b_{k j} + c_{k j} } | c = }} {{eqn | r = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j} + \sum_{k \mathop = 1}^n a_{i k} \circ c_{k j} | c = }} {{eqn | r = g_{i j} + h_{i j} | c = }} {{end-eqn}} Thus: :$\mathbf A \paren {\mathbf B + \mathbf C} = \paren {\mathbf A \mathbf B} + \paren {\mathbf A \mathbf C}$ A similar construction shows that: :$\paren {\mathbf B + \mathbf C} \mathbf A = \paren {\mathbf B \mathbf A} + \paren {\mathbf C \mathbf A}$ {{Qed}}	1
Let $m$ be a [[Definition:Positive Integer|positive integer]]. Then the [[Definition:Euclidean Space|Euclidean space]] $\R^m$, along with the [[Definition:Euclidean Norm|Euclidean norm]], forms a [[Definition:Banach Space|Banach space]] over $\R$.	1
Assert that $U: M \oplus M^\perp \to H: \left({m, m^\perp}\right) \mapsto m + m^\perp$ is an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]]. According to the definition of [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]], it is sufficient to check that $U$ is [[Definition:Surjection|surjective]] and that: :$\left\langle{ U \left({m, m^\perp}\right), U \left({n, n^\perp}\right) }\right\rangle_H = \left\langle{ \left({m, m^\perp}\right), \left({n, n^\perp}\right) }\right\rangle_{M \oplus M^\perp}$ First the [[Definition:Surjection|surjectivity]]: Denote by $P$ the [[Definition:Orthogonal Projection|orthogonal projection]] on $M$. Then for any $h \in H$, have $h = Ph + \left({h - Ph}\right)$. By [[Definition:Orthogonal Projection|definition]] of $P$, $Ph \in M$. Furthermore, by [[Properties of Orthogonal Projection]], $h - Ph \perp M$; that is, $h - Ph \in M^\perp$. It follows that $h = U \left({Ph, h - Ph}\right)$, showing that $U$ is a [[Definition:Surjection|surjection]]. It remains to check that $U$ preserves the [[Definition:Inner Product|inner product]]: {{begin-eqn}} {{eqn|l = \left\langle{ U \left({m, m^\perp}\right), U \left({n, n^\perp}\right) }\right\rangle_H |r = \left\langle{ m + m^\perp, n + n^\perp}\right\rangle_H }} {{eqn|r = \left\langle{m, n}\right\rangle_H + \left\langle{m^\perp, n}\right\rangle_H + \left\langle{m, n^\perp}\right\rangle_H + \left\langle{m^\perp, n^\perp}\right\rangle_H |c = [[Definition:Inner Product|Linearity]] of $\left\langle{\cdot, \cdot}\right\rangle_H$ }} {{eqn|r = \left\langle{m, n}\right\rangle_H + \left\langle{m^\perp, n^\perp}\right\rangle_H |c = Defining property of [[Definition:Orthocomplement|orthocomplement]] }} {{eqn|r = \left\langle{ \left({m, m^\perp}\right), \left({n, n^\perp}\right) }\right\rangle_{M \oplus M^\perp} |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle_{M \oplus M^\perp}$ }} {{end-eqn}} Hence $U$ is an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]], and so $M \oplus M^\perp$ is [[Definition:Isomorphism (Hilbert Spaces)|isomorphic]] to $H$. {{qed}} [[Category:Hilbert Spaces]] 5sfhxq1m9kc6yz967uacbnschftll3f	1
=== [[Definition:Dimension of Module|Module]] === {{:Definition:Dimension of Module}} === [[Definition:Dimension of Vector Space|Vector Space]] === {{:Definition:Dimension of Vector Space}} {{wtd|Infinite dimensional case}}	1
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\struct {G, +_G, \circ}_R$ be a [[Definition:Unitary Module|unitary $R$-module]]. Then the [[Definition:Singleton|singleton set]] $\set e$ consisting of the [[Definition:Zero Vector|zero vector]] is [[Definition:Linearly Dependent Set|linearly dependent]].	1
If $n$ is [[Definition:Prime Number|prime]], the result is immediate. Let $n$ be [[Definition:Composite Number|composite]]. Then by [[Composite Number has Two Divisors Less Than It]]: :$\exists r, s \in \Z: n = r s, 1 < r < n, 1 < s < n$ This being the case, the set $S_1 = \set {d: d \divides n, 1 < d < n}$ is [[Definition:Non-Empty Set|nonempty]], and [[Definition:Bounded Below Set|bounded below]] by $1$. By [[Set of Integers Bounded Below by Integer has Smallest Element]], $S_1$ has a [[Definition:Smallest Element|smallest element]], which we will call $p_1$. {{AimForCont}} $p_1$ is [[Definition:Composite Number|composite]]. By [[Composite Number has Two Divisors Less Than It]], there exist $a, b$ such that $a, b \divides p_1$ and $1 < a < p_1, 1 < b < p_1$. But by [[Divisor Relation on Positive Integers is Partial Ordering]], it follows that $a, b \divides n$ and hence $a, b \in S$. This [[Definition:Contradiction|contradicts]] the assertion that $p_1$ is the [[Definition:Smallest Element|smallest element]] of $S_1$. Thus, $p_1$ is necessarily [[Definition:Prime Number|prime]]. We may now write $n = p_1 n_1$, where $n > n_1 > 1$. If $n_1$ is [[Definition:Prime Number|prime]], the proof is complete. Otherwise, the set $S_2 = \set {d: d \divides n_1, 1 < d < n_1}$ is [[Definition:Non-Empty Set|nonempty]], and [[Definition:Bounded Below Set|bounded below]] by $1$. By the above argument, the [[Definition:Smallest Element|smallest element]] $p_2$ of $S_2$ is [[Definition:Prime Number|prime]]. Thus we may write $n_1 = p_2 n_2$, where $1 < n_2 < n_1$. This gives us $n = p_1 p_2 n_2$. If $n_2$ is [[Definition:Prime Number|prime]], we are done. Otherwise, we continue this process. Since $n > n_1 > n_2 > \cdots > 1$ is a [[Definition:Strictly Decreasing Sequence|(strictly) decreasing sequence]] of [[Definition:Positive Integer|positive integers]], there must be a [[Definition:Finite Set|finite number]] of $n_i$'s. That is, we will arrive at some [[Definition:Prime Number|prime number]] $n_{k - 1}$, which we will call $p_k$. This results in the [[Definition:Prime Decomposition|prime decomposition]] $n = p_1 p_2 \cdots p_k$. {{qed}}	1
By definition, every [[Definition:Real Polynomial Function|real polynomial function]] is a [[Definition:Linear Combination|linear combination]] of $B$. Suppose: :$\displaystyle \sum_{k \mathop = 0}^m \alpha_k I^k = 0, \alpha_m \ne 0$ Then by [[Definition:Differentiation|differentiating]] $m$ times, we obtain from [[Nth Derivative of Nth Power]]: :$m! \alpha_m = 0$ whence $\alpha_m = 0$ which is a contradiction. Hence $B$ is [[Definition:Linearly Independent Set|linearly independent]] and therefore is a [[Definition:Basis of Vector Space|basis]] for $\map P \R$. {{qed}}	1
By definition of [[Definition:Dot Product|dot product]]: :$\mathbf a \cdot \mathbf b = \norm {\mathbf a} \norm {\mathbf b} \cos \theta$ where $\theta$ is the [[Definition:Angle|angle]] between $\mathbf a$ and $\mathbf b$. When $\mathbf a$ and $\mathbf b$ be [[Definition:Perpendicular|perpendicular]], by definition $\theta = 90 \degrees$. The result follows by [[Cosine of Right Angle]], which gives that $\cos 90 \degrees = 0$. {{qed}}	1
The [[Cauchy-Binet Formula]] gives: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m}}\right) \det \left({\mathbf B_{j_1 j_2 \ldots j_m}}\right)$ where: :$\mathbf A$ is an [[Definition:Matrix|$m \times n$ matrix]] :$\mathbf B$ is an [[Definition:Matrix|$n \times m$ matrix]]. :For $1 \le j_1, j_2, \ldots, j_m \le n$: ::$\mathbf A_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. ::$\mathbf B_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. From the definition of [[Definition:Transpose of Matrix|transpose]] $\mathbf A^\intercal$ is an [[Definition:Matrix|$n \times m$ matrix]]. Hence the [[Cauchy-Binet Formula]] can be applied directly: :$\displaystyle \det \left({\mathbf A \mathbf A^\intercal}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m} }\right) \det \left({\mathbf A^\intercal_{j_1 j_2 \ldots j_m} }\right)$ Note that by construction: :$\mathbf A_{j_1 j_2 \ldots j_m}$ is a [[Definition:Square Matrix|square matrix]] Also, by definition of [[Definition:Transpose of Matrix|transpose]]: :$\mathbf A^\intercal_{j_1 j_2 \ldots j_m} = \left({\mathbf A_{j_1 j_2 \ldots j_m} }\right)^\intercal$ The result follows from [[Determinant of Transpose]]: :$\det \left({\mathbf A}\right) = \det \left({\mathbf A^\intercal}\right)$ {{qed}}	1
By the [[Reverse Triangle Inequality]], we have: :$\cmod {\norm {x_n} - \norm x} \le \norm {x_n - x}$ Hence by the [[Squeeze Theorem]] and [[Convergent Sequence Minus Limit]], $\norm {x_n} \to \norm x$ as $n \to \infty$. {{Qed}}	1
Let $f: \R \to \R$ be an [[Definition:Additive Function (Conventional)|additive function]]. Then: :$f \paren 0 = 0$	1
Let $R$ be a [[Definition:Noetherian Ring|noetherian]] [[Definition:Integral Domain|integral domain]]. Then $R$ is a [[Definition:Factorization Domain|factorization domain]].	1
{{tidy|}} Let $C$ be the [[Definition:Cofactor Matrix|cofactor matrix]] of $A$. Then by [[Definition:Adjugate Matrix|definition of adjugate matrix]], $C^T$ is the adjugate matrix of $A$. Therefore, {{begin-eqn}} {{eqn | l = A \cdot C^T | r = \det{A} \cdot I_n | c = [[Matrix Product with Adjugate Matrix]]. }} {{end-eqn}} Because $A$ is invertible, $A^{-1}$ exists. Because $A$ is invertible, $1/\det{A}$ exists by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]]. Therefore: :$A^{-1} = \frac{1}{\det{A}}\cdot C^T.$ Since $A \mathbf x = \mathbf b$ by hypothesis, :$\mathbf x = A^{-1} \mathbf b.$ Therefore, :$\mathbf x = \left(\frac{1}{\det{A}}\cdot C^T\right) \mathbf b.$ For each $r,s \in \{1, \dots, n\}$, let $A_{rs}$ denote the [[Definition:Element of Matrix|element]] of $C$ whose index is $(r,s)$. Then by the [[Definition:Transpose of Matrix|definition of transpose]], :$C^T = \begin{bmatrix} A_{11} & A_{21} & \dots & A_{n1} \\ A_{12} & A_{22} & \dots & A_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1n} & A_{2n} & \dots & A_{nn} \end{bmatrix}.$ By the [[Definition:Cofactor Matrix|definition of cofactor matrix]], $A_{rs}$ is the [[Definition:Cofactor/Element|cofactor]] of the element of $A$ whose index is $(r,s)$. Let $i \in \{1, \dots, n\}$. Then by [[Definition:Matrix Product (Conventional)|definition of matrix product]], :$x_i = \frac{1}{\det{A}}\left(\sum_{j = 1}^n A_{ji} b_j\right).$ Recall that $A_i$ is the matrix obtained from $A$ by replacing the $i$-th column with $\mathbf{b}$. Then if :$A = \begin{bmatrix} a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ a_{2,1} & a_{2,2} & \dots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \dots & a_{n,n} \end{bmatrix}$ then :$A_i = \begin{bmatrix} a_{1,1} & a_{1,2} & \dots & a_{1, i-1} & b_1 & a_{1, i+1} & \dots & a_{1,n} \\ a_{2,1} & a_{2,2} & \dots & a_{2, i-1} & b_2 & a_{2, i+1} & \dots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2} & \dots & a_{n, i-1} & b_n & a_{n, i+1} & \dots & a_{n,n} \end{bmatrix}.$ For all $j \in \{1, \dots, n\}$ the matrix obtained by deleting the $j$-th row and the $i$-th column of $A$ is equal to the matrix obtained by deleting the $j$-th row and $i$-th column of $A_i$. Therefore, by the definition of a cofactor, the cofactor of $(A_i)_{j,i}$ is equal to $A_{ji}$. By the [[Expansion Theorem for Determinants]], we now have :$\det{A_i} = \sum_{j=1}^n A_{ji} b_j.$ Therefore, :$x_i = \frac{\det{A_i}}{\det{A}}$ as desired. {{Namedfor|Gabriel Cramer|cat = Cramer}}	1
{{WLOG}}, we will only prove $OB$ [[Definition:Angle Bisector|bisects]] $\angle AOC$. We have: {{begin-eqn}} {{eqn | l = OA | r = OC | c = {{Defof|Rhombus}} }} {{eqn | l = BA | r = BC | c = {{Defof|Rhombus}} }} {{eqn | l = OB | r = OB | c = Common Side }} {{eqn | ll = \leadsto | l = \triangle OAB | o = \cong | r = \triangle OCB | c = [[SSS]] }} {{end-eqn}} Comparing corresponding angles gives: :$\angle AOB = \angle COB$ hence $OB$ [[Definition:Angle Bisector|bisects]] $\angle AOC$. {{qed}}	1
Let $\struct {X, \norm {\, \cdot \,}}$ and $\struct {Y, \norm {\, \cdot \,}}$ be [[Definition:Normed Vector Space|normed vector spaces]]. Let $V = X \times Y$ be a [[Definition:Direct Product of Vector Spaces/Finite Case|direct product of vector spaces]] $X$ and $Y$ together with [[Definition:Operation Induced by Direct Product|induced component-wise operations]]. Let $\norm {\tuple {x, y} }$ be the [[Definition:Direct Product Norm|direct product norm]]. Then $\norm {\tuple {x, y} }$ is a [[Definition:Norm on Vector Space|norm]] on $V$.	1
Let $\epsilon > 0$ be given. Then $\dfrac \epsilon 2 > 0$. Since $\sequence {x_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to $l$, we can find $N_1$ such that: :$\forall n > N_1: \norm {x_n - l} < \dfrac \epsilon 2$ Similarly, for $\sequence {y_n}$ we can find $N_2$ such that: :$\forall n > N_2: \norm {y_n - m} < \dfrac \epsilon 2$ Now let $N = \max \set {N_1, N_2}$. Then if $n > N$, both the above inequalities will be true. Thus $\forall n > N$: {{begin-eqn}} {{eqn | l = \norm {\paren {x_n + y_n} - \paren {l + m} } | r = \norm {\paren {x_n - l} + \paren {y_n - m} } | c = }} {{eqn | o = \le | r = \norm {x_n - l} + \norm {y_n - m} | c = {{NormAxiom|3}} }} {{eqn | o = < | r = \frac \epsilon 2 + \frac \epsilon 2 = \epsilon | c = }} {{end-eqn}} Hence: :$\sequence {x_n + y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to $l + m$. {{qed}} [[Category:Combination Theorem for Sequences in Normed Division Rings]] msbv7wuxm011galapa42gjw9m7l1zhn	1
Let $\GF$ be a [[Definition:Galois Field|Galois field]] with $p$ elements. Then the [[Definition:Order of Structure|order]] of the [[Definition:General Linear Group|general linear group]] $\GL {n, \GF}$ is: :$\displaystyle \prod_{j \mathop = 1}^n \paren {p^n - p^{j - 1} }$	1
Let $m \in M$ and $n \in N$ Then {{begin-eqn}} {{eqn | l = m \otimes_R n | r = \paren {m + 0} \otimes_R n | c = {{GroupAxiom|2}} }} {{eqn | r = m \otimes_R n + 0 \otimes_R n | c = {{Defof|Tensor Equality}} }} {{eqn | r = m \otimes_R \paren {n + 0} | c = {{GroupAxiom|2}} }} {{eqn | r = m \otimes_R n + m\otimes_R 0 | c = {{Defof|Tensor Equality}} }} {{eqn | r = m \otimes_R n + 0 \otimes_R \paren {n + 0} | c = {{GroupAxiom|2}} }} {{eqn | r = m \otimes_R n + m \otimes_R 0 + 0 \otimes_R n + 0 \otimes_R 0 | c = {{Defof|Tensor Equality}} }} {{eqn | r = m \otimes_R n + 0 \otimes_R 0 | c = {{Defof|Tensor Equality}} }} {{end-eqn}} Hence $0 \otimes_R n$, $m \otimes_R 0$ and $0 \otimes_R 0$ must all be [[Definition:Identity Element|identity elements]] for $M \otimes_R N$ as a [[Definition:Left Module|left module]]. {{qed}} [[Category:Tensor Algebra]] [[Category:Homological Algebra]] 3plttq0y0vknwtgupk53xvo8d8yew9f	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $J \subseteq R$ be a [[Definition:Right Ideal|right ideal]] of $R$. Let $\circ : J \times R \to J$ be the [[Definition:Restriction of Mapping|restriction]] of $\times$ to $J \times R$. Then $\struct {J, +, \circ}$ is a [[Definition:Right Module|right module]] over $\struct {R, +, \times}$.	1
Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{n p}$ Let $\mathbf A \mathbf B = \sqbrk c_{m p}$. Then from the definition of [[Definition:Matrix Product (Conventional)|matrix product]]: :$\displaystyle \forall i \in \closedint 1 m, j \in \closedint 1 p: c_{i j} = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j}$ So, let $\paren {\mathbf A \mathbf B}^\intercal = \sqbrk r_{p m}$. The dimensions are correct, because $\mathbf A \mathbf B$ is an $m \times p$ matrix, thus making $\paren {\mathbf A \mathbf B}^\intercal$ a $p \times m$ matrix. Thus: :$\displaystyle \forall j \in \closedint 1 p, i \in \closedint 1 m: r_{j i} = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j}$ Now, let $\mathbf B^\intercal \mathbf A^\intercal = \sqbrk s_{p m}$ Again, the dimensions are correct because $\mathbf B^\intercal$ is a $p \times n$ matrix and $\mathbf A^\intercal$ is an $n \times m$ matrix. Thus: :$\displaystyle \forall j \in \closedint 1 p, i \in \closedint 1 m: s_{j i} = \sum_{k \mathop = 1}^n b_{k j} \circ a_{i k}$ As the [[Definition:Underlying Structure of Matrix|underlying structure]] of $\mathbf A$ and $\mathbf B$ is a [[Definition:Commutative Ring|commutative ring]], then $a_{i k} \circ b_{k j} = b_{k j} \circ a_{i k}$. Note the order of the indices in the term in the summation sign on the {{RHS}} of the above. They are reverse what they would normally be because we are multiplying the transposes together. Thus it can be seen that $r_{j i} = s_{j i}$ and the result follows. {{qed}}	1
Let $\struct {F, +, \circ}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Let $X$ be [[Definition:Transcendental over Field|transcendental over $F$]]. Let $F \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] in $X$ over $F$. Then $F \sqbrk X$ is a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $h \in H$. Let $K \subseteq H$ be a [[Definition:Closed Set (Topology)|closed]], [[Definition:Convex Set (Vector Space)|convex]], [[Definition:Non-Empty Set|non-empty]] subset of $H$. Then there is a unique point $k_0 \in K$ such that: :$\norm {h - k_0} = \map d {h, K}$ where $d$ denotes [[Definition:Distance between Element and Subset of Metric Space|distance to a set]]. {{refactor|Own page, and prove it}} Furthermore, if $K$ is a [[Definition:Closed Linear Subspace|linear subspace]], this point is characterised by: :$\norm {h - k_0} = \map d {h, K} \iff \paren {h - k_0} \perp K$ where $\perp$ signifies [[Definition:Orthogonal (Hilbert Space)|orthogonality]].	1
This is a special case of a [[Definition:Direct Product of Vector Spaces|direct product of vector spaces]] where each of the $G_k$ is the [[Definition:Vector Space|$K$-vector space]] $K$. {{qed}}	1
Proved in [[Linear Combination of Integrals]]. {{qed}}	1
{{ProofWanted|a coherent understanding of exactly what it means would be useful here}}	1
{{ProofWanted|Background needed}}	1
Let $\struct {G, +_G}$ be an [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Unitary Module|unitary $R$-module]]. Let $x \in G, n \in \Z$. Then:	1
Let $\sequence {x_n} $ be a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$. Then: :$\exists K \in \R_{> 0} : \forall n, m : \map d {x_n , x_m} \le K$ By the definition of the [[Definition:Metric Induced by Norm on Division Ring|metric induced by a norm]] this is equivalent to: :$\exists K \in \R_{> 0} : \forall n, m : \norm {x_n - x_m} \le K$ Then $\forall n \in \N$: {{begin-eqn}} {{eqn | l = \norm {x_n} | r = \norm {x_n - x_1 + x_1} }} {{eqn | o = \le | r = \norm {x_n - x_1} + \norm {x_1} | c = [[Definition:Norm Axioms|Norm axiom (N3)]] (Triangle Inequality) }} {{eqn | o = \le | r = K + \norm {x_1} | c = Assumption of [[Definition:Bounded Sequence in Metric Space|bounded sequence]] }} {{end-eqn}} Hence the [[Definition:Sequence|sequence]] $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]] by $K + \norm {x_1}$ in the [[Definition:Normed Division Ring|normed division ring]] $\struct {R, \norm {\,\cdot\,} }$.	1
Let $\Z \sqbrk {\sqrt 2}$ denote the [[Definition:Set|set]]: :$\Z \sqbrk {\sqrt 2} := \set {a + b \sqrt 2: a, b \in \Z}$ that is, all numbers of the form $a + b \sqrt 2$ where $a$ and $b$ are [[Definition:Integer|integers]]. Let $\struct {\Z \sqbrk {\sqrt 2}, +, \times}$ be the [[Numbers of Type Integer a plus b root 2 form Subdomain of Reals|integral domain]] where $+$ and $\times$ are conventional [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]] on [[Definition:Real Number|real numbers]]. Then numbers of the form $a + b \sqrt 2$ such that $a^2 - 2 b^2 = \pm 1$ are all [[Definition:Unit of Ring|units]] of $\struct {\Z \sqbrk {\sqrt 2}, +, \times}$.	1
By the definition of a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] then: :$(1): \quad$ there exists a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]] $\phi: R \to R'$. :$(2): \quad \struct {R', \norm {\, \cdot \,}' }$ is a [[Definition:Complete Metric Space|complete metric space]]. :$(3): \quad \phi \sqbrk R$ is a [[Definition:Everywhere Dense|dense]] [[Definition:Topological Subspace|subspace]] in $\struct {R', \norm {\, \cdot \,}' }$. By [[Normed Division Ring is Dense Subring of Completion]], $\phi \sqbrk R$ is a [[Definition:Dense|dense]] [[Definition:Normed Division Subring|normed division subring]] of $R'$ and $\phi: R \to \map \phi R$ is an [[Definition:Isometric Isomorphism|isometric isomorphism]]. By [[Isometrically Isomorphic Non-Archimedean Division Rings]], $\struct {R, \norm {\, \cdot \,} }$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]] {{iff}} $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ is. So it remains to show that $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]] {{iff}} $\struct {R', \norm {\, \cdot \,}' }$ is. === Necessary Condition === Let $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ be [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]]. Let $x', y' \in R'$. By the definition of a [[Definition:Dense|dense subset]]: :$\map \cl {\phi \sqbrk R} = R'$ By [[Closure of Subset of Metric Space by Convergent Sequence]] then: :there exists a [[Definition:Sequence|sequence]] $\sequence {x_n'} \subseteq \phi \sqbrk R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x'$, that is, $\displaystyle \lim_{n \mathop \to \infty} x_n' = x'$ :there exists a [[Definition:Sequence|sequence]] $\sequence {y_n'} \subseteq \phi \sqbrk R$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $y'$, that is, $\displaystyle \lim_{n \mathop \to \infty} y_n' = y'$ By [[Sum Rule for Sequences in Normed Division Ring]] then: :$\displaystyle \lim_{n \mathop \to \infty} x_n' + y_n' = x' + y'$ By [[Modulus of Limit/Normed Division Ring|modulus of limit]] then: :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n'}' = \norm {x'}'$ :$\displaystyle \lim_{n \mathop \to \infty} \norm {y_n'}' = \norm {y'}'$ :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n' + y_n'}' = \norm {x' + y'}'$ By [[Maximum Rule for Real Sequences]] then: :$\displaystyle \lim_{n \mathop \to \infty} \max \set {\norm {x_n'}', \norm {y_n'}' } = \max \set {\norm {x'}', \norm {y'}'}$ Since $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]] then: :$\forall n: \norm {x_n' + y_n'}' \le \max \set {\norm {x_n'}', \norm {y_n'}' }$ By [[Inequality Rule for Real Sequences]] then: :$\displaystyle \norm {x' + y'}' = \lim_{n \mathop \to \infty} \norm {x_n' + y_n'}' \le \lim_{n \mathop \to \infty} \max \set {\norm {x_n'}', \norm {y_n'}' } = \max \set {\norm {x'}', \norm {y'}'}$ The result follows. {{qed|lemma}} === Sufficient Condition === Let $\struct {R', \norm {\, \cdot \,}' }$ be [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]]. By [[Subring of Non-Archimedean Division Ring]] then $\struct {\phi \sqbrk R, \norm {\, \cdot \,}' }$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]]. {{qed}}	1
Let: {{begin-eqn}} {{eqn | l = \prod_{k \mathop = 1}^n \paren {x - x_k} | r = a_nx^n + \sum_{m \mathop = 0}^{n-1} a_m x^m | c = Polynomial expansion in powers of $x$ }} {{eqn | r = x^n + \sum_{m \mathop = 0}^{n - 1} \paren {-1}^{n - m} \map {e_{n-m} } {x_1, \ldots, x_n} \, x^m | c = [[Viete's Formulas]] and {{Defof|Elementary Symmetric Function}} }} {{eqn | l = W_n | r = \begin{bmatrix} 1 & x_1 & \cdots & x_1^{n-1} \\ 1 & x_2 & \cdots & x_2^{n-1} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_1^{n-1} & \cdots & x_n^{n-1} \\ \end{bmatrix} | c = {{Defof|Vandermonde Matrix}} of [[Definition:Order of Square Matrix|Order $n$]] }} {{end-eqn}} Let $W_n$ have a [[Definition:Inverse Matrix|matrix inverse]] $W_n^{-1} = \begin {bmatrix} d_{ij} \end {bmatrix}$. Let $a_n = \map {e_0} {x_1, \ldots, x_n} = 1$. Then: {{begin-eqn}} {{eqn | n = 1 | l = d_{ij} | r = \dfrac {\displaystyle \sum_{k \mathop = 0}^{n - i} a_{i + k} \, x_j^k} {\displaystyle \prod_{m \mathop = 1, m \mathop \ne j }^n \paren {x_j - x_m} } | c = for $i, j = 1, \ldots, n$ }} {{eqn | r = \dfrac {\displaystyle \sum_{k \mathop = 0}^{n - i} \paren {-1}^{n - i - k} \map {e_{n - i - k} } {x_1, \ldots, x_n} \, x_j^k} {\displaystyle \prod_{m \mathop = 1, m \mathop \ne j }^n \paren {x_j - x_m} } | c = for $i, j = 1, \ldots, n$ }} {{end-eqn}}	1
{{proof wanted|Subspace of $B(H,K)$; completeness is nontrivial.}}	1
Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]]: :$e := \kappa_k \to \kappa_k + \lambda r_l$ Then $\kappa'_k$ is such that: :$\forall a'_{i k} \in \kappa'_k: a'_{i k} = a_{i k} + \lambda a_{i l}$ Now let $\map {e'} {\mathbf A'}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A'$ to $\mathbf A''$: :$e' := \kappa'_k \to \kappa'_k - \lambda \kappa'_l$ Applying $e'$ to $\mathbf A'$ we get: {{begin-eqn}} {{eqn | lo= \forall a''_{i k} \in \kappa''_k: | l = a''_{i k} | r = a'_{i k} - \lambda a'_{i l} | c = }} {{eqn | r = \paren {a_{i k} + \lambda a_{i l} } - \lambda a'_{i l} | c = }} {{eqn | r = \paren {a_{i k} + \lambda a_{i l} } - \lambda a_{i l} | c = as $\lambda a'_{i l} = \lambda a_{i l}$: [[Definition:Column of Matrix|column]] $l$ was not changed by $e$ }} {{eqn | r = a_{i k} | c = }} {{eqn | ll= \leadsto | lo= \forall a''_{i k} \in \kappa''_k: | l = a''_{i k} | r = a_{i k} | c = }} {{eqn | ll= \leadsto | l = \kappa''_{i k} | r = \kappa_{i k} | c = }} {{eqn | ll= \leadsto | l = \mathbf A'' | r = \mathbf A | c = }} {{end-eqn}} It is noted that for $e'$ to be an [[Definition:Elementary Column Operation|elementary column operation]], the only possibility is for it to be as defined.	1
Let $f: \R \to \R$ be a [[Definition:Monotone Real Function|monotone real function]] which is [[Definition:Additive Function (Conventional)|additive]], that is: :$\forall x, y \in \R: \map f {x + y} = \map f x + \map f y$ Then: :$\exists a \in \R: \forall x \in \R: \map f x = a x$	1
:The [[Definition:Sphere in Normed Division Ring|$r$-sphere of $x$]], $\map {S_r} x$, is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\norm {\,\cdot\,}$.	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\MM_R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $R$.]] Then $\struct {\map {\MM_R} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]].	1
By the definition of [[Definition:Null Space|null space]], $\mathbf 0$ is a [[Definition:Solution to System of Simultaneous Equations|solution]] {{iff}} the [[Definition:Null Space|null space]] contains the [[Definition:Zero Vector|zero vector]]. The result follows from [[Null Space Contains Zero Vector]]. {{qed}} [[Category:Linear Algebra]] [[Category:Null Spaces]] mp2ftq9clu23hsbzit9h6vwj15zfend	1
The [[Definition:Interior of Jordan Curve|interior]] of a [[Definition:Triangle (Geometry)|triangle]] embedded in $\R^2$ is a [[Definition:Convex Set (Vector Space)|convex set]].	1
Let the [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator on $A$ be $*$. Suppose $A'$ is a [[Definition:Real Algebra|real algebra]] whose [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator is $*'$. Then by definition: : $\forall a \in A': {a^*}' = a$ Let $a = \left({x, y}\right) \in A'$. Then by definition of the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]]: : $x, y \in A$ By definition of the [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator: : ${\left({x, y}\right)^*}' = \left({x^*, -y}\right)$ So for ${a^*}' = a$ it must be that $x = x^*$ and $y = -y$ from definition of [[Equality of Ordered Pairs]]. That is, $y = 0$. But $A'$ is a [[Definition:Star-Algebra|$*$-algebra]], which is by definition a [[Definition:Unitary Division Algebra|unitary division algebra]]. So $\exists y \in A: y = 1 \ne 0$. From that contradiction, it follows that $A'$ can not be a [[Definition:Real Algebra|real algebra]]. {{qed}}	1
Follows directly from [[Scalar Product with Product]]. {{qed}}	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number|complex numbers]]. Let $z_1 \times z_2$ denote the [[Definition:Complex Cross Product|(complex) cross product]] of $z_1$ and $z_2$. Then: :$\size {z_1 \times z_2} = \size {z_2 \times z_1}$ where $\size {\, \cdot \,}$ denotes the [[Definition:Absolute Value|absolute value function]].	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $M$ be a [[Definition:Free Module|free module]] over $A$ of [[Definition:Rank of Free Module|finite rank]] $n > 0$. Let $\phi : M \to M$ be a [[Definition:Linear Operator|linear operator]]. === Definition 1 === The '''characteristic polynomial''' of $\phi$ is the [[Definition:Characteristic Polynomial of Matrix|characteristic polynomial]] of the [[Definition:Relative Matrix of Linear Transformation|relative matrix]] of $\phi$ with respect to a [[Definition:Basis of Module|basis]] of $M$. === Definition 2 === Let $A[x]$ be the [[Definition:Polynomial Ring in One Variable|polynomial ring in one variable]] over $A$. Let $\operatorname{id}$ be the [[Definition:Identity Mapping|identity mapping]] on $M$. Let $M \otimes_A A[x]$ be the [[Definition:Extension of Scalars of Module|extension of scalars]] of $M$ to $A[x]$. The '''characteristic polynomial''' of $\phi$ is the [[Definition:Determinant of Linear Operator|determinant]] of the [[Definition:Linear Operator|linear operator]] $x\operatorname{id} - \phi$ on $M \otimes_A A[x]$.	1
:$y \ne 0_R \implies \norm{xy^{-1}} = \norm{y^{-1}x} = \dfrac {\norm{x}}{\norm{y}}$	1
Trivially, we have: :$\forall x \in \R: \map f {1 \cdot x} = 1 \map f x$ Next, suppose that: :$\map f {n x} = n \map f x$ By [[Definition:Additive Function (Conventional)|additivity]] of $f$, we have: {{begin-eqn}} {{eqn | l = \map f {\paren {n + 1} x} | r = \map f {n x + x} | c = }} {{eqn | r = \map f {n x} + \map f x = n \map f x + \map f x | c = }} {{eqn | r = \paren {n + 1} \map f x | c = }} {{end-eqn}} Hence by the [[Principle of Mathematical Induction]]: :$\forall n \in \N, x \in \R: \map f {n x} = n \map f x$ As [[Additive Function is Odd Function]] and [[Odd Function of Zero is Zero]], we conclude: :$\forall p \in \Z, x \in \R: \map f {p x} = p \map f x$ Let $p \ne 0$. By substituting $y = p x$, the above gives: :$\forall p \in \Z \setminus \set 0, y \in \R: \map f y = p \map f {\dfrac y p}$ In other words: :$\forall p \in \Z \setminus \set 0, y \in \R: \map f {\dfrac y p} = \dfrac 1 p \map f y$ Given $p, q \in \Z, q \ne 0$, we have: {{begin-eqn}} {{eqn | l = \map f {\dfrac p q} x | r = \map f {p \dfrac x q} | c = }} {{eqn | r = p \map f {\dfrac x q} | c = }} {{eqn | r = p \dfrac {\map f x } q | c = }} {{eqn | r = \dfrac p q \map f x | c = }} {{end-eqn}} Therefore we conclude: :$\forall r \in \Q, x \in \R: \map f {r x} = r \map f x$ {{qed}} [[Category:Additive Functions]] sod129ok9yo83743zxodu3g3r9dnleq	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \left\Vert{\mathbf u}\right\Vert \left\Vert{\mathbf u}\right\Vert \cos \angle \mathbf u, \mathbf u | c = [[Cosine Formula for Dot Product]] }} {{eqn | r = \left\Vert{\mathbf u}\right\Vert^2 \cos 0 | c = since the angle between a vector and itself is $0$ }} {{eqn | r = \left\Vert{\mathbf u}\right\Vert^2 | c = [[Cosine of Zero is One]] }} {{end-eqn}} {{qed}} === Note === Because this theorem is used to prove the general ($n$-dimensional) case of [[Cosine Formula for Dot Product]], this proof is circular the way we have defined the dot product. However, some texts use the cosine formula as the definition of the dot product and derive the sum of products form as a consequence. The two definitions are equivalent, so we have included this proof to show how the statement would be proved from the cosine definition.	1
By definition, there exists a [[Definition:Countable Basis|countable basis]] $\BB$ for $\tau$. Using the [[Axiom:Axiom of Countable Choice|axiom of countable choice]], we can obtain a [[Definition:Choice Function|choice function]] $\phi$ for $\BB \setminus \set \O$. Define: :$H = \set {\map \phi B: B \in \BB \setminus \set \O}$ By [[Image of Countable Set under Mapping is Countable]], it follows that $H$ is [[Definition:Countable Set|countable]]. It suffices to show that $H$ is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Let $x \in U \in \tau$. By [[Equivalence of Definitions of Analytic Basis]], there exists a $B \in \BB$ such that $x \in B \subseteq U$. Then $\map \phi B \in U$, and so $H \cap U$ is [[Definition:Non-Empty Set|non-empty]]. Hence, $x$ is an [[Definition:Adherent Point/Definition 1|adherent point]] of $H$. By [[Equivalence of Definitions of Adherent Point]], it follows that $x \in H^-$, where $H^-$ denotes the [[Definition:Closure (Topology)|closure]] of $H$. Therefore, $H^- = S$, and so $H$ is [[Definition:Everywhere Dense|everywhere dense]] in $T$ by definition. {{qed}} {{ACC}}	1
The [[Definition:Vector Cross Product|vector cross product]] is ''not'' [[Definition:Associative|associative]]. That is, in general: :$\mathbf a \times \left({\mathbf b \times \mathbf c}\right) \ne \left({\mathbf a \times \mathbf b}\right) \times \mathbf c$ for $\mathbf {a}, \mathbf {b}, \mathbf {c} \in \R^3$.	1
Let $n$ be an [[Definition:Integer|integer]] such that $n > 1$. Then $n$ can be expressed as the [[Definition:Integer Multiplication|product]] of one or more [[Definition:Prime Number|primes]].	1
Let $\eqclass {x_n} {}, \eqclass {y_n} {} \in \CC \,\big / \NN$ {{begin-eqn}} {{eqn | l = \norm {\eqclass {x_n} {} + \eqclass {y_n} {} } _1 | r = \norm {\eqclass {x_n + y_n} {} }_1 | c = Addition on quotient ring }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n + y_n} | c = Definition of $\norm {\,\cdot\,}_1$ }} {{end-eqn}} By {{NormAxiom|3}}: :$\forall n: \norm {x_n + y_n} \le \norm {x_n} + \norm {y_n}$ So: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} \norm {x_n + y_n} | o = \le | r = \lim_{n \mathop \to \infty} \norm {x_n} + \norm {y_n} | c = [[Inequality Rule for Real Sequences]] }} {{eqn | r = \lim_{n \mathop \to \infty} \norm {x_n} + \lim_{n \mathop \to \infty} \norm {y_n} | c = [[Sum Rule for Real Sequences]] }} {{eqn | r = \norm {\eqclass {x_n} {} }_1 + \norm {\eqclass {y_n} {} } _1 | c = Definition of $\norm {\,\cdot\,}_1$ }} {{end-eqn}} {{qed}}	1
Let $\Bbb F$ denote one of the [[Definition:Standard Number System|standard number systems]]. Let $\map \MM {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\Bbb F$. For $\mathbf A \in \map \MM {m, n}$ and $\lambda$ \in $\Bbb F$, let $\lambda \mathbf A$ be defined as the [[Definition:Matrix Scalar Product|matrix scalar product]] of $\lambda$ and $\mathbf A$. The [[Definition:Matrix Scalar Product|matrix scalar product]] is [[Definition:Associative Operation|associative]] on $\map \MM {m, n}$, in the following sense: For all $\mathbf A$ in $\map \MM {m, n}$ and $\lambda, \mu \in \Bbb F$: :$\paren {\lambda + \mu} \mathbf A = \lambda \mathbf A + \mu \mathbf A$	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$ of [[Definition:Dimension (Linear Algebra)|finite dimension]]. Let $f$ be a [[Definition:Nondegenerate Bilinear Form|nondegenerate bilinear form]] on $V$. Let $U\subset V$ be a [[Definition:Vector Subspace|subspace]]. Let $U^\perp$ be its [[Definition:Orthogonal Complement (Bilinear Form)|orthogonal complement]]. Then: :$\map \dim U + \map \dim U^\perp = \map \dim V$	1
Let $n \in \N$. Let $b_1, b_2, \dots, b_n$ be [[Definition:Real Number|real numbers]]. Let $\mathbf b = \tuple {b_1, b_2, \dots, b_n}^T$. Let $x_1, x_2, \dots, x_n$ be [[Definition:Real Number|real numbers]]. Let $\mathbf x = \tuple {x_1, x_2, \dots, x_n}^T$. Let $A$ be an [[Definition:Invertible Matrix|invertible]] $n \times n$ [[Definition:Matrix|matrix]] with coefficients in $\R$. For each $i \in \set {1, \dots, n}$, let $A_i$ be the matrix obtained by replacing the $i$th column with $\mathbf b$. Let: :$A \mathbf x = \mathbf b$ Then: :$\mathbf x_i = \dfrac {\map \det {A_i} } {\map \det A}$ for each $i \in \set {1, \dots, n}$.	1
Let $\mathbf A$ be a [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]]. Then its [[Definition:Inverse Matrix|inverse]] $\mathbf A^{-1}$ is also [[Definition:Proper Orthogonal Matrix|proper orthogonal]].	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\mathbf A$ be an [[Definition:Element|element]] of the [[Definition:Ring of Square Matrices|ring of square matrices]] $\struct {\map {\mathcal M_R} n, +, \times}$. The following definitions for $\mathbf A$ to be [[Definition:Non-Invertible Matrix|non-invertible]] are [[Definition:Logical Equivalence|equivalent]]:	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $R$ be a $k$-[[Definition:Unital Associative Commutative Algebra|algebra]] of [[Definition:Finite Dimensional Vector Space|finite dimension]] which is an [[Definition:Integral Domain|integral domain]]. Then $R$ is a [[Definition:Field (Abstract Algebra)|field]].	1
Let $\struct {R, \norm {\,\cdot\,}} $ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Let $\sequence {x_n} $ be a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in the [[Definition:Normed Division Ring|normed division ring]] $\struct {R, \norm {\,\cdot\,}}$ Then: :$\sequence {x_n} $ is a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$	1
Let $\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ be an [[Definition:Orthonormal Basis of Vector Space|orthonormal basis]] of a [[Definition:Vector Space|vector space]] $V$. Then: :$\forall i, j \in \set {1, 2, \ldots, n}: \mathbf e_i \cdot \mathbf e_j = \delta_{i j}$ where: :$\mathbf e_i \cdot \mathbf e_j$ denotes the [[Definition:Dot Product|dot product]] of $\mathbf e_i$ and $\mathbf e_j$ :$\delta_{i j}$ denotes the [[Definition:Kronecker Delta|Kronecker delta]].	1
{{begin-eqn}} {{eqn | l = \mathbf a \times \mathbf b | r = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ a_i & a_j & a_k \\ b_i & b_j & b_k \\ \end{vmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = -\begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ b_i & b_j & b_k \\ a_i & a_j & a_k \\ \end{vmatrix} | c = [[Determinant with Rows Transposed]] }} {{eqn | r = -\left({\mathbf b \times \mathbf a}\right) | c = {{Defof|Vector Cross Product}} }} {{end-eqn}} {{qed}}	1
Let $S$ be the [[Definition:Set|set]] of [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $2$]] whose [[Definition:Diagonal Element|diagonal elements]] are [[Definition:Zero (Number)|zero]]. Then the [[Definition:Algebraic Structure|algebraic structure]] $\struct {S, +, \times}$ is not a [[Definition:Ring (Abstract Algebra)|ring]]. Note that $\times$ denotes [[Definition:Matrix Product (Conventional)|conventional matrix multiplication]].	1
Let $\mathbf A$ be a [[Definition:Matrix|matrix]]. Then the '''nullity of $\mathbf A$''' is defined to be the [[Definition:Dimension of Vector Space|dimension]] of the [[Definition:Null Space|null space]] of $\mathbf A$.	1
=== Positive Definiteness === Let $\tuple {x , y} \in V$. Then: {{begin-eqn}} {{eqn | l = \norm {\tuple {x, y} } | r = \map \max {\norm x, \norm y} | c = {{defof|Direct Product Norm}} }} {{eqn | o = \ge | r = 0 | c = [[Definition:Norm on Vector Space|Norm Axiom]] $N1$: Positive Definiteness }} {{end-eqn}} Suppose $\norm {\tuple {x, y} } = 0$. Then: {{begin-eqn}} {{eqn | l = 0 | o = \le | r = \norm x | c = [[Definition:Norm on Vector Space|Norm Axiom]] $N1$: Positive Definiteness }} {{eqn | o = \le | r = \map \max {\norm x, \norm y} }} {{eqn | r = \norm {\tuple {x, y} } | c = {{defof|Direct Product Norm}} }} {{eqn | r = 0 }} {{end-eqn}} Hence, $\norm x = 0$ {{qed|lemma}} === Positive Homogeneity === {{begin-eqn}} {{eqn | l = \norm {\alpha \tuple {x, y} } | r = \norm {\tuple {\alpha x, \alpha y} } | c = [[Definition:Operation Induced by Direct Product|Induced component-wise operations]] }} {{eqn | r = \map \max {\norm {\alpha x}, \norm {\alpha y} } | c = {{defof|Direct Product Norm}} }} {{eqn | r = \map \max {\size \alpha \norm x, \size \alpha \norm y} | c = [[Definition:Norm on Vector Space|Norm Axiom]] $N2$ : Positive Homogeneity }} {{eqn | r = \size \alpha \max \set {\norm x, \norm y} }} {{eqn | r = \size \alpha \norm {\tuple {x ,y} } | c = {{defof|Direct Product Norm}} }} {{end-eqn}} {{qed|lemma}} === Triangle Inequality === {{begin-eqn}} {{eqn | l = \norm {x_1 + x_2} | o = \le | r = \norm {x_1} + \norm {x_2} }} {{eqn | o = \le | r = \map \max {\norm {x_1}, \norm {y_1} } + \map \max {\norm {x_2}, \norm {y_2} } }} {{eqn | r = \norm {\tuple {x_1, y_1} }+ \norm {\tuple {x_2, y_2} } | c = {{defof|Direct Product Norm}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \norm {y_1 + y_2} | o = \le | r = \norm {y_1} + \norm {y_2} }} {{eqn | o = \le | r = \map \max {\norm {x_1}, \norm {y_1} } + \map \max {\norm {x_2}, \norm {y_2} } }} {{eqn | r = \norm {\tuple {x_1, y_1} } + \norm {\tuple {x_2, y_2} } | c = {{defof|Direct Product Norm}} }} {{end-eqn}} Together: :$\map \max {\norm {x_1 + x_2}, \norm {y_1 + y_2}} \le \norm {\tuple {x_1, y_1} } + \norm {\tuple {x_2, y_2} }$ Then: {{begin-eqn}} {{eqn | l = \norm {\tuple {x_1, y_1} + \tuple {x_2, y_2} } | r = \norm {\tuple {x_1 + x_2, y_1 + y_2} } | c = [[Definition:Operation Induced by Direct Product|Induced component-wise operations]] }} {{eqn | r = \map \max {\norm {x_1 + x_2}, \norm {y_1 + y_2} } | c = {{defof|Direct Product Norm}} }} {{eqn | o = \le | r = \norm {\tuple {x_1, y_1} } + \norm {\tuple {x_2, y_2} } }} {{end-eqn}} {{qed}}	1
=== Case 1 === Suppose that $\mathbf v$ and $\mathbf w$ are not [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of each other. :[[File:AngleBetweenTwoVectors.png|400px]] From [[Construction of Triangle from Given Lengths]], it is [[Definition:Sufficient Condition|sufficient]] to show that sum of the lengths of any two sides is greater than the third side. Consider the side with length $\norm {\mathbf v}$. From the [[Triangle Inequality|triangle inequality for vectors]]: {{begin-eqn}} {{eqn | l = \norm {\mathbf v} | r = \norm {\mathbf {w + v - w} } }} {{eqn | o = < | r = \norm {\mathbf w} + \norm {\mathbf {v - w} } }} {{end-eqn}} Note that the equality is a [[Definition:Strict Inequality|strict inequality]] [[Triangle Inequality|because the vectors are not scalar multiples of each other]]. {{explain|Pick a better way to explain this than linking to the above}} Consider the side with length $\norm {\mathbf w}$. {{begin-eqn}} {{eqn | l = \norm {\mathbf w} | r = \norm {\mathbf {v + w - v} } }} {{eqn | o = < | r = \norm {\mathbf v} + \norm {\mathbf {w - v} } }} {{eqn | r = \norm {\mathbf v} + \norm {\mathbf {v - w} } }} {{end-eqn}} Lastly, Consider the side with length $\norm {\mathbf v - \mathbf w}$. {{begin-eqn}} {{eqn | l = \norm {\mathbf {v - w} } | r = \norm {\mathbf {v + \paren {-w} } } }} {{eqn | o = < | r = \norm {\mathbf v} + \norm {\mathbf {-w} } }} {{eqn | r = \norm {\mathbf v} + \norm {\mathbf w} }} {{end-eqn}} {{qed|lemma}} === Case 2 === Suppose that $\mathbf v$ and $\mathbf w$ ''are'' [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of each other. Then the existence of $\theta$ follows directly from the definition of the angle between vectors that are scalar multiples of each other. {{explain|Link to the above definition, or word it here.}} {{qed}}	1
The [[Definition:Quaternion|set of quaternions]] $\H$, with the operations of [[Definition:Quaternion Addition|addition]] and [[Definition:Quaternion Multiplication|multiplication]], forms a [[Definition:Vector Space|vector space]].	1
The [[Definition:Complex Number|complex numbers]] $\C$ are formed by the [[Definition:Complex Number/Construction from Cayley-Dickson Construction|Cayley-Dickson Construction]] from the [[Definition:Real Number|real numbers]] $\R$. From [[Real Numbers form Algebra]], we have that $\R$ forms: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]]. :$(2): \quad$ A [[Definition:Commutative Algebra|commutative algebra]]. :$(3): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]]. :$(4): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]] whose $*$ operator is the [[Definition:Identity Mapping|identity mapping]]. :$(5): \quad$ A [[Definition:Real Star-Algebra|real $*$-algebra]]. From [[Cayley-Dickson Construction forms Star-Algebra]], $\C$ is a [[Definition:Star-Algebra|$*$-algebra]]. From [[Cayley-Dickson Construction from Nicely Normed Algebra is Nicely Normed]], $\C$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. From [[Cayley-Dickson Construction from Real Algebra is Commutative]], $\C$ is a [[Definition:Commutative Algebra|commutative algebra]]. From [[Cayley-Dickson Construction from Commutative Associative Algebra is Associative]], $\C$ is an [[Definition:Associative Algebra|associative algebra]]. However, from [[Algebra from Cayley-Dickson Construction Never Real]], $\C$ is not a [[Definition:Real Algebra|real algebra]]. === Proof of Normed Division Algebra === Consider the element $\left({1, 0}\right)$ of $\R^2$. We have: {{begin-eqn}} {{eqn | o = | r = \left({x_1, x_2}\right) \times \left({1, 0}\right) | c = }} {{eqn | r = \left({x_1 \times 1 - 0 \times x_2, x_1 \times 0 + x_2 \times 1}\right) | c = }} {{eqn | r = \left({x_1, x_2}\right) | c = }} {{end-eqn}} As $\times$ has already been shown to be [[Definition:Commutative Operation|commutative]], it follows that $\left({1, 0}\right) \times \left({x_1, x_2}\right) = \left({x_1, x_2}\right)$. So $\left({1, 0}\right) \in \R^2$ functions as a [[Definition:Unit of Algebra|unit]]. That is, $\left({\R^2, \times}\right)$ is a [[Definition:Unitary Algebra|unitary algebra]]. <!-- From [[Inverses for Real Multiplication]], every element of $\left({\R, \times}\right)$ except $0$ has a [[Definition:Multiplicative Inverse|multiplicative inverse]]. So $\left({\R, \times}\right)$ is a [[Definition:Division Algebra|division algebra]] and hence a [[Definition:Unitary Division Algebra|unitary division algebra]]. --> We define a [[Definition:Norm on Vector Space|norm]] on $\left({\R^2, \times}\right)$ by: :$\forall \mathbf a = \left({a_1, a_2}\right) \in \R^2: \left \Vert {\mathbf a} \right \Vert = \sqrt {a_1^2 + a_2^2}$ This is a [[Definition:Norm on Vector Space|norm]] because: : $(1): \quad \left \Vert \mathbf x \right \Vert = 0 \iff \mathbf x = \mathbf 0$ : $(2): \quad \left \Vert \lambda \mathbf x \right \Vert = \left \vert \lambda \right \vert \left \Vert x \right \Vert$ : $(3): \quad \left \Vert x - y \right \Vert \le \left \Vert x - z \right \Vert + \left \Vert z - y \right \Vert$ It also follows that: : $\left \Vert x \times y \right \Vert = \left \vert x \times y \right \vert = \left \vert x \right \vert \times \left \vert y \right \vert = \left \Vert x \right \Vert \times \left \Vert y \right \Vert$ and so $\left({\R^2, \times}\right)$ is a [[Definition:Normed Division Algebra|normed division algebra]]. {{proofread|Make sure the above are all proved properly.}} {{qed}}	1
Let $\map {\R^3} {x, y, z}$ denote the [[Definition:Real Cartesian Space|real Cartesian space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]].. Let $\tuple {\mathbf i, \mathbf j, \mathbf k}$ be the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis on $\R^3$]]. Let $\mathbf f$ and $\mathbf g: \R^3 \to \R^3$ be [[Definition:Vector-Valued Function|vector-valued functions]] on $\R^3$: :$\mathbf f := \tuple {\map {f_x} {\mathbf x}, \map {f_y} {\mathbf x}, \map {f_z} {\mathbf x} }$ :$\mathbf g := \tuple {\map {g_x} {\mathbf x}, \map {g_y} {\mathbf x}, \map {g_z} {\mathbf x} }$ Let $\nabla \times \mathbf f$ denote the [[Definition:Curl Operator|curl]] of $f$. Then: :$\nabla \times \paren {\mathbf f + \mathbf g} = \nabla \times \mathbf f + \nabla \times \mathbf g$	1
Let $\Omega \subset \R^{n+k}$ be [[Definition:Open Set of Real Euclidean Space|open]]. Let $f : \Omega \to \R^k$ be [[Definition:Smooth Vector-Valued Function|smooth]]. Let $(a,b) \in \Omega$, with $a\in \R^n$ and $b\in \R^k$. Let $f(a,b) = 0$. For $(x_0,y_0)\in\Omega$, let $D_2 f(x_0,y_0)$ denote the [[Definition:Differential of Vector-Valued Function|differential]] of the function $y\mapsto f(x_0, y)$ at $y_0$. Let the [[Definition:Linear Mapping|linear map]] $D_2 f(a,b)$ be [[Definition:Invertible Linear Mapping|invertible]]. Then there exist [[Definition:Neighborhood|neighborhoods]] $U\subset\Omega$ of $a$ and $V\subset\R^k$ of $b$ such that there exists a unique [[Definition:Function|function]] $g : U \to V$ such that $f(x, g(x)) = 0$ for all $x\in U$. Moreover, $g$ is [[Definition:Smooth Vector-Valued Function|smooth]], and its [[Definition:Differential of Vector-Valued Function|differential]] satisfies: :$dg (x) = - \left( (D_2f)(x, g(x)) \right)^{-1} \circ (D_1 f)(x, g(x))$ for all $x\in U$.	1
For any $\lambda \in \R$, we define $f: \R \to \R$ as the [[Definition:Real Function|function]]: :$\displaystyle \map f \lambda = \sum {\paren {r_i + \lambda s_i}^2}$ Now: :$\map f \lambda \ge 0$ because it is the sum of [[Definition:Square Function|squares]] of [[Definition:Real Number|real numbers]]. Hence: {{begin-eqn}} {{eqn | l = \forall \lambda \in \R: \map f \lambda | o = \equiv | r = \sum {\paren {r_i^2 + 2 \lambda r_i s_i + \lambda^2 s_i^2} } \ge 0 | c = }} {{eqn | o = \equiv | r = \sum {r_i^2} + 2 \lambda \sum {r_i s_i} + \lambda^2 \sum {s_i^2} \ge 0 | c = }} {{end-eqn}} This is a [[Definition:Quadratic Equation|quadratic equation]] in $\lambda$. From [[Solution to Quadratic Equation]]: :$\displaystyle a \lambda^2 + b \lambda + c = 0: a = \sum {s_i^2}, b = 2 \sum {r_i s_i}, c = \sum {r_i^2}$ The [[Definition:Discriminant of Quadratic Equation|discriminant]] of this equation (that is $b^2 - 4 a c$) is: :$D := \displaystyle 4 \paren {\sum {r_i s_i} }^2 - 4 \sum {r_i^2} \sum {s_i^2}$ {{AimForCont}} $D$ is [[Definition:Strictly Positive Real Number|(strictly) positive]]. Then $\map f \lambda = 0$ has two [[Definition:Distinct Elements|distinct]] [[Definition:Real Number|real]] [[Definition:Root of Polynomial|roots]], $\lambda_1 < \lambda_2$, say. From [[Sign of Quadratic Function Between Roots]], it follows that $f$ is [[Definition:Strictly Negative Real Number|(strictly) negative]] somewhere between $\lambda_1$ and $\lambda_2$. But we have: :$\forall \lambda \in \R: \map f \lambda \ge 0$ From this [[Definition:Contradiction|contradiction]] it follows that: :$D \le 0$ which is the same thing as saying: :$\displaystyle \sum {r_i^2} \sum {s_i^2} \ge \paren {\sum {r_i s_i} }^2$ {{qed}}	1
Let $\left({V, \phi}\right)$ be a [[Definition:G-Module|$G$-module]] over a [[Definition:Field (Abstract Algebra)|field]] $k$. Let $W$ be a [[Definition:Vector Subspace|vector subspace]] of $V$. Then $\left({W, \phi_W}\right)$, where $\phi_W: G \times W \to W$ is the [[Definition:Restriction of Mapping|restriction]] of $\phi$ to $G \times W$, is a [[Definition:G-Submodule|$G$-submodule]] of $V$ [[Definition:Iff|iff]] $\phi \left({G, W}\right) \subseteq W$.	1
Let $\displaystyle \sum_{n \mathop = 1}^\infty z_n$ be an [[Definition:Absolutely Convergent Complex Series|absolutely convergent series in $\C$]]. Then $\displaystyle \sum_{n \mathop = 1}^\infty z_n$ is [[Definition:Convergent Series of Numbers|convergent]].	1
From [[Negative of Absolute Value]], we have for all $a \in \closedint a b$: :$-\size {\map f t} \le \map f t \le \size {\map f t}$ Thus from [[Relative Sizes of Definite Integrals]]: :$\displaystyle -\int_a^b \size {\map f t} \rd t \le \int_a^b \map f t \rd t \le \int_a^b \size {\map f t} \rd t$ Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\MM_R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $R$]]. Then $\struct {\map {\MM_R} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]]. However, for $n \ge 2$, $\struct {\map {\MM_R} n, +, \times}$ is not a [[Definition:Commutative Ring|commutative ring]].	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $T: H \to K$ be a [[Definition:Linear Transformation|linear transformation]]. Then $T$ is [[Definition:Compact Linear Transformation|compact]] iff its [[Definition:Adjoint Linear Transformation|adjoint]] $T^*$ is.	1
Let $\alpha_{ij}$ be [[Definition:Element|elements]] of a [[Definition:Field (Abstract Algebra)|field]] $F$, where $1 \le i \le m, 1 \le j \le n$. Let $n > m$. Then there exist $x_1, x_2, \ldots, x_n \in F$ not all zero, such that: :$\displaystyle \forall i: 1 \le i \le m: \sum_{j \mathop = 1}^n \alpha_{ij} x_j = 0$ Alternatively, this can be expressed as: If $n > m$, the following system of [[Definition:Homogeneous Linear Equations|homogeneous linear equations]]: {{begin-eqn}} {{eqn | l = 0 | r = \alpha_{11} x_1 + \alpha_{12} x_2 + \cdots + \alpha_{1n} x_n }} {{eqn | l = 0 | r = \alpha_{21} x_1 + \alpha_{22} x_2 + \cdots + \alpha_{2n} x_n }} {{eqn | o = \cdots }} {{eqn | l = 0 | r = \alpha_{m1} x_1 + \alpha_{m2} x_2 + \cdots + \alpha_{mn} x_n }} {{end-eqn}} has at least one solution such that not all of $x_1, \ldots, x_n$ is zero.	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$. Let $\BB$ and $\CC$ be [[Definition:Basis of Module|bases]] of $M$. Let $\mathbf M_{\BB, \CC}$ be the [[Definition:Change of Basis Matrix|change of basis matrix]] from $\BB$ to $\CC$. Let $m\in M$. Let $\sqbrk m_\BB$ and $\sqbrk m_\CC$ be its [[Definition:Coordinate Vector|coordinate vectors]] relative to $\BB$ and $\CC$ respectively. Then $\sqbrk m_\BB = \mathbf M_{\BB, \CC} \cdot \sqbrk m_\CC$.	1
By definition of [[Definition:Set Equality/Definition 2|set equality]], it suffices to prove the following two [[Definition:Subset|inclusions]]: :$\ds \bigcap_{i \mathop \in I} M_i^\perp \subseteq \paren {\vee \set {M_i : i \in I} }^\perp$ :$\paren {\vee \set {M_i : i \in I} }^\perp \subseteq \ds \bigcap_{i \mathop \in I} M_i^\perp$ === $\ds \bigcap_{i \mathop \in I} M_i^\perp$ is contained in $\paren {\vee \set {M_i : i \in I} }^\perp$ === By [[Orthocomplement is Closed Linear Subspace]] and [[Closed Linear Subspaces Closed under Intersection]], both spaces considered are [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$. By [[Orthocomplement Reverses Subset]], the required containment is equivalent to: :$\vee \set {M_i : i \in I} \subseteq \ds \paren {\bigcap_{i \mathop \in I} M_i^\perp }^\perp$ For $h \in \ds \bigcap_{i \mathop \in I} M_i^\perp$, by definition one has: :$h \perp M_i$ for all $i \in I$ That is: :$M_i \perp \ds \bigcap_{i \mathop \in I} M_i^\perp$ This is equivalent to saying that: :$M_i \subseteq \ds \paren {\bigcap_{i \mathop \in I} M_i^\perp }^\perp$ Definition $(2)$ of [[Definition:Closed Linear Span|closed linear span]] now grants the desired subset relation. {{qed|lemma}} === $\paren {\vee \set {M_i : i \in I} }^\perp$ is contained in $\ds \bigcap_{i \mathop \in I} M_i^\perp$ === By definition $(2)$ of [[Definition:Closed Linear Span|closed linear span]]: :$\forall i \in I: M_i \subseteq \vee \set {M_i^\perp : i \in I}$ By [[Orthocomplement Reverses Subset]], it follows that: :$\forall i \ni I: \paren {\vee \set {M_i : i \in I} }^\perp \subseteq M_i^\perp$ Therefore, by definition of [[Definition:Set Intersection|set intersection]]: :$\paren {\vee \set {M_i : i \in I} }^\perp \subseteq \displaystyle \bigcap_{i \mathop \in I} M_i^\perp$ {{qed|lemma}} Thus we have established that: :$h \in \paren {\vee \set {M_i : i \in I} }^\perp \iff h \in \ds \bigcap_{i \mathop \in I} M_i^\perp$ From the definition of [[Definition:Set Equality/Definition 1|set equality]], it follows that: :$\ds \bigcap_{i \mathop \in I} M_i^\perp = \paren {\vee \set {M_i : i \in I} }^\perp$ {{qed}}	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $V$ and $W$ be $K$-[[Definition:Vector Space|vector spaces]]. Let $\phi: V \to W$ be a [[Definition:Linear Transformation|linear transformation]]. Let the [[Definition:Kernel of Linear Transformation|kernel]] $\ker \phi$ be [[Definition:Finite Dimensional Vector Space|finite dimensional]]. Then the '''nullity of $\phi$''' is the [[Definition:Dimension of Vector Space|dimension]] of $\ker \phi$ and is denoted $\map \nu \phi$.	1
The [[Definition:P-adic Norm|$p$-adic norm]] forms a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] on the [[Definition:Rational Number|rational numbers]] $\Q$.	1
Let $P \left({\R}\right)$ be the [[Definition:Vector Space|vector space]] of all [[Definition:Real Polynomial Function|polynomial functions]] on the [[Definition:Real Number Line|real number line]] $\R$. Then the [[Definition:Differentiation|differentiation]] operator $D$ on $P \left({\R}\right)$ is a [[Definition:Linear Operator|linear operator]].	1
Let $a, b \in \R$ be [[Definition:Real Number|real numbers]]. Let $f: \R \to \R$ be the [[Definition:Real Function|real function]] defined as: :$\forall x \in \R: \map f x = a x + b$ Then $f$ is a [[Definition:Bijection|bijection]] {{iff}} $a \ne 0$.	1
From [[Dot Product Operator is Commutative]]: :$z_1 \circ z_2 = z_2 \circ z_1$ The result follows trivially. {{qed}}	1
Let $U$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$. By the definition of $T$, $U$ is [[Definition:Closed Set (Topology)|closed]]. As $U$ is [[Definition:Countable Set|countable]] but $S$ is [[Definition:Uncountable Set|uncountable]]: :$U \subsetneq S$ and so: :$U \ne S$ From [[Closed Set Equals its Closure]]: :$U^- = U$ where $U^-$ is the [[Definition:Closure (Topology)|closure]] of $U$. Thus: :$U^- \ne S$ So by definition, $U$ is not [[Definition:Everywhere Dense|everywhere dense]] in $T$. As $U$ is arbitrary, there is no [[Definition:Countable Set|countable set]] which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. The result follows by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $\mathbb S$ be the [[Definition:Set|set]] of all $S \subseteq G$ such that $S$ is a [[Definition:Generator of Vector Space|generator]] for $G$ and that $H \subseteq S \subseteq F$. Because $F \in \mathbb S$, it follows that $\mathbb S \ne \O$. Because $F$ is [[Definition:Finite Set|finite]], then so is every [[Definition:Element|element]] of $\mathbb S$. Let $R = \set {r \in \Z: r = \card S \in \mathbb S}$. That is, $R$ is the set of all the integers which are the number of [[Definition:Element|elements]] in [[Definition:Generator of Vector Space|generators]] for $G$ that are subsets of $F$. Let $n$ be the smallest [[Definition:Element|element]] of $R$. Let $B$ be an [[Definition:Element|element]] of $\mathbb S$ such that $\card B = n$. We note that as $H$ is a [[Definition:Linearly Independent Set|linearly independent set]], it does not contain $0$ by [[Subset of Module Containing Identity is Linearly Dependent]]. Then $0 \notin B$, or $B \setminus \set 0$ would be a [[Definition:Generator of Vector Space|generator]] for $G$ with $n - 1$ [[Definition:Element|elements]]. This would contradict the definition of $n$. Let $m = \card H$. Let $\sequence {a_n}$ be a [[Definition:Sequence of Distinct Terms|sequence of distinct vectors]] such that $H = \set {a_1, \ldots, a_m}$ and $B = \set {a_1, \ldots, a_n}$. Suppose $B$ were [[Definition:Linearly Dependent Set|linearly dependent]]. By [[Linearly Dependent Sequence of Vector Space]], there would exist $p \in \closedint 2 n$ and [[Definition:Scalar (Vector Space)|scalars]] $\mu_1, \ldots, \mu_{p - 1}$ such that $\displaystyle a_p = \sum_{k \mathop = 1}^{p - 1} \mu_k a_k$. As $H$ is [[Definition:Linearly Independent Set|linearly independent]], $p > m$ and therefore $B' = B \setminus \set {a_p}$ would contain $H$. {{explain|why $H$ being L.I. implies $p > m$}} Now if $\displaystyle x = \sum_{k \mathop = 1}^n \lambda_k a_k$, then: :$\displaystyle x = \sum_{k \mathop = 1}^{p - 1} \paren {\lambda_k + \lambda_p \mu_k} a_k + \sum_{k \mathop = p + 1}^n \lambda_k a_k$ Hence $B'$ would be a [[Definition:Generator of Vector Space|generator]] for $G$ containing $n - 1$ [[Definition:Element|elements]], which contradicts the definition of $n$. Thus $B$ must be [[Definition:Linearly Independent Set|linearly independent]] and hence is a [[Definition:Basis of Vector Space|basis]]. {{qed}} {{Proofread}}	1
Let $S$ be a [[Definition:Set|set]]. Then $\struct {\powerset S, *, \cap}$ is a [[Definition:Commutative and Unitary Ring|commutative ring with unity]], in which the [[Definition:Unity of Ring|unity]] is $S$. This ring is not an [[Definition:Integral Domain|integral domain]].	1
We have: {{begin-eqn}} {{eqn | l = \map {S_r} x | r = \set {y \in R : \norm {y - x} = r} | c = {{Defof|Sphere in Normed Division Ring}} }} {{eqn | r = \set {y \in R : \norm {y - x} \le r} \cap \set {y \in R : \norm{y - x} \ge r} }} {{eqn | r = \map { {B_r}^-} x \cap \paren {R \setminus \map {B_r} x} | c = {{Defof|Open Ball of Normed Division Ring}} and {{Defof|Closed Ball of Normed Division Ring}} }} {{end-eqn}} Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. By [[Topological Properties of Non-Archimedean Division Rings/Open Balls are Clopen|Open Balls Are Clopen]] then $\map {B_r} x$ is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in $d$. By [[Metric Induces Topology]] then $R \setminus \map {B_r} x$ is is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in $d$. By [[Topological Properties of Non-Archimedean Division Rings/Closed Balls are Clopen|Closed Balls Are Clopen]] then $\map { {B_r}^-} x$ is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in $d$. By [[Metric Induces Topology]] then the [[Definition:Intersection|intersection]] of a [[Definition:Finite Set|finite number]] of [[Definition:Open Set of Metric Space|open sets]] is [[Definition:Open Set of Metric Space|open]]. Hence $\map {S_r} x$ is [[Definition:Open Set of Metric Space|open]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$. By [[Intersection of Closed Sets is Closed in Topological Space]] then $\map {S_r} x$ is [[Definition:Closed Set of Metric Space|closed]] in $\struct {R, d}$. The result follows. {{qed}}	1
Let $\CC \closedint a b$ be the [[Definition:Space of Continuous on Closed Interval Real-Valued Functions|space of continuous on closed interval real-valued functions]]. Let $x \in \CC \closedint a b$ be a [[Definition:Continuous Real-Valued Vector Function|continuous real valued function]]. Let $\displaystyle \norm x_1 := \int_a^b \size {\map x t} \rd t$ be the [[Definition:P-Seminorm|1-seminorm]]. Then $\norm {\, \cdot \,}_1$ is a [[Definition:Norm on Vector Space|norm]] on $\CC \closedint a b$.	1
Let $T = \struct {S, \tau_p}$ be a [[Definition:Fort Space|Fort space]] on a [[Definition:Countably Infinite Set|countably infinite set]] $S$. Then $T$ is a [[Definition:Separable Space|separable space]].	1
[[File:Resultant-of-3-non-coplanar-vectors.png|400px]] Let $\mathbf {\hat a}$, $\mathbf {\hat b}$ and $\mathbf {\hat c}$ be [[Definition:Unit Vector|unit vectors]] in the [[Definition:Direction|directions]] of $\mathbf a$, $\mathbf b$ and $\mathbf c$ respectively. Let $O$ be a [[Definition:Point|point]] in [[Definition:Ordinary Space|space]]. Take $\vec {OP} := \mathbf r$. With $OP$ as its [[Definition:Space Diagonal|space diagonal]], construct a [[Definition:Parallelepiped|parallelepiped]] with [[Definition:Edge of Polyhedron|edges]] $OA$, $OB$ and $OC$ [[Definition:Parallel Lines|parallel]] to $\mathbf {\hat a}$, $\mathbf {\hat b}$ and $\mathbf {\hat c}$ respectively. Only one such [[Definition:Parallelepiped|parallelepiped]] can be so constructed. Let $x$, $y$ and $z$ be the [[Definition:Length of Line|length]] of the [[Definition:Edge of Polyhedron|edges]] $OA$, $OB$ and $OC$ respectively. Then: {{begin-eqn}} {{eqn | l = \mathbf r | r = \vec {OA} + \vec {AF} + \vec {FP} | c = }} {{eqn | r = \vec {OA} + \vec {OB} + \vec {OC} | c = }} {{eqn | r = x \mathbf {\hat a} + y \mathbf {\hat b} + z \mathbf {\hat c} | c = }} {{end-eqn}} Thus $\mathbf r$ is the [[Definition:Resultant|resultant]] of the $3$ [[Definition:Vector Quantity|vector quantities]] $x \mathbf {\hat a}$, $y \mathbf {\hat b}$ and $z \mathbf {\hat c}$ which, by construction, are [[Definition:Parallel Lines|parallel]] to $\mathbf a$, $\mathbf b$ and $\mathbf c$ respectively. The fact that only one [[Definition:Parallelepiped|parallelepiped]] can be constructed in the above proves [[Definition:Unique|uniqueness]]. {{qed}}	1
By definition, a [[Definition:Rescaling Matrix|rescaling matrix]] is also a [[Definition:Diagonal Matrix|diagonal matrix]]. Hence [[Inverse of Diagonal Matrix]] applies, and since $r$ is a [[Definition:Unit of Ring|unit]], it gives the desired result. {{qed}} [[Category:Matrix Algebra]] rdzg8jav5ovuegzpvdyrnr0ul16ur42	1
Let us write $\phi \sqbrk V$ for the [[Definition:Homomorphic Image|homomorphic image]] of $\phi$. From [[Homomorphic Image of R-Module is R-Module]], $\phi \sqbrk V$ is a [[Definition:Module|$K$-module]]. It thus suffices to show that $\phi \sqbrk V$ is [[Definition:Unitary Module|unitary]], since then it will be a [[Definition:Vector Space|$K$-vector space]]. To this end, let $1_K$ be the [[Definition:Unity of Ring|unity]] of $K$. Then for any $\map \phi {\mathbf v} \in \phi \sqbrk V$, compute: {{begin-eqn}} {{eqn | l = 1_K \circ_W \map \phi {\mathbf v} | r = \map \phi {1_K \circ_V \mathbf v} | c = $\phi$ is a [[Definition:Linear Transformation on Vector Space|linear transformation]] }} {{eqn | r = \map \phi {\mathbf v} | c = $V$ is a [[Definition:Vector Space|$K$-vector space]] }} {{end-eqn}} Hence $\phi \sqbrk V$ is [[Definition:Unitary Module|unitary]], and so a [[Definition:Vector Space|$K$-vector space]]. {{qed}}	1
Let $\left({G, \cdot}\right)$ be a [[Definition:Group|group]] and let $f: \left({V, \phi}\right) \to \left({V', \mu}\right)$ be a [[Definition:G-Module Homomorphism |homomorphism of $G$-modules]]. Then $\operatorname{Im} \left({f}\right)$ is a [[Definition:G-Submodule|$G$-submodule]] of $V'$.	1
Let $S \subseteq \N$ be the set of all [[Definition:Natural Numbers|natural numbers]] $n \in \N$ such that: :For any [[Definition:Finite Set|finite]] [[Definition:Generator of Module|generator]] $F$ of $V$ over $R$, if $\card {F \cap L} \ge n$, then $\card L \le \card F$. It is to be demonstrated that $S = \N$. That is, that $\card {F \cap L} \ge n \implies \card L \le \card F$ for all $n \in \N$. By [[Intersection is Subset]] and [[Cardinality of Subset of Finite Set]], we have $\card {F \cap L} \le \card F$. Hence, it is [[Definition:Vacuous Truth|vacuously true]] that $\card F + 1 \in S$. Therefore, $S$ is [[Definition:Non-Empty Set|non-empty]]. From the [[Well-Ordering Principle|well-ordering principle]], $S$ has a [[Definition:Smallest Element|smallest element]] $N$. If $N = 0$, the theorem immediately follows. {{AimForCont}} $N \ge 1$. Let $\card {F \cap L} \ge N - 1$. If $L \subseteq F$, the theorem follows from [[Cardinality of Subset of Finite Set]]. Otherwise, [[Definition:Existential Quantifier|there exists]] a $v \in L$ such that $v \notin F$. Let $F' = F \cup \set v$. By [[Intersection is Subset]], we have $F' \cap L \subseteq L$, so it follows by [[Subset of Linearly Independent Set]] that $F' \cap L$ is [[Definition:Linearly Independent Set|linearly independent]] over $R$. Also, by [[Intersection is Subset]]: :$F' \cap L \subseteq F'$ We have that $F'$ is a [[Definition:Generator of Module|generator]] of $V$ over $R$. By [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]], there exists a [[Definition:Basis of Vector Space|basis]] $B$ for $V$ such that: :$F' \cap L \subseteq B \subseteq F'$ Since $v \notin F$ is a [[Definition:Linear Combination of Subset|linear combination]] of $F$, it follows that $F'$ is [[Definition:Linearly Dependent Set|linearly dependent]] over $R$. By the definition of a [[Definition:Basis of Vector Space|basis]]: :$B \subsetneq F'$ By [[Cardinality of Subset of Finite Set]] and [[Cardinality is Additive Function]]: :$\card B < \card {F'} = \card F + 1$ Hence: :$\card B \le \card F$ We have that: {{begin-eqn}} {{eqn | l = \card {B \cap L} | o = \ge | r = \card {\paren {F' \cap L} \cap L} | c = [[Set Intersection Preserves Subsets]] and [[Cardinality of Subset of Finite Set]] }} {{eqn | r = \card {F' \cap L} | c = [[Intersection is Associative]] and [[Intersection is Idempotent]] }} {{eqn | r = \card {\paren {F \cap L} \cup \paren {\set v \cap L} } | c = [[Intersection Distributes over Union]] }} {{eqn | r = \card {\paren {F \cap L} \cup \set v} | c = [[Intersection with Subset is Subset]] }} {{eqn | o = \ge | r = N | c = [[Cardinality is Additive Function]] }} {{end-eqn}} Since $N \in S$, it follows by the definition of a [[Definition:Basis of Vector Space|basis]] that: :$\card L \le \card B \le \card F$ Hence: :$N - 1 \in S$ But this [[Definition:Contradiction|contradicts]] the [[Definition:Assumption|assumption]] that $N$ is the [[Definition:Smallest Element|smallest element]] of $S$. Therefore: :$N = 0$ Hence the result. {{qed}}	1
There exists a [[Definition:Basis of Vector Space|basis]] $B$ for $E$ such that $H \subseteq B$.	1
We [[Space of Continuous on Closed Interval Real-Valued Functions with Pointwise Addition and Pointwise Scalar Multiplication form Vector Space|have]] that $\struct {\map \CC I, +, \, \cdot \,}_\R$ is a [[Definition:Vector Space|vector space]]. By [[Differentiable Function is Continuous]], $\map {\CC^1} I \subset \map \CC I$. Let $f, g \in \map {\CC^1} I$. Let $\alpha \in \R$. Let $\map 0 x$ be a [[Definition:Real-Valued Function|real-valued function]] such that: :$\map 0 x : I \to 0$ [[Definition:Restriction of Operation|Restrict]] $\paren +$ to $\map {\CC^1} I \times \map {\CC^1} I$. [[Definition:Restriction of Operation|Restrict]] $\paren {\, \cdot \,}$ to $\R \times \map {\CC^1} I$. === Closure under vector addition === By [[Sum Rule for Derivatives]]: :$f + g \in \map {\CC^1} I$ {{qed|lemma}} === Closure under scalar multiplication === By [[Derivative of Constant Multiple]]: :$\alpha \cdot f \in \map {\CC^1} I$ {{qed|lemma}} === Nonemptiness === By [[Derivative of Constant]], a [[Definition:Constant Mapping|constant mapping]] is [[Definition:Differentiable Real-Valued Function|differentiable]]. Hence, $\map 0 x \in \map {\CC^1} I$. {{qed|lemma}} We have that $\map {\CC^1} I$ is closed under [[Definition:Restriction of Operation|restrictions]] of $\paren +$ and $\paren {\, \cdot \,}$ to $\map {\CC^1} I$. Also, $\map {\CC^1} I$ is [[Definition:Non-Empty Set|non-empty]]. By definition, $\struct {\map {\CC^1} I, +, \, \cdot \,}_\R$ is a [[Definition:Vector Subspace|vector subspace]] of $\struct {\map \CC I, +, \, \cdot \,}_\R$. Since $\struct {\map {\CC^1} I, +, \, \cdot \,}_\R$ satisfies [[Definition:Vector Space Axioms|vector space axioms]] under given [[Definition:Restriction of Operation|restrictions]], it is a [[Definition:Vector Space|vector space]]. {{qed}}	1
This is a reformulation of [[Krull's Theorem]]. {{qed}}	1
Let $x, y \in S$. Let $\lambda \in R$. Then: {{begin-eqn}} {{eqn | l = \map \phi {x + y} | r = \map \phi x + \map \phi y | c = {{Defof|Linear Transformation}} }} {{eqn | r = \map \psi x + \map \psi y | c = $x, y \in S$ }} {{eqn | r = \map \psi {x + y} | c = {{Defof|Linear Transformation}} }} {{eqn | l = \map \phi {\lambda \circ x} | r = \lambda \circ \map \phi x | c = {{Defof|Linear Transformation}} }} {{eqn | r = \lambda \circ \map \psi x | c = $x \in S$ }} {{eqn | r = \map \psi {\lambda \circ x} | c = {{Defof|Linear Transformation}} }} {{end-eqn}} Hence $x + y, \lambda \circ x \in S$. By [[Submodule Test]], $S$ is a [[Definition:Submodule|submodule]] of $G$. {{qed}}	1
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $\struct {\Q_p, \norm {\,\cdot\,}_p}$ be the [[Definition:P-adic Number|$p$-adic numbers]]. Then the [[Definition:P-adic Integers|$p$-adic integers]] $\Z_p$ is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:P-adic Metric on P-adic Numbers|$p$-adic metric]].	1
Let $\mathbf u$, $\mathbf v$ be non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Real Euclidean Space|Euclidean space]] $\R^n$. :$\mathbf u$ and $\mathbf v$ are '''perpendicular''' {{iff}} the [[Definition:Angle Between Vectors|angle between them]] is a [[Definition:Right Angle|right angle]].	1
{{begin-eqn}} {{eqn | l = \cmod {z - 1} | o = < | r = \cmod {z + 1} | c = }} {{eqn | ll= \leadstoandfrom | l = \cmod {z + 1} | o = > | r = \cmod {z - 1} | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {x + 1}^2 + y^2 | o = > | r = \paren {x - 1}^2 + y^2 | c = {{Defof|Complex Modulus}} }} {{eqn | ll= \leadstoandfrom | l = \paren {x + 1}^2 | o = > | r = \paren {x - 1}^2 | c = }} {{eqn | ll= \leadstoandfrom | l = x^2 + 2 x + 1 | o = > | r = x^2 - 2 x + 1 | c = }} {{eqn | ll= \leadstoandfrom | l = 4 x | o = > | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = x | o = > | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = \Re z | o = > | r = 0 | c = {{Defof|Real Part}} }} {{end-eqn}} {{qed}}	1
By [[P-adic Norm not Complete on Rational Numbers]], no [[Definition:P-adic Norm|$p$-adic norm]] $\norm{\,\cdot\,}_p$ on the [[Definition:Set|set]] of the [[Definition:Rational Number|rational numbers]], for any [[Definition:Prime Number|prime]] $p$, is [[Definition:Complete Normed Division Ring|complete]]. By [[Rational Number Space is not Complete Metric Space]], the [[Definition:Absolute Value|absolute value]] $\size{\,\cdot\,}$ on the [[Definition:Set|set]] of the [[Definition:Rational Number|rational numbers]] is not [[Definition:Complete Normed Division Ring|complete]]. By [[Norm is Complete Iff Equivalent Norm is Complete]], no [[Definition:Norm on Division Ring|norm]] is [[Definition:Complete Normed Division Ring|complete]] if it is [[Definition:Equivalent Division Ring Norms|equivalent]] to either the [[Definition:Absolute Value|absolute value]] $\size{\,\cdot\,}$ or the [[Definition:P-adic Norm|$p$-adic norm]] $\norm{\,\cdot\,}_p$ for some [[Definition:Prime Number|prime]] $p$. By [[Ostrowski's Theorem]], every [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Norm on Division Ring|norm]] is [[Definition:Equivalent Division Ring Norms|equivalent]] to either the [[Definition:Absolute Value|absolute value]] $\size{\,\cdot\,}$ or the [[Definition:P-adic Norm|$p$-adic norm]] $\norm{\,\cdot\,}_p$ for some [[Definition:Prime Number|prime]] $p$. The result follows. {{qed}}	1
We have: : [[Compact Complement Topology is Second-Countable]] : [[Second-Countable Space is Separable]] Hence the result. {{qed}}	1
Let $\struct {F, +, \circ}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Then the only [[Definition:Ideal of Ring|ideals]] of $\struct {F, +, \circ}$ are:$\struct {F, +, \circ}$ and $\set {0_F}$. That is, $\struct {F, +, \circ}$ has no non-[[Definition:Null Ideal|null]] [[Definition:Proper Ideal|proper ideals]].	1
Let $\map {\mathcal M_\R} n$ be a [[Definition:Matrix Space|$n \times n$ matrix space]] over the [[Definition:Real Number|set of real numbers $\R$]]. Then the set of all $n \times n$ real matrices $\map {\mathcal M_\R} n$ under [[Definition:Matrix Product (Conventional)|matrix multiplication (conventional)]] forms a [[Definition:Monoid|monoid]].	1
From [[Ring with Multiplicative Norm has No Proper Zero Divisors]], $R$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. From [[Finite Ring with No Proper Zero Divisors is Field]], $R$ is a [[Definition:Field (Abstract Algebra)|field]]. {{qed}} [[Category:Ring Theory]] [[Category:Field Theory]] [[Category:Norm Theory]] lwtk5hog282ltnhd0cbmwawzoi1ae1i	1
Let $\C^n$ be a [[Definition:Complex Vector Space|complex vector space]]. Let $S$ and $T$ be [[Definition:Linear Operator|linear operators]] on $\C^n$. Then the '''pointwise sum of $S$ and $T$''' is defined as: :$S + T: \C^n \to \C^n:$ ::$\forall u \in \C^n: \map {\paren {S + T} } u := \map S u + \map T u$ where $+$ on the {{RHS}} is [[Definition:Complex Addition|complex]] [[Definition:Vector Sum|vector addition]].	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $A$ be a [[Definition:Set|set]]. For each $a \in A$, let $f_a: A \to R$ be defined as: :$\forall x \in A: \map {f_a} x = \begin{cases} 1 & : x = a \\ 0 & : x \ne a \end{cases}$ Then $B = \set {f_a: a \in A}$ is a [[Definition:Basis (Linear Algebra)|basis]] of the [[Finite Submodule of Function Space]] $R^{\paren A}$.	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then: :$\forall \lambda \in F: \forall \mathbf v \in \mathbf V: \lambda \circ \mathbf v = \bszero \implies \paren {\lambda = 0_F \lor \mathbf v = \mathbf 0}$ where $\bszero \in \mathbf V$ is the [[Definition:Zero Vector|zero vector]].	1
From the [[Definition:Vector Space Axioms|vector space axioms]] we have that $\exists \mathbf 0 \in \mathbf V$. It remains to be proved that $\map T {\mathbf 0} = \mathbf 0'$: {{begin-eqn}} {{eqn | l = \map T {\mathbf 0} | r = \map T {\mathbf 0 + \mathbf 0} }} {{eqn | r = \map T {\mathbf 0} + \map T {\mathbf 0} | c = {{Defof|Linear Transformation on Vector Space}} }} {{eqn | ll= \leadsto | l = \mathbf 0' | r = \map T {\mathbf 0} | c = subtracting $\map T {\mathbf 0}$ from both sides }} {{end-eqn}} {{qed}}	1
Every [[Definition:Homogeneous Linear Equations|homogeneous system of linear equations]] has the [[Definition:Zero Vector|zero vector]] as a [[Definition:Solution to System of Simultaneous Equations|solution]].	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $\R$ or $\C$, and let $v \in V$. Then the [[Definition:Singleton|singleton]] $S = \left\{{v}\right\}$ is a [[Definition:Convex Set (Vector Space)|convex set]].	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathbf A = \sqbrk a_{m n}$ be an [[Definition:Matrix|$m \times n$ matrix]] over $\struct {R, +, \circ}$. Then the [[Definition:Matrix Space|matrix space $\map {\MM_R} {m, n}$ of all $m \times n$ matrices]] over $R$ is a [[Definition:Module|module]].	1
Let $\mathbf A = \left[{a}\right]_n$ be a [[Definition:Square Matrix|square matrix of order $n$]] over a [[Definition:Commutative Ring|commutative ring]] $R$. Then $\mathbf A$ can be converted to an [[Definition:Upper Triangular Matrix|upper]] or [[Definition:Lower Triangular Matrix|lower triangular matrix]] by [[Definition:Elementary Row Operation|elementary row operations]].	1
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]]. Let $M$ be [[Definition:Separable Space|separable]]. Then $M$ is [[Definition:Second-Countable Space|second-countable]].	1
{{begin-eqn}} {{eqn|l = \frac 1 {\phi \left({q}\right)} \sum_{\chi \mathop \in G^*} \langle \eta, \chi \rangle_G \chi \left({y}\right) |r = \frac 1 {\phi \left({q}\right)} \sum_{\chi \mathop \in G^*} \sum_{x \mathop \in G} \eta \left({x}\right) \overline \chi \left({x}\right) \chi \left({y}\right) |c = }} {{eqn|r = \frac 1 {\phi \left({q}\right)} \sum_{x \mathop \in G} \eta(x) \sum_{\chi \mathop \in G^*} \overline \chi \left({x}\right) \chi \left({y}\right) |c = }} {{eqn|r = \frac 1 {\phi \left({q}\right)} \eta \left({y}\right) \phi \left({y}\right) |c = [[Orthogonality Relations for Characters]] }} {{eqn|r = \eta \left({y}\right) |c = }} {{end-eqn}} {{qed}} [[Category:Fourier Analysis]] [[Category:Representation Theory]] dlnm1gr781v18q9tma21hb0389hk2s3	1
Let $\HH$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \subseteq \HH$ be a [[Definition:Subset|subset]] of $\HH$. Then the following identity holds: :$\paren {A^\perp}^\perp = \vee A$ Here $A^\perp$ denotes [[Definition:Orthocomplement|orthocomplementation]], and $\vee A$ denotes the [[Definition:Closed Linear Span|closed linear span]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $\left({M_i}\right)_{i \in I}$ be an [[Definition:Indexed Set|$I$-indexed set]] of [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$. Let $M_i$ and $M_j$ be [[Definition:Orthogonal (Hilbert Space)#Sets|orthogonal]] whenever $i \ne j$. Denote, for each $i \in I$, by $P_i$ the [[Definition:Orthogonal Projection|orthogonal projection]] onto $M_i$. Denote by $P$ the [[Definition:Orthogonal Projection|orthogonal projection]] onto the [[Definition:Closed Linear Span|closed linear span]] $\vee \left\{{M_i: i \in I}\right\}$ of the $M_i$. Then for all $h \in H$, $\displaystyle \sum \left\{{P_i h: i \in I}\right\} = Ph$, where $\displaystyle \sum$ denotes a [[Definition:Generalized Sum|generalized sum]].	1
Follows directly from the definition of [[Definition:Change of Basis Matrix|change of basis matrix]]. {{qed}} [[Category:Change of Basis]] 3ob4isdrjumnvvaa4nbh17c8srbaahz	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\left\{\left\langle {M_i,+_i,\circ_i} \right\rangle\right\}_{i \in I}$ be a [[Definition:Indexed Family|family]] of $R$-[[Definition:Module|modules]]. Let $\left\langle{M, +, \circ}\right\rangle$ be their [[Definition:Module Direct Product|direct product]]. Then $\left\langle{M, +, \circ}\right\rangle$ is a [[Definition:Module|module]].	1
Let $\mathbf x = a \mathbf 1 + b \mathbf i + c \mathbf j + d \mathbf k$ be a [[Definition:Quaternion|quaternion]]. Let $\overline {\mathbf x}$ be the [[Definition:Conjugate Quaternion|conjugate]] of $\mathbf x$. The [[Definition:Field Norm of Quaternion|field norm]] of $\mathbf x$: :$\map n {\mathbf x} := \size {\mathbf x \overline {\mathbf x} }$ is not a [[Definition:Norm on Division Ring|norm]] in the [[Definition:Abstract Algebra|abstract algebraic]] context of a [[Definition:Division Ring|division ring]].	1
:$(1): \quad M^\circ$ is an $\paren {n - m}$-dimensional subspace of $G^*$, and $M^{\circ \circ} = \map J M$ Let $\sequence {a_n}$ be an [[Definition:Ordered Basis|ordered basis]] of $G$ such that $\sequence {a_m}$ is an ordered basis of $M$. Let $\sequence {a'_n}$ be the [[Definition:Ordered Dual Basis|ordered dual basis]] of $G^*$. Let $\ds t' = \sum_{k \mathop = 1}^n \lambda_k a'_k \in M^\circ$. Then: {{begin-eqn}} {{eqn | lo= \forall j \in \closedint 1 m: | l = \lambda_j | r = \sum_{k \mathop = 1}^n \lambda_k a'_k \paren {a_j} | c = }} {{eqn | r = \paren {\sum_{k \mathop = 1}^n \lambda_k a'_k} \paren {a_j} | c = }} {{eqn | r = t' \paren {a_j} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} So $t'$ is a [[Definition:Linear Combination|linear combination]] of $\set {a'_k: m + 1 \le k \le n}$. But $a'_k$ clearly belongs to $M^\circ$ for each $k \in \closedint {m + 1} n$. Therefore $M^\circ$ has dimension $n - m$. {{qed|lemma}} When we apply this result to $M^\circ$ instead of $M$, it is seen that the [[Definition:Annihilator on Algebraic Dual|annihilator]] $M^{\circ \circ}$ of $M^\circ$ has dimension $n - \paren {n - m} = m$. But clearly $\map J M \subseteq M^{\circ \circ}$. As $J$ is an [[Definition:Vector Space Isomorphism|isomorphism]], $\map J M$ has dimension $m$. So by [[Dimension of Proper Subspace is Less Than its Superspace]], $\map J M = M^{\circ \circ}$. As a consequence, $\map {J^{-1} } {M^{\circ \circ} } = M$. Hence the result: $M^{\circ \circ} = \map J M$ {{qed|lemma}} :$(2) \quad \map {J^{-1} } {N^\circ}$ is an $\paren {n - p}$-dimensional subspace of $G$ If $N$ is a $p$-dimensional subspace of $G$, then $N^\circ$ and hence also $\map {J^{-1} } {N^\circ}$ have dimension $n - p$ by what has just been proved. {{qed|lemma}} By definition: $\paren {\map {J^{-1} } {N^\circ} }^\circ = \set {z' \in G: \forall x \in G: \forall t' \in N: \map {t'} x = 0: \map {z'} x = 0}$ Thus $N \subseteq \paren {\map {J^{-1} } {N^\circ} }^\circ$. But as $\paren {\map {J^{-1} } {N^\circ} }^\circ$ has dimension $n - \paren {n - p} = p$, it follows that $N = \paren {\map {J^{-1} } {N^\circ} }^\circ$ by [[Dimension of Proper Subspace is Less Than its Superspace]]. {{explain|Where in the above assertion is $(3)$ proved?}} :$(4) \quad$ Its inverse is the bijection $N \to \map {J^{-1} } {N^\circ}$. The final assertion follows by the definition of an [[Definition:Inverse Mapping/Definition 2|inverse mapping]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \frac \d {\d x} \left({\mathbf r \left({x}\right) \cdot \mathbf q \left({x}\right) }\right) | r = \frac \d {\d x} \left({\sum_{i \mathop = 1}^n r_i \left({x}\right) q_i \left({x}\right)}\right) | c = {{Defof|Dot Product}} }} {{eqn | r = \sum_{i \mathop = 1}^n \frac \d {\d x} \left({r_i \left({x}\right) q_i \left({x}\right)}\right) | c = [[Sum Rule for Derivatives/General Result|Sum Rule for Derivatives]] }} {{eqn | r = \sum_{i \mathop = 1}^n \left({ {r_i}' \left({x}\right) q_i \left({x}\right) + r_i \left({x}\right) {q_i}' \left({x}\right)}\right) | c = [[Product Rule for Derivatives]] }} {{eqn | r = \sum_{i \mathop = 1}^n {r_i}' \left({x}\right) q_i \left({x}\right) + \sum_{i \mathop = 1}^n r_i \left({x}\right) {q_i}' \left({x}\right) | c = [[Summation is Linear]] }} {{eqn | r = \mathbf r'\left({x}\right) \cdot \mathbf q \left({x}\right) + \mathbf r \left({x}\right) \cdot \mathbf q' \left({x}\right) | c = {{Defof|Dot Product}} }} {{end-eqn}} {{qed}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf B$ be the [[Definition:Square Matrix|matrix]] resulting from one [[Definition:Row of Matrix|row]] of $\mathbf A$ having been multiplied by a [[Definition:Constant|constant]] $c$. Then: :$\map \det {\mathbf B} = c \map \det {\mathbf A}$ That is, multiplying one [[Definition:Row of Matrix|row]] of a [[Definition:Square Matrix|square matrix]] by a [[Definition:Constant|constant]] multiplies its [[Definition:Determinant of Matrix|determinant]] by that [[Definition:Constant|constant]].	1
Let $E$ be a [[Definition:Vector Space|vector space]] of $n$ [[Definition:Dimension of Vector Space|dimensions]]. Let $H$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $E$. {{:Linearly Independent Set is Contained in some Basis}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H}\right)$ be a [[Definition:Self-Adjoint Operator|self-adjoint operator]]. Then all [[Definition:Eigenvalue|eigenvalues]] of $A$ are [[Definition:Real Number|real]].	1
Proof by [[Principle of Mathematical Induction|induction]]: Let the [[Vandermonde Determinant]] be presented in the form as defined by [[Vandermonde Determinant#Mirsky|Mirsky]]: Let $V_n = \begin{vmatrix} a_1^{n-1} & a_1^{n-2} & \cdots & a_1 & 1 \\ a_2^{n-1} & a_2^{n-2} & \cdots & a_2 & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_n^{n-1} & a_n^{n-2} & \cdots & a_n & 1 \\ \end{vmatrix}$ For all $n \in \N_{>0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({a_i - a_j}\right)$ $P(1)$ is true, as this just says $\begin{vmatrix} 1 \end{vmatrix} = 1$. === Basis for the Induction === $P(2)$ holds, as it is the case: : $V_2 = \begin{vmatrix} a_1 & 1 \\ a_2 & 1 \end{vmatrix}$ which evaluates to $V_2 = a_1 - a_2$. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: : $\displaystyle V_k = \prod_{1 \mathop \le i \mathop < j \mathop \le k} \left({a_i - a_j}\right)$ Then we need to show: : $\displaystyle V_{k+1} = \prod_{1 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: Take the determinant: :$V_{k+1} = \begin{vmatrix} x^k & x^{k-1} & \cdots & x^2 & x & 1 \\ a_2^k & a_2^{k-1} & \cdots & a_2^2 & a_2 & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ a_{k+1}^k & a_{k+1}^{k-1} & \cdots & a_{k+1}^2 & a_{k+1} & 1 \end{vmatrix}$ Let the [[Expansion Theorem for Determinants]] be used to expand $V_n$ in terms of the first row It can be seen that it is a [[Definition:Real Polynomial Function|polynomial]] in $x$ whose [[Definition:Degree (Polynomial)|degree]] is no greater than $k$. Let that polynomial be denoted $f \left({x}\right)$. Let any $a_r$ be substituted for $x$ in the determinant. Then two of its rows will be the same. From [[Square Matrix with Duplicate Rows has Zero Determinant]], the value of such a determinant will be $0$. Such a substitution in the determinant is equivalent to substituting $a_r$ for $x$ in $f \left({x}\right)$. Thus it follows that: :$f \left({a_2}\right) = f \left({a_3}\right) = \ldots = f \left({a_{k+1}}\right) = 0$ So $f \left({x}\right)$ is divisible by each of the factors $x - a_2, x - a_3, \ldots, x - a_{k+1}$. All these factors are distinct, otherwise the original determinant is zero. So: : $f \left({x}\right) = C \left({x - a_2}\right) \left({x - a_3}\right) \cdots \left({x - a_k}\right) \left({x - a_{k+1}}\right)$ As the degree of $f \left({x}\right)$ is no greater than $k$, it follows that $C$ is independent of $x$. From the [[Expansion Theorem for Determinants]], the coefficient of $x^k$ is: : $\begin{vmatrix} a_2^{k-1} & \cdots & a_2^2 & a_2 & 1 \\ \vdots & \ddots & \vdots & \vdots & \vdots \\ a_{k+1}^{k-1} & \cdots & a_{k+1}^2 & a_{k+1} & 1 \end{vmatrix}$. By the [[Vandermonde Determinant/Proof 2#Induction Hypothesis|induction hypothesis]], this is equal to: : $\displaystyle \prod_{2 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ This must be our value of $C$. So we have: : $\displaystyle f \left({x}\right) = \left({x - a_2}\right) \left({x - a_3}\right) \cdots \left({x - a_k}\right) \left({x - a_{k+1}}\right) \prod_{2 \mathop \le i \mathop < j \mathop \le k+1} \left({a_i - a_j}\right)$ Substituting $a_1$ for $x$, we retrieve the proposition $P \left({k+1}\right)$. So $P \left({k}\right) \implies P \left({k+1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: : $\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({a_i - a_j}\right)$ {{qed}}	1
:$\ds \paren {\sum_{k \mathop = 1}^m \lambda_k} \circ x = \sum_{k \mathop = 1}^m \paren {\lambda_k \circ x}$	1
From [[Additive Function is Linear for Rational Factors]]: :$f(q) = q f(1)$ for all $q\in\Q$. Without loss of generality, let :$f(q) = q$ for all $q\in\Q$. Since $f$ is not linear, let $\alpha\in\R\setminus\Q$ be such that :$f(\alpha) = \alpha+\delta$ for some $\delta \neq 0$. Consider an arbitrary nonempty circle in the plane. Let its centre be :$(x,y)$ where $x\neq y$ and $x,y\in\Q$ and its radius be $r>0$. We will show how to find a point of the graph of $f$ inside this circle. As $x\neq y$ and $r$ can be arbitrarily small, this will prove the theorem. Since $\delta\neq0$, let :$\beta = \frac{y - x}{\delta}$ Since $x\neq y$, :$\beta\neq0$. As [[Rationals are Everywhere Dense in Topological Space of Reals]], there exists a rational number $b\neq 0$ such that: :$\left\vert \beta - b \right\vert < \frac{r}{2 \left\vert\delta\right\vert}$ As [[Rationals are Everywhere Dense in Topological Space of Reals]], there also exists a rational number $a$ such that: :$\left\vert \alpha - a \right\vert < \frac{r}{2\left\vert b\right\vert} $ Now put: :$X = x + b (\alpha - a) \ $ :$ Y = f(X) \ $ Then: :$|X-x| = |b (\alpha - a)| < \frac{r}{2}$ so $X$ is in the circle. Then: {{begin-eqn}} {{eqn|l= Y |r=f(x + b (\alpha - a)) |c=Definition of $Y$ and $X$ }} {{eqn|r=f(x) + f(b \alpha) - f( b a) |c=[[Definition:Cauchy Functional Equation|Cauchy functional equation]] }} {{eqn|r=x + b f(\alpha) - b f(a) |c=[[Additive Function is Linear for Rational Factors]] }} {{eqn|r=y - \delta \beta + b f(\alpha) - b f(a) |c=Definition of $y$ }} {{eqn|r=y - \delta \beta + b (\alpha + \delta) - b a |c=[[Additive Function is Linear for Rational Factors]] }} {{eqn|r=y + b (\alpha - a) - \delta (\beta - b)}} {{end-eqn}} Therefore :$|Y-y| = |b (\alpha - a) - \delta (\beta - b)| \le |b (\alpha - a)| + |\delta (\beta - b)| \le r$ so $Y$ is in the circle as well. Hence the point $(X, Y)$ is inside the circle. {{qed}}	1
Every [[Definition:Hermitian Matrix|Hermitian matrix]] has [[Definition:Eigenvalue|eigenvalues]] which are all [[Definition:Real Number|real numbers]].	1
Let $A$ be a commutative ring. {{explain|Presumably a [[Definition:Commutative and Unitary Ring]] is actually required here?}} Let $I_1, \ldots, I_k$ for some $k \ge 1$ be pairwise coprime ideals in $A$, that is: : $\forall i \ne j: I_i + I_j = A$ Then there is an isomorphism of rings: :$A / \left({I_1 \cap \ldots \cap I_k}\right) \to A / I_1 \times \cdots \times A / I_k$ which is induced by the [[Definition:Ring Homomorphism|ring homomorphism]] $\phi: A \to A / I_1 \times \cdots \times A / I_k$ defined as: :$\phi \left({x}\right) = \left({x + I_1, \ldots, x + I_k}\right)$ which passes through the quotient. {{explain|what is meant by "passes through the quotient"?}}	1
Let $\tuple {k_1, k_2, \ldots, k_m}$ be an [[Definition:Ordered Tuple|ordered $m$-tuple]] of [[Definition:Integer|integers]]. Let $\map \epsilon {k_1, k_2, \ldots, k_m}$ denote the [[Definition:Sign of Ordered Tuple|sign]] of $\tuple {k_1, k_2, \ldots, k_m}$. Let $\tuple {l_1, l_2, \ldots, l_m}$ be the same as $\tuple {k_1, k_2, \ldots, k_m}$ except for $k_i$ and $k_j$ having been transposed. Then from [[Transposition is of Odd Parity]]: :$\map \epsilon {l_1, l_2, \ldots, l_m} = -\map \epsilon {k_1, k_2, \ldots, k_m}$ Let $\tuple {j_1, j_2, \ldots, j_m}$ be the same as $\tuple {k_1, k_2, \ldots, k_m}$ by arranged into non-decreasing order. That is: :$j_1 \le j_2 \le \cdots \le j_m$ Then it follows that: :$\map \det {\mathbf B_{k_1 \cdots k_m} } = \map \epsilon {k_1, k_2, \ldots, k_m} \map \det {\mathbf B_{j_1 \cdots j_m} }$ Hence: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A \mathbf B} | r = \sum_{1 \mathop \le l_1, \mathop \ldots \mathop , l_m \mathop \le m} \map \epsilon {l_1, \ldots, l_m} \paren {\sum_{k \mathop = 1}^n a_{1 k} b_{k l_1} } \cdots \paren {\sum_{k \mathop = 1}^n a_{m k} b_{k l_m} } | c = {{Defof|Determinant of Matrix}} }} {{eqn | r = \sum_{1 \mathop \le k_1, \mathop \ldots \mathop , k_m \mathop \le n} a_{1 k_1} \cdots a_{m k_m} \sum_{1 \mathop \le l_1, \mathop \ldots \mathop , l_m \mathop \le m} \map \epsilon {l_1, \ldots, l_m} b_{k_1 l_1} \cdots b_{k_m l_m} | c = }} {{eqn | r = \sum_{1 \mathop \le k_1, \mathop \ldots \mathop , k_m \mathop \le n} a_{1 k_1} \cdots a_{m k_m} \map \det {\mathbf B_{k_1 \cdots k_m} } | c = }} {{eqn | r = \sum_{1 \mathop \le k_1, \mathop \ldots \mathop , k_m \mathop \le n} a_{1 k_1} \map \epsilon {k_1, \ldots, k_m} \cdots a_{m k_m} \map \det {\mathbf B_{j_1 \cdots j_m} } | c = }} {{eqn | r = \sum_{1 \mathop \le j_1 \mathop \le j_2 \mathop \le \cdots \mathop \le j_m \le n} \map \det {\mathbf A_{j_1 \cdots j_m} } \map \det {\mathbf B_{j_1 \cdots j_m} } | c = }} {{end-eqn}} If two $j$s are equal: :$\map \det {\mathbf A_{j_1 \cdots j_m} } = 0$ {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] with [[Definition:Inner Product Norm|inner product norm]] $\norm {\, \cdot \,}$. Let $f_1, \ldots, f_n \in H$ be pairwise [[Definition:Orthogonal (Hilbert Space)|orthogonal]]. Then: :$\displaystyle \norm {\sum_{i \mathop = 1}^n f_i}^2 = \sum_{i \mathop = 1}^n \norm {f_i}^2$	1
The [[Definition:First Fundamental Form|first fundamental form]] for the element of [[Definition:Arc Length|arc length]] on a [[Definition:Curve|curved]] [[Definition:Surface|surface]] is given by: :$\d s^2 = E \rd u^2 + 2 F \d u \rd v + G \rd v^2$ where $E$, $F$ and $G$ are the [[Definition:Coefficient|coefficients]] of the [[Definition:First Fundamental Form|first fundamental form]].	1
Let $\sequence {x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $\struct {\map {\CC^1} I, \norm {\, \cdot \,}_{1, \infty} }$: :$\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall m, n \in \N: m, n \ge N: \norm {x_n - x_m}_{1, \infty} < \epsilon$ === $\sequence {x_n}_{n \mathop \in \N}$ converges in $\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$ to $x$ === $\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall m, n \in \N: m, n \ge N$ we have that: {{begin-eqn}} {{eqn | l = \norm {x_n - x_m}_\infty | o = \le | r = \norm {x_n - x_m}_\infty + \norm {x'_n - x'_m}_\infty }} {{eqn | r = \norm {x_n - x_m}_{1, \infty} | c = {{defof|C^k Norm}} }} {{eqn | o = < | r = \epsilon | c = {{defof|Cauchy Sequence}} }} {{end-eqn}} Hence, $\sequence {x_n}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$. We have that [[Space of Continuous on Closed Interval Real-Valued Functions with Supremum Norm form Banach Space|$\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$ is a Banach space]]. Therefore, $\sequence {x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]]. Denote this [[Definition:Limit of Sequence in Normed Vector Space|limit]] as $x$: :$\ds \lim_{n \mathop \to \infty} x_n = x$ where $x \in \map \CC I$. {{qed|lemma}} === $\sequence {x'_n}_{n \mathop \in \N}$ converges in $\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$ to $y$ === $\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall m, n \in \N: m, n \ge N$ we have that: {{begin-eqn}} {{eqn | l = \norm {x'_n - x'_m}_\infty | o = \le | r = \norm {x_n - x_m}_\infty + \norm {x'_n - x'_m}_\infty }} {{eqn | r = \norm {x_n - x_m}_{1, \infty} | c = {{defof|C^k Norm}} }} {{end-eqn}} Hence, $\sequence {x'_n}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$. We have that [[Space of Continuous on Closed Interval Real-Valued Functions with Supremum Norm form Banach Space|$\struct {\map \CC I, \norm {\, \cdot \,}_\infty}$ is a Banach space]]. Therefore, $\sequence {x'_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]]. Denote this [[Definition:Limit of Sequence in Normed Vector Space|limit]] as $y$: :$\ds \lim_{n \mathop \to \infty} x'_n = y$ where $y \in \map \CC I$. {{qed|lemma}} === $x$ is differentiable, and $y' = x$ === Let $t \in I$. By [[Fundamental Theorem of Calculus|fundamental theorem of calculus]] $\paren \star$: :$\ds \map {x_n} t - \map {x_n} a = \int_a^t \map {x'_n} \tau \rd \tau$ Consider the following difference: :$\ds \size {\map {x_n} t - \map {x_n} a - \int_a^t \map y \tau \rd \tau}$ Then: {{begin-eqn}} {{eqn | l = \size {\map {x_n} t - \map {x_n} a - \int_a^t \map y \tau \rd \tau} | r = \size {\int_a^t \paren {\map {x'_n} \tau - \map y \tau} \rd \tau} | c = Substitution of $\paren \star$ }} {{eqn | o = \le | r = \int_a^t \size {\map {x'_n} \tau - \map y \tau} \rd \tau }} {{eqn | o = \le | r = \int_a^t \sup_{T \mathop \in I} \size {\map {x'_n} T - \map y T} \rd \tau }} {{eqn | o = \le | r = \norm {x'_n - y}_\infty \paren {t - a} | c = {{defof|Supremum Norm}} }} {{end-eqn}} [[Definition:Absolute Value|Absolute value]] is a [[Definition:Continuous Real Function|continuous function]]. Therefore, we can take the [[Limit of Composite Function|limit of the composite function]]. Passing the limit, as $n$ goes to infinity, gives: {{begin-eqn}} {{eqn | l = \size {\map x t - \map x a - \int_a^t \map y \tau \rd \tau} | o = \le | r = \norm {y - y}_\infty \paren {t - a} | c = $\ds \sequence {x'_n}_{n \mathop \in \N}$ converges to $y$ }} {{eqn | r = 0 }} {{end-eqn}} In other words: :$\ds \map x t = \map x a + \int_a^t \map y \tau \rd \tau$ By [[Fundamental Theorem of Calculus|fundamental theorem of calculus]], $x$ is a [[Definition:Primitive of Real Function|primitive]] of $y$. Thus, $x' = y \in \map \CC I$. Hence, $x \in \map {\CC^1} I$. {{qed|lemma}} === $\sequence {x_n}_{n \mathop \in \N}$ converges in $\struct {\map {\CC^1} I, \norm {\, \cdot \,}_{1, \infty} }$ to $x$ === Let $\epsilon \in \R_{> 0}$. Let $N \in \N : \forall m, n \in \N : m, n > N \implies \norm { x_n - x_m}_{1, \infty} < \epsilon$. Then for all $t \in I$ we have that: {{begin-eqn}} {{eqn | l = \size {\map {x_n} t - \map {x_m} t} + \size {\map {x'_n} t - \map {x'_m} t} | o = \le | r = \sup_{t \mathop \in I} \size {\map {x_n} t - \map {x_m} t}+ \sup_{t \mathop \in I} \size {\map {x'_n} t - \map {x'_m} t} }} {{eqn | r = \norm {x_n - x_m}_\infty + \norm {x'_n - x'_m}_\infty | c = {{defof|Supremum Norm}} }} {{eqn | r = \norm {x_n - x_m}_{1, \infty} | c = {{defof|C^k Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} [[Definition:Absolute Value|Absolute value]] is a [[Definition:Continuous Real Function|continuous function]]. Therefore, we can take the [[Limit of Composite Function|limit of the composite function]]. Letting $m$ go to [[Definition:Infinity|infinity]], it follows that: :$\forall n > N : \size {\map {x_n} t - \map x t} + \size {\map {x'_n} t - \map {x'} t} < \epsilon$ Since $t \in I$ was arbitrary: {{begin-eqn}} {{eqn | l = \size {\map {x_n} t - \map x t} + \size {\map {x'_n} t - \map {x'} t} | o = \le | r = \sup_{t \mathop \in I} \size {\map {x_n} t - \map x t} + \sup_{t \mathop \in I} \size {\map {x'_n} t - \map {x'} t} }} {{eqn | r = \norm {x_n - x}_{1, \infty} | c = {{defof|Supremum Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} {{qed}}	1
Let $D$ be a [[Definition:Diagonal Matrix|diagonal matrix]]. Then $D$ is [[Definition:Symmetric Matrix|symmetric]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {R', \norm {\, \cdot \,}' }$ be a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] of $\struct {R, \norm {\, \cdot \,} }$ Then: :$R$ is a [[Definition:Field (Abstract Algebra)|field]] {{iff}} $R'$ is a [[Definition:Field (Abstract Algebra)|field]].	1
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $S$ be the [[Definition:Set|set]] of all [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\alpha$ denote the [[Definition:Relation|relation]] defined on $S$ by: :$\forall \mathbf A, \mathbf B \in S: \mathbf A \mathrel \alpha \mathbf B \iff \exists r, s \in \N: \mathbf A^r = \mathbf B^s$ Then $\alpha$ is an [[Definition:Equivalence Relation|equivalence relation]].	1
=== Proof of Existence === Proved in [[Matrix is Row Equivalent to Reduced Echelon Matrix]]. {{qed|lemma}} === Proof of Uniqueness === {{ProofWanted}} [[Category:Echelon Matrices]] tqbd2jjcoeedgo03370m94xifgo20hg	1
Let $\mathbf J = \sqbrk 1_n$ be a [[Definition:Square Ones Matrix|square ones matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Then $\mathbf J^2 = n \mathbf J$. That is: :$\begin{bmatrix} 1 & 1 & \cdots & 1 \\ 1 & 1 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 1 \end{bmatrix}^2 = \begin{bmatrix} n & n & \cdots & n \\ n & n & \cdots & n \\ \vdots & \vdots & \ddots & \vdots \\ n & n & \cdots & n \end{bmatrix}$	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be [[Definition:R-Algebraic Structure|$R$-algebraic structures]]. Let $\phi: S \to T$ be a [[Definition:Mapping|mapping]]. Then $\phi$ is an '''$R$-algebraic structure homomorphism''' {{iff}}: :$(1): \quad \forall k \in \closedint 1 n: \forall x, y \in S: \map \phi {x \ast_k y} = \map \phi x \odot_k \map \phi y$ :$(2): \quad \forall x \in S: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$ where $\closedint 1 n = \set {1, 2, \ldots, n}$ denotes an [[Definition:Integer Interval|integer interval]]. Note that this definition also applies to [[Definition:Module|modules]] and [[Definition:Vector Space|vector spaces]].	1
Recall that the [[Definition:P-adic Norm|$p$-adic norm]] is defined as: :$\forall q \in \Q: \norm r_p := \begin{cases} 0 & : r = 0 \\ p^{- k} & : r \ne 0 \end{cases}$ where: :$\displaystyle r = p^k \frac m n$ and: :$k, n \in \Z, m \in \Z_{\ne 0} : p \nmid m, n$ where $\nmid$ stands for [[Symbols:Number Theory/Does Not Divide|"does not divide"]]. We must show the following hold for all $r_1$, $r_2 \in \Q$: {{begin-axiom}} {{axiom | n = \text N 1 | q = \forall r \in \Q | ml= \norm r_p = 0 | mo= \iff | mr= x = 0 }} {{axiom | n = \text N 2 | q = \forall r_1, r_2 \in \Q | ml= \norm {r_1 r_2} | mo= = | mr= \norm {r_1}_p \times \norm {r_2}_p }} {{axiom | n = \text N 3 | q = \forall r_1, r_2 \in \Q | ml= \norm {r_1 + r_2}_p | mo= \le | mr= \norm {r_1}_p + \norm {r_2}_p }} {{end-axiom}} === Norm Axiom $(\text N 1)$ === Let $r \in \Q : r \ne 0$. Let $k, m\in \Z, n \in \Z_{\ne 0} : p \nmid m, n$. Suppose $r = 0$. By [[Definition:P-adic Norm|definition]]: :$\norm {r}_p = 0$ Suppose $\displaystyle r = p^k \frac m n \ne 0$ By [[Definition:P-adic Norm|definition]]: :$\displaystyle \norm {r}_p = \frac 1 {p^k} > 0$ Suppose $\norm r_p = 0$. By [[Definition:P-adic Norm|definition]]: :$r = 0$ {{qed|lemma}} === Norm Axiom $(\text N 2)$ === Suppose $r_1 = 0$ or $r_2 = 0$. From [[Definition:Norm Axioms|axiom $(\text N 1)$]], $\norm {r_1}_p = 0$ or $\norm {r_2}_p = 0$. Suppose $r_1 \ne 0 \ne r_2$. Let $k_1, k_2, m_1, m_2 \in \Z, n_1, n_2 \in \Z_{\ne 0} : p \nmid n_1, n_2, m_1, m_2$ Let $\displaystyle r_1 = p^{k_1} \frac {m_1} {n_1}, r_2 = p^{k_2} \frac {m_2} {n_2}$ Then: :$\displaystyle r_1 r_2 = p^{k_1 + k_2} \frac {m_1 m_2}{n_1 n_2}$ We have that $p \nmid m_1$, $p \nmid m_2$. Since $p$ is [[Definition:Prime Number|prime]]: :$p \nmid m_1 m_2$. Similarly: :$p \nmid n_1 n_2$. Therefore: {{begin-eqn}} {{eqn| l = \norm {r_1 r_2}_p | r = p^{- \paren {k_1 + k_2} } }} {{eqn| r = p^{-k_1} p^{-k_2} }} {{eqn| r = \norm {r_1}_p \norm {r_2}_p }} {{end-eqn}} {{qed|lemma}} === Norm Axiom $(\text N 3)$ === Suppose one of the following is true: :$r_1 = 0$ :$r_2 = 0$ :$r_1 + r_2 = 0$ Then the result is straightforward. Suppose $r_1 \ne 0$, $r_2 \ne 0$, $r_1 + r_2 \ne 0$. Let $\displaystyle r_1 = p^{k_1} \frac {m_1}{n_1}, r_2 = p^{k_2} \frac{m_2}{n_2}$ where: :$\displaystyle k_1, k_2, m_1, m_2 \in \Z, n_1, n_2 \in \Z_{\ne 0} : p \nmid m_1, m_2, n_1, n_2$ Then: {{begin-eqn}} {{eqn | l = r_1 + r_2 | r = \frac {p^{k_1} m_1 n_2 + p^{k_2} m_2 n_1} {n_1 n_2} }} {{eqn | r = p^{\map \min {k_1, k_2} } \frac {p^{k_1 \mathop - \map \min {k_1, k_2} } m_1 n_2 + p^{k_2 \mathop - \map \min {k_1, k_2} } n_1 m_2}{n_1 n_2} | c = {{Defof|Min Operation}} }} {{eqn | r = p^{\map \min {k_1, k_2} } \frac {\tilde m}{n_1 n_2} | c = $\displaystyle \tilde m := p^{k_1 \mathop - \map \min {k_1, k_2} } m_1 n_2 + p^{k_2 \mathop - \map \min {k_1, k_2} } n_1 m_2$ }} {{end-eqn}} By [[Fundamental Theorem of Arithmetic]]: :$\exists ! \tilde k \in \Z_{\ge 0} : \exists m \in \Z : p \nmid m : \tilde m = p^{\tilde k} m$ Obviously, $p \nmid n_1 n_2$ Hence: {{begin-eqn}} {{eqn | l = \norm {r_1 + r_2}_p | r = \frac 1 {p^{\tilde k + \map \min {k_1, k_2} } } }} {{eqn | o = \le | r = \frac 1 {p^{\map \min {k_1, k_2} } } }} {{eqn | r = \map \max {p^{-k_1}, p^{-k_2} } | c = {{Defof|Max Operation}} }} {{eqn | r = \map \max {\norm {r_1}_p, \norm {r_1}_p} }} {{eqn | o = \le | r = \map \max {\norm {r_1}_p, \norm {r_2}_p} + \map \min {\norm {r_1}_p, \norm {r_2}_p} }} {{eqn | r = \norm {r_1}_p + \norm {r_2}_p }} {{end-eqn}} {{qed|lemma}} All [[Definition:Norm Axioms|norm axioms]] are seen to be satisfied. Hence the result. {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf a | r = \size {\mathbf a} \hat {\mathbf a} | c = [[Vector Quantity as Scalar Product of Unit Vector Quantity]] }} {{eqn | ll= \leadsto | l = m \mathbf a | r = m \paren {\size {\mathbf a} \hat {\mathbf a} } | c = }} {{end-eqn}} Then: {{finish|hard to prove something trivial}} {{qed}}	1
Let $x \in \R$. Let $\epsilon \in \R_{\mathop > 0}$ Either $x \in \Q$ or $x \in \R \setminus \Q$. Suppose $x \in \R \setminus \Q$. Let $y := x$. Then: :$\size {x - y} < \epsilon$ Suppose $x \in \Q$. Let $\displaystyle n \in \N : n > \frac {\sqrt 2} \epsilon$ Let $\displaystyle y := x + \frac {\sqrt 2} n$ Then $y \in \R \setminus \Q$. Furthermore: {{begin-eqn}} {{eqn | l = \size {x - y} | r = \size {\frac {\sqrt 2} n} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} In both cases $x$ was arbitrary. Hence: :$\forall x \in \R : \exists \epsilon \in \R_{\mathop > 0} : \exists y \in \R \setminus \Q : \size {x - y} < \epsilon$ By [[Definition:Definition|definition]], $\R \setminus \Q$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] in $\R$. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of $\map {\MM_R} {m, n}$. Let $\sqbrk c_{m n} = \sqbrk a_{m n} + \sqbrk b_{m n}$. By definition of [[Definition:Matrix Entrywise Addition|matrix entrywise addition]]: :$\forall i \in \closedint 1 m, j \in \closedint 1 n: a_{i j} + b_{i j} = c_{i j}$ By {{Ring-axiom|A0}}, $R$ is [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Ring Addition|addition]]. Hence: :$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} \in R$ From the definition of [[Definition:Matrix Entrywise Addition|matrix entrywise addition]], $\sqbrk c_{m n}$ has the same [[Definition:Order of Matrix|order]] as both $\sqbrk a_{m n}$ and $\sqbrk b_{m n}$. Thus it follows that: :$\sqbrk c_{m n} \in \map {\MM_R} {m, n}$ Thus $\struct {\map {\MM_R} {m, n}, +}$, as it is defined, is [[Definition:Closure (Abstract Algebra)|closed]]. {{qed}}	1
Given $\epsilon > 0$: By the definition of a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] then: :$\exists N': \forall n, m > N', \norm {y_n - y_m} < \epsilon$ Hence $\forall n, m > \paren {N' + N}$: {{begin-eqn}} {{eqn | l = \norm {x_n - x_m } | r = \norm {y_{n - N} - y_{m - N} } | c = $n, m > N$ }} {{eqn | o = < | r = \epsilon | c = $n - N, m - N > N'$ }} {{end-eqn}} The result follows. {{qed}}	1
By definition, $\mathbf P \mathbf Q$ is a [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]] {{iff}} it is an [[Definition:Orthogonal Matrix|orthogonal matrix]] with a [[Definition:Determinant of Matrix|determinant]] of $1$. From [[Product of Orthogonal Matrices is Orthogonal Matrix]], $\mathbf P \mathbf Q$ is an [[Definition:Orthogonal Matrix|orthogonal matrix]]. By definition, $\mathbf P$ and $\mathbf Q$ both have a determinant of $1$. From [[Determinant of Matrix Product]]: :$\det \left({\mathbf P \mathbf Q}\right) = \det \left({\mathbf P}\right) \det \left({\mathbf Q}\right)$ Thus: :$\det \left({\mathbf P \mathbf Q}\right) = 1$ Hence the result. {{qed}} [[Category:Orthogonal Matrices]] renl7fwzyrwo2rltv5hg4vjq8pbpvku	1
Let $A$ be a [[Definition:Non-Trivial Ring|non-trivial]] [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Then its [[Definition:Spectrum of Ring|prime spectrum]] is [[Definition:Non-Empty Set|non-empty]]: :$\Spec A \ne \O$	1
Let $\mathbb J = \set {x \in \R: a < x < b}$ be an [[Definition:Open Real Interval|open interval]] of the [[Definition:Real Number Line|real number line]] $\R$. Let $\map {\CC^{\paren m} } {\mathbb J}$ be the set of all [[Definition:Continuous Real Function|continuous real functions]] on $\mathbb J$ in [[Definition:Differentiability Class|differentiability class]] $m$. Then $\struct {\map {\CC^{\paren m} } {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of the [[Definition:Vector Space|$\R$-vector space]] $\struct {\R^{\mathbb J}, +, \times}_\R$.	1
By [[Characterisation of Non-Archimedean Division Ring Norms]]: :$\norm{\,\cdot\,}$ is [[Definition:Archimedean Division Ring Norm|Archimedean]] $\iff \sup \set {\norm {n \cdot 1_R}: n \in \N_{\gt 0}} \gt 1$ By [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 2|Corollary 2]]: :$\sup \set {\norm{n \cdot 1_R}: n \in \N_{\gt 0} } \gt 1 \iff \sup \set {\norm {n \cdot 1_R}: n \in \N_{\gt 0}} = +\infty$ {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] of [[Definition:Countable|countable]] [[Definition:Dimension (Hilbert Space)|dimension]]. Let $A: H \to H$ be a [[Definition:Diagonalizable Operator|diagonalizable operator]]. Let $\left({\alpha_n}\right)_{n \in \N}$ be the [[Definition:Diagonalizable Operator#Value Set|value set]] of $A$, with respect to a suitable [[Definition:Basis (Hilbert Space)|basis]] $E = \left({e_n}\right)_{n \in \N}$ for $H$. Then $A$ is [[Definition:Compact Linear Operator|compact]] iff: :$\displaystyle \lim_{n \to \infty} \alpha_n = 0$	1
{{proof wanted}} {{Namedfor|David Hilbert|cat = Hilbert}} [[Category:Noetherian Rings]] [[Category:Commutative Algebra]] i278do2499undyyvkjephu7k1v4f1zm	1
Let $\sequence {x_n}$ be the [[Definition:Real Sequence|real sequence]] defined as $x_n = \paren {n + 1}^{1/n}$, using [[Definition:Real Exponential Function|exponentiation]]. Then $\sequence {x_n}$ [[Definition:Convergent Sequence|converges]] with a [[Definition:Limit of Sequence (Number Field)|limit]] of $1$.	1
:$\sequence {\lambda x_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] and $\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x_n} = \lambda l$	1
Let $\mathcal F$ be the [[Definition:Equivalence Class|equivalence class]] of $\mathcal A$ under the [[Definition:Relation|relation]] of [[Definition:Compatible Atlases|compatibility]]. By [[Compatibility of Atlases is Equivalence Relation]], this is indeed an [[Definition:Equivalence Relation|equivalence relation]]. By definition we have $\mathcal A \in \mathcal F$. By [[Relation Partitions Set iff Equivalence]], $\mathcal F$ is an [[Definition:Element|element]] of the [[Definition:Partition (Set Theory)|partition]] of [[Definition:Equivalence Class|equivalence classes]]. By definition, the [[Definition:Element|elements]] of a [[Definition:Partition (Set Theory)|partition]] are [[Definition:Pairwise Disjoint|pairwise disjoint]]. Therefore if $\mathcal G \ne \mathcal F$ is an [[Definition:Element|element]] of the [[Definition:Partition (Set Theory)|partition]], we must have: : $\mathcal A \notin \mathcal G$ Therefore $\mathcal A$ belongs to exactly one [[Definition:Differentiable Structure|differentiable structure]]. {{qed}} {{explain|Clarification is needed as to why this result should be categorised in [[:Category:Manifolds]].}} [[Category:Manifolds]] 1d6a163jw7q2sy421hs3o2njg2xs4ya	1
From the definition of the [[Definition:Combinatorial Matrix|combinatorial matrix]]: :$C_n = x \mathbf I_n + y \mathbf J_n$ where: :$\mathbf I_n$ is the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]] :$\mathbf J_n$ is the [[Definition:Square Ones Matrix|square ones matrix]] of [[Definition:Order of Square Matrix|order $n$]]. From [[Square of Ones Matrix]] we have $\mathbf J_n^2 = n \mathbf J_n$. Hence: {{begin-eqn}} {{eqn | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {x \mathbf I_n - y \mathbf J_n} | r = x^2 \mathbf I_n - n y^2 \mathbf J_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {x \mathbf I_n - y \mathbf J_n} + n y^2 \mathbf J_n | r = x^2 \mathbf I_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {x \mathbf I_n - y \mathbf J_n} + n y \mathbf I_n y \mathbf J_n | r = x^2 \mathbf I_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {x \mathbf I_n - y \mathbf J_n} + x n y \mathbf I_n + n y \mathbf I_n y \mathbf J_n | r = x^2 \mathbf I_n + x n y \mathbf I_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {x \mathbf I_n - y \mathbf J_n} + n y \mathbf I_n \paren {x \mathbf I_n + y \mathbf J_n} | r = x^2 \mathbf I_n + x n y \mathbf I_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {\paren {x + n y} \mathbf I_n - y \mathbf J_n} | r = x \paren {x + n y} \mathbf I_n | c = }} {{eqn | ll= \leadsto | l = \paren {x \mathbf I_n + y \mathbf J_n} \paren {\frac {\paren {x + n y} } {x \paren {x + n y} } \mathbf I_n - \frac y {x \paren {x + n y} } \mathbf J_n} | r = \mathbf I_n | c = }} {{end-eqn}} So we have found a specification for the matrix which, when multiplied by $C_n$, yields $\mathbf I_n$. By using the identities $\mathbf I_n = \sqbrk {\delta_{i j} }_n$ and $\mathbf J_n = \sqbrk 1_n$ we obtain the stated result: :$b_{i j} = \dfrac {-y + \delta_{i j} \paren {x + n y} } {x \paren {x + n y} }$ {{qed}}	1
Let $\struct {\Q, \tau_d}$ be the [[Definition:Rational Number Space|rational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Then $\struct {\Q, \tau_d}$ is [[Definition:Separable Space|separable]].	1
[[File:Curvature.png]] Let $X$ and $Y$ be two separate [[Definition:Face of Polyhedron|faces]] of a [[Definition:Polyhedron|polyhedron]] separated by the [[Definition:Edge of Polyhedron|edge]] $l$. Let $P$ be a [[Definition:Point|point]] on $X$ and let $Q$ be a [[Definition:Point|point]] on $Y$. The curvature inside an infinitesimal region $\delta a$ is given by the net angular displacement $\delta\theta$ a vector $v$ experiences as it is parallel transported along a closed path around $\delta a$. The curvature is then given by: :$R = \dfrac {\delta \theta} {\delta a}$ We must then prove that the vector $v$ experiences no net angular displacement as it is parallel transported from $P$ to $Q$ and back to $P$. The two open curves $r$ and $s$ make a closed curve. As the vector is parallel transported along the open curve $r$, it crosses the edge between the two faces $X$ and $Y$. In doing so, it gains a finite angular displacement $\delta\theta_1$. Then, when the vector is transported back along the open curve $s$, it gains another angular displacement $\delta\theta_2$. Notice that because it is not being transported the other way (from $Y$ to $X$), the new angular displacement will be: :$\delta\theta_2 = -\delta\theta_1$. The curvature inside the region $\delta a$ is therefore given by: {{begin-eqn}} {{eqn | l = R | r = \frac {\delta \theta_1 + \delta \theta_2} {\delta a} | c = }} {{eqn | r = \frac {\delta \theta_1 - \delta \theta_1} {\delta a} | c = }} {{eqn | r = \frac 0 {\delta a} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} The result follows. {{qed}} [[Category:Differential Geometry]] ceimnk356hrgednbl236ekkgzuqbxx2	1
=== (N1) : [[Definition:Positive Definite (Ring)|Positive Definiteness]] === {{begin-eqn}} {{eqn | lo= \forall x \in S: | l = \norm x_S | r = 0 }} {{eqn | ll= \leadstoandfrom | l = \norm x | r = 0 | c = Definition of $\norm x_S$ }} {{eqn | ll= \leadstoandfrom | l = x | r = 0 | c = [[Definition:Norm on Division Ring|Norm axiom (N1)]] }} {{end-eqn}} {{qed|lemma}} === (N2) : [[Definition:Multiplicative Function|Multiplicativity]] === {{begin-eqn}} {{eqn | lo= \forall x, y \in S: | l = \norm {x y}_S | r = \norm {x y} | c = Definition of $\norm {\, \cdot \,}_S$ }} {{eqn | r = \norm x \norm y | c = [[Definition:Norm on Division Ring|Norm axiom (N2)]] }} {{eqn | r = \norm x_S \norm y_S | c = Definition of $\norm {\, \cdot \,}_S$ }} {{end-eqn}} {{qed|lemma}} === (N3) : Triangle Inequality === {{begin-eqn}} {{eqn | lo= \forall x, y \in S: | l = \norm {x + y}_S | r = \norm {x + y} | c = Definition of $\norm {\, \cdot \,}_S$ }} {{eqn | o = \le | r = \norm x + \norm y | c = [[Definition:Norm on Division Ring|Norm axiom (N3)]] }} {{eqn | r = \norm x_S + \norm y_S | c = Definition of $\norm {\, \cdot \,}_S$ }} {{end-eqn}} {{qed}} [[Category:Division Rings]] [[Category:Norm Theory]] ky2s7b1nsucwi1zc3srsir3ym7gws58	1
:$\cmod z \ge \size {\map \Im z}$	1
Since $A$ is [[Definition:Normal Operator|normal]], we have by [[Kernel of Linear Transformation is Orthocomplement of Range of Adjoint/Corollary|Kernel of Linear Transformation is Orthocomplement of Range of Adjoint: Corollary]] that: :$\ker A = \left({\operatorname{ran} A}\right)^\perp$ and in particular, that: :$\ker A \subseteq \left({\operatorname{ran} A}\right)^\perp$ Now, by [[Orthocomplement of Subset of Orthocomplement is Superset]]: :$\operatorname{ran} A \subseteq \left({\ker A}\right)^\perp$ Applying this to the [[Definition:Normal Operator|normal operator]] $A - \lambda$, we find: :$\operatorname{ran} \left({A - \lambda}\right) \subseteq \left({\ker \left({A - \lambda}\right)}\right)^\perp$ {{MissingLinks|Link to result proving $A - \lambda$ normal (not up presently)}} We are now set up to prove that $\ker \left({A - \lambda}\right)$ is a [[Definition:Reducing Subspace|reducing subspace]] for $A$. Let $x \in \ker \left({A - \lambda}\right)$. Then: {{begin-eqn}} {{eqn|l = A x |r = \lambda x + \left({A - \lambda}\right) x }} {{eqn|r = \lambda x |c = Definition of [[Definition:Kernel of Linear Transformation|kernel]], $x \in \ker \left({A - \lambda}\right)$ }} {{end-eqn}} Therefore, $A \ker \left({A - \lambda}\right) \subseteq \ker \left({A - \lambda}\right)$; that is to say, $\ker \left({A - \lambda}\right)$ is [[Definition:Invariant Subspace|$A$-invariant]]. Now, let $x \in \left({\ker \left({A - \lambda}\right)}\right)^\perp$. Observe that: :$A x = \lambda x + \left({A - \lambda}\right) x$ Now $\left({A - \lambda}\right) x \in \operatorname{ran} \left({A - \lambda}\right)$, and by our derivation above, this means that: :$\left({A - \lambda}\right) x \in \left({\ker \left({A - \lambda}\right)}\right)^\perp$ In conclusion, since $\left({\ker \left({A - \lambda}\right)}\right)^\perp$ is a [[Definition:Linear Subspace|linear subspace]] of $H$, it follows that: :$\lambda x + \left({A - \lambda}\right) x \in \left({\ker \left({A - \lambda}\right)}\right)^\perp$ as desired. Hence both $\ker \left({A - \lambda}\right)$ and $\left({\ker \left({A - \lambda}\right)}\right)^\perp$ have been shown to be [[Definition:Invariant Subspace|$A$-invariant subspaces]] of $H$. That is, $\ker \left({A - \lambda}\right)$ is a [[Definition:Reducing Subspace|reducing subspace]] for $A$. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Every [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]] is [[Definition:Bounded Sequence in Normed Division Ring|bounded]].	1
A '''linear operator''' is a [[Definition:Linear Transformation|linear transformation]] from a [[Definition:Module|module]] into itself.	1
Let $\mathbf a = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}$, and $\mathbf b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}$. Then: {{begin-eqn}} {{eqn | l = \left\Vert{ \mathbf a \times \mathbf b }\right\Vert^2 | r = \left({ \mathbf a \times \mathbf b }\right) \cdot \left({ \mathbf a \times \mathbf b }\right) | c = {{Defof|Euclidean Norm}} }} {{eqn | r = \begin{bmatrix} a_2 b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} \cdot \begin{bmatrix} a_2 b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = a_2^2 b_3^2 + a_3^2 b_2^2 - 2a_2 a_3 b_2 b_3 + a_3^2 b_1^2 + a_1^2 b_3^2 - 2a_1 a_3 b_1 b_3 + a_1^2 b_2^2 + a_2^2 b_1^2 - 2a_1 a_2 b_1 b_2 | c = {{Defof|Dot Product}} }} {{eqn | r = \left({a_1^2 + a_2^2 + a_3^2}\right) \left({b_1^2 + b_2^2 + b_3^2}\right) - \left({a_1 b_1 + a_2 b_2 + a_3 b_3}\right)^2 | c = by algebraic manipulations }} {{eqn | r = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 - \left({\mathbf a \cdot \mathbf b}\right)^2 }} {{end-eqn}} This proves $(1)$. {{qed|lemma}} If $\mathbf a$ or $\mathbf b$ is the [[Definition:Zero Vector|zero vector]], then $\left\Vert{\mathbf a}\right\Vert = 0$, or $\left\Vert{\mathbf b}\right\Vert = 0$ by the [[Definition:Norm on Vector Space|positive definiteness norm axiom]]. By calculation, it follows that $\mathbf a \times \mathbf b$ is also the zero vector, so $\left\Vert{\mathbf a \times \mathbf b}\right\Vert = 0$. Hence, equality $(2)$ holds. If both $\mathbf a$ or $\mathbf b$ are non-zero [[Definition:Vector|vectors]], we continue the calculations from the first section: {{begin-eqn}} {{eqn | l = \left\Vert{ \mathbf a \times \mathbf b }\right\Vert^2 | r = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 - \left({\mathbf a \cdot \mathbf b}\right)^2 }} {{eqn | r = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 - \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 \cos^2 \theta | c = [[Cosine Formula for Dot Product]] }} {{eqn | r = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 \left({1 - \cos^2 \theta}\right) }} {{eqn | r = \left\Vert{\mathbf a}\right\Vert^2 \left\Vert{\mathbf b}\right\Vert^2 \sin^2 \theta | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} Equality $(2)$ now follows after taking the [[Definition:Principal Square Root|square root]] of both sides of the equality. This is possible as [[Square of Real Number is Non-Negative]]. {{qed}}	1
:$\mathbf u \cdot \mathbf v = \mathbf v \cdot \mathbf u$	1
The [[Rational Numbers form Metric Space|rational numbers $\Q$ form a metric space]]. We have that the [[Rationals are Everywhere Dense in Topological Space of Reals]]. We also have that the [[Rational Numbers are Countably Infinite]]. The result follows from the definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $b_1,\ldots, b_n$ [[Definition:Generator|generate]] $B$ over $A$. Let $c_1,\ldots, c_m$ generate $C$ over $B$. Then for any $x \in C$ there are $\beta_k \in B$, $k=1,\ldots, m$ such that: :$\displaystyle x = \sum_{k = 1}^m \beta_k c_k$ For any $i \in \{1,\ldots,m\}$ there exists $\alpha_{ij} \in A$, $j = 1,\ldots, n$ such that :$\displaystyle \beta_i = \sum_{j = 1}^n \alpha_{ij}b_j$ So :$\displaystyle x = \sum_{k = 1}^m \sum_{j = 1}^n \alpha_{kj} b_j c_k$ Therefore, $\{b_jc_k : j = 1,\ldots,n;\ k = 1,\ldots, m\}$ generates $C$ over $A$. {{qed}} [[Category:Commutative Algebra]] [[Category:Algebraic Number Theory]] sz9wj70oearyvyybtmztbg84dxkwuw5	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Module|module]] over $R$. The [[Definition:Module|$R$-module]] $\map {\LL_R} {G, R}$ of all [[Definition:Linear Form|linear forms]] on $G$ is usually denoted $G^*$ and is called the '''algebraic dual''' of $G$. === [[Definition:Algebraic Dual/Double Dual|Double Dual]] === {{:Definition:Algebraic Dual/Double Dual}}	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,} }$. By [[Modulus of Limit/Normed Division Ring|modulus of limit in normed division ring]], $\sequence {\norm {x_n} }$ is a [[Definition:Convergent Real Sequence|convergent sequence in $\R$]]. By [[Convergent Real Sequence is Bounded]], $\sequence {\norm {x_n} }$ is [[Definition:Bounded Real Sequence|bounded]]. That is: :$\exists M \in \R_{> 0}: \forall n, \norm {x_n} = \size {\norm {x_n} } \le M$ Thus, by definition, $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
Let $A \subseteq B \subseteq C$ be [[Definition:Ring Extension|extensions]] of [[Definition:Commutative and Unitary Ring|commutative rings with unity]]. Suppose that $C$ is [[Definition:Integral Ring Extension|integral]] over $B$, and $B$ is [[Definition:Integral Ring Extension|integral]] over $A$. Then $C$ is [[Definition:Integral Ring Extension|integral]] over $A$.	1
{{begin-eqn}} {{eqn | l = \mathbf f \left({x}\right) \cdot \dfrac {\d \mathbf f \left({x}\right)} {\d x} | r = \mathbf f \left({x}\right) \cdot \sum_{k \mathop = 0}^n \dfrac {\d f_k \left({x}\right)} {\d x} \mathbf e_k | c = {{Defof|Derivative of Vector-Valued Function}} }} {{eqn | r = \sum_{k \mathop = 0}^n f_k \left({x}\right) \dfrac {\d f_k \left({x}\right)} {\d x} | c = {{Defof|Dot Product}} }} {{eqn | n = 1 | r = \sum_{k \mathop = 0}^n f_k \left({x}\right) \dfrac {\d f_k \left({x}\right)} {\d x} | c = {{Defof|Dot Product}} }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \dfrac {\d \left\lvert{\mathbf f \left({x}\right)}\right\rvert} {\d x} | r = \dfrac \d {\d x} \sqrt {\sum_{k \mathop = 0}^n \left({f_k \left({x}\right)}\right)^2} | c = {{Defof|Vector Length}} }} {{eqn | r = \frac 1 {2 \sqrt {\displaystyle \sum_{k \mathop = 0}^n \left({f_k \left({x}\right)}\right)^2} } \dfrac \d {\d x} \sum_{k \mathop = 0}^n \left({f_k \left({x}\right)}\right)^2 | c = [[Chain Rule for Derivatives]], [[Derivative of Power]] }} {{eqn | r = \frac 1 {\left\lvert{\mathbf f \left({x}\right)}\right\rvert} \frac 1 2 \sum_{k \mathop = 0}^n \dfrac \d {\d x} \left({\left({f_k \left({x}\right)}\right)^2}\right) | c = {{Defof|Vector Length}}, [[Sum Rule for Derivatives/General Result|Sum Rule for Derivatives: General Result]] }} {{eqn | r = \frac 1 {\left\lvert{\mathbf f \left({x}\right)}\right\rvert} \frac 1 2 \sum_{k \mathop = 0}^n 2 f_k \left({x}\right) \dfrac {\d f_k \left({x}\right)} {\d x} | c = [[Chain Rule for Derivatives]], [[Derivative of Power]] }} {{eqn | r = \frac 1 {\left\lvert{\mathbf f \left({x}\right)}\right\rvert} \mathbf f \left({x}\right) \cdot \dfrac {\d \mathbf f \left({x}\right)} {\d x} | c = simplification, and from $(1)$ }} {{end-eqn}} Hence the result. {{qed}}	1
We prove the [[Definition:Norm on Division Ring|norm axioms]]. === Positive Definiteness === This is demonstrated in: : [[Complex Modulus equals Zero iff Zero]] : [[Complex Modulus is Non-Negative]] {{qed|lemma}} === Multiplicativity === Follows from [[Modulus of Product]]. {{qed|lemma}} === Triangle Inequality === Follows from [[Triangle Inequality for Complex Numbers]]. {{qed}} [[Category:Examples of Norms]] [[Category:Complex Modulus]] 5a2ojdgq19ezkpwv2hj8vxlwkl81k1r	1
Let $z = a + i b$. Then: {{begin-eqn}} {{eqn | l = z \overline z | r = a^2 + b^2 | c = [[Product of Complex Number with Conjugate]] }} {{eqn | r = \cmod z^2 | c = {{Defof|Complex Modulus}} }} {{end-eqn}} {{qed}}	1
Let $\epsilon > 0$ be given. Then $\dfrac \epsilon 2 > 0$. Since $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], we can find $N_1$ such that: :$\forall n, m > N_1: \norm{x_n - x_m} < \dfrac \epsilon 2$ Similarly, $\sequence {y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], we can find $N_2$ such that: : $\forall n, m > N_2: \norm{y_n - y_m} < \dfrac \epsilon 2$ Now let $N = \max \set {N_1, N_2}$. Then if $n, m > N$, both the above inequalities will be true. Thus $\forall n, m > N$: {{begin-eqn}} {{eqn | l = \norm {\paren {x_n + y_n} - \paren {x_m + y_m} } | r = \norm {\paren {x_n - x_m} + \paren {y_n - y_m} } | c = }} {{eqn | o = \le | r = \norm {x_n - x_m} + \norm {y_n - y_m} | c = [[Definition:Norm on Division Ring|Axiom $\text N 3$ of norm: Triangle Inequality]] }} {{eqn | o = < | r = \frac \epsilon 2 + \frac \epsilon 2 = \epsilon | c = }} {{end-eqn}} Hence: :$\sequence {x_n + y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequences in $R$]]. {{qed}}	1
We have: {{begin-eqn}} {{eqn | l = x | o = \in | r = \ideal {a^{n + 1} } | c = }} {{eqn | ll= \leadsto | lo= \exists r \in D: | l = x | r = r \circ a^{n + 1} | c = }} {{eqn | ll= \leadsto | l = x | r = \paren {r \circ a} \circ a^n | c = }} {{eqn | ll= \leadsto | l = x | o = \in | r = \ideal {a^n} | c = }} {{eqn | ll= \leadsto | l = \ideal {a^{n + 1} } | o = \subseteq | r = \ideal {a^n} | c = }} {{end-eqn}} It remains to be shown that $\ideal {a^{n + 1} } \ne \ideal {a^n}$. {{AimForCont}} $\ideal {a^{n + 1} } = \ideal {a^n}$. Then: {{begin-eqn}} {{eqn | l = \ideal {a^{n + 1} } | r = \ideal {a^n} | c = }} {{eqn | ll= \leadsto | l = a^n | o = \in | r = \ideal {a^{n + 1} } | c = }} {{eqn | ll= \leadsto | lo= \exists x \in D: | l = a^n | r = x \circ a^{n + 1} | c = }} {{eqn | ll= \leadsto | l = 1_D | r = x \circ a | c = [[Cancellation Law of Ring Product of Integral Domain]] }} {{end-eqn}} That is, $a$ is a [[Definition:Unit of Ring|unit]] of $D$. This [[Definition:Contradiction|contradicts]] the assertion that $a$ is a [[Definition:Proper Element of Ring|proper element]] of $D$. The result follows by [[Proof by Contradiction]]. {{qed}}	1
=== Proof of Existence === By the definition of [[Definition:Basis of Vector Space|basis]], $\mathcal B$ is a [[Definition:Spanning Set|spanning set]]. Hence the result, by the definition of a spanning set. {{qed|lemma}} === Proof of Uniqueness === {{AimForCont}} otherwise, that: :$\displaystyle \sum_{k \mathop = 1}^n \alpha_k \mathbf x_k = \mathbf x = \sum_{k \mathop = 1}^n \beta_k \mathbf x_k$ where $\alpha_i \ne \beta_i$ for some $1 \le i \le n$. Then: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \alpha_k \mathbf x_k - \sum_{k \mathop = 1}^n \beta_k \mathbf x_k | r = \mathbf x - \mathbf x }} {{eqn | ll = \leadsto | l = \sum_{k \mathop = 1}^n \paren {\alpha_k - \beta_k} \mathbf x_k | r = \mathbf 0 }} {{end-eqn}} However, we have that $\mathcal B = \set {\mathbf x_1, \mathbf x_2, \ldots, \mathbf x_n}$ is a [[Definition:Basis of Vector Space|basis]] for $V$. So, by definition, $\mathcal B$ is a [[Definition:Linearly Independent Set|linearly independent set]]. This means that, for $1 \le i \le n$: :$\alpha_i - \beta_i = 0$ and hence $\alpha_i = \beta_i$ for all $1 \le i \le n$. This contradicts our assumption that $\alpha_i \ne \beta_i$ for some $i$. Hence the result, from [[Proof by Contradiction]]. {{qed}}	1
Let $\mathbb W \subseteq \R^n$ such that $\mathbb W$ is a [[Definition:Vector Subspace|linear subspace of $\R^n$]]. Then $\mathbb W$ contains the [[Definition:Zero Vector|zero vector]]: :$\mathbf 0_{n \times 1} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \in \mathbb W$	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $T \in \map {B_{00} } {H, K}$ be a [[Definition:Bounded Linear Transformation|bounded]] [[Definition:Finite Rank Operator|finite rank operator]]. Then: :$T^* \in \map {B_{00} } {K, H}$ that is, the [[Definition:Adjoint Linear Transformation|adjoint]] of $T$ is also a [[Definition:Bounded Linear Transformation|bounded]] [[Definition:Finite Rank Operator|finite rank operator]].	1
The [[Definition:Ring Zero|zero]] of the [[Definition:Scalar Ring|scalar ring]] is called the '''zero scalar''' and usually denoted $0$, or, if it is necessary to distinguish it from the [[Definition:Identity Element|identity]] of $\struct {G, +_G}$, by $0_R$.	1
{{begin-eqn}} {{eqn | l = D_x \left({y \, \mathbf{z} }\right) | r = D_x \left({\begin{bmatrix} y \ z_1 \\ y \ z_2 \\ \vdots \\ y \ z_n \end{bmatrix} }\right) }} {{eqn | r = \begin{bmatrix} D_x \left({y \ z_1}\right) \\ D_x \left({y \ z_2 }\right) \\ \vdots \\ D_x \left({y \ z_n }\right) \end{bmatrix} | c = [[Differentiation of Vector-Valued Function Componentwise]] }} {{eqn | r = \begin{bmatrix} \dfrac {\d y} {\d x} z_1 + y \dfrac {\d z_1} {\d x} \\ \dfrac {\d y} {\d x}z_2 + y \dfrac {\d z_2} {\d x} \\ \vdots \\ \dfrac {\d y} {\d x} z_n + y \dfrac {\d z_n} {\d x} \end{bmatrix} | c = [[Product Rule for Derivatives|Product Rule for Derivatives of Real Functions]] }} {{eqn | r = \begin{bmatrix} \dfrac {\d y} {\d x} z_1 \\ \dfrac {\d y} {\d x} z_2 \\ \vdots \\ \dfrac {\d y} {\d x} z_n \end{bmatrix} + \begin{bmatrix} y \dfrac {\d z_1} {\d x} \\ y \dfrac {\d z_2} {\d x} \\ \vdots \\ y \dfrac {\d z_n} {\d x} \end{bmatrix} }} {{eqn | r = \dfrac {\d y} {\d x} \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{bmatrix} + y \begin{bmatrix} \dfrac {\d z_1} {\d x} \\ \dfrac {\d z_2} {\d x} \\ \vdots \\ \dfrac {\d z_n} {\d x} \end{bmatrix} }} {{eqn | r = \frac {\d y} {\d x} \mathbf z + y \frac {\d \mathbf z} {\d x} | c = [[Differentiation of Vector-Valued Function Componentwise]] }} {{end-eqn}} {{qed}} {{proofread}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm{\,\cdot\,}$, Let $x, y \in R$ and $\norm x \ne \norm y$. Then: :$\norm {x + y} = \norm {x - y} = \norm {y - x} = \max \set {\norm x, \norm y}$	1
$L$ can be expressed by the equation: :$z = \dfrac {m z_1 + n z_2} {m + n}$ This form of $L$ is known as the '''symmetric form'''.	1
Let $G$ be the [[Definition:Set|set]] of [[Definition:Order of Square Matrix|order $2$]] [[Definition:Square Matrix|square matrices]]: :$G = \set {I, A, B, C}$ where: :$I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}, \quad B = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}, \quad C = \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix}$ Then the [[Definition:Algebraic Structure|algebraic structure]] $\struct {G, \times}$, where $\times$ denotes [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]], forms the [[Definition:Klein Four-Group|Klein four-group]].	1
The [[Definition:Closure (Topology)|closure]] of the set $S$ (trivially) equals $S$. That is, $S$ is [[Definition:Everywhere Dense|everywhere dense]] in $T$. But as $S$ is [[Definition:Countable Set|countable]] it follows by definition that $T$ is [[Definition:Separable Space|separable]]. {{qed}}	1
Let $\sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Let $\sqbrk b_{m n} = \mathbf I_m \sqbrk a_{m n}$. Then: {{begin-eqn}} {{eqn | ll= \forall i \in \closedint 1 m, j \in \closedint 1 n | l = b_{i j} | r = \sum_{k \mathop = 1}^m \delta_{i k} a_{k j} | c = where $\delta_{i k}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: $\delta_{i k} = 1_R$ when $i = k$ otherwise $0_R$ }} {{eqn | r = a_{i j} | c = }} {{end-eqn}} Thus $\sqbrk b_{m n} = \sqbrk a_{m n}$ and $\mathbf I_m$ is shown to be a [[Definition:Left Identity|left identity]]. {{qed}} [[Category:Unit Matrix is Identity for Matrix Multiplication]] csco6xzhtl62zukbf5e63iq3i03fg0u	1
The [[Definition:Vector Cross Product|vector cross product]] is [[Definition:Anticommutative|anticommutative]]: :$\forall \mathbf a, \mathbf b \in \R^3: \mathbf a \times \mathbf b = -\left({\mathbf b \times \mathbf a}\right)$	1
Let the [[Definition:Initial Point of Vector|initial point]] of $\mathbf r$ be $\tuple {x_1, y_1, z_1}$. Let the [[Definition:Terminal Point of Vector|terminal point]] of $\mathbf r$ be $\tuple {x_2, y_2, z_2}$. Thus, by definition of the [[Definition:Component of Vector|components of $\mathbf r$]], the [[Definition:Magnitude|magnitude]] of $\mathbf r$ equals the [[Definition:Distance (Linear Measure)|distance]] between $\tuple {x_1, y_1, z_1}$ and $\tuple {x_2, y_2, z_2}$. The result follows from [[Distance Formula in 3 Dimensions]]. {{qed}}	1
Let: :$\operatorname{N} \left({\mathbf A}\right) = \left\{{\mathbf x \in \R^n: \mathbf {Ax} = \mathbf 0}\right\}$ be the [[Definition:Null Space|null space]] of $\mathbf A$, where: :$ \mathbf A_{m \times n} = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}$, $\mathbf x_{n \times 1} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$, $\mathbf 0_{m \times 1} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ are [[Definition:Matrix|matrices]]. Then $\operatorname{N} \left({\mathbf A}\right)$ is a [[Definition:Vector Subspace|linear subspace]] of $\R^n$.	1
Let $z_1 = a_1 + i a_2, z_2 = b_1 + i b_2$. Then from the definition of the [[Definition:Modulus of Complex Number|modulus]], the above equation translates into: :$\paren {\paren {a_1 + b_1}^2 + \paren {a_2 + b_2}^2}^{\frac 1 2} \le \paren { {a_1}^2 + {a_2}^2}^{\frac 1 2} + \paren { {b_1}^2 + {b_2}^2}^{\frac 1 2}$ This is a special case of [[Minkowski's Inequality]], with $n = 2$. {{qed}}	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ be [[Definition:Norm on Division Ring|norms]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :$\exists \alpha \in \R_{\gt 0}: \forall n \in \N: \norm n_1 = \norm n_2^\alpha$ Then $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ are [[Definition:Equivalent Division Ring Norms|equivalent]]	1
Let $\C$ be the [[Definition:Field of Complex Numbers|field of complex numbers]]. Let $\F$ be a [[Definition:Subfield|subfield]] of $\C$. Let $V$ be a [[Definition:Vector Space|vector space]] over $\F$ Let $\left \langle {\cdot, \cdot} \right \rangle : V \times V \to \mathbb F$ be a [[Definition:Mapping|mapping]]. Then $\left \langle {\cdot, \cdot} \right \rangle : V \times V \to \mathbb F$ is '''positive''' [[Definition:Iff|iff]]: :$\forall x \in V: \quad \left \langle {x, x} \right \rangle = 0 \implies x = \mathbf 0_V$ where $\mathbf 0_V$ denotes the [[Definition:Zero Vector|zero vector]] of $V$.	1
Then: :$\forall x, y, z \in R: \norm {x - y}_p \le \max \set {\norm {x - z}_p, \norm {y - z}_p}$	1
The [[Definition:Complex Number|set of complex numbers]] $\C$ forms an [[Definition:Algebra over Field|algebra]] over the [[Definition:Field of Real Numbers|field of real numbers]]. This [[Definition:Algebra over Field|algebra]] is: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]]. :$(2): \quad$ A [[Definition:Commutative Algebra|commutative algebra]]. :$(3): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]]. :$(4): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. However, $\C$ is not a [[Definition:Real Algebra|real algebra]].	1
By [[Integral Ring Extension is Integral over Intermediate Ring]], $E$ is [[Definition:Integral Ring Extension|integral]] over $A$. Let $a \in A$ be [[Definition:Nonzero Ring Element|nonzero]]. Because $E$ is a [[Definition:Field (Abstract Algebra)|field]], $a$ is a [[Definition:Unit of Ring|unit]] of $E$. By [[Ring Element is Unit iff Unit in Integral Extension]], $a$ is a [[Definition:Unit of Ring|unit]] of $A$. Thus $A$ is a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$ of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n > 0$. Let $f$ be a [[Definition:Bilinear Form|bilinear form]] on $V$. Let $\operatorname{rad} \left({V}\right)$ be the [[Definition:Radical of Bilinear Form|radical]] of $V$. Let $\operatorname{rk} \left({f}\right)$ be the [[Definition:Rank of Bilinear Form|rank]] of $f$. Then: : $\dim \left({\operatorname{rad} \left({V}\right)}\right) = n - \operatorname{rk} \left({f}\right)$ where $\dim$ denotes [[Definition:Dimension (Linear Algebra)|dimension]].	1
Let $\struct {R, +, \circ}$ be the [[Definition:Trivial Ring|trivial ring]] over an [[Definition:Underlying Set of Structure|underlying set]]. Let $\map {\MM_R} n$ denote the [[Definition:Matrix Space|$n \times n$ matrix space]] over $R$. Then [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]] is [[Definition:Commutative Operation|commutative]] over $\map {\MM_R} n$: :$\forall \mathbf A, \mathbf B \in \map {\MM_R} n: \mathbf {A B} = \mathbf {B A}$	1
{{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2}^2 | r = \paren {\cmod {z_1} \cos \theta_1 + \cmod {z_2} \cos \theta_2}^2 + \paren {\cmod {z_1} \sin \theta_1 + \cmod {z_2} \sin \theta_2}^2 | c = {{Defof|Complex Modulus}} }} {{eqn | r = 2 \cmod {z_1} \cmod {z_2} \cos \theta_1 \cos \theta_2 + \cmod {z_1}^2 \cos^2 \theta_1 + \cmod {z_2}^2 \cos^2 \theta_2 | c = }} {{eqn | o = | ro= + | r = 2 \cmod {z_1} \cmod {z_2} \sin \theta_1 \sin \theta_2 + \cmod {z_1}^2 \sin^2 \theta_1 + \cmod {z_2}^2 \sin^2 \theta_2 | c = }} {{eqn | r = 2 \cmod {z_1} \cmod {z_2} \paren {\cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2} + \cmod {z_1}^2 + \cmod {z_2}^2 | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \cmod {z_1} \cmod {z_2} \, \map \cos {\theta_1 - \theta_2} | c = [[Cosine of Difference]] }} {{end-eqn}} {{qed}}	1
An [[Definition:Identity Matrix|identity matrix]] is an example of a [[Definition:Permutation Matrix|permutation matrix]].	1
Let: : $\mathcal A = \left \langle {a_m} \right \rangle$ : $\mathcal B = \left \langle {b_n} \right \rangle$ : $\mathcal C = \left \langle {c_p} \right \rangle$ Let: : $\left[{\alpha}\right]_{m n} = \left[{f; \left \langle {b_n} \right \rangle, \left \langle {a_m} \right \rangle}\right]$ and: : $\left[{\beta}\right]_{n p} = \left[{g; \left \langle {c_p} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ Then: {{begin-eqn}} {{eqn | l = \left({g \circ f}\right) \left({a_j}\right) | r = v \left({f \left({a_j}\right)}\right) | c = }} {{eqn | r = v \left({\sum_{k \mathop = 1}^n \alpha_{k j} b_k}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \alpha_{k j} v \left({b_k}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \alpha_{k j} \left({\sum_{i \mathop = 1}^p \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{k \mathop = 1}^n \left({\sum_{i \mathop = 1}^p \alpha_{k j} \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{i \mathop = 1}^p \left({\sum_{k \mathop = 1}^n \alpha_{k j} \beta_{i k} c_i}\right) | c = }} {{eqn | r = \sum_{i \mathop = 1}^p \left({\sum_{k \mathop = 1}^n \alpha_{k j} \beta_{i k} }\right) c_i | c = }} {{end-eqn}} {{qed}}	1
From [[Canonical Basis of Free Module on Set is Basis]], $R^{\paren I}$ has a [[Definition:Basis of Module|basis]]. {{qed}} [[Category:Free Modules]] 09a67r1n3513hjmtc6sbhi8db56nhxo	1
Let the [[Definition:Unity of Ring|unity]] of $D$ be $1_D$. By definition of [[Definition:Principal Ideal of Ring|principal ideal]]: :$\ideal a = \displaystyle \set {\sum_{i \mathop = 1}^n r_i \circ a \circ s_i: n \in \N, r_i, s_i \in D}$ Let $x, y \in J$. By definition of [[Definition:Linear Combination|linear combination]]: {{begin-eqn}} {{eqn | l = x | r = \sum_{i \mathop = 1}^n r_i \circ a_i | c = for some $n \in \N$ and for some $r_i \in D$ where $i \in \set {1, 2, \dotsc, n}$ }} {{eqn | r = r_1 \circ a_1 + r_2 \circ a_2 + \dotsb + r_n \circ a_n | c = for some $r_1, r_2, \dotsc, r_n \in D$ }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = y | r = \sum_{i \mathop = 1}^n s_i \circ a_i | c = for some $n \in \N$ and for some $s_i \in D$ where $i \in \set {1, 2, \dotsc, n}$ }} {{eqn | r = s_1 \circ a_1 + s_2 \circ a_2 + \dotsb + s_n \circ a_n | c = for some $s_1, s_2, \dotsc, s_n \in R$ }} {{eqn | ll= \leadsto | l = -y | r = -\sum_{i \mathop = 1}^n s_i \circ a_i | c = }} {{eqn | r = -1_D \times \sum_{i \mathop = 1}^n s_i \circ a_i | c = [[Product with Ring Negative/Corollary|Product with Ring Negative: Corollary]] }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {-1_D} \times \paren {s_i \circ a_i} | c = {{Ring-axiom|D}} }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {-\paren {s_i \circ a_i} } | c = [[Product with Ring Negative/Corollary|Product with Ring Negative: Corollary]] }} {{eqn | r = \sum_{i \mathop = 1}^n s_i \circ \paren {-a_i} | c = [[Product with Ring Negative]] }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = x + \paren {-y} | r = \sum_{i \mathop = 1}^n r_i \circ a_i + \sum_{i \mathop = 1}^n s_i \circ \paren {-a_i} | c = }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {r_i \circ a_i + s_i \circ \paren {-a_i} } | c = }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {r_i \circ a_i + \paren {-\paren {s_i \circ a_i} } } | c = }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {r_i \circ a_i + \paren {-s_i} \circ a_i} | c = }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {r_i + \paren {-s_i} } \circ a_i | c = {{Ring-axiom|D}} }} {{eqn | r = \sum_{i \mathop = 1}^n t_i \circ a_i | c = where $t_i - r_1 + \paren {-s_i}$ }} {{eqn | o = \in | r = J | c = as $t_i \in D$ }} {{end-eqn}} Then we have: {{begin-eqn}} {{eqn | l = x \circ y | r = \paren {\sum_{i \mathop = 1}^n r_i \circ a_i} \circ \paren {\sum_{i \mathop = 1}^n s_i \circ \a_i } | c = }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {t_i \circ a_i} | c = where $t_i \in D$ for $i \in \set {1, 2, \dotsc, n}$ }} {{eqn | r = \sum_{i \mathop = 1}^n \paren {a_i \circ t_i} | c = as $\circ$ is [[Definition:Commutative Operation|commutative]] in an [[Definition:Integral Domain|integral domain]] }} {{end-eqn}} {{explain|There exists (or ought to) some convolution result which proves the above -- I just haven't found it yet.}} Thus by the [[Test for Ideal]], $J$ is an [[Definition:Ideal of Ring|ideal]] of $D$. As $D$ is a [[Definition:Principal Ideal Domain|principal ideal domain]], it follows that $J$ is a [[Definition:Principal Ideal of Ring|principal ideal]]. Thus by definition of [[Definition:Principal Ideal of Ring|principal ideal]]: :$J = \ideal x$ for some $x \in D$. {{qed}}	1
By definition of the [[Definition:Zero Vector Quantity|zero vector]], the [[Definition:Magnitude|magnitude]] of $\mathbf r$ is equal to [[Definition:Zero (Number)|zero]]. By [[Magnitude of Vector Quantity in terms of Components]]: :$\size {\mathbf r} = \sqrt {x^2 + y^2 + z^2} = 0$ where $\size {\mathbf r}$ denotes the [[Definition:Magnitude|magnitude]] of $\mathbf r$. As each of $x$, $y$ and $z$ are [[Definition:Real Number|real numbers]], each of $x^2$, $y^2$ and $z^2$ is [[Definition:Non-Negative Real Number|non-negative]]. so in order for $\sqrt {x^2 + y^2 + z^2} = 0$, it must follow that each of $x$, $y$ and $z$ is [[Definition:Zero (Number)|zero]]. Hence the result. {{qed}}	1
From the [[Cauchy-Schwarz Inequality/Complex Numbers|Complex Number form of the Cauchy-Schwarz Inequality]], we have: :$\displaystyle \sum \left|{w_i}\right|^2 \left|{z_i}\right|^2 \ge \left|{\sum w_i z_i}\right|^2$ where all of $w_i, z_i \in \C$. As elements of $\R$ are also elements of $\C$, it follows that: :$\displaystyle \sum \left|{r_i}\right|^2 \left|{s_i}\right|^2 \ge \left|{\sum r_i s_i}\right|^2$ where all of $r_i, s_i \in \R$. But from the [[Definition:Modulus of Complex Number|definition of modulus]], it follows that: :$\displaystyle \forall r_i \in \R: \left|{r_i}\right|^2 = r_i^2$ Thus: :$\displaystyle \sum {r_i^2} \sum {s_i^2} \ge \left({\sum {r_i s_i}}\right)^2$ where all of $r_i, s_i \in \R$. {{qed}}	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,}}$. By the definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence in a normed division ring]], $\sequence {x_n} $ is [[Definition:Convergent Sequence in Metric Space|convergent]] to the [[Definition:Limit of Sequence (Metric Space)|limit]] $l$ in $\struct {R, d}$. By [[Convergent Sequence in Metric Space is Bounded]], $\sequence {x_n} $ is a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in $\struct {R, d}$. By [[Sequence is Bounded in Norm iff Bounded in Metric]], $\sequence {x_n} $ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in $\struct {R, \norm {\,\cdot\,} }$. {{qed}}	1
Let: :$\map {\mathrm N} {\mathbf A} = \set {\mathbf x \in \R^n: \mathbf A \mathbf x = \mathbf 0}$ be the [[Definition:Null Space|null space]] of $\mathbf A$, where: :$ \mathbf A_{m \times n} = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m 1} & a_{m 2} & \cdots & a_{m n} \\ \end{bmatrix}$ is a [[Definition:Matrix|matrix]] in the [[Definition:Matrix Space|matrix space]] $\map {\MM_\R} {m, n}$. Then the [[Definition:Null Space|null space]] of $\mathbf A$ contains the [[Definition:Zero Vector|zero vector]]: :$\mathbf 0 \in \map {\mathrm N} {\mathbf A}$ where: :$\mathbf 0 = \mathbf 0_{m \times 1} = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$	1
$H$ is a [[Definition:Basis of Vector Space|basis]] for $E$ {{iff}} it contains exactly $n$ [[Definition:Element|elements]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$. Let $\mathcal A$ and $\mathcal B$ be [[Definition:Ordered Basis|ordered bases]] of $M$. Let $\mathbf P$ be the [[Definition:Change of Basis Matrix|change of basis matrix]] from $\mathcal A$ to $\mathcal B$. Then $\mathbf P$ is [[Definition:Invertible Matrix|invertible]], and its [[Definition:Inverse Matrix|inverse]] $\mathbf P^{-1}$ is the [[Definition:Change of Basis Matrix|change of basis matrix]] from $\mathcal B$ to $\mathcal A$.	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\MM_F} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $F$]]. Then $\struct {\map {\MM_F} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]], but is not a [[Definition:Commutative Ring|commutative ring]].	1
Let $F \sqbrk X$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms]] over the [[Definition:Field (Abstract Algebra)|field]] $F$. Let $\map d X$ and $\map {d'} X$ be [[Definition:Polynomial Form|polynomial forms]] in $F \sqbrk X$. Then $\map d X$ is an [[Definition:Associate in Integral Domain|associate]] of $\map {d'} X$ {{iff}} $\map d X = c \cdot \map {d'} X$ for some $c \in F, c \ne 0_F$. Hence any two [[Definition:Polynomial Form|polynomials]] in $F \sqbrk X$ have a unique [[Definition:Monic Polynomial|monic]] [[Definition:Greatest Common Divisor of Polynomials|GCD]].	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \sum_{i \mathop = 1}^n u_i^2 | c = Definition of [[Definition:Dot Product/Definition 1|Dot Product]] }} {{eqn | o = \ge | r = 0 | c = as $u_i \in \R$ it follows that $u_i^2 \ge 0$ }} {{end-eqn}} {{qed}}	1
Let $J$ be an [[Definition:Ideal of Ring|ideal]] of $\Z$. Then $J$ is a [[Definition:Subring|subring]] of $\Z$, and so $\left({J, +}\right)$ is a [[Definition:Subgroup|subgroup]] of $\left({\Z, +}\right)$. But by [[Integers under Addition form Infinite Cyclic Group]], the group $\left({\Z, +}\right)$ is [[Definition:Cyclic Group|cyclic]], generated by $1$. Thus by [[Subgroup of Cyclic Group is Cyclic]], $\left({J, +}\right)$ is cyclic, generated by some $m \in \Z$. Therefore from the definition of [[Definition:Principal Ideal of Ring|principal ideal]], $J = \left\{{k m: k \in \Z}\right\} = \left({m}\right)$, and is thus a [[Definition:Principal Ideal of Ring|principal ideal]]. {{qed}}	1
=== Proof of Existence === Follows from the [[Definition:Vector Space Axioms|vector space axioms]]. {{qed|lemma}} === Proof of Uniqueness === Let $\mathbf v$ have inverses $\mathbf x$ and $\mathbf y$. Then: {{begin-eqn}} {{eqn | l = \mathbf v + \mathbf x | r = \mathbf 0 }} {{eqn | lo= \land | l = \mathbf v + \mathbf y | r = \mathbf 0 }} {{eqn | ll= \leadsto | l = \mathbf v + \mathbf x | r = \mathbf v + \mathbf y }} {{eqn | ll= \leadsto | l = \mathbf x | r = \mathbf y | c = [[Vectors are Left Cancellable]] }} {{end-eqn}} {{qed}}	1
By [[Vector Space has Basis between Linearly Independent Set and Spanning Set]], there exists a [[Definition:Basis of Vector Space|basis]] $C \subset G$. By [[Bases of Vector Space have Equal Cardinality]], there exists a [[Definition:Bijection|bijection]] between $B$ and $C$. By [[Composite of Injections is Injection]], composing this [[Definition:Bijection|bijection]] with the [[Definition:Inclusion Mapping|inclusion]] of $C$ in $G$, we obtain an [[Definition:Injection|injection]] from $B$ to $G$. {{qed}} [[Category:Vector Spaces]] o50qamytl06tbrbu0zgnt3j68uncmuj	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. By the definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence in a normed division ring]] then: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$ in $\struct {R, \norm {\,\cdot\,} }$ {{iff}} $\sequence {x_n}$ [[Definition:Convergent Sequence in Metric Space|converges]] to $x$ in $\struct {R, d}$. By [[Convergent Subsequence of Cauchy Sequence|Convergent Subsequence of Cauchy Sequence in Metric Space]]: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Metric Space|converges]] to $x$ in $\struct {R, d}$ {{iff}} $\sequence {x_n}$ has a [[Definition:Subsequence|subsequence]] that [[Definition:Convergent Sequence in Metric Space|converges]] to $x$ In $\struct {R, d}$. By the definition of a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence in a normed division ring]]: :$\sequence {x_n}$ has a [[Definition:Subsequence|subsequence]] that [[Definition:Convergent Sequence in Metric Space|converges]] to $x$ In $\struct {R, d}$ {{iff}} $\sequence {x_n}$ has a [[Definition:Subsequence|subsequence]] that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$ in $\struct {R, \norm {\,\cdot\,} }$. The result follows. {{qed}}	1
Let $\mathbb A$ be one of the [[Definition:Standard Number System|standard number systems]] $\N,\Z,\Q,\R,\C$. Let $a,b$ be [[Definition:Integer|integers]]. Let $\left[{a \,.\,.\, b}\right]$ denote the [[Definition:Integer Interval|integer interval]] between $a$ and $b$. Let $f : \left[{a \,.\,.\, b}\right] \to \mathbb A$ be a [[Definition:Mapping|mapping]]. Let $|\cdot|$ denote the [[Definition:Standard Absolute Value|standard absolute value]]. Let $\vert f \vert$ be the [[Definition:Absolute Value of Mapping|absolute value]] of $f$. Then we have the [[Definition:Inequality|inequality]] of [[Definition:Indexed Summation|indexed summations]]: :$\displaystyle \left\vert \sum_{i \mathop = a}^b f(i) \right\vert \leq \sum_{i \mathop = a}^b \vert f(i) \vert$	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number|complex numbers]] such that: :$\cmod {z_1 + z_2} = \cmod {z_1 - z_2}$ Then $\dfrac {z_2} {z_1}$ is [[Definition:Wholly Imaginary|wholly imaginary]].	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\Gamma_1$ be a [[Definition:Column Operation|column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$. Let $\Gamma_2$ be a [[Definition:Column Operation|column operation]] which transforms $\mathbf B$ to another new [[Definition:Matrix|matrix]] $\mathbf C \in \map \MM {m, n}$. Then there exists another [[Definition:Column Operation|column operation]] $\Gamma$ which transforms $\mathbf A$ back to $\mathbf C$ such that $\Gamma$ consists of $\Gamma_1$ followed by $\Gamma_2$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H}\right)$ be a [[Definition:Self-Adjoint Operator|self-adjoint operator]]. Then the [[Definition:Norm on Bounded Linear Transformation|norm]] of $A$ satisfies: :$\left\Vert{A}\right\Vert = \sup \left\{{ \left\vert{ \left\langle{Ah, h}\right\rangle_H }\right\vert: h \in H, \left\Vert{h}\right\Vert_H = 1 }\right\}$	1
A pair of [[Definition:Simultaneous Equations|simultaneous linear equations]] of the form: {{begin-eqn}} {{eqn | l=a x + b y | r=c }} {{eqn | l=d x + e y | r=f }} {{end-eqn}} where $ ae \ne b d$, has as its only [[Definition:Simultaneous Equations#Solution|solution]]: {{begin-eqn}} {{eqn | l=x | r= \frac {c e - b f} {a e - b d} }} {{eqn | l=y | r=\frac {a f - c d} {a e - b d} }} {{end-eqn}}	1
Let $L \subset \R^n$ be an [[Definition:Integral Lattice|integral lattice]]. Let $B = \left({v_1, \ldots, v_n}\right)$ be an [[Definition:Ordered Basis|ordered basis]] for $L$. The '''fundamental domain''' of $L$ associated to $B$ is the set: :$\displaystyle \left\{ {\sum_{i \mathop = 1}^n \lambda_i v_i: \forall i: 0 \le \lambda_i < 1}\right\}$ [[Category:Definitions/Linear Algebra]] [[Category:Definitions/Geometry of Numbers]] nh0m2th0ored6dymzrx9lt7pjpnoojz	1
Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ERO} 3 | t = Interchange [[Definition:Row of Matrix|rows]] $i$ and $j$ | m = r_i \leftrightarrow r_j }} {{end-axiom}}	1
For every $t\in N$, the [[Definition:Mapping|mapping]]: :$f_t: M \to M : x \mapsto f \left({x, t}\right)$ is a [[Definition:Contraction Mapping|contraction]]. By the [[Banach Fixed-Point Theorem]], there exists a [[Definition:unique|unique]] $g \left({t}\right) \in M$ such that $f_t \left({g \left({t}\right)}\right) = g \left({t}\right)$. We show that $g$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]]. Let $K < 1$ be a [[Definition:Uniform Lipschitz Constant|uniform Lipschitz constant]] for $f$. Let $s, t \in N$. Then {{begin-eqn}} {{eqn | l = d \left({g \left({s}\right), g \left({t}\right)}\right) | r = d \left({f \left({g \left({s}\right), s}\right), f \left({g \left({t}\right), t}\right)}\right) | c = Definition of $g$ }} {{eqn | o = \le | r = d \left({f \left({g \left({s}\right), s}\right), f \left({g \left({t}\right), s}\right)}\right) + d \left({f \left({g \left({t}\right), s}\right), f \left({g \left({t}\right), t}\right)}\right) | c = {{Defof|Metric}} }} {{eqn | o = \le | r = K \cdot d \left({g \left({s}\right), g \left({t}\right)}\right) + d \left({f \left({g \left({t}\right), s}\right), f \left({g \left({t}\right), t}\right)}\right) | c = $f$ is a [[Definition:Uniform Contraction Mapping|uniform contraction]] }} {{end-eqn}} and thus: :$d \left({g \left({s}\right), g \left({t}\right)}\right) \le \dfrac 1 {1 - K} d \left({f \left({g \left({t}\right), s}\right), f \left({g \left({t}\right), t}\right)}\right)$ The continuity of $g$ now follows from that of $f$ using the definition of [[Definition:Product Metric|product metric]] {{qed}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G_1, +_1, \circ_1}_R, \struct {G_2, +_2, \circ_2}_R, \ldots, \struct {G_n, +_n, \circ_n}_R$ be [[Definition:Module|$R$-modules]]. Let: :$\ds G = \prod_{k \mathop = 1}^n G_k$ be their [[Definition:Module Direct Product|direct product]]. Then $G$ is a [[Definition:Module|module]].	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathbf A$ be a [[Definition:Matrix|matrix]] over $R$. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose of $\mathbf A$]]. Let the [[Definition:Column of Matrix|columns]] of $\mathbf A^\intercal$ be members of a [[Definition:Vector Space|vector space]]. The '''row space of $\mathbf A$''' is defined as the [[Definition:Column Space|column space]] of $\mathbf A^\intercal$.	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $M$ and $N$ be [[Definition:Module|modules]] over $R$. Let $B : M \times N \to R$ be a [[Definition:Bilinear Mapping|bilinear mapping]]. The '''annihilator of $D \subseteq M$''', denoted $\operatorname{Ann}_N \left({D}\right)$ is the set: :$\left\{{n \in N : \forall d \in D: B \left({d, n}\right) = 0}\right\}$	1
Let $\mathbf a$ be a [[Definition:Vector Quantity|vector quantity]]. Let $0 \mathbf a$ denote the [[Definition:Scalar Multiplication on Vector Quantity|scalar product]] of $\mathbf a$ with $0$. Then: :$0 \mathbf a = \bszero$ where $\bszero$ denotes the [[Definition:Zero Vector Quantity|zero vector]].	1
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ be an [[Definition:R-Algebraic Structure|$R$-algebraic structure]]. Let $\phi: S \to S$ be an [[Definition:R-Algebraic Structure Homomorphism|$R$-algebraic structure homomorphism]] from $S$ to itself. Then $\phi$ is an [[Definition:R-Algebraic Structure Endomorphism|$R$-algebraic structure endomorphism]]. This definition continues to apply when $S$ is a [[Definition:Module|module]], and also when it is a [[Definition:Vector Space|vector space]].	1
Let the [[Definition:Field of Complex Numbers|field of complex numbers]] be denoted $\struct {\C, +, \times}$. By [[Complex Numbers under Addition form Abelian Group]], $\struct {\C, +}$ is an [[Definition:Abelian Group|abelian group]]. From [[Complex Multiplication Distributes over Addition]]: {{begin-eqn}} {{eqn | lo= \forall x, y, z \in \C: | l = x \times \paren {y + z} | r = x \times y + x \times z }} {{eqn | l = \paren {y + z} \times x | r = y \times x + z \times x }} {{end-eqn}} From [[Complex Multiplication is Associative]]: :$\forall x, y, z \in \C: x \times \paren {y \times z} = \paren {x \times y} \times z$ From [[Complex Multiplication Identity is One]]: :$\forall x \in \C: 1 \times x = x$ Therefore $\struct {\C, +, \times}$ forms a [[Definition:Vector Space|vector space]]. {{qed}}	1
From [[Null Ring is Ideal]] and [[Ring is Ideal of Itself]], it is always the case that $\set {0_R}$ and $\struct {R, +, \circ}$ are [[Definition:Ideal of Ring|ideals]] of $\struct {R, +, \circ}$. Let $a \in R^*$, where $R^* := R \setminus \set {0_R}$. Let $\ideal a$ be the [[Definition:Principal Ideal of Ring|principal ideal of $R$ generated by $a$]]. We have that $\ideal a$ is a [[Definition:Non-Null Ideal|non-null ideal]] and hence $\ideal a = R$. Thus $1_R \in \ideal a$. Thus $\exists x \in R: x \circ a = 1_R$ by the definition of [[Definition:Principal Ideal of Ring|principal ideal]]. Therefore $a$ is [[Definition:Invertible Element|invertible]]. As $a$ is arbitrary, it follows that all such $a$ are [[Definition:Invertible Element|invertible]]. Thus by definition $\struct {R, +, \circ}$ is a [[Definition:Division Ring|division ring]] such that $\circ$ is [[Definition:Commutative Operation|commutative]]. The result follows by definition of [[Definition:Field (Abstract Algebra)|field]]. {{Qed}}	1
Let $\mathcal L$ be the [[Definition:Straight Line|straight line]] defined by the [[Equation of Straight Line in Plane/General Equation|general equation]]: :$\alpha_1 x + \alpha_2 y = \beta$ Then $\mathcal L$ can be described by the equation: :$y = m x + c$ where: {{begin-eqn}} {{eqn | l = m | r = -\dfrac {\alpha_1} {\alpha_2} | c = }} {{eqn | l = c | r = \dfrac {\beta} {\alpha_2} | c = }} {{end-eqn}} such that $m$ is the [[Definition:Slope of Straight Line|slope]] of $\mathcal L$ and $c$ is the [[Definition:Y-Intercept|$y$-intercept]].	1
Define polynomial root sets $\set {1,2,\ldots, n}$ and $\set { 0,-1,\ldots,-n+1}$ for [[Definition:Cauchy Matrix]]. Let: {{begin-eqn}} {{eqn | l = H | r = \paren {\begin{smallmatrix}\displaystyle 1 & \dfrac {1} {2} & \cdots & \dfrac {1} {n} \\ \dfrac {1} {2} & \dfrac 1 {3} & \cdots & \dfrac {1} {n+1} \\ \vdots & \vdots & \cdots & \vdots \\ \dfrac {1} {n} & \dfrac {1} {n+1} & \cdots & \dfrac {1} {2n-1} \\ \end{smallmatrix} } | c = [[Definition:Hilbert Matrix|Hilbert matrix]] of order $n$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = H | r = -P V_x^{-1} V_y Q^{-1} | c = [[Vandermonde Matrix Identity for Cauchy Matrix]] and [[Hilbert Matrix is Cauchy Matrix]] }} {{end-eqn}} Definitions of [[Definition:Vandermonde Matrix|Vandermonde matrices]] $V_x$, $V_y$ and [[Definition:Diagonal Matrix|diagonal matrices]] $P$, $Q$: :$\displaystyle V_x=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ 1 & 2 & \cdots & n \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 2^{n-1} & \cdots & n^{n-1} \\ \end{smallmatrix} },\quad V_y=\paren {\begin{smallmatrix} \displaystyle 1 & 1 & \cdots & 1 \\ 0 & -1 & \cdots & -n+1 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & \paren {-1}^{n-1} & \cdots & \paren {-n+1}^{n-1} \\ \end{smallmatrix} }$ [[Definition:Vandermonde Matrix|Vandermonde matrices]] :$\displaystyle P= \paren {\begin{smallmatrix} p_1(1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p_n(n) \\ \end{smallmatrix} }, \quad Q= \paren {\begin{smallmatrix} p(0) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p(-n+1) \\ \end{smallmatrix} }$ [[Definition:Diagonal Matrix|Diagonal matrices]] Definitions of polynomials $p$, $p_1$, $\ldots$, $p_n$: :$\displaystyle p(x) = \prod_{i \mathop = 1}^n \paren {x - i}$ :$\displaystyle p_k(x) = \dfrac{ \map p x}{x-k} = \prod_{i \mathop = 1,i \mathop \ne k}^n \, \paren {x - i}$, $1 \mathop \le k \mathop \le n$	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $N$ be an [[Definition:Module|$R$-module]]. Let $\left({M_i}\right)_{i \mathop \in I}$ be a family of $R$-modules. Let $M = \displaystyle \bigoplus_{i \mathop \in I} M_i$ be their [[Definition:Module Direct Sum|direct sum]]. Let $\left({\psi_i}\right)_{i \mathop \in I}$ be a family of $R$-module morphisms $M_i \to N$. Then there exists a unique morphism: :$\Psi: M \to N$ such that: : $\forall i: \psi_i = \Psi \circ \iota_i$ where $\iota_i: M_i \to M$ is the $i$th canonical injection.	1
:[[File:Scalar-product-distributes-over-vector-addition.png|400px]] Let $\mathbf a = \vec {OP}$ and $\mathbf b = \vec {PQ}$. Then: :$\vec {OQ} = \mathbf a + \mathbf b$ Let $P'$ and $Q'$ be [[Definition:Point|points]] on $OP$ and $OQ$ respectively so that: :$OP' : OP = OQ' : OQ = m$ Then $P'Q'$ is [[Definition:Parallel Lines|parallel]] to $PQ$ and $m$ times it in [[Definition:Length of Line|length]]. Thus: :$\vec {P'Q'} = m \mathbf b$ which shows that: {{begin-eqn}} {{eqn | l = m \paren {\mathbf a + \mathbf b} | r = \vec {OQ'} | c = }} {{eqn | r = \vec {OP} + \vec {OP'} | c = }} {{eqn | r = m \mathbf a + m \mathbf b | c = }} {{end-eqn}} {{qed}}	1
This follows as $\map {\MM_R} {m, n}$ is a direct instance of the module given in the [[Definition:Module of All Mappings|module of all mappings]], where $\map {\MM_R} {m, n}$ is the $R$-module $R^{\closedint 1 m \times \closedint 1 n}$. The $S$ of that example is the set $\closedint 1 m \times \closedint 1 n$, while the $G$ of that example is the $R$-module $R$. {{finish}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]]. === $(1)$ implies $(2)$ === Let $x \in R$ be a [[Definition:Unit of Ring/Definition 1|unit of $\struct {R, +, \circ}$ by definition 1]]. Then by definition: :$\exists y \in R: x \circ y = 1_R = y \circ x$ That is, by definition of [[Definition:Divisor of Ring Element|divisor]]: :$x \divides 1_R$ Thus $x$ is a [[Definition:Unit of Ring/Definition 2|unit of $\struct {R, +, \circ}$ by definition 2]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Let $x \in R$ be a [[Definition:Unit of Ring/Definition 2|unit of $\struct {R, +, \circ}$ by definition 2]]. Then by definition: :$x \divides 1_R$ By definition of [[Definition:Divisor of Ring Element|divisor]]: :$\exists t \in R: 1_R = t \circ x$ Thus $x$ is a [[Definition:Unit of Ring/Definition 1|unit of $\struct {R, +, \circ}$ by definition 1]]. {{qed}} [[Category:Units of Rings]] l4gkmgz1c66ajs0aoygzxdbic0xbwud	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct {G, +_G, \circ}_R$ be a [[Definition:Module|module]] over $R$. Let $\phi: \struct {G, +_G, \circ}_R \to \struct {R, +_R, \circ}_R$ be a [[Definition:Linear Transformation|linear transformation]] from $G$ to the [[Definition:Module on Cartesian Product|$R$-module $R$]]. Then $\phi$ is called a '''linear form on $G$'''.	1
Let $C_n$ be the [[Definition:Combinatorial Matrix|combinatorial matrix]] of [[Definition:Order of Square Matrix|order $n$]] given by: :$C_n = \begin{bmatrix} x + y & y & \cdots & y \\ y & x + y & \cdots & y \\ \vdots & \vdots & \ddots & \vdots \\ y & y & \cdots & x + y \end{bmatrix}$ Then its [[Definition:Inverse Matrix|inverse]] $C_n^{-1} = \sqbrk b_n$ can be specified as: :$b_{i j} = \dfrac {-y + \delta_{i j} \paren {x + n y} } {x \paren {x + n y} }$ where $\delta_{i j}$ is the [[Definition:Kronecker Delta|Kronecker delta]].	1
The proof that $M$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]] is straightforward. {{stub|The proof that $M$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]].}} The relation: : $\left[{v \circ u; \left \langle {c_m} \right \rangle, \left \langle {a_p} \right \rangle}\right] = \left[{v; \left \langle {c_m} \right \rangle, \left \langle {b_n} \right \rangle}\right] \left[{u; \left \langle {b_n} \right \rangle, \left \langle {a_p} \right \rangle}\right]$ follows from [[Relative Matrix of Composition of Linear Mappings]]. {{qed}} {{Proofread}}	1
[[Definition:Equivalent Matrices|Matrix equivalence]] is an [[Definition:Equivalence Relation|equivalence relation]].	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G_1, +_1, \circ_1}_R, \struct {G_2, +_2, \circ_2}_R, \ldots, \struct {G_n, +_n, \circ_n}_R$ be [[Definition:Unitary Module|unitary $R$-modules]]. Let: :$\displaystyle G = \prod_{k \mathop = 1}^n G_k$ be their [[Definition:Module Direct Product|direct product]]. Then $G$ is a [[Definition:Unitary Module|unitary module]].	1
Let $\struct {\R, \tau_d}$ be the [[Definition:Real Number Line with Euclidean Topology|real number line with the usual (Euclidean) topology]]. Then $\struct {\R, \tau_d}$ is [[Definition:Separable Space|separable]].	1
Let the line $L = \alpha_1 x_1 + \alpha_2 x_2 = \beta$ be homogeneous. Then the [[Definition:Origin|origin]] $\tuple {0, 0}$ lies on the line $L$. That is, $\alpha_1 0 + \alpha_2 0 = \beta \implies \beta = 0$. Let the [[Equation of Straight Line in Plane|equation]] of $L$ be $L = \alpha_1 x_1 + \alpha_2 x_2 = 0$. Then $0 = \alpha_1 0 + \alpha_2 0 \in L$ and so $\tuple {0, 0}$ lies on the line $L$. Hence $L$ is [[Definition:Homogeneous (Analytic Geometry)|homogeneous]]. {{qed}}	1
Let $\struct {R_1, \norm {\, \cdot \,}_1 }, \struct {R_2, \norm {\, \cdot \,}_2 }$ be [[Definition:Normed Division Ring|normed division rings]]. Let $\phi: R_1 \to R_2$ be a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Homomorphism|ring homomorphism]]. Then: :$\forall x \in R_1 : \norm{\map \phi x}_2 = \norm x_1$	1
From [[Ring is Module over Itself]], it follows that: :$\struct {R, +, \circ}_R$ is an [[Definition:Module|$R$-module]]. :If $\struct {R, +, \circ}$ has a [[Definition:Unity of Ring|unity]], then $\struct {R, +, \circ}_R$ is [[Definition:Unitary Module|unitary]]. Now the theorem follows directly from [[Subring Module]]. {{qed}}	1
Let $\struct {G, +_G}$ be an [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Module|$R$-module]]. Let $x \in G, \lambda \in R, n \in \Z$. Let $\sequence {x_m}$ be a [[Definition:Sequence|sequence of elements of $G$]]. Let $\sequence {\lambda_m}$ be a [[Definition:Sequence|sequence of elements of $R$]] that is, [[Definition:Scalar (Module)|scalars]]. Then:	1
Let $\mathbf A$ be an $m \times n$ [[Definition:Matrix|matrix]]. Let $i, j \in \closedint 1 m: i \ne j$ Let $r_k$ denote the $k$th [[Definition:Row of Matrix|row]] of $\mathbf A$ for $1 \le k \le m$: :$r_k = \begin {pmatrix} a_{k 1} & a_{k 2} & \cdots & a_{k n} \end {pmatrix}$ Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] acting on $\mathbf A$ as: {{begin-axiom}} {{axiom | n = \text {ERO} 3 | t = Interchange [[Definition:Row of Matrix|rows]] $i$ and $j$ | m = r_i \leftrightarrow r_j }} {{end-axiom}} Then $e$ can be expressed as a [[Definition:Finite Sequence|finite sequence]] of exactly $4$ instances of the other two [[Definition:Elementary Row Operation|elementary row operations]]. {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Row of Matrix|row]] $i$ by $\lambda$ | m = r_i \to \lambda r_i }} {{axiom | n = \text {ERO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Row of Matrix|row]] $j$ to [[Definition:Row of Matrix|row]] $i$ | m = r_i \to r_i + \lambda r_j }} {{end-axiom}}	1
Let $\left({p}\right)$ be a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$. Let $p = f g$ be any [[Definition:Factorization|factorization]] of $p$. We must show that one of $f, g$ is a [[Definition:Unit of Ring|unit]]. Suppose that neither of $f, g$ is a [[Definition:Unit of Ring|unit]]. First it will be shown that: :$\left({p}\right) \subsetneqq \left({f}\right)$ Let $x \in \left({p}\right)$. That is: :$\exists q \in D: x = p q$ Then: :$x = f g q \in \left({f}\right)$ so: :$\left({p}\right) \subseteq \left({f}\right)$ Now suppose $f \in \left({p}\right)$. Then: :$\exists r \in D: f = r p$ and so from $p = f g$ above: :$f = r g f$ Therefore: :$r g = 1$ and so $g$ is a [[Definition:Unit of Ring|unit]]. This is a contradiction. Thus: :$f \notin \left({p}\right)$ and clearly: :$f \in \left({f}\right)$ so: :$\left({p}\right) \subsetneqq \left({f}\right)$ as claimed. {{Handwaving|"Clearly" needs to be replaced by a link to the definition which specifies this fact.}} Therefore, since $\left({p}\right)$ is [[Definition:Maximal Ideal of Ring|maximal]], we must have: :$\left({f}\right) = D$ But we assumed that $f$ is not a [[Definition:Unit of Ring|unit]]. So there is no $h \in D$ such that $f h = 1$. Therefore: :$1 \notin \left({f}\right) = \left\{{f h: h \in D}\right\}$ and: :$\left({f}\right) \subsetneqq D$ This is a contradiction. Therefore at least one of $f, g$ must be a [[Definition:Unit of Ring|unit]]. This completes the proof. {{qed}}	1
:$\paren {c \mathbf u + \mathbf v} \cdot \mathbf w = c \paren {\mathbf u \cdot \mathbf w} + \paren {\mathbf v \cdot \mathbf w}$	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]]: $0$. Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. Let $\sequence {x_{n_r} }$ be a [[Definition:Subsequence|subsequence]] of $\sequence {x_n}$. Then: :$\sequence {x_{n_r} }$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]].	1
A [[Definition:Vector (Linear Algebra)|vector]] is defined as an element of a [[Definition:Vector Space|vector space]]. We have that $\R^n$, with the operations of [[Definition:Vector Sum|vector addition]] and [[Definition:Scalar Multiplication on Vector Space|scalar multiplication]], form a [[Definition:Real Vector Space|real vector space]]. Hence a '''vector in $\R^n$''' is defined as any [[Definition:Element|element]] of $\R^n$. {{expand|While it is possible to identify a vector by a tuple in $\R^n$ we need to explain that the vector is not the point.}} === [[Definition:Vector (Euclidean Space)/Plane Vector|$\R^2$: Plane Vector]] === {{:Definition:Vector (Euclidean Space)/Plane Vector}} === [[Definition:Vector (Euclidean Space)/Space Vector|$\R^3$: Space Vector]] === {{:Definition:Vector (Euclidean Space)/Space Vector}}	1
The [[Definition:Ring of Integers Modulo m|ring of integers modulo $2$]] and the [[Definition:Parity Ring|parity ring]] are [[Definition:Isomorphic Algebraic Structures|isomorphic]].	1
We have: {{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2}^2 | r = \paren {z_1 + z_2} \paren {\overline {z_1} + \overline {z_2} } | c = [[Modulus in Terms of Conjugate]] and [[Sum of Complex Conjugates]] }} {{eqn | r = z_1 \overline {z_1} + z_2 \overline {z_2} + z_1\overline {z_2} + \overline {z_1} z_2 | c = }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \, \map \Re {z_1 \overline {z_2} } | c = [[Modulus in Terms of Conjugate]] and [[Sum of Complex Number with Conjugate]] }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \, \cmod{ z_1 } \cmod{ z_2 } \map \cos {\theta_1 - \theta_2} | c = [[Product of Complex Numbers in Polar Form]] and [[Argument of Conjugate of Complex Number]] }} {{end-eqn}} {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over one of the [[Definition:Standard Number System|standard number systems]]. For $\mathbf A, \mathbf B \in \map \MM {m, n}$, let $\mathbf A + \mathbf B$ be defined as the [[Definition:Matrix Entrywise Addition|matrix entrywise sum]] of $\mathbf A$ and $\mathbf B$. The operation $+$ is [[Definition:Associative Operation|associative]] on $\map \MM {m, n}$. That is: :$\paren {\mathbf A + \mathbf B} + \mathbf C = \mathbf A + \paren {\mathbf B + \mathbf C}$ for all $\mathbf A$, $\mathbf B$ and $\mathbf C$ in $\map \MM {m, n}$.	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime number]] $p$. Let $\size{\,\cdot\,}$ be the [[Definition:Absolute Value|absolute value]] on the [[Definition:Rational Numbers|rationals $\Q$]]. Then $\norm {\,\cdot\,}_p$ and $\size{\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. That is, the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_p$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\size {\,\cdot\,}$.	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]]. Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence in Normed Division Ring|null sequences]]. That is: :$\NN = \set {\sequence {x_n}: \displaystyle \lim_{n \mathop \to \infty} x_n = 0 }$ Then $\NN$ is a [[Definition:Ideal of Ring|ring ideal]] of $\CC$ that is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] and a [[Definition:Maximal Right Ideal of Ring|maximal right ideal]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\C$. Let $A \in B \left({H}\right)$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Let $B$ and $C$ be the [[Definition:Real Part (Linear Operator)|real]] and [[Definition:Imaginary Part (Linear Operator)|imaginary parts]] of $A$, respectively. Then $A = B + iC$.	1
{{begin-eqn}} {{eqn | l = \map \tr {\mathbf A} + \map \tr {\mathbf B} | r = \sum_{k \mathop = 1}^n a_{kk} + \sum_{k \mathop = 1}^n b_{kk} | c = {{Defof|Trace of Matrix}} }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {a_{kk} + b_{kk} } | c = [[Sum of Summations equals Summation of Sum]] }} {{eqn | r = \map \tr {\mathbf A + \mathbf B} | c = {{Defof|Matrix Entrywise Addition}}, {{Defof|Trace of Matrix}} }} {{end-eqn}} {{qed}}	1
Let $y \in D$ be non-zero. Then, using the principal ideal property, for some $f \in D \sqbrk X$ we have: :$\gen {y, X} = \gen f \subseteq D \sqbrk X$ Therefore: :$\exists p, q \in D \sqbrk X: y = f p, X = f q$ By [[Properties of Degree]] we conclude that $f = a$ and $q = b + c X$ for some $a, b, c \in D$. Substituting into the equation $X = f q$ we obtain: :$X = a b + a c X$ which implies that: :$a c = 1$ That is: :$a \in D^\times$ where $D^\times$ denotes the [[Definition:Group of Units of Ring|group of units]] of $D$. Therefore: :$\gen f = \gen 1 = D \sqbrk X$ Therefore: :$\exists r, s \in D \sqbrk X: r y + s X = 1$ If $d$ is the constant term of $r$, then we have $y d = 1$. Therefore $y \in D^\times$. Our choice of $y$ was arbitrary, so this shows that $D^\times \supseteq D \setminus \set 0$, which says precisely that $D$ is a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $A^\times$ be the [[Definition:Group of Units of Ring|group of units]] of $A$. Let $\map {\operatorname {Jac} } A$ be the [[Definition:Jacobson Radical|Jacobson radical]] of $A$. Then: :$\map {\operatorname {Jac} } A = \set {a \in A: 1_A - a x \in A^\times \text{ for all } x \in A}$ where $1_A$ is the [[Definition:Unity of Ring|unity]] of $A$.	1
Let $\displaystyle \forall i \in \closedint 1 m: \sum _{j \mathop = 1}^n {\alpha_{i j} x_j} = \beta_i$ be a system of [[Definition:Simultaneous Linear Equations|simultaneous linear equations]]. where all of $\alpha_1, \ldots, a_n, x_1, \ldots x_n, \beta_i, \ldots, \beta_m$ are elements of a [[Definition:Field (Abstract Algebra)|field]] $K$. Then $x = \tuple {x_1, x_2, \ldots, x_n}$ is a [[Definition:Solution to System of Simultaneous Equations|solution]] of this system {{iff}}: :$\sqbrk \alpha_{m n} \sqbrk x_{n 1} = \sqbrk \beta_{m 1}$ where $\sqbrk a_{m n}$ is an [[Definition:Matrix|$m \times n$ matrix]].	1
From [[Gaussian Integers form Integral Domain]], we have that $\struct {\Z \sqbrk i, +, \times}$ is an [[Definition:Integral Domain|integral domain]]. Let $a, d \in \Z \sqbrk i$ such that $d \ne 0$. Suppose $\cmod a \ge \cmod d$. Reference to an [[Definition:Argand Diagram|Argand diagram]] shows that one of: :$a + d, a - d, a + i d, a - i d$ is closer to the [[Definition:Origin|origin]] than $a$ is. So it is possible to subtract [[Definition:Gaussian Integer|Gaussian integer]] [[Definition:Multiple of Ring Element|multiples]] of $d$ from $a$ until the [[Definition:Square Function|square]] of the [[Definition:Complex Modulus|modulus]] of the remainder drops below $\cmod d^2$. That remainder can only take [[Definition:Integer|integer]] values. Thus a [[Division Theorem]] result follows: :$\exists q, r \in \Z \sqbrk i: a = q d + r$ where $\cmod r < \cmod d$. Let $J$ be an arbitrary non-[[Definition:Null Ideal|null]] [[Definition:Ideal of Ring|ideal]] of $\Z \sqbrk i$. Let $d$ be an [[Definition:Element|element]] of minimum [[Definition:Complex Modulus|modulus]] in $J$. Then the [[Division Theorem]] can be used to prove that $J = \ideal d$. {{finish|The above is the outline only.}}	1
From the definition of [[Definition:Negative Matrix|negative matrix]], we have: :$\forall i, j \in \closedint 1 n: \sqbrk {-a}_{i j} = -a_{i j}$ If $\mathbf A$ is an [[Definition:Upper Triangular Matrix|upper triangular matrix]], we have: :$\forall i > j: a_{i j} = 0$ Hence: :$\forall i > j: \sqbrk {-a}_{i j} = -a_{i j} = 0$ and so $-\mathbf A$ is itself [[Definition:Upper Triangular Matrix|upper triangular]]. Similarly, if $\mathbf A$ is a [[Definition:Lower Triangular Matrix|lower triangular matrix]], we have: :$\forall i < j: a_{i j} = 0$ Hence: :$\forall i < j: \sqbrk {-a}_{i j} = -a_{i j} = 0$ and so $-\mathbf A$ is itself [[Definition:Lower Triangular Matrix|lower triangular]]. {{Qed}} [[Category:Negative Matrices]] [[Category:Triangular Matrices]] ivy08olfqqzixab53xua25egndy4xer	1
Let $\mathbf A = \sqbrk a_n$ and $\mathbf B = \sqbrk b_n$ be [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order]] $n$. Let $\mathbf A$ and $\mathbf B$ be [[Definition:Similar Matrices|similar]]. Then: :$\map \tr {\mathbf A} = \map \tr {\mathbf B}$ where $\map \tr {\mathbf A}$ denotes the [[Definition:Trace of Matrix|trace]] of $\mathbf A$.	1
=== Positive definiteness === {{begin-eqn}} {{eqn | l = \norm {x}_\infty | r = \sup_{t \mathop \in I} \size {\map x t} | c = {{defof|Supremum Norm on Space of Continuous on Closed Interval Real-Valued Functions}} }} {{eqn | r = \max_{t \mathop \in I} \size {\map x t} | c = [[Weierstrass Extreme Value Theorem]] }} {{eqn | o = \ge | r = \size {\map x t} | c = {{defof|Max Operation}} }} {{eqn | o = \ge | r = 0 | c = [[Complex Modulus is Non-Negative]] }} {{end-eqn}} Suppose $\norm {x}_\infty = 0$. Then: {{begin-eqn}} {{eqn | l = 0 | r = \norm {x}_\infty }} {{eqn | r = \sup_{t \mathop \in I} \size {\map x t} | c = {{defof|Supremum Norm on Space of Continuous on Closed Interval Real-Valued Functions}} }} {{eqn | r = \max_{t \mathop \in I} \size {\map x t} | c = [[Weierstrass Extreme Value Theorem]] }} {{eqn | o = \ge | r = \size {\map x t} | c = {{defof|Max Operation}} }} {{eqn | o = \ge | r = 0 | c = [[Complex Modulus is Non-Negative]] }} {{eqn | ll = \leadsto | l = \size {\map x t} | r = 0 }} {{eqn | ll = \leadsto | l = \map x t | r = 0 | c = [[Complex Modulus equals Zero iff Zero]] }} {{end-eqn}} Therefore: :$\displaystyle \forall t \in I : \map x t = 0$ === Positive homogeneity === Let $\alpha \in \R$. {{begin-eqn}} {{eqn | l = \norm {\alpha \cdot x}_\infty | r = \sup_{t \mathop \in I} \size {\map {\paren {\alpha \cdot x} } t} | c = {{defof|Supremum Norm on Space of Continuous on Closed Interval Real-Valued Functions}} }} {{eqn | r = \max_{t \mathop \in I} \size {\map {\paren {\alpha \cdot x} } t} | c = [[Weierstrass Extreme Value Theorem]] }} {{eqn | r = \max_{t \mathop \in I} \size {\alpha {\map x t} } | c = {{defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \max_{t \mathop \in I} \size \alpha \size {\map x t} | c = [[Absolute Value of Product]] }} {{eqn | r = \size \alpha \max_{t \mathop \in I} \size {\map x t} }} {{eqn | r = \size \alpha \sup_{t \mathop \in I} \size {\map x t} | c = [[Weierstrass Extreme Value Theorem]] }} {{eqn | r = \size \alpha \norm {x}_\infty | c = {{defof|Supremum Norm on Space of Continuous on Closed Interval Real-Valued Functions}} }} {{end-eqn}} === Triangle inequality === {{begin-eqn}} {{eqn | l = \size {\map {\paren {x_1 + x_2} } t} | r = \size {\map {x_1} t + \map {x_2} t} | c = {{defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | o = \le | r = \size {\map {x_1} t} + \size {\map {x_2} t} | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | o = \le | r = \max_{t \mathop \in I} \size {\map {x_1} t} + \max_{t \mathop \in I} \size {\map {x_2} t} | c = {{defof|Max Operation}} }} {{eqn | r = \sup_{t \mathop \in I} \size {\map {x_1} t } + \sup_{t \mathop \in I} \size { \map {x_2} t } | c = [[Weierstrass Extreme Value Theorem]] }} {{eqn | r = \norm {x_1}_\infty + \norm {x_2}_\infty | c = {{defof|Supremum Norm on Space of Continuous on Closed Interval Real-Valued Functions}} }} {{end-eqn}}	1
Let $\displaystyle \map f {e_i} = \sum_{j \mathop = 1}^n c_{ij} e_j$ Let $A$ be the [[Definition:Matrix|matrix]] [[Definition:Relative Matrix|relative]] to the basis $\tuple {e_1, \ldots, e_n}$. Then by the above assumption, $A_{ij} = c_{ij}$. Then: {{begin-eqn}} {{eqn | l = \map \tr f | r = \map \tr A | c = {{Defof|Trace of Linear Operator}} }} {{eqn | r = \sum_{i \mathop = 1}^n A_{ii} | c = {{Defof|Trace of Matrix}} }} {{eqn | r = \sum_{i \mathop = 1}^n c_{ii} | c = from above }} {{end-eqn}} Now it remains to show that $c_{ii} = e_i^* \paren {\map f {e_i} }$: {{begin-eqn}} {{eqn | l = e_i^* \paren {\map f {e_i} } | r = e_i^* \paren {\sum_{j \mathop = 1}^n c_{ij} e_j} | c = From above assumption }} {{eqn | r = \sum_{j \mathop = 1}^n c_{ij} e_i^* \paren {e_j} | c = $e_i$ is a [[Definition:Linear Form|linear form]] }} {{eqn | r = \map {c_{ii} } 1 | c = {{Defof|Ordered Dual Basis}}: other terms vanish }} {{eqn | r = c_{ii} | c = }} {{end-eqn}} {{qed}}	1
By definition of the [[Definition:Unit Matrix|unit matrix]]: :$I_{a b} = \delta_{a b}$ where: :$I_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf I$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$. By definition, $\mathbf E$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $m$]] formed by applying $e$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$. That is, all [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $i$ of $\mathbf I$ are to have the corresponding [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $j$ added to them after the latter have been [[Definition:Ring Product|multiplied]] by $\lambda$. By definition of [[Definition:Unit Matrix|unit matrix]]: :all [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $i$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{i i}$, which is $1$. :all [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $j$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{j j}$, which is $1$. Thus in $\mathbf E$: :where $b \ne i$, $E_{a b} = \delta_{a b}$ :where $b = i$: ::$E_{a b} = \delta_{a b}$ where $a \ne j$ ::$E_{a b} = \delta_{a b} + \lambda \cdot 1$ where $a = j$ That is: :$E_{a b} = \delta_{a b}$ for all [[Definition:Element of Matrix|elements]] of $\mathbf E$ except where $b = i$ and $a = j$, at which [[Definition:Element of Matrix|element]]: :$E_{a b} = \delta_{a b} + \lambda$ That is: :$E_{a b} = \delta_{a b} + \lambda \cdot \delta_{b i} \cdot \delta_{j a}$ Hence the result. {{qed}}	1
{{ProofWanted}} [[Category:Bilinear Forms]] 8hy9bc6d969xfy7za814ga2i1fdehey	1
:$\psi$ is a [[Definition:Ring Isomorphism|ring isomorphism]].	1
From the [[Combination Theorem for Cauchy Sequences/Multiple Rule|Multiple Rule for Normed Division Ring Sequences]]: :$\sequence {a x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] :$\sequence {b y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. The result now follows directly from the [[Combination Theorem for Cauchy Sequences/Sum Rule|Sum Rule for Normed Division Ring Sequences]]: :$\sequence {a x_n + b y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. {{qed}}	1
The vectors $\left\Vert{\mathbf u}\right\Vert \mathbf v$ and $\left\Vert{\mathbf v}\right\Vert \mathbf u$ have equal length from [[Vector Times Magnitude Same Length As Magnitude Times Vector]]. Thus $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ is the diagonal of a [[Definition:Rhombus|rhombus]]. The result follows from [[Diagonals of Rhombus Bisect Angles]]. {{qed}}	1
Let $\mathbf A$ be [[Definition:Invertible Matrix|invertible]]. Let $1_R$ denote the [[Definition:Unity of Ring|unity]] of $R$. Let $\mathbf I_n$ denote the [[Definition:Unit Matrix|unit matrix of order $n$]]. Then: {{begin-eqn}} {{eqn | l = 1_R | r = \map \det {\mathbf I_n} | c = [[Determinant of Unit Matrix]] }} {{eqn | r = \map \det {\mathbf A \mathbf B} | c = {{Defof|Inverse Matrix}} }} {{eqn | r = \map \det {\mathbf A} \, \map \det {\mathbf B} | c = [[Determinant of Matrix Product]] }} {{end-eqn}} This shows that: :$\map \det {\mathbf B} = \dfrac 1 {\map \det {\mathbf A} }$ {{qed|lemma}}	1
By [[Elementary Matrix corresponding to Elementary Column Operation/Scale Column and Add|Elementary Matrix corresponding to Elementary Column Operation: Scale Column and Add]], $\mathbf E_2$ is of the form: :$E_{a b} = \delta_{a b} + \lambda \cdot \delta_{b i} \cdot \delta_{j a}$ where: :$E_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf E$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$ :$\delta_{a b}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: ::$\delta_{a b} = \begin {cases} 1 & : \text {if $a = b$} \\ 0 & : \text {if $a \ne b$} \end {cases}$ Because $i \ne j$ it follows that: :if $a = i$ and $b = j$ then $a \ne b$ Hence when $a = b$ we have that: :$\delta_{b i} \cdot \delta_{j a} = 0$ Hence the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf E_2$ are all equal to $1$. We also have that $\delta_{b i} \cdot \delta_{j a} = 1$ {{iff}} $a = i$ and $b = j$. Hence, all [[Definition:Element of Matrix|elements]] of $\mathbf E_2$ apart from the [[Definition:Diagonal Element|diagonal elements]] and $a_{i j}$ are equal to $0$. Thus $\mathbf E_2$ is a [[Definition:Triangular Matrix|triangular matrix]] (either [[Definition:Upper Triangular Matrix|upper]] or [[Definition:Lower Triangular Matrix|lower]]). From [[Determinant of Triangular Matrix]], $\map \det {\mathbf E_2}$ is equal to the [[Definition:Multiplication|product]] of all the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf E_2$. But as we have seen, these are all equal to $1$. Hence the result. {{qed}}	1
A [[Definition:Banach Space|Banach space]] is a [[Definition:Normed Vector Space|normed vector space]], where a [[Definition:Cauchy Sequence|Cauchy sequence]] [[Definition:Convergent Sequence in Normed Vector Space|converges]] {{WRT}} the supplied [[Definition:Norm on Vector Space|norm]]. To prove the theorem, we need to show that a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $\struct {\ell^p, \norm {\,\cdot\,}_p}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]]. We take a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] $\sequence {x_n}_{n \mathop \in \N}$ in $\struct {\ell^p, \norm {\,\cdot\,}_p}$. Then we consider the $k$th component and show, that a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$ with the [[Definition:Limit of Real Sequence|limit]] $x^{\paren k}$ and denote the entire set as $\mathbf x$. Finally, we show that $\sequence {\mathbf x_n}_{n \in \N}$, composed of components $x_n^{\paren k},$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] in $\struct {\ell^p, \norm {\,\cdot\,}_p}$ with the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $\mathbf x$. Let $\sequence {\mathbf x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\ell^p, \norm{\, \cdot \,}_p}$. Denote the $k$th component of $\mathbf x_n$ by $x_n^{\paren k}$. === $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ converges in $\struct {\R, \size {\, \cdot \,}}$=== Let $\epsilon >0$. Then: :$\displaystyle \exists N \in \N : \forall m,n \in \N : m,n > N : \norm {\mathbf x_n - \mathbf x_m}_p < \epsilon$ For same $N, m, n$ consider $\size {x_n^{\paren k} - x_m^{\paren k} } $: {{begin-eqn}} {{eqn | l = \size {x_n^{\paren k} - x_m^{\paren k} } | r = \paren {\size {x_n^{\paren k} - x_m^{\paren k} }^p}^{\frac 1 p} }} {{eqn | o = \le | r = \paren {\sum_{k \mathop = 0}^\infty \size {x_n^{\paren k} - x_m^{\paren k} }^p}^{\frac 1 p} }} {{eqn | r = \norm {\mathbf x_n - \mathbf x_m}_p | c = {{defof|P-Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Hence, $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\R, \size {\, \cdot \,}}$. From [[Real Number Line is Complete Metric Space]], $\R$ is a [[Definition:Complete Metric Space|complete metric space]]. Consequently, $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ [[Definition:Convergent Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$. {{qed|lemma}} Denote the [[Definition:Limit of Real Sequence|limit]] $\displaystyle \lim_{n \mathop \to \infty} \sequence {x_n^{\paren k}}_{n \mathop \in \N} = x^{\paren k}$. Denote $\sequence {x^{\paren k}}_{k \mathop \in \N} = \mathbf x$. === $\mathbf x$ belongs to $\ell^p$ === Let $\epsilon > 0$. Then: :$\exists N \in \N : \forall n,m \in \N : n,m > N : \norm {\mathbf x_n - \mathbf x_m}_p < \epsilon$. Let $K \in \N$, $1 \le p < \infty$. Then: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^K \size {x_n^{\paren k} - x_m^{\paren k} }^p | o = \le | r = \sum_{k \mathop = 1}^\infty \size {x_n^{\paren k} - x_m^{\paren k} }^p }} {{eqn | r = \norm {\mathbf x_n - \mathbf x_m}_p^p }} {{eqn | o = < | r = \epsilon^p }} {{eqn | o = < | r = \infty }} {{end-eqn}} Take the [[Definition:Limit of Rational Sequence|limit]] $m \to \infty$: :$\displaystyle \sum_{k \mathop = 1}^K \size {x_n^{\paren k} - x^{\paren k}}^p < \infty$ Since $K$ was arbitrary, we can take the [[Definition:Limit of Real Sequence|limit]] $K \to \infty$. By [[Definition:P-Sequence Space|definition]], $\forall k \in \N : x_n^{\paren k} - x^{\paren k} \in \ell^p$. In other words, $\mathbf x_n - \mathbf x \in \ell^p$. But, by [[Definition:Assumption|assumption]], $\mathbf x_n \in \ell^p$. Therefore: :$\paren {\mathbf x - \mathbf x_n} + \mathbf x_n = \mathbf x \in \ell^p$ {{qed|lemma}} === $\sequence {\mathbf x_n}_{n \mathop \in \N}$ converges in $\struct {\ell^p, \norm {\, \cdot \,}_p}$ to $\mathbf x$=== Let $1 \le p < \infty$. Let $\epsilon > 0$. Fix $N \in \N$, $m > N$. Then: :$\displaystyle \forall n > N : \norm {\mathbf x_n - \mathbf x_m}_p < \epsilon$ We have that $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$. Take the [[Definition:Limit of Real Sequence|limit]] $m \to \infty$: {{begin-eqn}} {{eqn | l = \lim_{m \mathop \to \infty} \norm {\mathbf x_n - \mathbf x_m}_p | r = \lim_{m \mathop \to \infty} \paren {\sum_{k \mathop = 0}^\infty \size {x_n^{\paren k} - x_m^{\paren k} }^p }^{\frac 1 p} }} {{eqn | r = \paren {\sum_{k \mathop = 0}^\infty \size {x_n^{\paren k} - x^{\paren k} }^p }^{\frac 1 p} | c = [[Limit of Composite Function]] }} {{eqn | r = \norm {\mathbf x_n - \mathbf x}_p }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} So $\sequence {\mathbf x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] in $\struct {\ell^p, \norm {\, \cdot \,}_p}$. {{qed}}	1
=== Bilinearity of $\oplus'$ === First we need to show that $\oplus'$ is [[Definition:Bilinear Mapping|bilinear]]. $(1): \quad$ Let $\left({a_1, b_1}\right), \left({a_2, b_2}\right), \left({c, d}\right) \in A'$. Then: {{begin-eqn}} {{eqn | o = | r = \left({\left({a_1, b_1}\right) + \left({a_2, b_2}\right)}\right) \oplus' \left({c, d}\right) | c = }} {{eqn | r = \left({a_1 + a_2, b_1 + b_2}\right) \oplus' \left({c, d}\right) | c = }} {{eqn | r = \left({\left({a_1 + a_2}\right) \oplus c - d \oplus \left({b_1 + b_2}\right)^*, \left({a_1 + a_2}\right)^* \oplus d + c \oplus \left({b_1 + b_2}\right)}\right) | c = }} {{eqn | r = \left({a_1 \oplus c + a_2 \oplus c - d \oplus b_1^* - d \oplus b_2^*, a_1^* \oplus d + a_2^* \oplus d + c \oplus b_1 + c \oplus b_2}\right) | c = as $\oplus$ is [[Definition:Bilinear Mapping|bilinear]] }} {{eqn | r = \left({a_1 \oplus c - d \oplus b_1^*, a_1^* \oplus d + c \oplus b_1}\right) + \left({a_2 \oplus c - d \oplus b_2^*, a_2^* \oplus d + c \oplus b_2}\right) | c = }} {{eqn | r = \left({\left({a_1, b_1}\right) \oplus' \left({c, d}\right)}\right) + \left({\left({a_2, b_2}\right) \oplus' \left({c, d}\right)}\right) | c = }} {{end-eqn}} Similarly (and equally tediously) we can show that: :$\left({c, d}\right) \oplus' \left({\left({a_1, b_1}\right) + \left({a_2, b_2}\right)}\right) = \left({\left({c, d}\right) \oplus' \left({a_1, b_1}\right)}\right) + \left({\left({c, d}\right) \oplus' \left({a_2, b_2}\right)}\right)$ $(2): \quad$ Let $\left({a, b}\right), \left({c, d}\right) \in A'$ and $\alpha, \beta \in \R$. Then: {{begin-eqn}} {{eqn | o = | r = \left({\alpha \left({a, b}\right)}\right) \oplus' \left({c, d}\right) | c = }} {{eqn | r = \left({\alpha a, \alpha b}\right) \oplus' \left({c, d}\right) | c = as $\oplus$ is [[Definition:Bilinear Mapping|bilinear]] }} {{eqn | r = \left({\alpha a \oplus c - d \oplus \alpha b^*, \alpha a^* \oplus d + c \oplus \alpha b}\right) | c = }} {{eqn | r = \alpha \left({a \oplus c - d \oplus b^*, a^* \oplus d + c \oplus b}\right) | c = as $\oplus$ is [[Definition:Bilinear Mapping|bilinear]] }} {{eqn | r = \alpha \left({a, b}\right) \oplus' \left({c, d}\right) | c = }} {{end-eqn}} Similarly: :$\left({a, b}\right) \oplus' \left({\left({c, d}\right) \beta}\right) = \left({a, b}\right) \oplus' \left({c, d}\right) \beta$ So $\oplus'$ has been shown to be a [[Definition:Bilinear Mapping|bilinear mapping]]. === Conjugate Nature of $*'$ === We have that: :$\forall \left({a, b}\right) \in A': {\left({a, b}\right)^*}' = \left({a^*, -b}\right)$ So: {{begin-eqn}} {{eqn | o = | r = {\left({ {\left({a, b}\right)^*}'}\right)^*}' | c = }} {{eqn | r = {\left({a^*, -b}\right)^*}' | c = }} {{eqn | r = \left({\left({a^*}\right)^*, -\left({-b}\right)}\right) | c = }} {{eqn | r = \left({a, b}\right) | c = as $*$ is a [[Definition:Conjugation (Abstract Algebra)|conjugation]] on $A$ }} {{end-eqn}} Finally: {{begin-eqn}} {{eqn | o = | r = {\left({\left({a, b}\right) \oplus' \left({c, d}\right)}\right)^*}' | c = }} {{eqn | r = {\left({a \oplus c - d \oplus b^*, a^* \oplus d + c \oplus b}\right)^*}' | c = }} {{eqn | r = \left({\left({a \oplus c - d \oplus b^*}\right)^*, -\left({a^* \oplus d + c \oplus b}\right)}\right) | c = }} {{eqn | r = \left({c^* \oplus a^* - b \oplus d^*, -\left({c \oplus b + a^* \oplus d}\right)}\right) | c = }} {{eqn | r = \left({c^*, -d}\right) \oplus' \left({a^*, -b}\right) | c = }} {{eqn | r = {\left({c, d}\right)^*}' \oplus' {\left({a, b}\right)^*}' | c = }} {{end-eqn}} thus proving that $*'$ is a [[Definition:Conjugation (Abstract Algebra)|conjugation]] on $A'$. Hence the result. {{qed}}	1
Utilizing the [[Definition:Vector Space Axioms|vector space axioms]]: {{begin-eqn}} {{eqn | l = \mathbf c + \mathbf a | r = \mathbf c + \mathbf b }} {{eqn | ll = \implies | l = \mathbf a + \mathbf c | r = \mathbf b + \mathbf c }} {{eqn | ll = \implies | l = \mathbf a | r = \mathbf b | c = [[Vectors are Right Cancellable]] }} {{end-eqn}} {{qed}} [[Category:Vector Algebra]] 99qrr8db0lqjny0a9ktic3ydue0x6ux	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct {S, +, *}$ be a [[Definition:Ring with Unity|ring with unity]]. Let $f: R \to S$ be a [[Definition:Ring Homomorphism|ring homomorphism]]. Let the [[Definition:Image of Set under Mapping|image]] of $f$ be a [[Definition:Subset|subset]] of the [[Definition:Center of Ring|center]] of $S$. Let $\struct {S_R, *}$ be the [[Definition:Algebra Defined by Ring Homomorphism|algebra defined by the ring homomorphism]] $f$. Then $\struct {S_R, *}$ is an [[Definition:Associative Algebra|associative algebra]].	1
Let $\mathbf u$, $\mathbf v$ be non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Real Euclidean Space|Euclidean space]] $\R^n$. Then $\mathbf u$ and $\mathbf v$ are [[Definition:Orthogonal (Linear Algebra)|orthogonal]] {{iff}} they are [[Definition:Perpendicular (Linear Algebra)|perpendicular]].	1
Let $A = \left({A_F, \oplus}\right)$ be a [[Definition:Star-Algebra|$*$-algebra]]. Let $A' = \left({A_F, \oplus'}\right)$ be constructed from $A$ using the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]]. Then $A$ is a [[Definition:Real Star-Algebra|real algebra]] {{iff}} $A'$ is a [[Definition:Commutative Algebra|commutative algebra]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\C$. Let $A \in B \left({H}\right)$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Suppose that: :$\forall h \in H: \left\langle{Ah, h}\right\rangle_H = 0$ Then $A$ is the [[Definition:Zero Operator|zero operator]].	1
Let $\map \det {\mathbf A}$ be [[Definition:Unit of Ring|invertible]] in $R$. From [[Matrix Product with Adjugate Matrix]]: {{begin-eqn}} {{eqn | l = \mathbf A \cdot \adj {\mathbf A} | r = \map \det {\mathbf A} \cdot \mathbf I_n }} {{eqn | l = \adj {\mathbf A} \cdot \mathbf A | r = \map \det {\mathbf A} \cdot \mathbf I_n }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = \mathbf A \cdot \paren {\map \det {\mathbf A}^{-1} \cdot \adj {\mathbf A} } | r = \mathbf I_n }} {{eqn | l = \paren {\map \det {\mathbf A}^{-1} \cdot \adj {\mathbf A} } \cdot \mathbf A | r = \mathbf I_n }} {{end-eqn}} Thus $\mathbf A$ is [[Definition:Invertible Matrix|invertible]], and: :$\mathbf A^{-1} = \map \det {\mathbf A}^{-1} \cdot \adj {\mathbf A}$ {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\struct {S, \norm {\, \cdot \,}_S}$ be a [[Definition:Normed Division Subring|normed division subring]] of $\struct {R, \norm {\, \cdot \,}}$. Then the [[Definition:Inclusion Mapping|inclusion mapping]] $i : S \to R$ is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]].	1
Let $D$ be a [[Definition:Principal Ideal Domain|principal ideal domain]] whose [[Definition:Ring Zero|zero]] is $0_D$. Let $J \subseteq D$ be a [[Definition:Nonzero Ideal of Ring|nonzero]] [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]]. Then $J$ is [[Definition:Maximal Ideal of Ring|maximal]].	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\left\vert{z^n}\right\vert = \left\vert{z}\right\vert^n$ $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \left\vert{z^0}\right\vert | r = \left\vert{1}\right\vert | c = }} {{eqn | r = 1 | c = }} {{eqn | r = \left\vert{z}\right\vert^0 | c = }} {{end-eqn}} Thus $P \left({0}\right)$ is seen to hold. === Basis for the Induction === $P \left({1}\right)$ is the case: {{begin-eqn}} {{eqn | l = \left\vert{z^1}\right\vert | r = \left\vert{z}\right\vert | c = }} {{eqn | r = \left\vert{z}\right\vert^1 | c = }} {{end-eqn}} Thus $P \left({1}\right)$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Basis for the Induction === $P \left({2}\right)$ is the case: : $\left\vert{z^2}\right\vert = \left\vert{z}\right\vert^2$ which is demonstrated in [[Square of Complex Modulus equals Complex Modulus of Square]]. Thus $P \left({2}\right)$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\left\vert{z^k}\right\vert = \left\vert{z}\right\vert^k$ from which it is to be shown that: :$\left\vert{z^{k + 1} }\right\vert = \left\vert{z}\right\vert^{k + 1}$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \left\vert{z^{k + 1} }\right\vert | r = \left\vert{z^k \cdot z}\right\vert | c = }} {{eqn | r = \left\vert{z^k}\right\vert \cdot \left\vert{z}\right\vert | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | r = \left\vert{z}\right\vert^k \cdot \left\vert{z}\right\vert | c = [[Power of Complex Modulus equals Complex Modulus of Power#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \left\vert{z}\right\vert^{k + 1} | c = }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \ Z_{\ge 0}: \left\vert{z^n}\right\vert = \left\vert{z}\right\vert^n$ {{qed}}	1
{{proof wanted}} [[Category:Dedekind Domains]] 4chme548wp166f2psqg7whw3rfq5f49	1
{{MissingLinks}} {{proofread|Use of injectivity of $g$ here is implicit; but I ''think'' it's still rigorous without details: second opinion wecome}} By the definition of the [[Definition:Matrix|matrix]] $A$: : $A \circ f = g \circ \phi$. Therefore if $x \in \ker \phi$ we have: :$A f \left({x}\right) = g \left({\phi \left({x}\right)}\right) = g \left({0}\right) = 0$ This shows that: :$f \left[{\ker \phi}\right] \subseteq N \left({A}\right)$ Now let $x = \left({x_1, \ldots, x_n}\right) \in N \left({A}\right)$. Let $y = x_1 e_1 + \cdots + x_n e_n \in V$. We have: :$g \circ \phi \left({y}\right) = A \circ f \left({y}\right) = A \left({x_1, \ldots, x_n}\right)^T = 0$ so $\phi \left({y}\right) = 0 f_1 + \cdots + 0 f_m = 0$. This shows that $y \in \ker \phi$. Since $x = f \left({y}\right)$, we have shown that $N \left({A}\right) \subseteq f \left({\ker \phi}\right)$. Therefore $f \left({\ker \phi}\right) = N \left({A}\right)$. We deduce immediately from the definitions that $\ker \phi \subseteq f^{-1} \left[{N \left({A}\right)}\right]$. Now suppose that $x \in f^{-1} \left[{N \left({A}\right)}\right]$. Then: :$f \left({x}\right) \in N \left({A}\right)$ Therefore: :$g \circ \phi \left({x}\right) = A \circ f \left({x}\right) = 0$ so $\phi \left({x}\right) = 0 f_1 + \cdots + 0 f_m = 0$. This shows that: :$f^{-1} \left[{N \left({A}\right)}\right] \subseteq \ker \phi$ as required. {{Qed}} [[Category:Null Spaces]] e2zc7tcpsda1ptoyvlgvmp6erkhx24e	1
Let $I = \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $\struct {\map {\CC^k} I, +, \, \cdot \,}_\R$ be the [[Space of Real-Valued k-times Differentiable on Closed Interval Functions with Pointwise Addition and Pointwise Scalar Multiplication forms Vector Space|vector space of real-valued functions, k-times differentiable on]] $I$. Let $x \in \map {\CC^k} I$ be a [[Definition:Real-Valued Function|real-valued function]] of [[Definition:Differentiability Class|differentiability class]] $k$. Let $\norm {\, \cdot \,}_{\map {C^k} I}$ be the [[Definition:C^k Norm|$C^k$ norm]] on $I$. Then $\norm {\, \cdot \,}_{\map {C^k} I}$ is a [[Definition:Norm on Vector Space|norm]] on $\struct {\map {\CC^k} I, +, \, \cdot \,}_\R$.	1
Let $\map {B_1} 0 = \set {x \in X : \norm x < 1}$ be an [[Definition:Open Ball in Normed Vector Space|open ball]]. Let $\map {B_1^-} 0 = \set {x \in X : \norm x \le 1}$ be a [[Definition:Closed Ball in Normed Vector Space|closed ball]]. Then: :$\displaystyle X = \Bbb S \bigcup \relcomp X {\Bbb S}$ where :$\displaystyle \relcomp X {\Bbb S} = \map {B_1} 0 \bigcup \paren {X \setminus \map {B_1^-} 0}$ is the [[Definition:Relative Complement|relative complement]] of $\Bbb S$ in $X$. We have that [[Closed Ball is Closed in Normed Vector Space]]. By [[Definition:Closed Set in Normed Vector Space|definition]], $X \setminus \map {B_1^-} 0$ is [[Definition:Open Set in Normed Vector Space|open]]. Furthermore, [[Open Ball is Open Set in Normed Vector Space]]. By [[Union of Open Sets of Normed Vector Space is Open]], $\relcomp X {\Bbb S}$ is [[Definition:Open Set in Normed Vector Space|open]]. By definition, $\Bbb S$ is [[Definition:Closed Set in Normed Vector Space|closed]]. {{qed}}	1
=== Necessary Condition === By definition of [[Definition:Linear Combination of Subset|linear combination of subset]]: :Every [[Definition:Linear Combination of Sequence|linear combination]] of $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ is a [[Definition:Linear Combination of Subset|linear combination]] of $\set {a_k: 1 \mathop \le k \mathop \le n}$. {{qed|lemma}} === Sufficient Condition === Let $b$ be a [[Definition:Linear Combination of Subset|linear combination]] of $\set {a_k: 1 \mathop \le k \mathop \le n} = \set {a_1, a_2, \ldots, a_n}$. Then there exists: :a [[Definition:Finite Sequence|sequence]] $\sequence {c_j}_{1 \mathop \le j \mathop \le m}$ of elements of $\set {a_1, a_2, \ldots, a_n}$ and: :a [[Definition:Finite Sequence|sequence]] $\sequence {\mu_j}_{1 \mathop \le j \mathop \le m}$ of [[Definition:Scalar (Module)|scalars]] such that: ::$\displaystyle b = \sum_{j \mathop = 1}^m \mu_j c_j$ For each $k \in \closedint 1 n$, let $\lambda_k$ be defined as follows. If: :$a_k \in \set {c_1, c_2, \ldots, c_m}$ and: :$a_i \ne a_j$ for all indices $i$ such that $1 \le i < k$ let $\lambda_k$ be the sum of all [[Definition:Scalar (Module)|scalars]] $\mu_j$ such that $c_j = a_k$. If: :$a_k \notin \set {c_1, c_2, \ldots, c_m}$ or: :$a_i = a_j$ for some index $i$ such that $1 \le i < k$ let $\lambda_k = 0$. It follows that: :$\displaystyle b = \sum_{j \mathop = 1}^m \mu_j c_j = \sum_{k \mathop = 1}^n \lambda_k a_k$ Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ and $\sequence {b_j}_{1 \mathop \le j \mathop \le m}$ be [[Definition:Finite Sequence|sequences]] of elements of $G$ such that $\set {a_1, a_2, \ldots, a_n}$ and $\set {b_1, b_2, \ldots, b_m}$ are identical. Then as a consequence of the above: :an element is a [[Definition:Linear Combination of Sequence|linear combination]] of $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ {{iff}}: :it is a [[Definition:Linear Combination of Subset|linear combination]] of $\set {a_k: 1 \mathop \le k \mathop \le n}$ {{qed}} {{explain|The above lacks coherence. Source is to be revisited and reinterpreted.}}	1
First consider the classical form of the [[Definition:Vandermonde Matrix|Vandermonde matrix]]: :$W_n = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 1} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 1} \\ \end{bmatrix}$ By [[Vandermonde Determinant]], the [[Definition:Determinant of Matrix|determinant]] of $W_n$ is: :$\displaystyle \map \det {W_n} = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {x_i - x_j} \ne 0$ Since this is non-zero, by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], the [[Definition:Inverse Matrix|inverse matrix]], denoted $B = \sqbrk {b_{i j} }$, is guaranteed to exist. Using the definition of the [[Definition:Matrix Product (Conventional)|matrix product]] and the [[Definition:Inverse Matrix|inverse]]: :$\displaystyle \sum_{k \mathop = 1}^n b_{k j} x_i^{k - 1} = \delta_{i j}$ That is, if $\map {P_j} x$ is the [[Definition:Polynomial (Analysis)|polynomial]]: :$\displaystyle \map {P_j} x := \sum_{k \mathop = 1}^n b_{k j}x^{k - 1}$ then: :$\map {P_j} {x_1} = 0, \ldots, \map {P_j} {x_{j - 1} } = 0, \map {P_j} {x_j} = 1, \map {P_j} {x_{j + 1} } = 0, \ldots, \map {P_j} {x_n} = 0$ By the [[Lagrange Interpolation Formula]], the $j$th [[Definition:Row of Matrix|row]] of $B$ is composed of the [[Definition:Polynomial Coefficient|coefficients]] of the $j$th [[Definition:Lagrange Basis Polynomial|Lagrange basis polynomial]]: :$\displaystyle \map {P_j} x = \sum_{k \mathop = 1}^n b_{k j} x^{k - 1} = \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \frac {x - x_m} {x_j - x_m}$ Identifying the $k$th order [[Definition:Polynomial Coefficient|coefficient]] in these two [[Definition:Polynomial (Analysis)|polynomial]]s yields: :$b_{k j} = \begin{cases} \paren {-1}^{n - k} \paren {\dfrac {\displaystyle \sum_{\substack {1 \mathop \le m_1 \mathop < \ldots \mathop < m_{n - k} \mathop \le n \\ m_1, \ldots, m_{n - k} \mathop \ne j} } x_{m_1} \cdots x_{m_{n - k} } } {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m} } } & : 1 \le k < n \\ \qquad \qquad \qquad \dfrac 1 {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m}} & : k = n \end{cases}$ which gives: :$b_{k j} = \begin{cases} \paren {-1}^{k - 1} \paren {\dfrac {\displaystyle \sum_{\substack {1 \mathop \le m_1 \mathop < \ldots \mathop < m_{n - k} \mathop \le n \\ m_1, \ldots, m_{n - k} \mathop \ne j} } x_{m_1} \cdots x_{m_{n - k} } } {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_m - x_j} } } & : 1 \le k < n \\ \qquad \qquad \qquad \dfrac 1 {\displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne j} } \paren {x_j - x_m} } & : k = n \end{cases}$ For the general case, we observe that by simple multiplication: :$\displaystyle V_n = \begin {pmatrix} \begin {bmatrix} x_1 & 0 & \cdots & 0 \\ 0 & x_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & x_n \end {bmatrix} \cdot W_n \end {pmatrix}^\intercal$ So by [[Inverse of Matrix Product]] and [[Inverse of Diagonal Matrix]]: :$\displaystyle V_n^{-1} = \begin {bmatrix} x_1^{-1} & 0 & \cdots & 0 \\ 0 & x_2^{-1} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & x_n^{-1} \end {bmatrix} \cdot \paren {W_n^{-1} }^\intercal$ Let $c_{k j}$ denote the $\tuple {k, j}$th coefficient of $V_n^{-1}$. Since the first matrix in the product expression for $V_n^{-1}$ above is [[Definition:Diagonal Matrix|diagonal]]: :$c_{kj} = \dfrac 1 {x_k} b_{j k}$ which establishes the result. {{Qed}}	1
From [[Product of Change of Basis Matrices]] and [[Change of Basis Matrix Between Equal Bases]]: * $\left[{I_M; \mathcal A, \mathcal B}\right] \left[{I_M; \mathcal B, \mathcal A}\right] = \left[{I_M; \mathcal A, \mathcal A}\right] = I_n$ * $\left[{I_M; \mathcal B, \mathcal A}\right] \left[{I_M; \mathcal A, \mathcal B}\right] = \left[{I_M; \mathcal B , \mathcal B}\right] = I_n$ Hence the result. {{qed}}	1
Let $\struct {S, \cdot}$ be an [[Definition:Algebraic Structure|algebraic structure]]. Let $\map {\MM_S} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. For $\mathbf A, \mathbf B \in \map {\MM_S} {m, n}$, let $\mathbf A \circ \mathbf B$ be defined as the [[Definition:Hadamard Product|Hadamard product]] of $\mathbf A$ and $\mathbf B$. The operation $\circ$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Closure (Abstract Algebra)|closed]] on $\struct {S, \cdot}$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $E = \left\{{e_1, \ldots, e_n}\right\}$ be an [[Definition:Orthonormal Subset|orthonormal subset]] of $H$. Let $M = \vee E$, the [[Definition:Closed Linear Span|closed linear span]] of $E$. Then the [[Definition:Orthogonal Projection|orthogonal projection]] $P$ onto $M$ satisfies, $\forall h \in H$: :$Ph = \displaystyle \sum_{k=1}^n \left\langle{h, e_k}\right\rangle e_k$	1
{{begin-eqn}} {{eqn | l = \left({c \mathbf u + \mathbf v}\right) \times \mathbf w | r = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ c u_i + v_i & c u_j + v_j & c u_k + v_k \\ w_i & w_j & w_k \end{vmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ c u_i & c u_j & c u_k \\ w_i & w_j & w_k \end{vmatrix} + \begin{vmatrix} \mathbf i& \mathbf j & \mathbf k \\ v_i & v_j & v_k \\ w_i & w_j & w_k \end{vmatrix} | c = [[Determinant as Sum of Determinants]] }} {{eqn | r = c \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ u_i & u_j & u_k \\ w_i & w_j & w_k \end{vmatrix} + \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ v_i & v_j & v_k \\ w_i & w_j & w_k \end{vmatrix} | c = [[Determinant with Row Multiplied by Constant]] }} {{eqn | r = c \left({\mathbf u \times \mathbf w}\right) + \mathbf v \times \mathbf w | c = {{Defof|Vector Cross Product}} }} {{end-eqn}} {{qed}} [[Category:Vector Cross Product]] bhw03ywmrmsthq70e9dmqa9snur0fg6	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A0}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Closed Algebraic Structure|closed]]. The result then follows directly from [[Closure of Hadamard Product]]. {{qed}}	1
Let $p > 3$. Then there exists $a \in \Z: 1 < a < p-1$. Consider the [[Definition:Sequence|sequence]] $\sequence {x_n} \subseteq \Q$ where $x_n = a^{p^n}$ for some $a \in \Z: 1 < a < p-1$. Let $n \in \N$. Then: :$\norm {a^{p^{n + 1} } - a^{p^n} }_p = \norm {a^{p^n} (a^{p^n \left({p - 1}\right)} - 1) }_p$ From the [[Euler's Theorem/Corollary 1|corollary to Euler's Theorem]]: :$a^{p^n \left({p - 1}\right)} - 1 \equiv 0 \pmod {p^n}$ so: :$\norm {a^{p^n} \paren {a^{p^n \paren {p - 1} } - 1} }_p \le p^{-n} \xrightarrow {n \to \infty} 0$ That is: :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_{n + 1} - x_n}_p = 0$ By [[Characterisation of Cauchy Sequence in Non-Archimedean Norm]] :$\sequence {x_n }$ is a [[Definition:Cauchy Sequence|cauchy sequence]] in $\struct {\Q, \norm {\,\cdot\,}_p }$. {{AimForCont}} $\sequence {x_n}$ [[Definition:Convergent Sequence|converges]] to some $x \in \Q$. That is: :$x = \displaystyle \lim_{n \mathop \to \infty} x_n$ By [[Modulus of Limit/Normed Division Ring|Modulus of Limit on a Normed Division Ring]]: :$\displaystyle \lim_{n \mathop \to \infty} \norm {x_n }_p = \norm {x }_p$ Since $\forall n, p \nmid a^{p^n} = x_n$, then: :$ \norm {x_n }_p = 1$ So: :$\norm x_p = \displaystyle \lim_{n \mathop \to \infty} \norm {x_n}_p = 1$ By Axiom (N1) of a [[Definition:Norm on Division Ring|norm on a division ring]] then: :$x \ne 0$. Since: {{begin-eqn}} {{eqn | l = x | r = \lim_{n \mathop \to \infty} x_n | c = Definition of $x$ }} {{eqn | r = \lim_{n \mathop \to \infty} x_{n + 1} | c = [[Limit of Subsequence equals Limit of Sequence]] }} {{eqn | r = \lim_{n \mathop \to \infty} \paren {x_n}^p | c = Definition of $x_n$ }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} x_n}^p | c = [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|Product rule for Normed Division Rings]] }} {{eqn | r = x^p | c = Definition of $x$ }} {{end-eqn}} and $x \ne 0$ then: :$x^{p-1} = 1$ So: :$x = 1$ or $x = -1$ and so $a-x$ is an integer: :$0 < a - x < p$ It follows that: :$p \nmid \paren {a - x}$ and so: :$\norm {x - a}_p = 1$ Since $x_n \to x$ as $n \to \infty$ then: :$\exists N: \forall n > N: \norm {x_n - x}_p < \norm {x - a}_p$ That is: :$\exists N: \forall n > N: \norm {a^{p^n} - x}_p < \norm {x - a}_p$ Let $n > N$: {{begin-eqn}} {{eqn | l = \norm {x - a}_p | r = \norm {x - a^{p^n} + a^{p^n} - a}_p }} {{eqn | o = \le | r = \max \set {\norm {x - a^{p^n} }_p, \norm {a^{p^n} - a}_p} | c = [[P-adic Norm is Non-Archimedean Norm]] }} {{end-eqn}} As $\norm {x - a^{p^n} }_p < \norm {x - a}_p$: {{begin-eqn}} {{eqn | l = \norm {x - a}_p | r = \norm {a^{p^n} - a}_p | c = [[Three Points in Ultrametric Space have Two Equal Distances]] }} {{eqn | r = \norm a_p \norm {a^{p^n - 1} - 1}_p | c = Axiom (N2) of a [[Definition:Norm on Division Ring|norm on a division ring]] }} {{eqn | r = \norm {a^{p^n - 1} - 1}_p | c = as $\norm a_p = 1$ }} {{eqn | o = < | r = 1 | c = [[Fermat's Little Theorem/Corollary 4|corollary 4 to Fermat's Little Theorem]] }} {{end-eqn}} This [[Definition:Contradiction|contradicts]] the earlier assertion that $\norm {x - a}_p = 1$. In conclusion: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] that does not [[Definition:Convergent Sequence|converge]] in $\struct {\Q, \norm {\,\cdot\,}_p }$.	1
From [[Equation for Line through Two Points in Complex Plane/Parametric Form 1|Equation for Line through Two Points in Complex Plane: Parametric Form 1]]: :$z = z_1 + t \paren {z_2 - z_1}$ Letting: {{begin-eqn}} {{eqn | l = z | r = x + i y }} {{eqn | l = z_1 | r = x_1 + i y_1 }} {{eqn | l = z_2 | r = x_2 + i y_2 }} {{end-eqn}} the parametric equations follow by equating [[Definition:Real Part|real parts]] and [[Definition:Imaginary Part|imaginary parts]]. {{qed}}	1
Let $G$ be an [[Definition:Module|$R$-module]]. Let $\left \langle {a_n} \right \rangle := \left \langle {a_j} \right \rangle_{1 \mathop \le j \mathop \le n}$ be a [[Definition:Finite Sequence|sequence of elements]] of $G$ of [[Definition:Length of Sequence|length]] $n$. An [[Definition:Element|element]] $b \in G$ is a '''linear combination''' of $\left \langle {a_n} \right \rangle$ {{iff}}: : $\displaystyle \exists \left \langle {\lambda_n} \right \rangle \subseteq R: b = \sum_{k \mathop = 1}^n \lambda_k a_k$	1
By hypothesis there is a [[Definition:Basis of Vector Space|basis]] $B$ of $E$ with $n$ elements. Then $H \cup B$ is a [[Definition:Generator of Module|generator]] for $E$. So by [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]] there exists a [[Definition:Basis of Vector Space|basis]] $C$ of $E$ such that $H \subseteq C \subseteq H \cup B$. {{Qed}}	1
Let $\mathbf A, \mathbf B$ be [[Definition:Square Matrix|square matrices of order $n$]] over a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] $\left({R, +, \circ}\right)$. Suppose that: : $\mathbf A \mathbf B = \mathbf I_n$ where $\mathbf I_n$ is the [[Definition:Unit Matrix|unit matrix of order $n$]]. Then $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible matrices]], and furthermore: :$\mathbf B = \mathbf A^{-1}$ where $\mathbf A^{-1}$ is the [[Definition:Inverse Matrix|inverse]] of $\mathbf A$.	1
This result follows trivially from [[Countable Space is Separable]].	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]] such that $z_2 \ne 0$. Let $\cmod z$ denote the [[Definition:Complex Modulus|modulus]] of $z$. Then: :$\cmod {\dfrac {z_1} {z_2} } = \dfrac {\cmod {z_1} } {\cmod {z_2} }$	1
Let $A$ be an [[Definition:Ring Extension|extension]] of a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] $\struct {R, +, \circ}$. Let $C$ be the [[Definition:Integral Closure|integral closure]] of $R$ in $A$. Then $C$ is a [[Definition:Subring|subring]] of $A$.	1
Follows from [[Equivalent Matrices have Equal Rank]]. {{qed}}	1
Let $\mathbf x = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$. Let $\mathbf I_n$ be the [[Definition:Unit Matrix|unit matrix of order $n$]]. Then: {{begin-eqn}} {{eqn | l = \mathbf x_{n \times 1} | r = \mathbf I_n \mathbf x_{n \times 1} | c = {{Defof|Left Identity}} }} {{eqn | r = \begin {bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \\ \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix} | c = [[Unit Matrix is Unity of Ring of Square Matrices#Lemma: Left Identity|Unit Matrix is Identity:Lemma]] }} {{eqn | r = \begin {bmatrix} \mathbf e_1 & \mathbf e_2 & \cdots & \mathbf e_n \end {bmatrix} \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix} | c = {{Defof|Standard Ordered Basis on Vector Space}} }} {{eqn | r = \sum_{i \mathop = 1}^n \mathbf e_i x_i | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | l = \map T {\mathbf x} | r = \map T {\sum_{i \mathop =1}^n \mathbf e_i x_i} }} {{eqn | r = \sum_{i \mathop = 1}^n \map T {\mathbf e_i} x_i | c = {{Defof|Linear Transformation on Vector Space}} }} {{eqn | r = \begin {bmatrix} \map T {\mathbf e_1} & \map T {\mathbf e_2} & \cdots & \map T {\mathbf e_n} \end {bmatrix} \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix} | c = {{Defof|Matrix Product (Conventional)}} }} {{end-eqn}} That $\mathbf A_T$ is $m \times n$ follows from each $\map T {\mathbf e_i}$ being an [[Definition:Element|element]] of $\R^m$ and thus having $m$ [[Definition:Row of Matrix|rows]]. {{qed}}	1
Let $\det: \GL {n, \R} \to \struct {\R_{\ne 0}, \times}$ be the [[Definition:Group Homomorphism|group homomorphism]]: :$\mathbf A \mapsto \map \det {\mathbf A}$ where $\map \det {\mathbf A}$ is the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. This is demonstrated to be a [[Definition:Group Homomorphism|homomorphism]] in [[General Linear Group to Determinant is Homomorphism]] From the [[General Linear Group to Determinant is Homomorphism/Corollary|the corollary to General Linear Group to Determinant is Homomorphism]], the [[Definition:Kernel of Group Homomorphism|kernel]] of $\phi$ is $\SL {n, \R}$. Thus from [[Kernel is Normal Subgroup of Domain]], $\SL {n, \R}$ is [[Definition:Normal Subgroup|normal]] in $\GL {n, \R}$. From the [[First Isomorphism Theorem for Groups]]: :$\Img \phi \cong \GL {n, \R} / \SL {n, \R}$ By definition, the [[Definition:Image of Mapping|image]] of $\phi$ is the [[Definition:Multiplicative Group of Real Numbers|multiplicative group of real numbers]]. Hence the result. {{qed}}	1
{{missingLinks}} We have an injective morphism: :$\dfrac A {f^{-1} \left({\mathfrak m}\right)} \to \dfrac B {\mathfrak m}$ We have that $\dfrac B {\mathfrak m}$ is a field extension of $k$ which is finitely generated Thus, by [[Zariski's Lemma]], $\dfrac B {\mathfrak m}$ is a finite field extension. By [[Subalgebra of Finite Field Extension is Field]], $\dfrac A {f^{-1} \left({\mathfrak m}\right)}$ is a field. Thus $f^{-1} \left({\mathfrak m}\right)$ is a maximal ideal. {{qed}}	1
Since $\norm {x_0}_1 < 1$ then $\norm {x_0}_2 < 1$ and: :$\log \norm {x_0}_1 < 0$ :$\log \norm {x_0}_2 < 0$ Hence $\alpha > 0$ Since $\forall x \in R: \norm x_1 < 1 \iff \norm x_2 < 1$: ===== [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.1|Lemma 1]] ===== {{:Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.1}} ===== [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.2|Lemma 2]] ===== {{:Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.2}} Let $x \in R, x \ne 0_R$. ===== Case 1 ===== Let $\norm x_1 = 1$. By [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.2|Lemma 2]] then: :$\norm x_2 = 1$ Hence: :$\norm x_1 = 1 = 1^\alpha = \norm x_2^\alpha$ {{qed|lemma}} ===== Case 2 ===== Let $\norm x_1 \ne 1$. By [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.2|Lemma 2]] then: :$\norm x_2 \ne 1$ Let $\beta = \dfrac {\log \norm x_1 } {\log \norm x_2 }$. Then $\norm x_1 = \norm x_2^\beta$. ====== [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.3|Lemma 3]] ====== {{:Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.3}} Hence $\norm x_1 = \norm x_2^\alpha$. </onlyinclude> {{qed}} [[Category:Equivalence of Definitions of Equivalent Division Ring Norms]] d424573c8eu49vw4soo5v51psipeox4	1
Let: :$\alpha = x_1 + i y_1$ :$\beta = x_2 + i y_2$ Then: {{begin-eqn}} {{eqn | o = | r = \left\lvert{\alpha + \beta}\right\rvert^2 + \left\lvert{\alpha - \beta}\right\rvert^2 | c = }} {{eqn | r = \left\lvert{\left({x_1 + i y_1}\right) + \left({x_2 + i y_2}\right)}\right\rvert^2 + \left\lvert{\left({x_1 + i y_1}\right) - \left({x_2 + i y_2}\right)}\right\rvert^2 | c = Definition of $\alpha$ and $\beta$ }} {{eqn | r = \left\lvert{\left({x_1 + x_2}\right) + i \left({y_1 + y_2}\right)}\right\rvert^2 + \left\lvert{\left({x_1 - x_2}\right) + i \left({y_1 - y_2}\right)}\right\rvert^2 | c = {{Defof|Complex Addition}}, {{Defof|Complex Subtraction}} }} {{eqn | r = \left({x_1 + x_2}\right)^2 + \left({y_1 + y_2}\right)^2 + \left({x_1 - x_2}\right)^2 + \left({y_1 - y_2}\right)^2 | c = {{Defof|Complex Modulus}} }} {{eqn | r = {x_1}^2 + 2 x_1 x_2 + {x_2}^2 + {y_1}^2 + 2 y_1 y_2 + {y_2}^2 + {x_1}^2 - 2 x_1 x_2 + {x_2}^2 + {y_1}^2 - 2 y_1 y_2 + {y_2}^2 | c = [[Square of Sum]], [[Square of Difference]] }} {{eqn | r = 2 {x_1}^2 + 2 {x_2}^2 + 2 {y_1}^2 + 2 {y_2}^2 | c = simplifying }} {{eqn | r = 2 \left({ {x_1}^2 + {y_1}^2}\right) + 2 \left({ {x_2}^2 + {y_2}^2}\right) | c = simplifying }} {{eqn | r = 2 \left\lvert{x_1 + i y_1}\right\rvert^2 + 2 \left\lvert{x_2 + i y_2}\right\rvert^2 | c = {{Defof|Complex Modulus}} }} {{eqn | r = 2 \left\lvert{\alpha}\right\rvert^2 + 2 \left\lvert{\beta}\right\rvert^2 | c = Definition of $\alpha$ and $\beta$ }} {{end-eqn}} {{qed}}	1
A [[Definition:Sequentially Compact Space|sequentially compact]] [[Definition:Metric Space|metric space]] is [[Definition:Separable Space|separable]].	1
From the [[Reverse Triangle Inequality/Real and Complex Fields/Proof 1|Reverse Triangle Inequality]]: :$\cmod {x - y} \ge \cmod {\cmod x - \cmod y}$ By the definition of both [[Definition:Absolute Value|absolute value]] and [[Definition:Complex Modulus|complex modulus]]: :$\cmod {\cmod x - \cmod y} \ge 0$ As: :$\cmod x - \cmod y = \pm \cmod {\cmod x - \cmod y}$ it follows that: :$\cmod {\cmod x - \cmod y} \ge \cmod x - \cmod y$ Hence the result. {{qed}}	1
Let $A$ be the [[Definition:Set|set]] of all [[Definition:Real Sequence|real sequences]] $\sequence {x_i}$ such that the [[Definition:Series of Numbers|series]] $\displaystyle \sum_{i \mathop \ge 0} x_i^2$ is [[Definition:Convergent Series of Numbers|convergent]]. Let $\ell^2 = \struct {A, d_2}$ be the [[Definition:Hilbert Sequence Space|Hilbert sequence space on $\R$]]. Then $\ell^2$ is a [[Definition:Separable Space|separable space]].	1
Then the [[Definition:Quotient Ring|quotient ring]] $\CC \,\big / \NN$ is a [[Definition:Field (Abstract Algebra)|field]].	1
Let $T = \struct {S, \tau}$ be a [[Definition:Uncountable Finite Complement Topology|finite complement topology]] on an [[Definition:Uncountable Set|uncountable]] set $S$. We have that a [[Finite Complement Topology is Separable]]. But we also have that an [[Uncountable Finite Complement Space is not First-Countable]]. Hence the result, by [[Proof by Counterexample]]. {{qed}}	1
Let $T$ be [[Definition:Separable Space|separable]]. {{AimForCont}} $S$ is [[Definition:Uncountable Set|uncountable]]. Then by [[Uncountable Discrete Space is not Separable]], $T$ is not [[Definition:Separable Space|separable]]. Hence the result by [[Proof by Contradiction]]. {{qed}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Module|module]] over $R$. Let $M$ be a [[Definition:Submodule|submodule]] of $G$. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Then the [[Definition:Annihilator on Algebraic Dual|annihilator]] $M^\circ$ of $M$ is a [[Definition:Submodule|submodule]] of $G^*$. Similarly, let $N$ be a [[Definition:Submodule|submodule]] of $G^*$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. Then the [[Definition:Annihilator on Algebraic Dual|annihilator]] $N^\circ$ of $N$ is a [[Definition:Submodule|submodule]] of $G^{**}$.	1
From [[G-Submodule Test]] it suffices to prove that $\mu \left({G, \operatorname{Im} \left({f}\right) }\right) \subseteq \operatorname{Im} \left({f}\right)$. In other words: for any $g \in G$ and $w \in \operatorname{Im} \left({f}\right)$, it is to be shown that $\mu \left({g, w}\right) \in \operatorname{Im} \left({f}\right)$. Assume that $g \in G$ and $w\in\operatorname{Im} \left({f}\right)$. Then there exists a $v \in V$ such that $f \left({v}\right) = w$. By definition of [[Definition:G-Module Homomorphism|homomorphism]], have: :$\mu \left({g, w}\right) = \mu \left({g, f \left({v}\right) }\right) = f \left({\phi \left({g, v}\right) }\right)$ Hence, for all $g \in G$ and $w\in\operatorname{Im} \left({f}\right)$, $\mu \left({g, w}\right) \in \operatorname{Im} \left({f}\right)$. By [[G-Submodule Test]], it follows that $\operatorname{Im} \left({f}\right)$ is a [[Definition:G-Submodule|$G$-submodule]] of $V'$. {{qed}} [[Category:Representation Theory]] lrqe10cldex5s9jftgmv10fkv100lhw	1
From [[Elementary Row Operations as Matrix Multiplications]], an [[Definition:Elementary Row Operation|elementary row operation]] on $\mathbf A$ is equivalent to [[Definition:Matrix Product (Conventional)|matrix multiplication]] by the [[Definition:Elementary Row Matrix|elementary row matrices]] corresponding to the [[Definition:Elementary Row Operation|elementary row operations]]. From [[Determinant of Elementary Row Matrix]], the [[Definition:Determinant of Matrix|determinants]] of those [[Definition:Elementary Row Matrix|elementary row matrices]] are as follows: === [[Determinant of Elementary Row Matrix/Scale Row|Scale Row]] === {{:Determinant of Elementary Row Matrix/Scale Row}} === [[Determinant of Elementary Row Matrix/Scale Row and Add|Add Scalar Product of Row to Another]] === {{:Determinant of Elementary Row Matrix/Scale Row and Add}} === [[Determinant of Elementary Row Matrix/Exchange Rows|Exchange Rows]] === {{:Determinant of Elementary Row Matrix/Exchange Rows}} Hence the result. {{qed}}	1
Let $\mathbf I$ denote the [[Definition:Unit Matrix|unit matrix]] of arbitrary [[Definition:Order of Square Matrix|order]] $n$. By [[Determinant of Unit Matrix]]: :$\map \det {\mathbf I} = 1$ Let $\rho$ be the [[Definition:Permutation on n Letters|permutation]] on $\tuple {1, 2, \ldots, n}$ which [[Definition:Transposition|transposes]] $i$ and $j$. From [[Parity of K-Cycle]], $\map \sgn \rho = -1$. By definition we have that $\mathbf E_3$ is $\mathbf I$ with [[Definition:Column of Matrix|columns]] $i$ and $j$ [[Definition:Transposition|transposed]]. By the definition of a [[Definition:Determinant of Matrix|determinant]]: :$\displaystyle \map \det {\mathbf I} = \sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k k} }$ By [[Permutation of Determinant Indices]]: :$\displaystyle \map \det {\mathbf E_3} = \sum_\lambda \paren {\map \sgn \rho \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k \map \rho k} }$ We can take $\map \sgn \rho = -1$ outside the summation because it is constant, and so we get: {{begin-eqn}} {{eqn | l = \map \det {\mathbf E_3} | r = \map \sgn \rho \sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k \map \rho k} } | c = }} {{eqn | r = -\sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k k} } | c = }} {{eqn | r = -\map \det {\mathbf I} | c = }} {{end-eqn}} Hence the result. {{qed}}	1
Let $s, t \in \closedint 1 m$ such that $s \ne t$. === Case $1$ === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] $r_s \to \lambda r_s$: :$E_{ik} = \begin{cases} \delta_{ik} & : i \ne s \\ \lambda \delta_{ik} & : i = s \end{cases}$ where $\delta$ denotes the [[Definition:Kronecker Delta|Kronecker delta]]. Then: {{begin-eqn}} {{eqn | l = \sqbrk {E A}_{i j} | r = \sum_{k \mathop = 1}^m E_{i k} A_{k j} | c = }} {{eqn | r = \begin{cases} A_{i j} & : i \ne r \\ \lambda A_{i j} & : i = r \end{cases} | c = }} {{eqn | ll= \leadsto | l = \mathbf {E A} | r = e \paren {\mathbf A} | c = }} {{end-eqn}} {{qed|lemma}} === Case $2$ === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] $r_s \to r_s + \lambda r_t$: :$E_{i k} = \begin{cases} \delta_{i k} & : i \ne s \\ \delta_{s k} + \lambda \delta_{t k} & : i = s \end{cases}$ where $\delta$ denotes the [[Definition:Kronecker Delta|Kronecker delta]]. Then: {{begin-eqn}} {{eqn | l = \sqbrk {E A}_{i j} | r = \sum_{k \mathop = 1}^m E_{i k} A_{k j} | c = }} {{eqn | r = \begin {cases} A_{i j} & : i \ne s \\ A_{i j} + \lambda A_{t j} & : i = s \end {cases} | c = }} {{eqn | ll= \leadsto | l = \mathbf {E A} | r = e \paren {\mathbf A} | c = }} {{end-eqn}} {{qed|lemma}} === Case $3$ === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] $r_s \leftrightarrow r_t$: By [[Exchange of Rows as Sequence of Other Elementary Row Operations]], this [[Definition:Elementary Row Operation|elementary row operation]] can be expressed as: :$e_1 e_2 e_3 e_4 \paren {\mathbf A} = e \paren {\mathbf A}$ where the $e_i$ are [[Definition:Elementary Row Operation|elementary row operation]] of the other two types. For each $e_i$, let $\mathbf E_i = e_i \paren {\mathbf I}$. Then: {{begin-eqn}} {{eqn | l = e \paren {\mathbf A} | r = e_1 e_2 e_3 e_4 \paren {\mathbf A} | c = Definition of $e$ }} {{eqn | r = \mathbf E_1 \mathbf E_2 \mathbf E_3 \mathbf E_4 \mathbf A | c = Cases $1$ and $2$ }} {{eqn | r = \mathbf E_1 \mathbf E_2 \mathbf E_3 e_4 \paren {\mathbf I} \mathbf A }} {{eqn | r = \mathbf E_1 \mathbf E_2 e_3 e_4 \paren {\mathbf I} \mathbf A }} {{eqn | r = \mathbf E_1 e_2 e_3 e_4 \paren {\mathbf I} \mathbf A }} {{eqn | r = e_1 e_2 e_3 e_4 \paren {\mathbf I} \mathbf A }} {{eqn | r = e \paren {\mathbf I} \mathbf A }} {{eqn | r = \mathbf {E A} }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = x \in \map {B_\epsilon} {a; \norm {\,\cdot\,} } | o = \leadstoandfrom | r = \norm {x - a} < \epsilon | c = {{Defof|Open Ball of Normed Division Ring|open ball}} in $\struct {R, \norm {\,\cdot\,} }$ }} {{eqn | o = \leadstoandfrom | r = \map d {x, a} < \epsilon | c = {{Defof|Metric Induced by Norm on Division Ring|metric induced}} by $\norm {\,\cdot\,}$ }} {{eqn | o = \leadstoandfrom | r = x \in \map {B_\epsilon} {a; d } | c = {{Defof|Open Ball|open ball}} in $\struct {R, d}$ }} {{end-eqn}} The result follows from [[Definition:Equality of Sets|Equality of Sets]]. {{qed}}	1
Let $\mathbf a = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}$, and $\mathbf b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}$. Then the [[Definition:Dot Product|dot product]] of $\mathbf a$ and $\mathbf a \times \mathbf b$ is: {{begin-eqn}} {{eqn | l = \mathbf a \cdot \left({\mathbf a \times \mathbf b}\right) | r = a_1 \left({a_2 b_3 - a_3 b_2}\right) + a_2 \left({a_3 b_1 - a_1 b_3}\right) + a_3 \left({a_1 b_2 - a_2 b_1}\right) | c = {{Defof|Dot Product}} and {{Defof|Vector Cross Product}} }} {{eqn | r = a_1 a_2 b_3 - a_1 a_3 b_2 + a_2 a_3 b_1 - a_1 a_2 b_3 + a_1 a_3 b_2 - a_2 a_3 b_1 }} {{eqn | r = 0 }} {{end-eqn}} Since the [[Definition:Dot Product|dot product]] is equal to zero, the vectors are [[Definition:Orthogonal (Linear Algebra)|orthogonal]] by definition. Similarly, $\mathbf b$ and $\mathbf a \times \mathbf b$ are [[Definition:Orthogonal (Linear Algebra)|orthogonal]]: {{begin-eqn}} {{eqn | l = \mathbf b \cdot \left({\mathbf a \times \mathbf b}\right) | r = b_1 \left({a_2 b_3 - a_3 b_2}\right) + b_2 \left({a_3 b_1 - a_1 b_3}\right) + b_3 \left({a_1 b_2 - a_2 b_1}\right) }} {{eqn | r = a_2 b_1 b_3 - a_3 b_1 b_2 + a_3 b_1 b_2 - a_1 b_2 b_3 + a_1 b_2 b_3 - a_2 b_1 b_3 }} {{eqn | r = 0 }} {{end-eqn}} {{qed}}	1
From [[Elementary Column Matrix for Inverse of Elementary Column Operation is Inverse]] it is demonstrated that: :if $\mathbf E$ is the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to an [[Definition:Elementary Column Operation|elementary column operation]] $e$ then: :the [[Definition:Inverse of Elementary Column Operation|inverse]] of $e$ corresponds to an [[Definition:Elementary Column Matrix|elementary column matrix]] which is the [[Definition:Inverse Matrix|inverse]] of $\mathbf E$. So as $\mathbf E$ has an [[Definition:Inverse Matrix|inverse]], [[Definition:A Priori|a priori]] it is [[Definition:Invertible Matrix|invertible]]. {{qed}}	1
We have that the [[Definition:Arens-Fort Space|Arens-Fort space]] is an [[Definition:Expansion of Topology|expansion]] of a [[Definition:Countable Fort Space|countable Fort space]]. So $S$ is [[Definition:Countable Set|countable]]. The result follows from [[Countable Space is Separable]]. {{qed}}	1
From the [[Equation of Straight Line in Plane/Normal Form|Normal Form of Equation of Straight Line in Plane]], a general [[Definition:Straight Line|straight line]] can be expressed in the form: :$x \cos \alpha + y \sin \alpha = p$ where: :$p$ is the [[Definition:Length of Line|length]] of a [[Definition:Perpendicular|perpendicular]] $\mathcal P$ from $\mathcal L$ to the [[Definition:Origin|origin]] :$\alpha$ is the [[Definition:Angle|angle]] made between $\mathcal P$ and the [[Definition:X-Axis|$x$-axis]]. As $\mathcal L$ is [[Definition:Horizontal Line|horizontal]], then by definition $\mathcal P$ is [[Definition:Vertical Line|vertical]]. By definition, the [[Definition:Vertical Line|vertical line]] through the [[Definition:Origin|origin]] is the [[Definition:Y-Axis|$y$-axis]] itself. Thus: :$\alpha$ is a [[Definition:Right Angle|right angle]], that is $\alpha = \dfrac \pi 2 = 90 \degrees$ :$p = b$ Hence the [[Definition:Equation of Geometric Figure|equation]] of $\mathcal L$ becomes: {{begin-eqn}} {{eqn | l = x \cos \dfrac \pi 2 + y \sin \dfrac \pi 2 | r = b | c = }} {{eqn | ll= \leadsto | l = x \times 0 + y \times 1 | r = b | c = [[Sine of Right Angle]], [[Cosine of Right Angle]] }} {{eqn | ll= \leadsto | l = y | r = b | c = }} {{end-eqn}} Hence the result. {{qed}} [[Category:Equations of Straight Lines in Plane]] j9g6se6w41ekcnkgdentv3erwbwo7u9	1
:$\sequence {x_n y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Take the [[Definition:Real Vector Space|$\R$-vector space]] $\left({\R^2, +, \times}\right)_\R$. Let $S$ be a [[Definition:Vector Subspace|subspace]] of $\left({\R^2, +, \times}\right)_\R$. Then $S$ is one of: : $(1): \quad \left({\R^2, +, \times}\right)_\R$ : $(2): \quad \left\{{0}\right\}$ : $(3): \quad$ A line through the origin.	1
Let $\struct {R, \norm {\,\cdot\,}_R}$ and $\struct {S, \norm {\,\cdot\,}_S}$ be [[Definition:Normed Division Ring|normed division rings]]. Let $\phi:R \to S$ be a mapping. Then $\phi:R \to S$ is an [[Definition:Isometric Isomorphism|isometric isomorphism]] {{iff}} $\phi^{-1}: S \to R$ is also an [[Definition:Isometric Isomorphism|isometric isomorphism]].	1
Let $V$ be a [[Definition:Banach Space|Banach space]]. Let $\left({v_n}\right)_{n \in \N}$ be a [[Definition:Sequence|sequence]] of elements in $V$. Then the following two statements are equivalent: :$(1): \qquad \displaystyle \sum_{n=1}^\infty \left\Vert{v_n}\right\Vert$ converges ([[Definition:Absolutely Convergent Series|absolute convergence]]) :$(2): \qquad \displaystyle \sum \left\{{v_n: n \in \N}\right\}$ converges (generalised or [[Definition:Generalized Sum|net convergence]])	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\mathbf N = \left({a_{ij} }\right)$ be an $n \times n$ [[Definition:Matrix|matrix]] with entries in $A$. Let $\mathbf I_n$ denote the $n \times n$ [[Definition:Unit Matrix|unit matrix]]. Let $p_N \left({x}\right)$ be the [[Definition:Determinant of Matrix|determinant]] $\det \left({x \cdot \mathbf I_n - \mathbf N}\right)$. Then: : $p_N \left({N}\right) = \mathbf 0$ as an $n \times n$ [[Definition:Zero Matrix|zero matrix]]. That is: :$ N^n + b_{n-1} N^{n-1} + \cdots + b_1 N + b_0 = \mathbf 0$ where the $b_i$ are the coefficients of $p_N \left({x}\right)$.	1
Let $m, n \in \N_{>0}$ be non-[[Definition:Zero (Number)|zero]] [[Definition:Natural Number|natural numbers]] such that $m > n$. Let $S$ be the [[Definition:Set|set]] of all [[Definition:Matrix|matrices]] of [[Definition:Order of Matrix|order $m \times n$]]. Then the [[Definition:Algebraic Structure|algebraic structure]] $\struct {S, +, \times}$ is not a [[Definition:Ring (Abstract Algebra)|ring]]. Note that $\times$ denotes [[Definition:Matrix Product (Conventional)|conventional matrix multiplication]].	1
From the definition of a [[Definition:Module|module]], the [[Definition:Group|group]] $\left({H, +_H}\right)$ is [[Definition:Abelian Group|abelian]]. Therefore we can apply [[Inverse Mapping in Induced Structure]] to show that $- \phi: G \to H$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]]. Then: {{begin-eqn}} {{eqn | l=\left({- \phi}\right) \left({\lambda \circ x}\right) | r=-\phi \left({\lambda \circ x}\right) | c= }} {{eqn | r=- \lambda \circ \phi \left({x}\right) | c= }} {{eqn | r=\lambda \circ \left({- \phi \left({x}\right)}\right) | c= }} {{eqn | r=\lambda \circ \left({- \phi}\right) \left({x}\right) | c= }} {{end-eqn}} {{qed}}	1
{{TFAE|def = Dot Product}} Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Real Euclidean Space|real Euclidean space]] $\R^n$.	1
Since a [[Definition:BasisBasis of Vector Space|basis]] is, by definition, both [[Definition:Linearly Independent Set|linearly independent]] and a [[Definition:Generator of Vector Space|generator]], the result follows directly from [[Size of Linearly Independent Subset is at Most Size of Finite Generator]]. {{qed}}	1
Follows directly from the definition of [[Definition:Matrix Product (Conventional)|matrix multiplication]]: :$\displaystyle \forall i \in \closedint 1 m, j \in \closedint 1 p: c_{i j} = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j}$ In this case, $m = n$ and $a_{i k} = b_{k j} = 1$. Hence: :$\displaystyle c_{i j} = \sum_{k \mathop = 1}^n 1 \times 1 = n$ {{qed}} [[Category:Matrix Algebra]] 4o70eshdycot8iszwvk6jr2n1p8qn7j	1
Let $\left({G, +_G, \circ}\right)_R$ and $\left({H, +_H, \circ}\right)_R$ be [[Definition:Module|$R$-modules]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation|linear transformation]]. Let $- \phi$ be the negative of $\phi$ as defined in [[Induced Structure Inverse]]. Then $- \phi: G \to H$ is also a [[Definition:Linear Transformation|linear transformation]].	1
The [[Definition:Euclidean Space|Euclidean space]] $\R^m$ is a [[Definition:Vector Space|vector space]] over $\R$. That the [[Definition:Norm Axioms (Vector Space)|norm axioms]] are satisfied is proven in [[Euclidean Space is Normed Space]]. Then we have [[Euclidean Space is Complete Metric Space]]. The result follows by the definition of a [[Definition:Banach Space|Banach space]]. {{qed}}	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\mathbf V$ be a [[Definition:Vector Space|vector space over $K$]]. Let $S \subseteq \mathbf V$ be a [[Definition:Subset|subset]] of $\mathbf V$. $S$ is a '''generator of $\mathbf V$''' {{iff}} every element of $\mathbf V$ is a [[Definition:Linear Combination|linear combination]] of [[Definition:Element|elements]] of $S$.	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $x \in X$. Let $\epsilon \in \R_{> 0}$. Let $\map {B_\epsilon^-} x$ be the [[Definition:Closed Ball in Normed Vector Space|closed $\epsilon$-ball]] of $x$ in $M$. Then $\map {B_\epsilon^-} x$ is a [[Definition:Closed Set in Normed Vector Space|closed set]] of $M$.	1
We first consider the case where $L$ is [[Definition:Finite Set|finite]]. Let $S \subseteq \N$ be the set of all $n \in \N$ such that: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le n$, then $\card L \le \card F$ where: :$L \setminus F$ denotes the [[Definition:Set Difference|set difference]] between $L$ and $F$ :$\card L$ and $\card F$ denote the [[Definition:Cardinality|cardinality]] of $L$ and $F$ respectively. We use the [[Principle of Finite Induction]] to prove that $S = \N$. === Basis of the Induction === Let $\card {L \setminus F} \le 0$. Then from [[Cardinality of Empty Set]]: :$L \setminus F = \O$ By [[Set Difference with Superset is Empty Set]]: :$L \subseteq F$ By [[Cardinality of Subset of Finite Set]]: :$\card L \le \card F$ Hence: :$0 \in S$ This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === It is to be shown that if $k \in S$ where $k \ge 1$, then it follows that $k + 1 \in S$. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le k$, then $\card L \le \card F$ It is to be demonstrated that it follows that: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le k + 1$, then $\card L \le \card F$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: Assume the [[Size of Linearly Independent Subset is at Most Size of Finite Generator/Proof 1#Induction Hypothesis|induction hypothesis]] that $n \in S$. Let $F$ be a [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] of $V$ such that: :$\card {L \setminus F} = n + 1$ Let $v \in L \setminus F$. Let $L' = L \cap \paren {F \cup \set v}$. By [[Intersection is Subset]]: :$L' \subseteq L$ By [[Subset of Linearly Independent Set]], it follows that $L'$ is [[Definition:Linearly Independent Set|linearly independent]] over $R$. Also by [[Intersection is Subset]]: :$L' \subseteq F \cup \set v$ Therefore, by [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]]: :[[Definition:Existential Quantifier|there exists]] a [[Definition:Basis (Linear Algebra)|basis]] $B$ of $V$ such that: ::$L' \subseteq B \subseteq F \cup \set v$ Since $v \notin F$ is a [[Definition:Linear Combination of Subset|linear combination]] of $F$, it follows that $F \cup \set v$ is [[Definition:Linearly Dependent Set|linearly dependent]] over $R$. Therefore: :$B \subsetneq F \cup \set v$ By [[Cardinality of Subset of Finite Set]]: :$\card B < \card {F \cup \set v} = \card F + 1$ Hence: :$\card B \le \card F$ We have that: {{begin-eqn}} {{eqn | l = \card {L \setminus B} | o = \le | r = \card {L \setminus L'} | c = [[Relative Complement inverts Subsets]] and [[Cardinality of Subset of Finite Set]] }} {{eqn | r = \card {L \setminus \paren {F \cup \set v} } | c = [[Set Difference with Intersection is Difference]] }} {{eqn | r = \card {\paren {L \setminus F} \setminus \set v} | c = [[Set Difference with Union]] }} {{eqn | r = n + 1 - 1 | c = [[Cardinality of Set Difference with Subset]] }} {{eqn | r = n }} {{end-eqn}} Since $n \in S$: :$\card L \le \card B \le \card F$ Hence: :$n + 1 \in S$ and so the [[Definition:Induction Step|induction step]] has been completed. By [[Set Difference is Subset]]: :$L \setminus F \subseteq L$ From [[Subset of Finite Set is Finite]]: :$L \setminus F$ is [[Definition:Finite Set|finite]]. Therefore, we can apply the fact that $S = \N$ to conclude that: :$\card L \le \card F$ Let $L$ be [[Definition:Infinite Set|infinite]]. Then by [[Set is Infinite iff exist Subsets of all Finite Cardinalities]], there exists a [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $L' \subseteq L$ such that: :$\card {L'} = \card F + 1$ By [[Subset of Linearly Independent Set]], it follows that $L'$ is [[Definition:Linearly Independent Set|linearly independent]] over $R$. It is proven above that this is impossible. Hence the result. {{qed}}	1
Let $u: K^n \to K^m$ be the [[Definition:Linear Transformation|linear transformation]] such that $\mathbf A$ is the [[Definition:Relative Matrix|matrix of $u$ relative to]] the [[Definition:Standard Ordered Basis|standard ordered bases]] of $K^n$ and $K^m$. Let $\rho \left({\mathbf A}\right)$ be the [[Definition:Rank of Matrix|rank]] of $\mathbf A$. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Similar notations on $u$ denote the [[Definition:Rank of Linear Transformation|rank]] and [[Definition:Transpose of Linear Transformation|transpose]] of $u$. We have $\rho \left({\mathbf A}\right) = \rho \left({u}\right)$ and $\rho \left({\mathbf A^\intercal}\right) = \rho \left({u^\intercal}\right)$, but $\rho \left({u^\intercal}\right) = \rho \left({u}\right)$ from [[Rank and Nullity of Transpose]]. {{finish}}	1
Let $\epsilon > 0$ be given. By the definition of a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]: :$\exists N: \forall n, m > N: \norm {x_n - x_m} < \epsilon$ So :$\exists N: \forall n > N: \norm {x_{n + 1} - x_n} < \epsilon$ Hence the result follows: :$\lim_{n \mathop \to \infty} \norm {x_{n + 1} - x_n} = 0$.	1
Let $\mathbf A = \sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf 0_R | r = \sqbrk a_{m n} + \sqbrk {0_R}_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {a + 0_R}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf A + \mathbf 0_R | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \mathbf 0_R + \mathbf A | r = \sqbrk {0_R}_{m n} + \sqbrk a_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {0_R + a}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf 0_R + \mathbf A | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} {{qed}}	1
Let $V$ be a vector space with norm $\norm {\, \cdot \,}$. The function $\norm {\, \cdot \,}: V \to \R$ is continuous.	1
Two [[Definition:Hilbert Space|Hilbert spaces]] are [[Definition:Isomorphism (Hilbert Spaces)|isomorphic]] iff they have the same [[Definition:Dimension (Hilbert Space)|dimension]].	1
=== [[Unit Matrix is Identity for Matrix Multiplication/Left|Lemma: Left Identity]] === {{:Unit Matrix is Identity for Matrix Multiplication/Left}}{{qed|lemma}} === [[Unit Matrix is Identity for Matrix Multiplication/Right|Lemma: Right Identity]] === {{:Unit Matrix is Identity for Matrix Multiplication/Right}}{{qed|lemma}} Thus: $\mathbf A \mathbf I_n = \mathbf A = \mathbf I_n \mathbf A$ Hence, by definition, $\mathbf I_n$ is an [[Definition:Identity Element|identity element]] for [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]] over $\map {\MM_R} n$. That $\mathbf I_n$ is ''the'' [[Definition:Identity Element|identity element]] follows from [[Identity is Unique]]. {{qed}}	1
From the [[Multiple Rule for Sequences in Normed Division Ring]], we have: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x_n} = \lambda l$ :$\displaystyle \lim_{n \mathop \to \infty} \paren {\mu y_n} = \mu m$ The result now follows directly from the [[Sum Rule for Sequences in Normed Division Ring]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x_n + \mu y_n} = \lambda l + \mu m$ {{qed}}	1
A direct corollary of [[Orthogonal Group is Subgroup of General Linear Group]]. {{qed}}	1
Let $\C$ be the [[Definition:Complex Plane|complex plane]]. Let $L$ be a [[Definition:Straight Line|straight line]] in $\C$. Then $L$ may be written as: :$\beta z + \overline \beta \overline z + \gamma = 0$ where $\gamma$ is [[Definition:Real Number|real]] and $\beta$ may be [[Definition:Complex Number|complex]].	1
Let $z = x + iy$. {{begin-eqn}} {{eqn | l = \cmod {\exp z} | r = \cmod {\map \exp {x + iy} } }} {{eqn | r = \cmod {\paren {\exp x} \paren {\exp i y} } | c = [[Exponential of Sum/Complex Numbers|Exponential of Sum]] }} {{eqn | r = \cmod {\exp x} \cmod {\exp i y} | c = [[Complex Modulus of Product of Complex Numbers|Modulus of Product]] }} {{eqn | r = \cmod {\exp x} | c = [[Modulus of Exponential of Imaginary Number is One]] }} {{eqn | r = e^x | c = [[Exponential of Real Number is Strictly Positive]] }} {{eqn | r = \map \exp {\Re z} | c = {{Defof|Real Part}} }} {{end-eqn}} {{qed}}	1
In $I - A$, $I$ is the [[Definition:Identity Operator|identity operator]] on $H$. Hence: {{begin-eqn}} {{eqn|l = \left({I - A}\right)^2 |r = I^2 - IA - AI + A^2 }} {{eqn|r = I^2 - 2A + A^2 |c = $I$ is the [[Definition:Identity Operator|identity operator]] }} {{eqn|r = I - A |c = $I, A$ are [[Definition:Idempotent Operator|idempotent]] }} {{end-eqn}} That is, $I - A$ is [[Definition:Idempotent Operator|idempotent]]. {{qed}}	1
:$\norm {\, \cdot \,}_1$ satisfies {{NormAxiom|1}} That is: :$\forall \eqclass {x_n} {} \in \CC \,\big / \NN: \norm {\eqclass {x_n} {} }_1 = 0 \iff \eqclass {x_n} {} = \eqclass {0_R} {} $	1
This follows directly from: * [[Sequentially Compact Metric Space is Totally Bounded]] * [[Totally Bounded Metric Space is Separable]]. {{qed}} {{ACC|Totally Bounded Metric Space is Separable}} [[Category:Separable Spaces]] [[Category:Metric Spaces]] [[Category:Sequentially Compact Spaces]] 5ngsevn0ih6kfj87bntzl2pno8t3wip	1
Let $T$ be [[Definition:Separable Space|separable]]. {{AimForCont}} $S$ is [[Definition:Uncountable Set|uncountable]]. Then by [[Uncountable Discrete Space is not Separable]], $T$ is not [[Definition:Separable Space|separable]]. Hence the result by [[Proof by Contradiction]]. {{qed}}	1
=== Definition 1 iff Definition 2 === By definition, the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\, \cdot \,}$ is the [[Definition:Mapping|mapping]] $d: R \times R \to \R_{\ge 0}$ defined as: :$\map d {x, y} = \norm {x - y}$ From [[Metric Induced by Norm on Normed Division Ring is Metric]], $d$ is a [[Definition:Metric|metric]]. By definition of a [[Definition:Convergent Sequence in Metric Space|convergent sequence]] in a [[Definition:Metric Space|metric space]], $\sequence{x_n}$ [[Definition:Convergent Sequence in Metric Space|converges]] to $x \in R$ {{iff}}: :$\forall \epsilon \in \R_{>0}: \exists N \in \R_{>0}: \forall n \in \N: n > N \implies \map d {x_n, x} < \epsilon$ {{iff}}: :$\forall \epsilon \in \R_{>0}: \exists N \in \R_{>0}: \forall n \in \N: n > N \implies \norm {x_n - x} < \epsilon$ The result follows. {{qed|lemma}} === Definition 1 iff Definition 3 === Let $x \in R$. By definition of [[Definition:Norm on Division Ring|norm on a division ring]], $\norm {\, \cdot \,}$ is a [[Definition:Mapping|mapping]] $\norm {\, \cdot \,}:R \to \R_{\ge 0}$. Then: :$\forall n \in \N: \size {\norm{x_n - x} - 0} = \size {\norm{x_n - x}} = \norm{x_n - x}$ By definition of [[Definition:Convergent Real Sequence|convergence]] of a [[Definition:Real Sequence|real sequence]], the [[Definition:Real Sequence|real sequence]] $\sequence {\norm {x_n - x} }$ [[Definition:Convergent Real Sequence|converges]] to $0$ in the [[Definition:Real Numbers|reals]] $\R$ {{iff}} :$\forall \epsilon \in \R_{>0}: \exists N \in \R_{>0}: n > N \implies \size {\norm{x_n - x} - 0} < \epsilon$ {{iff}}: :$\forall \epsilon \in \R_{>0}: \exists N \in \R_{>0}: n > N \implies \norm{x_n - x} < \epsilon$ The result follows. {{qed}}	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\mathcal M_K \left({m, n}\right)$ be the [[Definition:Matrix Space|$m \times n$ matrix space]] over $K$. Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]] of [[Definition:Rank of Matrix|rank]] $r$ over $K$. Then: :$\mathbf A \equiv \begin{cases} \left[{0_K}\right]_{m n} & : r = 0 \\ & \\ \begin{bmatrix} \mathbf I_r & \mathbf 0 \\ \mathbf 0 & \mathbf 0 \end{bmatrix} & : 0 < r < \min \left\{{n, m}\right\} \\ & \\ \begin{bmatrix} \mathbf I_r & \mathbf 0 \end{bmatrix} & : r = m < n \\ & \\ \begin{bmatrix} \mathbf I_r \\ \mathbf 0 \end{bmatrix} & : r = n < m \\ & \\ \mathbf I_r & : r = m = n \end{cases}$ Thus there are exactly $\min \left\{{m, n}\right\} + 1$ [[Definition:Equivalence Class|equivalence classes]] for the relation of [[Definition:Matrix Equivalence|equivalence]] on $\mathcal M_K \left({m, n}\right)$, one of which contains only the [[Definition:Zero Matrix|zero matrix]].	1
=== [[Definition:Closure (Abstract Algebra)/Algebraic Structure|Algebraic Structures]] === {{:Definition:Closure (Abstract Algebra)/Algebraic Structure}} === [[Definition:Closure (Abstract Algebra)/Scalar Product|Scalar Product]] === {{:Definition:Closure (Abstract Algebra)/Scalar Product}} [[Category:Definitions/Abstract Algebra]] [[Category:Definitions/Linear Algebra]] cx9tvgqnuptsin06zh38wl1671k2lmh	1
Let $G$ and $H$ be a [[Definition:Vector Space|$K$-vector space]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Then $\phi$ is a [[Definition:Vector Space Monomorphism|monomorphism]] {{iff}} for every [[Definition:Linearly Independent Sequence|linearly independent sequence]] $\sequence {a_n}$ of [[Definition:Vector (Linear Algebra)|vectors]] of $G$, $\sequence {\map \phi {a_n} }$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]] of [[Definition:Vector (Linear Algebra)|vectors]] of $H$.	1
The [[Definition:Module on Cartesian Product|$R$-module $R^n$]] is [[Definition:Dimension (Linear Algebra)|$n$-dimensional]].	1
Let $x = p + 1$. Then $p \nmid x$ and: :$x = p + 1 > p > 0$ {{qed}} [[Category:P-adic Norm not Complete on Rational Numbers]] klt8vnjgaediwd7qt6bk8eururfgx8p	1
If $R$ has a nontrivial decomposition $R = R_1 \times R_2$ then $\tuple {1_R, 0_R}$ is a non-trivial idempotent element of $R$. {{explain|non-trivial decomposition, non-trivial element}} Conversely suppose there is $0_R, 1_R \ne e \in R$ with $e^2 = e$. Let $R_1 = \ideal e$, the [[Definition:Ideal of Ring|ideal]] [[Definition:Generator|generated]] by $e$, and $R_2 = R / \ideal e$. Since $e \paren {e - 1_R} = 0_R$, it follows by definition that $e$ is a [[Definition:Zero Divisor of Ring|zero divisor]]. So by [[Unit Not Zero Divisor]] it is not a [[Definition:Unit of Ring|unit]]. Therefore, $1 \notin R_1$ and $\ideal e \subsetneqq R$. Also $R_1 \cap R_2 = \set {0_R}$ so the product is direct (that is, the [[Universal Property for Direct Products]] is satisfied). Finally we define the "gluing [[Definition:Ring Homomorphism|homomorphism]]" $\phi : R \to R_1 \times R_2$ by :$\phi: a \mapsto \tuple {a e, a + \ideal e}$ which is easily shown to be an [[Definition:Ring Isomorphism|isomorphism]]. {{qed}} [[Category:Commutative Algebra]] fh03qu2of6klj77b3msijjski733boh	1
By [[Convergent Sequence in Normed Division Ring is Bounded]], $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. Suppose $\norm {x_n} \le K$ for $n = 1, 2, 3, \ldots$. Let $M = \max \set {K, \norm m}$. Then: :$\norm m \le M$ and: :$\forall n: \norm{x_n} \le M$ Let $\epsilon > 0$ be given. Then $\dfrac \epsilon {2 M} > 0$. As $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$, we can find $N_1$ such that: :$\forall n > N_1: \norm {x_n - l} < \dfrac \epsilon {2 M}$ Similarly, for $\sequence {y_n}$ we can find $N_2$ such that: :$\forall n > N_2: \norm {y_n - m} < \dfrac \epsilon {2 M}$ Now let $N = \max \set {N_1, N_2}$. Then if $n > N$, both the above inequalities will be true. Thus $\forall n > N$: {{begin-eqn}} {{eqn | l = \norm {x_n y_n - l m} | r = \norm {x_n y_n - x_n m + x_n m - l m} | c = }} {{eqn | o = \le | r = \norm {x_n y_n - x_n m} + \norm {x_n m - l m} | c = [[Definition:Norm on Division Ring|Axiom (N3) of norm]] (Triangle Inequality). }} {{eqn | r = \norm {x_n \paren {y_n - m } } + \norm {\paren {x_n - l } m} | c = }} {{eqn | o = \le | r = \norm {x_n} \cdot \norm {y_n - m} + \norm {x_n - l} \cdot \norm m | c = [[Definition:Norm on Division Ring|Axiom (N2) of norm]] (Multiplicativity). }} {{eqn | o = \le | r = M \cdot \norm {y_n - m} + \norm {x_n - l} \cdot M | c = since $\sequence {x_n}$ is bounded by $M$ and $\norm m \le M$ }} {{eqn | o = \le | r = M \cdot \dfrac \epsilon {2 M} + \dfrac \epsilon {2 M} \cdot M | c = }} {{eqn | r = \dfrac \epsilon 2 + \dfrac \epsilon 2 | c = }} {{eqn | r = \epsilon | c = }} {{end-eqn}} Hence: :$\sequence {x_n y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]]. It follows that: :$\displaystyle \lim_{n \mathop \to \infty} \paren {x_n y_n} = l m$ {{qed}}	1
{{ProofWanted|hard slog, unless someone has a technique}}	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\mathbf A \in R^{n \times n}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Then $\mathbf A$ is [[Definition:Invertible Matrix|invertible]] {{iff}} its [[Definition:Determinant of Matrix|determinant]] is [[Definition:Unit of Ring|invertible]] in $R$. If $R$ is one of the [[Definition:Standard Number Field|standard number fields]] $\Q$, $\R$ or $\C$, this translates into: :$\mathbf A$ is [[Definition:Invertible Matrix|invertible]] {{iff}} its [[Definition:Determinant of Matrix|determinant]] is [[Definition:Zero (Number)|non-zero]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be an [[Definition:Module|$R$-module]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. Let the [[Definition:Mapping|mapping]] $J: G \to G^{**}$ be the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]] from $G$ into $G^{**}$. For each $x \in G$, $x^\wedge: G^* \to R$ is defined as: :$\forall t' \in G^*: \map {x^\wedge} {t'} = \map {t'} x$ Let the [[Definition:Mapping|mapping]] $J: G \to G^{**}$ be the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]] from $G$ into $G^{**}$ defined as: :$\forall x \in G: \map J x = x^\wedge$ where for each $x \in G$, $x^\wedge: G^* \to R$ is defined as: :$\forall t' \in G^*: \map {x^\wedge} {t'} = \map {t'} x$ Then: :$(1): \quad x^\wedge \in G^{**}$ :$(2): \quad J$ is a [[Definition:Linear Transformation|linear transformation]].	1
For any $\lambda \in \R$, we define $f: \R \to \R$ as the [[Definition:Real Function|function]]: :$\displaystyle \map f \lambda = \sum {\paren {r_i + \lambda s_i}^2}$ Now: :$\map f \lambda \ge 0$ because it is the sum of [[Definition:Square Function|squares]] of [[Definition:Real Number|real numbers]]. Hence: {{begin-eqn}} {{eqn | l = \forall \lambda \in \R: \map f \lambda | o = \equiv | r = \sum {\paren {r_i^2 + 2 \lambda r_i s_i + \lambda^2 s_i^2} } \ge 0 | c = }} {{eqn | o = \equiv | r = \sum {r_i^2} + 2 \lambda \sum {r_i s_i} + \lambda^2 \sum {s_i^2} \ge 0 | c = }} {{end-eqn}} This is a [[Definition:Quadratic Equation|quadratic equation]] in $\lambda$. From [[Solution to Quadratic Equation]]: :$\displaystyle a \lambda^2 + b \lambda + c = 0: a = \sum {s_i^2}, b = 2 \sum {r_i s_i}, c = \sum {r_i^2}$ The [[Definition:Discriminant of Quadratic Equation|discriminant]] of this equation (that is $b^2 - 4 a c$) is: :$D := \displaystyle 4 \paren {\sum {r_i s_i} }^2 - 4 \sum {r_i^2} \sum {s_i^2}$ {{AimForCont}} $D$ is [[Definition:Strictly Positive Real Number|(strictly) positive]]. Then $\map f \lambda = 0$ has two [[Definition:Distinct Elements|distinct]] [[Definition:Real Number|real]] [[Definition:Root of Polynomial|roots]], $\lambda_1 < \lambda_2$, say. From [[Sign of Quadratic Function Between Roots]], it follows that $f$ is [[Definition:Strictly Negative Real Number|(strictly) negative]] somewhere between $\lambda_1$ and $\lambda_2$. But we have: :$\forall \lambda \in \R: \map f \lambda \ge 0$ From this [[Definition:Contradiction|contradiction]] it follows that: :$D \le 0$ which is the same thing as saying: :$\displaystyle \sum {r_i^2} \sum {s_i^2} \ge \paren {\sum {r_i s_i} }^2$ {{qed}}	1
Let $\phi$ be a [[Definition:Linear Operator|linear operator]] on the [[Definition:Real Vector Space|real vector space]] of [[Definition:Dimension of Vector Space|two dimensions]] $\R^2$. Then $\phi$ is completely determined by an [[Definition:Ordered Tuple|ordered tuple]] of $4$ [[Definition:Real Number|real numbers]].	1
Let $C$ be a [[Definition:Curve|curve]] embedded in a [[Definition:Plane|plane]] defined by [[Definition:Polar Coordinates|polar coordinates]]. Let $P$ be the [[Definition:Point|point]] at $\polar {r, \theta}$. Then the [[Definition:Angle|angle]] $\psi$ made by the [[Definition:Tangent to Curve|tangent]] to $C$ at $P$ with the [[Definition:Radial Coordinate|radial coordinate]] is given by: :$\tan \psi = r \dfrac {\d \theta} {\d r}$	1
Let $\struct {X, \norm {\, \cdot \,}}$ is a [[Definition:Normed Vector Space|normed vector space]]. Let $D \subseteq X$ be a [[Definition:Subset|subset]] of $X$. Let $D^-$ be the [[Definition:Closure/Normed Vector Space|closure]] of $D$. Then $D$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] iff $D^- = X$.	1
Let $\struct {S, \cdot}$ be an [[Definition:Algebraic Structure|algebraic structure]]. Let $\map {\MM_S} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. For $\mathbf A, \mathbf B \in \map {\MM_S} {m, n}$, let $\mathbf A \circ \mathbf B$ be defined as the [[Definition:Hadamard Product|Hadamard product]] of $\mathbf A$ and $\mathbf B$. The operation $\circ$ is [[Definition:Commutative Operation|commutative]] on $\map {\MM_S} {m, n}$ {{iff}} $\cdot$ is [[Definition:Commutative Operation|commutative]] on $\struct {S, \cdot}$.	1
First of all, note that [[Absolutely Convergent Generalized Sum Converges]] assures us that the left expression in above equality is defined. Suppose that there exists an $\epsilon > 0$ such that $\displaystyle \left\Vert{\sum \left\{{v_i: i \in I}\right\}}\right\Vert > \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\} + \epsilon$. This supposition is seen to be equivalent to $\displaystyle \left\Vert{\sum \left\{{v_i: i \in I}\right\}}\right\Vert > \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\}$. {{explain|Expand below sentence to be more clear}} Then, by definition of a [[Definition:Generalized Sum|generalized sum]], there necessarily exists a [[Definition:Finite|finite]] [[Definition:Subset|subset]] $F$ of $I$ with: :$\displaystyle \left\Vert{\sum_{i \in F} v_i}\right\Vert > \left\Vert{\sum \left\{{v_i: i \in I}\right\}}\right\Vert - \epsilon > \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\}$ However, using the standard triangle equality on this ''finite'' sum (i.e., axiom '''N3''' for a [[Definition:Norm on Vector Space|norm]], repetitively), we also have: :$\displaystyle \left\Vert{\sum_{i \in F} v_i}\right\Vert \le \sum_{i \in F} \left\Vert{v_i}\right\Vert \le \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\}$ Here the second inequality follows from [[Generalized Sum is Monotone]]. These two estimates constitute a contradiction, and therefore such an $\epsilon$ cannot exist. Hence $\displaystyle \left\Vert{\sum \left\{{v_i: i \in I}\right\}}\right\Vert \le \sum \left\{{\left\Vert{v_i}\right\Vert: i \in I}\right\}$. {{qed}} [[Category:Generalized Sums]] [[Category:Banach Spaces]] [[Category:Triangle Inequality]] abiohhjxt7y574z92vr2w74cszc9tgy	1
{{begin-eqn}} {{eqn | l = \paren {\sum_{n \mathop = 0}^\infty {\size {x_n} }^p }^{\frac 1 p} | o = \ge | r = \paren { {\size {x_i}^p }_{i \mathop \in \N_0} }^{\frac 1 p} }} {{eqn | r = {\size {x_i} }_{i \mathop \in \N_0} }} {{eqn | ll= \leadsto | l = \frac 1 p \map \ln {\sum_{n \mathop = 0}^\infty {\size {x_n} }^p} | o = \ge | r = \map \ln {\size {x_i}_{i \mathop \in \N_0} } }} {{eqn | ll= \leadsto | l = \frac 1 p \map \ln {\sum_{n \mathop = 0}^\infty {\size {x_n} }^p} \sum_{i \mathop = 0}^\infty {\size {x_i} }^p | o = \ge | r = \sum_{i \mathop = 0}^\infty {\size {x_i} }^p \map \ln {\size {x_i} } | c = [[Definition:Real Multiplication|Multiply]] both sides by $\size {x_i}$ and [[Definition:Summation|sum]] over $i \in \N_0$ }} {{end-eqn}} By [[Derivative of P-Norm wrt P|derivative of p-norm {{WRT}} $p$]]: {{begin-eqn}} {{eqn | l = \dfrac \d {\d p} \norm {\mathbf x}_p | r = \frac {\norm {\mathbf x}_p} p \paren {\frac {\sum_{n \mathop = 0}^\infty \size {x_n}^p \map \ln {\size {x_n} } } {\norm {\bf x}_p^p} - \map \ln {\norm {\bf x}_p} } }} {{eqn | r = \frac 1 {p \norm {\mathbf x}_p^{p \mathop - 1} } \paren {\sum_{n \mathop = 0}^\infty {\size {x_n} }^p \map \ln {\size {x_n} } - \sum_{n \mathop = 0}^{\infty} {\size {x_n} }^p \frac 1 p \sum_{i \mathop = 0}^\infty \map \ln {\size {x_i} } } }} {{end-eqn}} By [[P-Norm is Norm]], $\norm {\mathbf x}_p > 0$ for $\mathbf x \ne \sequence 0$. By previously derived [[Definition:Inequality|inequality]], the [[Definition:Term (Algebra)|term]] in [[Definition:Parenthesis|parenthesis]] is [[Definition:Negative Real Function|negative]]. Hence: :$\forall p \ge 1: \dfrac \d {\d p} \norm {\mathbf x}_p < 0$ {{qed}} [[Category:P-Norms]] tinom8aldd9i9ojcszzph3derdxyc3t	1
Let $A = \left({A_F, \oplus}\right)$ be a [[Definition:Star-Algebra|$*$-algebra]]. Let $A' = \left({A_F, \oplus'}\right)$ be constructed from $A$ using the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]]. Then $A'$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]] {{iff}} $A$ is also a [[Definition:Nicely Normed Star-Algebra|nicely normed algebra]].	1
There are no [[Definition:Sequence|sequences]] at all of $n$ terms of the [[Definition:Empty Set|empty set]] for any $n > 0$. Hence the result holds [[Definition:Vacuous Truth|vacuously]]. {{qed}}	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Then the [[Definition:Empty Set|empty set]] $\O$ is [[Definition:Closed Set in Normed Vector Space|closed]] in $M$.	1
Let $\mathbf A = \sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \mathbf 0_R | r = \sqbrk a_{m n} + \sqbrk {0_R}_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {a + 0_R}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf A + \mathbf 0_R | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \mathbf 0_R + \mathbf A | r = \sqbrk {0_R}_{m n} + \sqbrk a_{m n} | c = Definition of $\mathbf A$ and $\mathbf 0_R$ }} {{eqn | r = \sqbrk {0_R + a}_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk a_{m n} | c = {{Ring-axiom|A3}} is $0_R$ }} {{eqn | ll= \leadsto | l = \mathbf 0_R + \mathbf A | r = \mathbf A | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $E$. By definition of [[Definition:Dimension of Vector Space|dimension of vector space]], $E$ has a [[Definition:Basis of Vector Space|basis]] with exactly $n$ [[Definition:Element|elements]]. By [[Sufficient Conditions for Basis of Finite Dimensional Vector Space]], $B$ is a [[Definition:Generator of Module|generator]] for $E$. Then by [[Size of Linearly Independent Subset is at Most Size of Finite Generator]], $H$ has at most $n$ elements. {{Qed}}	1
Let $\C$ be the [[Definition:Complex Plane|complex plane]]. Let $L$ be the [[Definition:Infinite Straight Line|infinite straight line]] in $\C$ which is the [[Definition:Locus|locus]] of the equation: :$l x + m y = 1$ Then $L$ may be written as: :$\map \Re {a z} = 1$ where $a$ is the [[Definition:Point|point]] in $\C$ defined as: :$a = l - i m$	1
Let $\left \langle {a_n} \right \rangle$ be a [[Definition:Sequence|sequence of elements]] of $G$ such that: : $\displaystyle \forall \left \langle {\lambda_n} \right \rangle \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0_R$ That is, the only way to make $e$ with a [[Definition:Linear Combination|linear combination]] of $\left \langle {a_n} \right \rangle$ is by making all the elements of $\left \langle {\lambda_n} \right \rangle$ equal to $0_R$. Such a sequence is '''linearly independent'''. === [[Definition:Linearly Independent/Sequence/Real Vector Space|Linearly Independent Sequence on a Real Vector Space]] === {{:Definition:Linearly Independent/Sequence/Real Vector Space}}	1
[[Definition:Space of Bounded Sequences|Space of bounded sequences]] with [[Definition:Supremum Norm|supremum norm]] forms a [[Definition:Normed Vector Space|normed vector space]].	1
Let $\Omega\subset\R^k$ be [[Definition:Open Set of Real Euclidean Space|open]]. Let $f: \Omega \to \R^n$ be an [[Definition:Immersion|immersion]]. Let $p \in \Omega$. Then: :$k \le n$ and there exists a [[Definition:Local Diffeomorphism|local diffeomorphism]] $\phi$ around $\map f p$ such that: :$\phi \circ \map f x = \tuple {x, 0}$ for all $x$ in a [[Definition:Neighborhood (Topology)|neighborhood]] of $p$.	1
Let: {{begin-eqn}} {{eqn | l = C | r = \begin{bmatrix} \dfrac 1 {x_1 - y_1} & \dfrac 1 {x_1 - y_2} & \cdots & \dfrac 1 {x_1 - y_n} \\ \dfrac 1 {x_2 - y_1} & \dfrac 1 {x_2 - y_2} & \cdots & \dfrac 1 {x_2 - y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n - y_1} & \dfrac 1 {x_n - y_2} & \cdots & \dfrac 1 {x_n - y_n} \\ \end{bmatrix} }} {{end-eqn}} To be proved: {{begin-eqn}} {{eqn | l = \det \paren {C} | r = \dfrac {\displaystyle \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right) \left({y_i - y_j}\right)} {\displaystyle \prod_{1 \mathop \le i, \, j \mathop \le n} \left({x_i - y_j}\right)} | c = Knuth (1997) replacing $y_k \to -y_k$ in $C$ and $\det \paren {C}$ }} {{end-eqn}} Assume hereafter that set $\set {x_1,\ldots,x_n,y_1,\ldots,y_n}$ consists of distinct values, because otherwise $\det \paren {C}$ is undefined or zero. '''Preliminaries''': [[Vandermonde Matrix Identity for Cauchy Matrix]] supplies matrix equation :$\displaystyle (1)\quad - C = PV_x^{-1} V_y Q^{-1}$ :Definitions of symbols: ::$\displaystyle V_x=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ x_1 & x_2 & \cdots & x_n \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1} \\ \end{smallmatrix} },\quad V_y=\paren {\begin{smallmatrix} 1 & 1 & \cdots & 1 \\ y_1 & y_2 & \cdots & y_n \\ \vdots & \vdots & \ddots & \vdots \\ y_1^{n-1} & y_2^{n-1} & \cdots & y_n^{n-1} \\ \end{smallmatrix} }$ [[Definition:Vandermonde Matrix|Vandermonde matrices]] ::$\displaystyle P= \paren {\begin{smallmatrix} p_1(x_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p_n(x_n) \\ \end{smallmatrix} }, \quad Q= \paren {\begin{smallmatrix} p(y_1) & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & p(y_n) \\ \end{smallmatrix} }$ [[Definition:Diagonal Matrix|Diagonal matrices]] ::$\displaystyle p(x) = \prod_{i \mathop = 1}^n \paren {x - x_i}, \quad \displaystyle p_k(x) = \prod_{i \mathop = 1,i \mathop \ne k}^n \, \paren {x - x_i}, \quad 1 \mathop \le k \mathop \le n$ [[Definition:Polynomial/Complex Numbers|Polynomials]] '''Determinant of $C$ Calculation''': {{begin-eqn}} {{eqn | l = C | r = \paren {-I}\,P\,V_x^{-1}\, V_y\, Q^{-1} | c = [[Vandermonde Matrix Identity for Cauchy Matrix]] Symbol $I$ is the $n\times n$ identity matrix. }} {{eqn | l = (2)\quad \det \paren {C} | r = \det \paren { -I } \det \paren {P} \det \paren { V_x^{-1} } \det \paren { V_y } \det \paren { Q^{-1} } | c = [[Determinant of Matrix Product]] }} {{eqn | r = \paren { -1 }^n \det \paren { I } \det \paren {P} \det \paren { V_x^{-1} } \det \paren { V_y } \det \paren { Q^{-1} } | c = [[Effect of Elementary Row Operations on Determinant]] Factor constant $-1$ from all rows of $-I$. }} {{eqn | r = \paren { -1 }^n \det \paren { I } \dfrac { \det \paren {P} \det \paren { V_y } } { \det \paren { Q } \det \paren { V_x } } | c = [[Matrix Product with Adjugate Matrix]] and [[Determinant of Matrix Product]] }} {{end-eqn}} '''Lemma''': $\det \paren {P} = \paren {-1}^m \paren { \det \paren {V_x} }^2$ where $m=\frac 1 2 n \paren {n-1}$ :'''Details''': Determinant $\det \paren {P}$ expands to: {{begin-eqn}} {{eqn | l = \,\,\prod_{j \mathop = 1}^n \map {p_j} {x_j} | r = \prod_{j \mathop = 1}^n \,\, \prod_{k \mathop = 1,\, k \mathop \neq j}^n \paren { x_j - x_k } | c = Definition of polynomials $\map {p_j} x$. }} {{end-eqn}} :Pair factors $\paren {x_r - x_s}$ and $\paren {x_s - x_r}$ into factor $- \paren {x_s - x_r}^2$, then: {{begin-eqn}} {{eqn | l = \,\,\det \paren {P} | r = \paren {-1}^m \, \paren { \prod_{1 \mathop \leq r \mathop \lt s \mathop \le n} \paren { x_s - x_r }^2 } | c = where $m = 1 + \cdots + \paren {n-1} = \frac 1 2 n \paren {n-1}$ }} {{eqn | r = \paren {-1}^m \, \paren { \det \paren {V_x} }^2 | c = [[Vandermonde Determinant]] }} {{end-eqn}} {{qed|lemma}} Apply the '''Lemma''' to equation (2): {{begin-eqn}} {{eqn | l = \det \paren {C} | r = \paren { -1 }^{n+m} \, \paren { \dfrac { \det \paren {V_x} \det \paren { V_y } } { \det \paren { Q } } } | c = $m = \frac 1 2 n \paren {n-1}$ }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \, \prod_{1 \mathop \le m \mathop \lt k \mathop \le n} \paren { y_k - y_m } } { \displaystyle \prod_{k \mathop = 1}^n \map {p} {y_k} } | c = [[Vandermonde Determinant]] and [[Determinant of Diagonal Matrix]] }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_k - y_m } } { \displaystyle \prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { y_k - x_j } } | c = Definition of $\map p x$. }} {{eqn | r = \paren { -1 }^{n+m} \,\, \dfrac { \displaystyle \paren {-1}^{m}\prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_m - y_k } } { \displaystyle \paren {-1}^{n^2}\,\,\prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { x_j - y_k } } | c = Factor out changed signs. }} {{eqn | r = \left. \dfrac { \displaystyle \prod_{1 \mathop \le m \mathop \lt k \mathop \le n}^{\phantom n} \paren { x_k - x_m } \paren { y_m - y_k } } { \displaystyle \prod_{k \mathop = 1}^n \prod_{j \mathop = 1}^n \paren { x_j - y_k } } \right. | c = All signs cancel. }} {{end-eqn}} {{qed}}	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A2}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Commutative Operation|commutative]]. The result then follows directly from [[Commutativity of Hadamard Product]]. {{qed}}	1
Let $M$ be a [[Definition:Compact Space|compact]] [[Definition:Dimension (Geometry)|$2$-dimensional]] [[Definition:Riemannian Manifold|Riemannian manifold]] with [[Definition:Boundary (Topology)|boundary]] $\partial M$. Let $\Kappa$ be the [[Definition:Gaussian Curvature|Gaussian curvature]] of $M$. Let $k_g$ be the [[Definition:Geodesic Curvature|geodesic curvature]] of $\partial M$. Then : :$\displaystyle \int_M \kappa \, \mathrm d A + \int_{\partial M} k_g \, \mathrm d s = 2 \pi \chi\left({M}\right)$ where: :$\mathrm d A$ is the [[Definition:Area Element|element of area]] of the [[Definition:Surface|surface]] :$\mathrm d s$ is the [[Definition:Line Element|line element]] along $\partial M$ :$\chi\left({M}\right)$ is the [[Definition:Euler Characteristic|Euler characteristic]] of $M$.	1
{{ProofWanted|This needs to be made considerably less clumsy}}	1
For all $m \in \N: m \ge 2$, the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]: :$\struct {\Z_m, +_m, \times_m}$ is a [[Definition:Commutative and Unitary Ring|commutative ring with unity $\eqclass 1 m$]]. The [[Definition:Ring Zero|zero]] of $\struct {\Z_m, +_m, \times_m}$ is $\eqclass 0 m$.	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 2}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$D_n = F_{n + 1}$ === Basis for the Induction === $P \left({2}\right)$ is the case: {{begin-eqn}} {{eqn | l = D_2 | r = \begin{vmatrix} 1 & -1 \\ 1 & 1 \end{vmatrix} | c = }} {{eqn | r = \left({1 \times 1}\right) - \left({-1 \times 1}\right) | c = {{Defof|Determinant of Order 2}} }} {{eqn | r = 2 | c = }} {{eqn | r = F_3 | c = {{Defof|Fibonacci Number}} }} {{end-eqn}} Thus $P \left({2}\right)$ is seen to hold. $P \left({3}\right)$ is the case: {{begin-eqn}} {{eqn | l = D_3 | r = \begin{vmatrix} 1 & -1 & 0 \\ 1 & 1 & -1 \\ 0 & 1 & 1 \end{vmatrix} | c = }} {{eqn | r = \left({1 \times \begin{vmatrix} 1 & -1 \\ 1 & 1 \end{vmatrix} }\right) - \left({\left({-1}\right) \times \begin{vmatrix} 1 & -1 \\ 0 & 1 \end{vmatrix} }\right) + \left({0 \times \begin{vmatrix} 1 & 1 \\ 0 & 1 \end{vmatrix} }\right) | c = [[Expansion Theorem for Determinants]]: expanding by $1$st [[Definition:Row of Matrix|row]] }} {{eqn | r = 1 \times D_2 + 1 \times \left({1 \times 1}\right) | c = {{Defof|Determinant of Order 2}} }} {{eqn | r = 2 + 1 | c = from above }} {{eqn | r = F_3 + F_2 | c = {{Defof|Fibonacci Number}} }} {{eqn | r = F_4 | c = {{Defof|Fibonacci Number}} }} {{end-eqn}} Thus $P \left({3}\right)$ is seen to hold. $P \left({2}\right)$ and $P \left({3}\right)$ together form the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({k}\right)$ and $P \left({k - 1}\right)$ are true, where $k \ge 3$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: {{begin-eqn}} {{eqn | l = D_k | r = F_{k + 1} }} {{eqn | l = D_{k - 1} | r = F_k }} {{end-eqn}} from which it is to be shown that: :$D_{k + 1} = F_{k + 2}$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: Recall the definition of $D_n$: :$D_{k + 1} = \underbrace {\begin{vmatrix} 1 & -1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 1 & 1 & -1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 1 & -1 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & -1 & \cdots & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 & \cdots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & 0 & \cdots & 1 & -1 & 0 \\ 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & -1 \\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 \\ \end{vmatrix} }_{k + 1 \text { columns} }$ Expanding by $1$st [[Definition:Row of Matrix|row]] using the [[Expansion Theorem for Determinants]] requires the evaluation of $2$ [[Definition:Cofactor of Element|cofactors]]: :$1 \times C_{1 1}$ and: :$\left({-1}\right) \times C_{1 2}$ where $C_{r s}$ denotes the [[Definition:Determinant of Matrix|determinant]] of [[Definition:Order of Determinant|order $k$]] obtained from $D_{k + 1}$ by deleting row $r$ and column $s$. By the construction of $D_{k + 1}$, it can be seen that $C_{1 1}$ is $D_k$. The structure of $C_{1 2}$ is seen to be: :$C_{1 2} = \left({-1}\right) \times \underbrace {\begin{vmatrix} 1 & -1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & -1 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 1 & -1 & \cdots & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & \cdots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & 1 & -1 & 0 \\ 0 & 0 & 0 & 0 & \cdots & 1 & 1 & -1 \\ 0 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 \\ \end{vmatrix} }_{k \text { columns} }$ By [[Determinant with Unit Element in Otherwise Zero Column]]: :$C_{1 2} = \left({-1}\right) \times \underbrace {\begin{vmatrix} 1 & -1 & 0 & \cdots & 0 & 0 & 0 \\ 1 & 1 & -1 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 1 & \cdots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & 1 & -1 & 0 \\ 0 & 0 & 0 & \cdots & 1 & 1 & -1 \\ 0 & 0 & 0 & \cdots & 0 & 1 & 1 \\ \end{vmatrix} }_{k - 1 \text { columns} }$ which equals $-D_{k - 1}$. Putting this together, we have: {{begin-eqn}} {{eqn | l = D_{k + 1} | r = 1 \times C_{1 1} + \left({-1}\right) \times C_{1 2} | c = [[Expansion Theorem for Determinants]] }} {{eqn | r = C_{1 1} - C_{1 2} | c = }} {{eqn | r = D_k - \left({-D_{k - 1} }\right) | c = }} {{eqn | r = D_k + D_{k - 1} | c = }} {{eqn | r = F_{k + 1} + F_k | c = [[Matrix whose Determinant is Fibonacci Number#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = F_{k + 2} | c = {{Defof|Fibonacci Number}} }} {{end-eqn}} So $P \left({k}\right) \land P \left({k - 1}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{\ge 2}: D_n = F_{n + 1}$ {{qed}}	1
No [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Set|set]] of the [[Definition:Rational Number|rational numbers]] is [[Definition:Complete Normed Division Ring|complete]].	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Vector (Euclidean Space)|vectors]] in the [[Definition:Real Euclidean Space|Euclidean space]] $\R^3$. Let $\mathbf a \times \mathbf b$ denote the [[Definition:Vector Cross Product|vector cross product]]. Then: :$(1): \quad$ $\mathbf a$ and $\mathbf a \times \mathbf b$ are [[Definition:Orthogonal (Linear Algebra)|orthogonal]]. :$(2): \quad$ $\mathbf b$ and $\mathbf a \times \mathbf b$ are [[Definition:Orthogonal (Linear Algebra)|orthogonal]].	1
Let $\map \MM 1$ denote the [[Definition:Matrix Space|matrix space]] of [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order]] $1$. Let $\map \MM {1, n}$ denote the [[Definition:Matrix Space|matrix space]] of [[Definition:Order of Square Matrix|order]] $1 \times n$. Let $\mathbf A = \begin {pmatrix} a \end {pmatrix} \in \map \MM 1$ and $\mathbf B = \begin {pmatrix} b_1 & b_2 & \cdots & b_n \end{pmatrix} \in \map \MM {1, n}$. Let $\mathbf C = \mathbf A \mathbf B$ denote the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A$ with $\mathbf B$. Let $\mathbf D = a \mathbf B$ denote the [[Definition:Matrix Scalar Product|matrix scalar product]] of $a$ with $\mathbf B$. Then $\mathbf C = \mathbf D$.	1
=== Necessary Condition === Assume that $W$ is a [[Definition:G-Submodule|$G$-submodule]] of $V$. Hence by definition $\phi_W: G \times W \to W$ is a [[Definition:Linear Group Action|linear action]] on $W$. Also by definition, $\phi_W \left({G, W}\right) = \phi \left({G, W}\right) \subseteq W$. {{qed|lemma}} === Sufficient Condition === Assume now that $\phi \left({G, W}\right) = \phi_W \left({G, W}\right) \subseteq W$. Then it is correct to define $\phi_W: G \times W \to W$; it is a well-defined [[Definition:Mapping|mapping]]. We need to check if $\phi_W$ is a [[Definition:Linear Group Action|linear action]] on $W$: Assume $a,b \in W$ and $g \in G$; in particular, then, $a,b \in V$ and: {{begin-eqn}} {{eqn|l = \phi_W \left({g, a + b}\right) |r = \phi \left({g, a + b}\right)| |c = Definition of $\phi_W$ }} {{eqn|r = \phi \left({g, a}\right) + \phi \left({g, b}\right) |c = $\phi$ is a [[Definition:Linear Group Action|linear action]] on $V$ }} {{eqn|r = \phi_W \left({g, a}\right) + \phi_W \left({g, b}\right) |c = Definition of $\phi_W$ }} {{end-eqn}} Further, assume $\lambda \in k$ and $g\in G$, then: {{begin-eqn}} {{eqn|l = \phi_W \left({g, \lambda b}\right) |r = \phi \left({g, \lambda b}\right) |c = Definition of $\phi_W$ }} {{eqn|r = \lambda \phi \left({g, b}\right) |c = $\phi$ is a [[Definition:Linear Group Action|linear action]] on $V$ }} {{eqn|r = \lambda \phi_W \left({g, b}\right) |c = Definition of $\phi_W$ }} {{end-eqn}} Thus $W$ is a [[Definition:G-Submodule|$G$-submodule]] of $V$. {{qed}} [[Category:Representation Theory|{{SUBPAGENAME}}]] kjp6yspncfxbw87ek5l5gkmlxk27le1	1
First it is shown that: :$\displaystyle \map \det {\mathbf A} = \sum_{\mu \mathop \in S_n} \paren {\map \sgn \mu \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \lambda k, \map \mu k} }$ Let $\nu: \N_{> 0} \to \N_{> 0}$ be a [[Definition:Permutation|permutation]] on $\N_{> 0}$ such that $\lambda \circ \nu = \mu$. The product can be rearranged as: :$\displaystyle \prod_{k \mathop = 1}^n a_{\map \lambda k, \map \mu k} = a_{\map \lambda 1, \map \mu 1} a_{\map \lambda 2, \map \mu 2} \cdots a_{\map \lambda n, \map \mu n} = a_{1, \map \nu 1} a_{2, \map \nu 2} \cdots a_{n, \map \nu n} = \prod_{k \mathop = 1}^n a_{k, \map \nu k}$ from {{Field-axiom|M2}}. By [[Parity Function is Homomorphism]]: :$\map \sgn \mu \map \sgn \lambda = \map \sgn \lambda \map \sgn \nu \map \sgn \lambda = \map {\sgn^2} \lambda \map \sgn \nu = \map \sgn \nu$ and so: :$\displaystyle \map \det {\mathbf A} = \sum_{\nu \mathop \in S_n} \paren {\map \sgn \nu \prod_{k \mathop = 1}^n a_{k, \map \nu k} }$ which is the usual definition for the [[Definition:Determinant of Matrix|determinant]]. Next it is to be shown: :$\displaystyle \map \det {\mathbf A} = \sum_{\mu \mathop \in S_n} \paren {\map \sgn \mu \map \sgn \lambda \prod_{k \mathop = 1}^n a_{\map \mu k, \map \lambda k} }$ Let $\nu: \N_{> 0} \to \N_{> 0}$ be a [[Definition:Permutation|permutation]] on $\N_{> 0}$ such that $\mu \circ \nu = \lambda$. The result follows via a similar argument. {{Qed}} [[Category:Determinants]] ncgpkk0vowuqgjdvv37pev1rssrws1s	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\overline z$ denote the [[Definition:Complex Conjugate|complex conjugate]] of $z$. Let $\cmod z$ denote the [[Definition:Complex Modulus|modulus]] of $z$. Then: :$\cmod z = \cmod {\overline z}$	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $K$ be a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$. Let $P_K$ denote the [[Definition:Orthogonal Projection|orthogonal projection]] on $K$. Then $P_K$ has the following properties: :$(1):\qquad P_K$ is a [[Definition:Linear Transformation|linear transformation]] on $H$. :$(2):\qquad \forall h\in H: \left\|{P_K(h)}\right\| \le \left\|{h}\right\|$ :$(3):\qquad P_K \circ P_K = P_K$ :$(4):\qquad \ker P_K = K^{\perp}$ and $\operatorname{ran} P_K = K$ {{wtd|I am not entirely happy with this setup of the page, with four results stated. Strictly speaking, each should have its own page. But I am not up to thinking of a name for each of the results separately. Probably I will put up four transcluded proof pages. Lastly, note that these four results are one theorem $I.2.7$ in Conway. --LF<br/><br/>Four transcluded subpages is a technique that's been used before for this sort of linked set of results. The fact they're all part of the same proof in Conway is incidental. -- pm}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct{G, +_G, \circ}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Let $\circ' : G \times R \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: x \circ' \lambda = \lambda \circ x$ Then $\struct{G, +_G, \circ'}$ is not necessarily a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$	1
:$\sequence {x_n + y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
By [[Inverse of Algebraic Structure Isomorphism is Isomorphism]] then: :$\phi: R \to S$ is an [[Definition:Ring Isomorphism|ring isomorphism]] {{iff}} $\phi^{-1}: S \to R$ is also an [[Definition:Ring Isomorphism|ring isomorphism]]. Let $d_R$ and $d_S$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norms]] $\norm {\,\cdot\,}_R$ and $\norm {\,\cdot\,}_S$ respectively. By [[Inverse of Isometry of Metric Spaces is Isometry]] then: :$\phi: \struct {R, d_R} \to \struct {S, d_S}$ is an [[Definition:Isometry (Metric Spaces)|isometry]] {{iff}} $\phi^{-1}: \struct {S, d_S} \to \struct {R, d_R}$ is also an [[Definition:Isometry (Metric Spaces)|isometry]]. The result follows. {{qed}} [[Category:Normed Division Rings]] py0czj6bpbe3o9u87vjpmzmaqi3wrxw	1
By definition, a [[Definition:Topological Space|topological space]] $T = \struct {S, \tau}$ is [[Definition:Separable Space|separable]] if there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. The [[Definition:Closure (Topology)|closure]] of $S$ in $S$ is trivially $S$. So, by definition, $S$ is [[Definition:Everywhere Dense|everywhere dense]] in $S$. As $S$ is [[Definition:Countable Set|countable]] by definition, the result follows. {{qed}} [[Category:Separable Spaces]] [[Category:Countable Sets]] 5sagq1qbpwtck3uym0x314bh0a672vu	1
=== Existence === By [[Morphism from Ring with Unity to Module]], for all $i$ there exists a morphism $\psi_i:R\to M$ with $\psi_i(1)=m_i$. By [[Universal Property of Direct Sum of Modules]], there exists a morphism $\Psi:R^{(I)}\to M$ such that $\Psi\circ\iota_i=\psi_i$ for all $i$. Thus $\Psi(e_i)=\Psi(\iota_i(1))=\psi_i(1)=m_i$ for all $i$. We have $(r_i)_{i\in I}=\sum_{i\in I}r_ie_i$, so the expression for $\Psi$ follows by linearity. === Uniqueness === Let $\Psi$ be such a morphism. Then $\Psi\circ\iota_i$ sends $1$ to $m_i$. By [[Morphism from Ring with Unity to Module]], $\Psi\circ\iota_i=\psi_i$, with $\psi_i$ as above. By [[Universal Property of Direct Sum of Modules]], $\Psi$ is determined by $\Psi\circ\iota_i$. Thus $\Psi$ is unique. {{qed}} [[Category:Module Theory]] [[Category:Direct Products]] [[Category:Universal Properties]] p7el9r1q9guxfa9y6efy0lo3ejn7eh2	1
Let $R$ be a [[Definition:Division Ring|division ring]] with [[Definition:Unity of Ring|unity]] $1_R$. Let $\norm {\,\cdot\,}_1$ and $\norm {\,\cdot\,}_2$ be [[Definition:Equivalent Division Ring Norms|equivalent]] [[Definition:Norm/Division Ring|norms]] on $R$. Then $\norm {\,\cdot\,}_1$ and $\norm {\,\cdot\,}_2$ are either both [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] or both [[Definition:Archimedean Division Ring Norm|Archimedean]].	1
{{ProofWanted}} use [[Transition Mapping Between Charts is Homeomorphism]] [[Category:Manifolds]] il4ktsasdf6tjtm7to57hdoa83u74ty	1
From [[Test for Right Ideal]], the following need to be proved: : $(1): \quad G \ne \O$ : $(2): \quad \forall \mathop {\mathbf X}, \mathop{\mathbf Y} \in G: \mathbf X + \paren {-\mathbf Y} \in G$ : $(3): \quad \forall \mathop{\mathbf J} \in G, \mathop{\mathbf R} \in \map {\MM_S} 2: \mathbf J \times \mathbf R \in G$ === Condition $(1): \quad G \ne \O$ === By definition of $G$: :$\quad \begin {bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \in G$ {{qed|lemma}} === Condition $(2): \quad \forall \mathop {\mathbf X}, \mathop{\mathbf Y} \in G: \mathbf X + \paren {-\mathbf Y} \in G$ === Let: :$\quad \mathbf X = \begin{bmatrix} x_1 & x_2 \\ 0 & 0 \end{bmatrix}, \quad \mathbf Y = \begin{bmatrix} y_1 & y_2 \\ 0 & 0 \end{bmatrix} \in G$ Then: :$\quad \mathbf X - \mathbf Y = \begin {bmatrix} x_1 - y_1 & x_2 - y_2 \\ 0 & 0 \end{bmatrix} \in G$ {{qed|lemma}} === Condition $(3): \quad \forall \mathop{\mathbf J} \in G, \mathop{\mathbf R} \in \map {\MM_S} 2: \mathbf J \times \mathbf R \in G$ === Let: :$\quad \mathbf J = \begin{bmatrix} j_1 & j_2 \\ 0 & 0 \end{bmatrix} \in G, \quad \mathbf R = \begin{bmatrix} r_{1 1} & r_{2 1} \\ r_{1 2} & r_{2 2} \end{bmatrix} \in \map {\MM_S} 2$ Then: :$\quad \mathbf J \times \mathbf R = \begin{bmatrix} j_1 \times r_{1 1} + j_2 \times r_{1 2} & j_1 \times r_{2 1} + j_2 \times r_{2 2} \\ 0 & 0 \end{bmatrix} \in G$ {{qed}} [[Category:Right Module Does Not Necessarily Induce Left Module over Ring]] 6rd5qz1edmk5oex5kjo6387gtlyxct0	1
Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]]. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] $\mathbf A$. Let $1 \le j_1, j_2, \ldots, j_m \le n$. Let $\mathbf A_{j_1 j_2 \ldots j_m}$ denote the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. Let $\mathbf A^\intercal_{j_1 j_2 \ldots j_m}$ denote the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf A^\intercal$. Then: :$\displaystyle \det \left({\mathbf A \mathbf A^\intercal}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \left({\det \left({\mathbf A_{j_1 j_2 \ldots j_m} }\right)}\right)^2$ where $\det$ denotes the [[Definition:Determinant of Matrix|determinant]].	1
By [[Limit of Subsequence equals Limit of Sequence/Normed Division Ring|Limit of Subsequence equals Limit of Sequence]] then $\sequence {y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} y_n = l$ Let $\epsilon > 0$ be given. Let $\epsilon' = \dfrac {\epsilon {\norm l}^2 } {2}$. Then: :$ \epsilon' > 0$ As $\sequence {y_n} \to l$, as $n \to \infty$, we can find $N_1$ such that: :$\forall n > N_1: \norm {y_n - l} < \epsilon'$ As $\sequence {y_n}$ converges to $l \ne 0$, by [[Sequence Converges to Within Half Limit/Normed Division Ring|Sequence Converges to Within Half Limit]]: :$\exists N_2 \in \N: \forall n > \N_2: \dfrac {\norm l} 2 < \norm {y_n}$ or equivalently: :$\exists N_2 \in \N: \forall n > \N_2: 1 < \dfrac {2 \norm {y_n} } {\norm l}$ Let $N = \max \set {N_1, N_2}$. Then $\forall n > N$: :$(1): \quad \norm {y_n - l} < \epsilon'$ :$(2): \quad 1 < \dfrac {2 \norm {y_n} } {\norm l}$ Hence: {{begin-eqn}} {{eqn | l = \norm { {y_n }^{-1} - l^{-1} } | o = < | r = \dfrac {2 \norm {y_n} } {\norm l} \norm { {y_n}^{-1} - l^{-1} } | c = from $(1)$ }} {{eqn | r = \dfrac 2 {\norm l^2} \paren {\norm { {y_n } } \norm { {y_n}^{-1} - l^{-1} } \norm l} | c = multiplying and dividing by $\norm l$ }} {{eqn | r = \dfrac 2 {\norm l^2} \norm {y_n \paren { {y_n}^{-1} - l^{-1} } l} | c = {{NormAxiom|2}} }} {{eqn | r = \dfrac 2 {\norm l^2} \norm {y_n y_n^{-1} l - y_n l^{-1} l} | c = {{Ring-axiom|D}} }} {{eqn | r = \dfrac 2 {\norm l^2} \norm {l - y_n} | c = [[Definition:Division Ring|Inverse Property of Division Ring]] }} {{eqn | o = < | r = \dfrac 2 {\norm l^2} \epsilon' | c = from $(2)$ }} {{eqn | r = \dfrac {2} {\norm l^2} \paren { \dfrac {\epsilon \norm l^2 } 2} | c = Definition of $\epsilon'$ }} {{eqn | r = \epsilon | c = cancelling terms }} {{end-eqn}} Hence: :$\sequence { {y_n}^{-1} }$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with $\displaystyle \lim_{n \mathop \to \infty} {y_n}^{-1} = l^{-1}$. {{qed}} [[Category:Combination Theorem for Sequences in Normed Division Rings]] s9m3b9wd0jhdnb1n3cmd1fpdiz6vbae	1
Suppose that $(1)$ and $(2)$ hold. From $(1)$, we obtain for every $\lambda \in K$ and $u \in U$ that $\lambda u \in U$. An application of $(2)$ yields the condition of the [[One-Step Vector Subspace Test]]. Hence $U$ is a [[Definition:Vector Subspace|vector subspace]] of $V$. {{qed}}	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Then: :$-\cmod z \le \cmod z$ where $\cmod z$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z$. The equality holds {{iff}} $z = 0$.	1
Let $x_0 \in R$. Let $\epsilon > 0$ be given. Let $x \in R$ such that: :$\map d {x, x_0} < \epsilon$ Then: {{begin-eqn}} {{eqn | l = \map d {-x, -x_0} | r = \norm {-x - \paren {-x_0} } | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm {-x + x_0} }} {{eqn | r = \norm {x_0 - x} | c = $+$ is [[Definition:Commutative Operation|Commutative]]. }} {{eqn | r = \map d {x_0, x} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \map d {x, x_0} | c = [[Definition:Metric Space Axioms|Metric Space Axiom $(\text M 2)$]] }} {{eqn | r = \epsilon | o = < }} {{end-eqn}} Since $x_0$ and $\epsilon$ were arbitrary, by the definition of [[Definition:Continuous Mapping (Metric Space)|continuity]] then the [[Definition:Mapping|mapping]]: :$\eta: \struct {R, d} \to \struct {R, d} : \map \eta x = -x$ is [[Definition:Continuous Mapping (Metric Space)|continuous]]. {{qed}}	1
=== Necessary Condition === Let $\norm {\,\cdot\,}_R$ be a [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]]. Then for all $x,y \in R$: {{begin-eqn}} {{eqn | l = \norm {x + y}_S | r = \norm {\map \phi {\map {\phi^{-1} } x} + \map \phi {\map {\phi^{-1} } y} }_S | c = $\phi$ is a [[Definition:Bijection|bijection]] }} {{eqn | r = \norm {\map {\phi^{-1} } x + \map {\phi^{-1} } y}_R | c = $\phi$ is an [[Definition:Isometry (Metric Spaces)|isometry]]. }} {{eqn | o = \le | r = \max \set {\norm {\map {\phi^{-1} } x}_R, \norm {\map {\phi^{-1} } y}_R} | c = $\norm {\,\cdot\,}_R$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]]. }} {{eqn | r = \max \set {\norm {\map \phi {\map {\phi^{-1} } x} }_S, \norm {\map \phi {\map {\phi^{-1} } y} }_S} | c = $\phi$ is an [[Definition:Isometry (Metric Spaces)|isometry]]. }} {{eqn | r = \max \set {\norm x_S, \norm y_S} | c = $\phi$ is a [[Definition:Bijection|bijection]]. }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === Let $\norm {\, \cdot \,}_S$ be a [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]]. By [[Inverse of Isometric Isomorphism]], $\phi^{-1}: S \to R$ is an [[Definition:Isometric Isomorphism|isometric isomorphism]]. By the '''[[Isometrically Isomorphic Non-Archimedean Division Rings#Necessary Condition|necessary condition]]''', $\norm {\, \cdot \,}_R$ is [[Definition:Non-Archimedean Division Ring Norm|non-archimedean]]. {{qed}} [[Category:Normed Division Rings]] m3vzfu8nxtrx8s5lgtieza28ol5q02r	1
From [[Combination Theorem for Cauchy Sequences/Multiple Rule|Multiple Rule for Normed Division Ring Sequences]]: :$\sequence {-y_n} = \sequence {\paren {-1} y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. From [[Combination Theorem for Cauchy Sequences/Sum Rule|Sum Rule for Normed Division Ring Sequences]]: :$\sequence {x_n - y_n} = \sequence {x_n + \paren {-y_n} }$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. {{qed}}	1
This follows from [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]]. It suffices to find: :A [[Definition:Linearly Independent Set|linearly independent]] [[Definition:Subset|subset]] $L\subset V$ :A [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $S\subset V$ with $L\subset S$. By [[Empty Set is Linearly Independent]], we make take $L = \O$ and $S$ any [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]], which exists because $V$ is [[Definition:Finitely Generated Module|finitely generated]]. {{qed}}	1
Denote the [[Definition:Triangle (Geometry)|triangle]] as $\triangle$, and the [[Definition:Interior of Jordan Curve|interior]] of the [[Definition:Boundary (Geometry)|boundary]] of $\triangle$ as $\Int \triangle$. From [[Boundary of Polygon is Jordan Curve]], it follows that the [[Definition:Boundary (Geometry)|boundary]] of $\triangle$ is equal to the [[Definition:Image of Mapping|image]] of a [[Definition:Jordan Curve|Jordan curve]], so $\Int \triangle$ is well-defined. Denote the [[Definition:Vertex (Geometry)|vertices]] of $\triangle$ as $A_1, A_2, A_3$. For $i \in \set {1, 2, 3}$, put $j = i \bmod 3 + 1$, $k = \paren {i + 1} \bmod 3 + 1$, and: :$U_i = \set {A_i + s t \paren {A_j - A_i} + \paren {1 - s} t \paren {A_k - A_i} : s \in \openint 0 1, t \in \R_{>0} }$ Suppose that the [[Definition:Angle Between Vectors|angle]] $\angle A_i$ between is $A_j - A_i$ and $A_k - A_i$ is [[Definition:Convex Angle|non-convex]]. As $\angle A_i$ is an [[Definition:Internal Angle|internal angle]] in $\triangle$, it follows from [[Definition:Polygon|definition of polygon]] that $\angle A_i$ cannot be [[Definition:Zero Angle|zero]] or [[Definition:Straight Angle|straight]]. Then $\angle A_i$ is larger than a [[Definition:Straight Angle|straight angle]], which is impossible by [[Sum of Angles of Triangle Equals Two Right Angles]]. It follows that $\angle A_i$ is [[Definition:Convex Angle|convex]]. From [[Characterization of Interior of Triangle]], it follows that: :$\displaystyle \Int \triangle = \bigcap_{i \mathop = 1}^3 U_i$ From [[Interior of Convex Angle is Convex Set]], it follows for $i \in \set {1, 2, 3}$ that $U_i$ is a [[Definition:Convex Set (Vector Space)|convex set]]. The result now follows from [[Intersection of Convex Sets is Convex Set (Vector Spaces)]]. {{qed}} [[Category:Vector Spaces]] 62yculhuqp577pbsv8offiogbpmwcf4	1
Let the [[Definition:Unity of Ring|unity]] of $K$ be $1$, and the [[Definition:Ring Zero|zero]] of $K$ be $0$. Consider the [[Definition:Vector (Linear Algebra)|vectors]]: {{begin-eqn}} {{eqn | l = \mathbf e_1 | o = := | r = \underbrace {\tuple {1, 0, \ldots, 0} }_{n \text { coordinates} } | c = }} {{eqn | l = \mathbf e_2 | o = := | r = \underbrace {\tuple {0, 1, \ldots, 0} }_{n \text { coordinates} } | c = }} {{eqn | o = \vdots | c = }} {{eqn | l = \mathbf e_n | o = := | r = \underbrace {\tuple {0, 0, \ldots, 1} }_{n \text { coordinates} } | c = }} {{end-eqn}} Thus $\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\mathbf V$. From [[Standard Ordered Basis is Basis]], $\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ is a [[Definition:Basis of Vector Space|basis]] of $\mathbf V$. The result follows from [[Linearly Independent Set is Basis iff of Same Cardinality as Dimension]]. {{qed}}	1
Let $z = x + i y \in \C$ be an arbitrary [[Definition:Complex Number|complex number]]. {{AimForCont}} the contrary: {{begin-eqn}} {{eqn | l = \size x + \size y | o = > | r = \sqrt 2 \cmod z | c = }} {{eqn | ll= \leadsto | l = \paren {\size x + \size y}^2 | o = > | r = 2 \cmod z^2 | c = squaring both sides }} {{eqn | ll= \leadsto | l = \size x^2 + 2 \size x \, \size y + \size y^2 | o = > | r = 2 \cmod z^2 | c = multiplying out }} {{eqn | ll= \leadsto | l = x^2 + 2 \size x \, \size y + y^2 | o = > | r = 2 \cmod z^2 | c = {{Defof|Absolute Value}} }} {{eqn | ll= \leadsto | l = x^2 + 2 \size x \, \size y + y^2 | o = > | r = 2 \paren {x^2 + y^2} | c = {{Defof|Complex Modulus}} }} {{eqn | ll= \leadsto | l = 2 \size x \, \size y | o = > | r = x^2 + y^2 | c = }} {{eqn | ll= \leadsto | l = 2 \size x \, \size y | o = > | r = \size x^2 + \size y^2 | c = {{Defof|Absolute Value}} }} {{eqn | ll= \leadsto | l = \size x^2 - 2 \size x \, \size y + \size y^2 | o = < | r = 0 | c = rearranging }} {{eqn | ll= \leadsto | l = \paren {\size x - \size y}^2 | o = < | r = 0 | c = factoring }} {{end-eqn}} But as $\size x$ and $\size y$ are both [[Definition:Real Number|real]] this cannot happen. Thus our initial assumption $\size x + \size y > \sqrt 2 \cmod z$ is false. Hence the result. {{qed}}	1
From: :[[Integers form Ring]] :[[Rational Numbers form Ring]] :[[Real Numbers form Ring]] :[[Complex Numbers form Ring]] the [[Definition:Standard Number System|standard number systems]] $\Z$, $\Q$, $\R$ and $\C$ are [[Definition:Ring (Abstract Algebra)|rings]] whose [[Definition:Zero Element|zero]] is the [[Definition:Zero (Number)|number $0$ (zero)]]. Hence we can apply [[Zero Matrix is Identity for Matrix Entrywise Addition over Ring]]. {{qed|lemma}} The above cannot be applied to the [[Definition:Natural Numbers|natural numbers]] $\N$, as they do not form a [[Definition:Ring (Abstract Algebra)|ring]]. However, from [[Natural Numbers under Addition form Commutative Monoid]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\N, +}$ is a [[Definition:Commutative Monoid|commutative monoid]] whose [[Definition:Identity Element|identity]] is [[Definition:Zero (Number)|$0$ (zero)]]. By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Addition|addition of numbers]]. The result follows from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
We have: {{begin-eqn}} {{eqn | l = \paren {\mathbf A^{-1} }^\intercal \mathbf A^\intercal | r = \paren {\mathbf A \mathbf A^{-1} }^\intercal | c = [[Transpose of Matrix Product]] }} {{eqn | r = \mathbf I^\intercal | c = {{Defof|Inverse Matrix}}: $\mathbf I$ denotes [[Definition:Unit Matrix|Unit Matrix]] }} {{eqn | r = \mathbf I | c = {{Defof|Unit Matrix}} }} {{end-eqn}} Hence $\paren {\mathbf A^{-1} }^\intercal$ is an [[Definition:Inverse Matrix|inverse]] of $\mathbf A^\intercal$. From [[Inverse of Square Matrix over Field is Unique]]: :$\paren {\mathbf A^{-1} }^\intercal = \paren {\mathbf A^\intercal}^{-1}$ {{qed}}	1
Consider the [[Definition:Set|set]] $H$ of all points of $\ell^2$ which have [[Definition:Finite Set|finitely many]] [[Definition:Rational Number|rational]] [[Definition:Coordinate of Ordered Tuple|coordinates]] and all the rest [[Definition:Zero (Number)|zero]]. Then $H$ forms a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $A$ which is [[Definition:Everywhere Dense|(everywhere) dense]]. {{finish|Demonstrate that it is [[Definition:Everywhere Dense|(everywhere) dense]].}} The result follows by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,}} $ be a [[Definition:Normed Division Ring|normed division ring]]. Every [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence]] in $R$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Let $\mathbf v, \mathbf w$ be two non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Euclidean Space)|vectors in $\R^n$]]. === Case 1 === Suppose that $\mathbf v$ and $\mathbf w$ are not [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of each other: :$\neg \exists \lambda \in \R: \mathbf v = \lambda \mathbf w$ Then the '''angle between $\mathbf v$ and $\mathbf w$''' is defined as follows: Describe a [[Definition:Triangle (Geometry)|triangle]] with lengths corresponding to: :$\norm {\mathbf v}, \norm {\mathbf w}, \norm {\mathbf v - \mathbf w}$ where $\norm {\, \cdot \,}$ denotes [[Definition:Vector Length|vector length]]: :[[File:AngleBetweenTwoVectors.png|600px]] The [[Definition:Angle|angle]] formed between the two [[Definition:Side of Polygon|sides]] with [[Definition:Linear Measure|lengths]] $\norm {\mathbf v}$ and $\norm {\mathbf w}$ is called the '''angle between vectors $\mathbf v$ and $\mathbf w$'''. By convention, the angle is taken between $0$ and $\pi$. === Case 2 === Suppose that $\mathbf v$ and $\mathbf w$ ''are'' [[Definition:Scalar Multiplication on Vector Space|scalar multiples]] of each other: :$\exists \lambda \in \R: \mathbf v = \lambda \mathbf w$ As $\mathbf v$ and $\mathbf w$ as non-zero, $\lambda \ne 0$. If $\lambda > 0$, then the angle between $\mathbf v$ and $\mathbf w$ is defined as a [[Definition:Zero Angle|zero angle]], that is: :$\theta = 0$ If $\lambda < 0$, then the angle between $\mathbf v$ and $\mathbf w$ is defined as a [[Definition:Straight Angle|straight angle]], that is: :$\theta = \pi$	1
Let $\mathbf u \cdot \mathbf u = 0$. Then: {{begin-eqn}} {{eqn | l = 0 | r = \norm {\mathbf u }^2 \cos \angle \mathbf u, \mathbf u | c = {{Defof|Dot Product|index = 2}} }} {{eqn | r = \norm {\mathbf u}^2 \cos 0 | c = }} {{eqn | r = \norm {\mathbf u}^2 | c = }} {{end-eqn}} The only way for this to happen is if: :$\norm {\mathbf u} = 0$ which implies: :$\mathbf u = \mathbf 0$ Now suppose $\mathbf u = \mathbf 0$. Then: {{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \mathbf 0 \cdot \mathbf 0 | c = }} {{eqn | r = \norm {\mathbf 0}^2 \cos \angle \mathbf 0, \mathbf 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed}}	1
We have that [[P-Norm is Norm/Complex Numbers|$p$-norm is a norm on complex numbers]]. Since [[Definition:Real Numbers|real numbers]] are [[Definition:Complex Number/Wholly Real|wholy real complex numbers]], the same results holds. {{qed}} [[Category:Examples of Norms]] [[Category:P-Norms]] d0t8j9qm69yeccq7h5c3xktgx0b9b4j	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\struct {G, +_G, \circ}_K$ be an [[Definition:Vector Space|vector space]] over $K$. {{TFAE| def = Basis of Vector Space}}	1
Let $\struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ be an [[Definition:Absolutely Convergent Series|absolutely convergent series]] in $X$. Suppose $X$ is a [[Definition:Banach Space|Banach space]]. Then $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ is [[Definition:Convergent Series|convergent]].	1
We have: :$\Psi(\left\langle{r_i}\right\rangle_{i \mathop \in I}) = 0$ {{iff}}: :$\displaystyle \sum_{i \mathop \in I} r_i m_i = 0$ {{explain|Justify the above statement.}} Thus [[Definition:Injection|injectivity]] and [[Definition:Linearly Independent|linearly independent]] are equivalent. {{explain|Justify the above statement.}} {{qed}} [[Category:Module Theory]] 2u5c0gexeulic4vtazjnvahb6v1ek06	1
Let $z = x + i y$. By definition of the [[Definition:Complex Plane|complex plane]], it can be represented by the [[Definition:Point|point]] $\tuple {x, y}$. By the [[Distance Formula]], the [[Definition:Distance (Linear Measure)|distance]] $d$ of $z$ from the [[Definition:Origin|origin]] is: {{begin-eqn}} {{eqn | l = d | r = \sqrt {\paren {x - 0}^2 + \paren {y - 0}^2} | c = }} {{eqn | r = \sqrt {x^2 + y^2} | c = }} {{end-eqn}} which is precisely the [[Definition:Complex Modulus|modulus]] of $z$. {{qed}}	1
Let $\struct {K, +, \times}$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $L / K$ be a [[Definition:Field Extension|field extension]] over $K$. Let $\struct {L, +, \times}_K$ be the a [[Definition:Vector Space on Field Extension|vector space of $L$]] over $K$. Then $\struct {L, +, \times}_K$ is a [[Definition:Vector Space|vector space]].	1
Let us write $\phi \left({G}\right)$ to denote the [[Definition:Homomorphic Image|homomorphic image]] of $\phi$. From [[Image of Group Homomorphism is Subgroup]], $\phi \left({G}\right)$ is a [[Definition:Subgroup|subgroup]] of $\left({H, +_H}\right)$. For any $\phi \left({g}\right)$ and $\phi \left({g'}\right)$ in $\phi \left({G}\right)$, we have: {{begin-eqn}} {{eqn|l = \phi \left({g}\right) +_H \phi \left({g'}\right) |r = \phi \left({g +_G g'}\right) |c = $\phi$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] }} {{eqn|r = \phi \left({g' +_G g'}\right) |c = $\left({G, +_G}\right)$ is an [[Definition:Abelian Group|abelian group]] }} {{eqn|r = \phi \left({g'}\right) +_H \phi \left({g}\right) |c = $\phi$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] }} {{end-eqn}} hence $\phi \left({G}\right)$ is an [[Definition:Abelian Group|abelian group]]. Now we can turn to showing that $\phi \left({G}\right)$ is an [[Definition:Module|$R$-module]]. To do this, we take the [[Definition:Module|$R$-module]] axioms in turn. === Proof of $(1)$ === It is to be shown that for all $\lambda \in R$ and $\phi \left({g}\right), \phi \left({g'}\right) \in \phi \left({G}\right)$: :$\lambda \circ_H \left({\phi \left({g}\right) +_H \phi \left({g'}\right)}\right) = \left({\lambda \circ_H \phi \left({g}\right)}\right) +_H \left({\lambda \circ_H \phi \left({g'}\right)}\right)$ Compute, using that $\phi$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] repetitively: {{begin-eqn}} {{eqn|l = \lambda \circ_H \left({\phi \left({g}\right) +_H \phi \left({g'}\right)}\right) |r = \lambda \circ_H \phi \left({g +_G g'}\right) }} {{eqn|r = \phi \left({\lambda \circ_G \left({g +_G}\right)}\right) }} {{eqn|r = \phi \left({\left({\lambda \circ_G g}\right) +_G \left({\lambda \circ_G g'}\right)}\right) |c = $G$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = \phi \left({\lambda \circ_G g}\right) +_H \phi \left({\lambda \circ_G g'}\right) }} {{eqn|r = \left({\lambda \circ_H \phi \left({g}\right)}\right) +_H \left({\lambda \circ_H \phi \left({g'}\right)}\right) }} {{end-eqn}} {{qed|lemma}} === Proof of $(2)$ === It is to be shown that for all $\lambda, \mu \in R$ and $\phi \left({g}\right) \in \phi \left({G}\right)$: :$\left({\lambda +_R \mu}\right) \circ_H \phi \left({g}\right) = \left({\lambda \circ_H \phi \left({g}\right)}\right) +_H \left({\mu \circ_H \phi \left({g}\right)}\right)$ Compute, using that $\phi$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] repetitively: {{begin-eqn}} {{eqn|l = \left({\lambda +_R \mu}\right) \circ_H \phi \left({g}\right) |r = \phi \left({\left({\lambda +_R \mu}\right) \circ_G g}\right) }} {{eqn|r = \phi \left({\left({\lambda \circ_G g}\right) +_G \left({\mu \circ_G g}\right)}\right) |c = $G$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = \phi \left({\lambda \circ_G g}\right) +_H \phi \left({\mu \circ_G g}\right) }} {{eqn|r = \left({\lambda \circ_H \phi \left({g}\right)}\right) +_H \left({\mu \circ_H \phi \left({g}\right)}\right) }} {{end-eqn}} {{qed|lemma}} === Proof of $(3)$ === It is to be shown that for all $\lambda, \mu \in R$ and $\phi \left({g}\right) \in \phi \left({G}\right)$: :$\left({\lambda \times_R \mu}\right) \circ_H \phi \left({g}\right) = \lambda \circ_H \left({\mu \circ_H \phi \left({g}\right)}\right)$ Compute, using that $\phi$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]] repetitively: {{begin-eqn}} {{eqn|l = \left({\lambda \times_R \mu}\right) \circ_H \phi \left({g}\right) |r = \phi \left({\left({\lambda \times_R \mu}\right) \circ_G g}\right) }} {{eqn|r = \phi \left({\lambda \circ_G \left({\mu \circ_G g}\right)}\right) |c = $G$ is an [[Definition:Module|$R$-module]] }} {{eqn|r = \lambda \circ_H \phi \left({\mu \circ_G g}\right) }} {{eqn|r = \lambda \circ_H \left({\mu \circ_H \phi \left({g}\right)}\right) }} {{end-eqn}} {{qed|lemma}} Having verified that $\phi \left({G}\right)$ satisfies the three axioms, we conclude it is an [[Definition:Module|$R$-module]]. {{qed}}	1
Follows directly from [[Vector Subspace Dimension One Less]]. {{qed}}	1
Let us verify the [[Definition:Norm on Vector Space|norm]] axioms in turn. === Axiom $(\text N 1)$ === {{begin-eqn}} {{eqn | o = | r = x = 0_V | c = }} {{eqn | ll= \leadstoandfrom | o = | r = \innerprod x x = 0 | c = Property $(5)$ of [[Definition:Inner Product|Inner Product]] }} {{eqn | ll= \leadstoandfrom | o = | r = \innerprod x x^{1 / 2} = 0 }} {{eqn | ll= \leadstoandfrom | o = | r = \norm x = 0 | c = {{Defof|Inner Product Norm}} }} {{end-eqn}} {{Qed|lemma}} === Axiom $(\text N 2)$ === Part $(2)$ of [[Properties of Semi-Inner Product]]. {{Qed|lemma}} === Axiom $(\text N 3)$ === Part $(3)$ of [[Properties of Semi-Inner Product]]. {{Qed|lemma}} Hence all the properties of a [[Definition:Norm on Vector Space|norm]] have been shown to hold. {{Qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Ring Zero|zero]]: $0$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\, \cdot \,}$ to the following [[Definition:Limit of Sequence (Normed Division Ring)|limit]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = l$ Let $\sequence {x_{n_r} }$ be a [[Definition:Subsequence|subsequence]] of $\sequence {x_n}$. Then: :$\sequence {x_{n_r} }$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] and $\displaystyle \lim_{r \mathop \to \infty} x_{n_r} = l$	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $S$ be a [[Definition:Division Subring|division subring]] of $R$. Then: :$\struct {S, \norm {\, \cdot \,}_S}$ is a [[Definition:Normed Division Subring|normed division subring]] of $\struct {R, \norm {\, \cdot \,} }$ where $\norm {\, \cdot \,}_S$ is the [[Definition:Norm on Division Ring|norm]] $\norm{\,\cdot\,}$ [[Definition:Restriction/Mapping|restricted]] to $S$.	1
{{begin-eqn}} {{eqn | l = \map f {x_1} | r = x_1^k - \paren {x_1^k + p} }} {{eqn | r = \paren {x_1^k - x_1^k} - p }} {{eqn | r = -p }} {{eqn | o = \equiv | r = 0 \pmod p }} {{end-eqn}} {{qed}} [[Category:P-adic Norm not Complete on Rational Numbers]] 4y313yewjfwjz6avent2oo6zfqueega	1
Let $e_1$ be the [[Definition:Elementary Row Operation|elementary row operation]] $\text {ERO} 1$: {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda \ne 0$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Row of Matrix|row]] $k$ by $\lambda$ | m = r_k \to \lambda r_k }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_1$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e_1$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_1$ is: :$\map \det {\mathbf E_1} = \lambda$	1
Let $\mathbf E$ be an [[Definition:Elementary Column Matrix|elementary column matrix]]. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E$ is as follows:	1
Let $z_1, z_2, \dotsc, z_n \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\cmod z$ be the [[Definition:Modulus of Complex Number|modulus]] of $z$. Then: :$\cmod {z_1 + z_2 + \dotsb + z_n} \le \cmod {z_1} + \cmod {z_2} + \dotsb + \cmod {z_n}$	1
{{AimForCont}} $n$ has two [[Definition:Prime Decomposition|prime factorizations]]: :$n = p_1 p_2 \dots p_r = q_1 q_2 \dots q_s$ where $r \le s$ and each $p_i$ and $q_j$ is prime with $p_1 \le p_2 \le \dots \le p_r$ and $q_1 \le q_2 \le \dots \le q_s$. Since $p_1 \divides q_1 q_2 \dots q_s$, it follows from [[Euclid's Lemma for Prime Divisors]] that $p_1 = q_j$ for some $1 \le j \le s$. Thus: :$p_1 \ge q_1$ Similarly, since $q_1 \divides p_1 p_2 \dots p_r$, from [[Euclid's Lemma for Prime Divisors]]: :$q_1 \ge p_1$ Thus, $p_1 = q_1$, so we may cancel these [[Definition:Common Divisor of Integers|common factors]], which gives: :$p_2 p_3 \cdots p_r = q_2 q_3 \dots q_s$ This process is repeated to show that: :$p_2 = q_2, p_3 = q_3, \ldots, p_r = q_r$ If $r < s$, we arrive at $1 = q_{r + 1} q_{r + 2} \cdots q_s$ after canceling all [[Definition:Common Divisor of Integers|common factors]]. But by [[Divisors of One]], the only [[Definition:Divisor of Integer|divisors]] $1$ are $1$ and $-1$. Hence $q_{r + 1}, q_{r + 2}, \ldots, q_s$ cannot be [[Definition:Prime Number|prime numbers]] From that [[Proof by Contradiction|contradiction]] it follows that $r = s$. Thus: :$p_1 = q_1, p_2 = q_2, \ldots, p_r = q_s$ which means the two [[Definition:Prime Decomposition|factorizations]] are identical. Therefore, the [[Definition:Prime Decomposition|prime factorizations]] of $n$ is [[Definition:Unique|unique]]. {{Qed}}	1
Let: :$\forall g \in G: \rho \left({g}\right) \circ f = f \circ \rho \left({g}\right)$ Let $v$ be a [[Definition:Vector Space|vector]] $v \in V$. Then: : $\rho \left({g}\right) \left({f \left({v}\right)}\right) = f \left({\rho \left({g}\right) \left({v}\right)}\right)$ Using the properties from [[Correspondence between Linear Group Actions and Linear Representations]]: :there exists a [[Definition:G-Module|$G$-module]] $\left({V, \phi}\right)$ associated with $\rho$ such that: ::$\phi \left({g, v}\right) = \rho \left({g}\right) \left({v}\right)$ Applying the last formula: :$\rho \left({g}\right) \left({f \left({v}\right)}\right) = \phi \left({g, f \left({v}\right)}\right)$ and: :$f \left({\phi \left({g, v}\right)}\right) = f \left({\rho \left({g}\right) \left({v}\right)}\right)$ Thus our assumption is equivalent to: :$f \left({\phi \left({g, v}\right)}\right) = \phi \left({g, f \left({v}\right)}\right)$ Hence, by definition of [[Definition:G-Module Homomorphism|$G$-module homomorphism]], $f: V \to V$ is a [[Definition:G-Module Homomorphism|$G$-module homomorphism]]. {{qed}} [[Category:Representation Theory]] seiecfhsn2xwcc0n9y12p95801ucz5a	1
Let us verify that the definition of $\circ$ is [[Definition:Well-Defined Operation|well-defined]]. Let $\left[\!\left[a\right]\!\right]_m=\left[\!\left[b\right]\!\right]_m$. Then we need to show that: :$\forall x \in R : \left[\!\left[a\right]\!\right]_m \circ x = \left[\!\left[b\right]\!\right]_m \circ x$ By the definition of [[Definition:Congruence Modulo Integer|congruence]]: :$\left[\!\left[a\right]\!\right]_m = \left[\!\left[b\right]\!\right]_m \iff \exists k \in \Z : a = b + k m$ Then: {{begin-eqn}} {{eqn | l = \left[\!\left[a\right]\!\right]_m \circ x | r = a \cdot x | c = Definition of $\circ$ }} {{eqn | r = \left(b+km\right) \cdot x }} {{eqn | r = b \cdot x + km \cdot x | c = [[Powers of Group Elements/Sum of Indices|Powers of Group Elements: Sum of Indices]] }} {{eqn | r = b \cdot x + k \cdot \left( m \cdot x \right) | c = [[Powers of Group Elements/Product of Indices|Powers of Group Elements: Product of Indices]] }} {{eqn | r = b \cdot x + k \cdot 0_R | c = [[Characteristic times Ring Element is Ring Zero]] }} {{eqn | r = b \cdot x + 0_R | c = [[Power of Identity is Identity]] }} {{eqn | r = b \cdot x }} {{eqn | r = \left[\!\left[b\right]\!\right]_m \circ x | c = Definition of $\circ$ }} {{end-eqn}} Thus, the definition of $\circ$ is [[Definition:Well-Defined Operation|well-defined]]. {{qed|lemma}} Let us verify that $\left({R, +, \circ}\right)_{\Z_m}$ is a [[Definition:Unitary Module|unitary $\Z_m$-module]] by verifying the axioms in turn. === Axiom $(1)$ === We need to show that: :$\left[\!\left[{a}\right]\!\right]_m \circ \left({x + y}\right) = {\left[\!\left[{a}\right]\!\right]_m \circ x} + {\left[\!\left[{a}\right]\!\right]_m \circ y}$ {{begin-eqn}} {{eqn | l = \left[\!\left[a\right]\!\right]_m \circ \left({x + y}\right) | r = a \cdot \left(x +y\right) | c = Definition of $\circ$ }} {{eqn | r = a \cdot x + a \cdot y | c = [[Power of Product in Abelian Group]] }} {{eqn | r = \left[\!\left[a\right]\!\right]_m \circ x + \left[\!\left[a\right]\!\right]_m \circ y | c = Definition of $\circ$ }} {{end-eqn}} {{qed|lemma}} === Axiom $(2)$ === We need to show that: :$\left( {\left[\!\left[a\right]\!\right]_m +_m \left[\!\left[b\right]\!\right]_m}\right) \circ x = \left[\!\left[a\right]\!\right]_m \circ x + \left[\!\left[b\right]\!\right]_m \circ x$ {{begin-eqn}} {{eqn | l = \left( {\left[\!\left[a\right]\!\right]_m +_m \left[\!\left[b\right]\!\right]_m}\right) \circ x | r = \left[\!\left[ a+b \right]\!\right]_m \circ x | c = {{Defof|Modulo Addition}} }} {{eqn | r = \left(a+b\right) \cdot x | c = Definition of $\circ$ }} {{eqn | r = a \cdot x + b \cdot x | c = [[Powers of Group Elements/Sum of Indices|Powers of Group Elements: Sum of Indices]] }} {{eqn | r = \left[\!\left[a\right]\!\right]_m \circ x + \left[\!\left[b\right]\!\right]_m \circ x | c = Definition of $\circ$ }} {{end-eqn}} {{qed|lemma}} === Axiom $(3)$ === We need to show that: :$\left({\left[\!\left[a\right]\!\right]_m \times_m \left[\!\left[b\right]\!\right]_m}\right) \circ x = \left[\!\left[a\right]\!\right]_m \circ \left({\left[\!\left[b\right]\!\right]_m \circ x}\right)$ {{begin-eqn}} {{eqn | l = \left({\left[\!\left[a\right]\!\right]_m \times_m \left[\!\left[b\right]\!\right]_m}\right) \circ x | r = \left[\!\left[ a \times b \right]\!\right]_m \circ x | c = {{Defof|Modulo Multiplication}} }} {{eqn | r = \left(a \times b\right) \cdot x | c = Definition of $\circ$ }} {{eqn | r = a \cdot \left({b \cdot x}\right) | c = [[Powers of Group Elements/Product of Indices|Powers of Group Elements: Product of Indices]] }} {{eqn | r = \left[\!\left[a\right]\!\right]_m \circ \left({\left[\!\left[b\right]\!\right]_m \circ x}\right) | c = Definition of $\circ$ }} {{end-eqn}} {{qed|lemma}} === Axiom $(4)$ === We need to show that: :$\left[\!\left[1\right]\!\right]_m \circ x = x$ since [[Modulo Multiplication has Identity|$\left[\!\left[1\right]\!\right]_m$ is the unity of $\Z_m$]]. That is, that $1 \cdot x = x$. This follows from the definition of [[Definition:Power of Group Element|power of group element]]. {{qed|lemma}} Having verified all four axioms, we have shown that $\left({R, +, \circ}\right)_{\Z_m}$ is a [[Definition:Unitary Module|unitary $\Z_m$-module]]. {{qed}} [[Category:Unitary Modules]] [[Category:Group Theory]] ipltlbrpyn2938lcspwwaozmqnj4eza	1
Let $x, y \in R$. {{begin-eqn}} {{eqn | l = \norm {x + y} | r = \norm {x - \paren {-y} } }} {{eqn | r = \map d {x, - y} | c = {{Defof|Metric Induced by Norm on Division Ring|Metric Induced by $\norm {\, \cdot \,}$}} }} {{eqn | r = \max \set {\map d {x, 0}, \map d {0, -y} } | o = \le | c = {{Defof|Non-Archimedean Metric}} }} {{eqn | r = \max \set {\norm {x - 0 }, \norm {0 - \paren {-y} } } | c = {{Defof|Metric Induced by Norm on Division Ring|Metric Induced by $\norm {\, \cdot \,}$}} }} {{eqn | r = \max \set {\norm x, \norm y} }} {{end-eqn}}	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. By definition, $\Gamma$ is a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] on $\mathbf A$. Let $\sequence e_k$ denote a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] $\tuple {e_1, e_2, \ldots, e_k}$ applied on $\mathbf A$ in order: first $e_1$, then $e_2$, then $\ldots$, then $e_k$. Let $\Gamma_k$ be the [[Definition:Column Operation|column operation]] which consists of $\sequence e_k$. Let $\mathbf E_k$ denote the [[Definition:Elementary Column Matrix|elementary column matrix]] of [[Definition:Order of Square Matrix|order]] $n$ formed by applying $e_k$ to the [[Definition:Unit Matrix|unit matrix]] $I_n$. For all $r \in \Z_{>0}$, let $\map P r$ be the [[Definition:Proposition|proposition]]: :For all $\Gamma_r$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf K_r$ of [[Definition:Order of Square Matrix|order $n$]] such that: ::$\mathbf A \mathbf K_r = \mathbf B_r$ :where: ::$\Gamma_r$ is a [[Definition:Column Operation|column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B_r \in \map \MM {m, n}$. ::$\mathbf K_r$ is the [[Definition:Matrix Product (Conventional)|product]] of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Matrix|elementary column matrices]]: :::$\mathbf K_r = \mathbf E_1 \mathbf E_2 \dotsb \mathbf E_{r - 1} \mathbf E_r$ === Basis for the Induction === $\map P 1$ is the case where $\Gamma_1$ is a single-[[Definition:Term of Sequence|term]] [[Definition:Finite Sequence|sequence]] consisting of one [[Definition:Elementary Column Operation|elementary column operation]] $e_1$. Let $e_1$ be an [[Definition:Elementary Column Operation|elementary column operation]] operating on $\mathbf A$, which transforms $\mathbf A$ into $\mathbf B_1$. By definition, there exists [[Definition:Unique|exactly one]] [[Definition:Elementary Column Matrix|elementary column matrix]] $\mathbf E_1$ of [[Definition:Order of Square Matrix|order $m$]] such that $\mathbf E_1$ is the result of applying $e_1$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$ of [[Definition:Order of Square Matrix|order $n$]]. From the [[Elementary Column Operations as Matrix Multiplications/Corollary|corollary to Elementary Column Operations as Matrix Multiplications]]: :$\mathbf A \mathbf E_1 = \mathbf B_1$ By [[Elementary Column Matrix is Invertible]], $E_1$ is [[Definition:Invertible Matrix|invertible]]. Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :For all $\Gamma_k$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf K_k$ of [[Definition:Order of Square Matrix|order $n$]] such that: ::$\mathbf A \mathbf K_k = \mathbf B_k$ from which it is to be shown that: :For all $\Gamma_{k + 1}$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf K_{k + 1}$ of [[Definition:Order of Square Matrix|order $n$]] such that: ::$\mathbf A \mathbf K_{k + 1} = \mathbf B_{k + 1}$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: By definition, $\Gamma_{k + 1}$ is a [[Definition:Column Operation|column operation]] consisting of a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] $\tuple {e_1, e_2, \ldots, e_k, e_{k + 1} }$ applied on $\mathbf A$ in order. Thus $\Gamma_{k + 1}$ consists of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] $\tuple {e_1, e_2, \ldots, e_k}$ applied on $\mathbf A$ in order, followed by a further [[Definition:Elementary Column Operation|elementary column operation]] $e_{k + 1}$. By the [[Column Operation is Equivalent to Post-Multiplication by Product of Elementary Matrices#Induction Hypothesis|induction hypothesis]], there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf K_k$ of [[Definition:Order of Square Matrix|order $m$]] such that: :$\mathbf A \mathbf K_k = \mathbf B_k$ where $\mathbf B_k \in \map \MM {m, n}$ is the result of applying $\sequence e_k$ to $\mathbf A$ in order. Let $e_{k + 1}$ be applied to $\mathbf B_k$. By definition, there exists [[Definition:Unique|exactly one]] [[Definition:Elementary Column Matrix|elementary column matrix]] $\mathbf E_{k + 1}$ of [[Definition:Order of Square Matrix|order $m$]] such that $\mathbf E_{k + 1}$ is the result of applying $e_{k + 1}$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$ of [[Definition:Order of Square Matrix|order $m$]]. Then: {{begin-eqn}} {{eqn | l = \mathbf B_{k + 1} | r = \mathbf B_k \mathbf E_{k + 1} | c = [[Elementary Column Operations as Matrix Multiplications/Corollary|Corollary to Elementary Column Operations as Matrix Multiplications]] }} {{eqn | r = \paren {\mathbf A \mathbf K_k} \mathbf E_{k + 1} | c = }} {{eqn | r = \mathbf A \paren {\mathbf K_k \mathbf E_{k + 1} } | c = [[Matrix Multiplication is Associative]] }} {{end-eqn}} By [[Product of Matrices is Invertible iff Matrices are Invertible]], $\mathbf K_k \mathbf E_{k + 1}$ is [[Definition:Invertible Matrix|invertible]]. We have that $\mathbf K_k$ is the [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] resulting from the application of $\sequence e_k$ on $\mathbf I_m$. Thus $\mathbf K_k \mathbf E_{k + 1}$ is the [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] resulting from the application of $\sequence e_{k + 1}$ on $\mathbf I_m$. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore, for every [[Definition:Column Operation|column operation]] $\Gamma$ which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R$ of [[Definition:Order of Square Matrix|order $m$]] such that: :$\mathbf A \mathbf K = \mathbf B$ where: :$\mathbf K$ is the [[Definition:Matrix Product (Conventional)|product]] of a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Matrix|elementary column matrices]]. {{qed}}	1
{{AimForCont}} $n$ has two [[Definition:Prime Decomposition|prime factorizations]]: :$n = p_1 p_2 \dots p_r = q_1 q_2 \dots q_s$ where $r \le s$ and each $p_i$ and $q_j$ is prime with $p_1 \le p_2 \le \dots \le p_r$ and $q_1 \le q_2 \le \dots \le q_s$. Since $p_1 \divides q_1 q_2 \dots q_s$, it follows from [[Euclid's Lemma for Prime Divisors]] that $p_1 = q_j$ for some $1 \le j \le s$. Thus: :$p_1 \ge q_1$ Similarly, since $q_1 \divides p_1 p_2 \dots p_r$, from [[Euclid's Lemma for Prime Divisors]]: :$q_1 \ge p_1$ Thus, $p_1 = q_1$, so we may cancel these [[Definition:Common Divisor of Integers|common factors]], which gives: :$p_2 p_3 \cdots p_r = q_2 q_3 \dots q_s$ This process is repeated to show that: :$p_2 = q_2, p_3 = q_3, \ldots, p_r = q_r$ If $r < s$, we arrive at $1 = q_{r + 1} q_{r + 2} \cdots q_s$ after canceling all [[Definition:Common Divisor of Integers|common factors]]. But by [[Divisors of One]], the only [[Definition:Divisor of Integer|divisors]] $1$ are $1$ and $-1$. Hence $q_{r + 1}, q_{r + 2}, \ldots, q_s$ cannot be [[Definition:Prime Number|prime numbers]] From that [[Proof by Contradiction|contradiction]] it follows that $r = s$. Thus: :$p_1 = q_1, p_2 = q_2, \ldots, p_r = q_s$ which means the two [[Definition:Prime Decomposition|factorizations]] are identical. Therefore, the [[Definition:Prime Decomposition|prime factorizations]] of $n$ is [[Definition:Unique|unique]]. {{Qed}}	1
By [[Determinant of Transpose]]: :$\det \mathbf Q^\intercal = \det \mathbf Q$ Then: {{begin-eqn}} {{eqn | l = \mathbf Q \mathbf Q^\intercal | r = \mathbf I | c = [[Product of Orthogonal Matrix with Transpose is Identity]] }} {{eqn | ll= \leadsto | l = \map \det {\mathbf Q \mathbf Q^\intercal} | r = \det \mathbf I | c = }} {{eqn | ll= \leadsto | l = \map \det {\mathbf Q \mathbf Q^\intercal} | r = 1 | c = [[Determinant of Unit Matrix]] }} {{eqn | ll= \leadsto | l = \det \mathbf Q \det \mathbf Q^\intercal | r = 1 | c = [[Determinant of Matrix Product]] }} {{end-eqn}} Hence the result. {{qed}}	1
:$\displaystyle \lim_{n \mathop \to \infty} y_n^{-1} = l^{-1}$	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Division Ring|division ring]] $K$. Let $U \subseteq V$ be a non-[[Definition:Empty Set|empty]] [[Definition:Subset|subset]] of $V$ such that: :$(1): \qquad \forall u \in U, \lambda \in K: \lambda u \in U$ :$(2): \qquad \forall u, v \in U: u + v \in U$ Then $U$ is a [[Definition:Vector Subspace|subspace]] of $V$.	1
Let $P: \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma$ be a [[Equation of Plane|plane]] in $\R^3$. Then the plane $P'$ is [[Definition:Parallel Planes|parallel]] to $P$ iff there is a $\gamma' \in \R$ such that: :$P' = \left\{{ \left({x_1, x_2, x_3}\right) \in \R^3 : \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma' }\right\}$	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix of order $n$]]. Let $\map \det {\mathbf A}$ denote the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Take the [[Definition:Elementary Row Operation|elementary row operations]]: {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Row of Matrix|row]] $i$ by $\lambda$ | m = r_i \to \lambda r_i }} {{axiom | n = \text {ERO} 2 | t = For some $\lambda$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Row of Matrix|row]] $j$ to [[Definition:Row of Matrix|row]] $i$ | m = r_i \to r_i + \lambda r_j }} {{axiom | n = \text {ERO} 3 | t = Exchange [[Definition:Row of Matrix|rows]] $i$ and $j$ | m = r_i \leftrightarrow r_j }} {{end-axiom}} Applying $\text {ERO} 1$ has the effect of multiplying $\map \det {\mathbf A}$ by $\lambda$. Applying $\text {ERO} 2$ has no effect on $\map \det {\mathbf A}$. Applying $\text {ERO} 3$ has the effect of multiplying $\map \det {\mathbf A}$ by $-1$.	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $\left \langle{\cdot, \cdot}\right \rangle$ be a [[Definition:Semi-Inner Product|semi-inner product]] on $V$. Denote, for $x \in V$, $\left\Vert{x}\right\Vert := \left\langle{x, x}\right\rangle^{1 / 2}$. Then, $\forall x, y \in V, a \in \Bbb F$: :$(1): \quad \left\Vert{x + y}\right\Vert \le \left\Vert{x}\right\Vert + \left\Vert{y}\right\Vert$ :$(2): \quad \left\Vert{a x}\right\Vert = \left|{a}\right| \left\Vert{x}\right\Vert$	1
The [[Definition:Taxicab Norm|taxicab norm]] is a [[Definition:Norm on Vector Space|norm]] on the [[Definition:Real Number|real]] and [[Definition:Complex Number|complex numbers]].	1
=== Construction of Real Vector Space === From the definition, a [[Definition:Vector Space|vector space]] is a [[Definition:Unitary Module|unitary module]] whose [[Definition:Scalar Ring of Unitary Module|scalar ring]] is a [[Definition:Field (Abstract Algebra)|field]]. In order to call attention to the precise scope of the operators, let [[Definition:Real Addition|real addition]] and [[Definition:Real Multiplication|real multiplication]] be expressed as $+_\R$ and $\times_\R$ respectively. Then we can express the [[Definition:Field of Real Numbers|field of real numbers]] as $\struct {\R, +_\R, \times_\R}$. From [[Real Numbers under Addition form Abelian Group]], $\struct {\R, +}$ is a [[Definition:Group|group]]. Again, in order to call attention to the precise scope of the operator, let [[Definition:Real Addition|real addition]] be expressed on $\struct {\R, +}$ as $+_G$. That is, the [[Definition:Group|group]] under consideration is $\struct {\R, +_G}$. Consider the [[Definition:Cartesian Product|cartesian product]]: :$\displaystyle \R^n = \prod_{i \mathop = 1}^n \struct {\R, +_G} = \underbrace {\struct {\R, +_G} \times \cdots \times \struct {\R, +_G} }_{n \text{ copies} }$ Let: :$\mathbf a = \tuple {a_1, a_2, \ldots, a_n}$ :$\mathbf b = \tuple {b_1, b_2, \ldots, b_n}$ be arbitrary elements of $\R^n$. Let $\lambda$ be an arbitrary element of $\R$. Let $+$ be the [[Definition:Binary Operation|binary operation]] defined on $\R^n$ as: :$\mathbf a + \mathbf b = \tuple {a_1 +_G b_1, a_2 +_G b_2, \ldots, a_n +_G b_n}$ Also let $\cdot$ be the [[Definition:Binary Operation|binary operation]] defined on $\R \times \R^n$ as: :$\lambda \cdot \mathbf a = \tuple {\lambda \times_\R a_1, \lambda \times_\R a_2, \ldots, \lambda \times_\R a_n}$ In this context, $\lambda \times_\R a_j$ is defined as [[Definition:Real Multiplication|real multiplication]], as is appropriate (both $\lambda$ and $a_j$ are [[Definition:Real Number|real numbers]]). With this set of definitions, the structure $\struct {\R^n, +, \cdot}$ is a [[Definition:Vector Space|vector space]], as is shown in [[Real Vector Space is Vector Space#Proof of Real Vector Space|Proof of Real Vector Space]] below. === Proof of Real Vector Space === In order to show that $\struct {\R^n, +, \cdot}$ is a [[Definition:Vector Space|vector space]], we need to show that: $\forall \mathbf x, \mathbf y \in \R^n, \forall \lambda, \mu \in \R$: : $(1): \quad \lambda \cdot \paren {\mathbf x + \mathbf y} = \paren {\lambda \cdot \mathbf x} + \paren {\lambda \cdot \mathbf y}$ : $(2): \quad \paren {\lambda +_\R \mu} \cdot x = \paren {\lambda \cdot \mathbf x} + \paren {\mu \cdot \mathbf x}$ : $(3): \quad \paren {\lambda \times_\R \mu} \cdot x = \lambda \cdot \paren {\mu \cdot \mathbf x}$ : $(4): \quad \forall \mathbf x \in \R^n: 1 \cdot \mathbf x = \mathbf x$. where $1$ in this context means the [[Definition:One|real number one]]. From [[External Direct Product of Groups is Group]], we have that $\struct {\R^n, +}$ is a [[Definition:Group|group]] in its own right. Let: :$\mathbf x = \tuple {x_1, x_2, \ldots, x_n}$ :$\mathbf y = \tuple {y_1, y_2, \ldots, y_n}$ Checking the criteria in turn: $(1): \quad \lambda \cdot \paren {\mathbf x + \mathbf y} = \paren {\lambda \cdot \mathbf x} + \paren {\lambda \cdot \mathbf y}$: {{begin-eqn}} {{eqn | l = \lambda \cdot \paren {\mathbf x + \mathbf y} | r = \tuple {\lambda \times_\R \paren {x_1 +_G y_1}, \lambda \times_\R \paren {x_2 +_G y_2}, \ldots, \lambda \times_\R \paren {x_n +_G y_n} } | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \tuple {\paren {\lambda \times_\R x_1 +_G \lambda \times_\R y_1}, \paren {\lambda \times_\R x_2 +_G \lambda \times_\R y_2}, \ldots, \paren {\lambda \times_\R x_n +_G \lambda \times_\R y_n} } | c = [[Real Multiplication Distributes over Addition|$\times_\R$ distributes over $+_G$]] }} {{eqn | r = \tuple {\lambda \times_\R x_1, \lambda \times_\R x_2, \ldots, \lambda \times_\R x_n} + \tuple {\lambda \times_\R y_1, \lambda \times_\R y_2, \ldots, \lambda \times_\R y_n} | c = Definition of $+$ over $\R^n$ }} {{eqn | r = \lambda \cdot \tuple {x_1, x_2, \ldots, x_n} + \lambda \cdot \tuple {y_1, y_2, \ldots, y_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \lambda \cdot \mathbf x + \lambda \cdot \mathbf y | c = Definition of $\mathbf x$ and $\mathbf y$ }} {{end-eqn}} So $(1)$ has been shown to hold. $(2): \quad \paren {\lambda +_\R \mu} \cdot x = \paren {\lambda \cdot \mathbf x} + \paren {\mu \cdot \mathbf x}$: {{begin-eqn}} {{eqn | l = \paren {\lambda +_\R \mu} \cdot x | r = \tuple {\paren {\lambda +_\R \mu} \times_\R x_1, \paren {\lambda +_\R \mu} \times_\R x_2, \ldots, \paren {\lambda +_\R \mu} \times_\R x_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \tuple {\paren {\lambda \times_\R x_1 +_G \mu \times_\R x_1}, \paren {\lambda \times_\R x_2 +_G \mu \times_\R x_2}, \ldots, \paren {\lambda \times_\R x_n +_G \mu \times_\R x_n} } | c = [[Real Multiplication Distributes over Addition|$\times_\R$ distributes over $+_\R$]], and $+_\R$ is the same operation as $+_G$ }} {{eqn | r = \tuple {\lambda \times_\R x_1, \lambda \times_\R x_2, \ldots, \lambda \times_\R x_n} + \tuple {\mu \times_\R x_1, \mu \times_\R x_2, \ldots, \mu \times_\R x_n} | c = Definition of $+$ over $\R^n$ }} {{eqn | r = \lambda \cdot \tuple {x_1, x_2, \ldots, x_n} + \mu \cdot \tuple {x_1, x_2, \ldots, x_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \lambda \cdot \mathbf x + \mu \cdot \mathbf x | c = Definition of $\mathbf x$ and $\mathbf y$ }} {{end-eqn}} So $(2)$ has been shown to hold. $(3): \quad \paren {\lambda \times_\R \mu} \cdot x = \lambda \cdot \paren {\mu \cdot \mathbf x}$: {{begin-eqn}} {{eqn | l = \paren {\lambda \times_\R \mu} \cdot x | r = \tuple {\paren {\lambda \times_\R \mu} \times_\R x_1, \paren {\lambda \times_\R \mu} \times_\R x_2, \ldots, \paren {\lambda \times_\R \mu} \times_\R x_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \tuple {\lambda \times_\R \paren {\mu \times_\R x_1}, \lambda \times_\R \paren {\mu \times_\R x_2}, \ldots, \lambda \times_\R \paren {\mu \times_\R x_n} } | c = [[Real Multiplication is Associative]] }} {{eqn | r = \lambda \cdot \tuple {\mu \times_\R x_1, \mu \times_\R x_2, \ldots, \mu \times_\R x_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \lambda \cdot \paren {\mu \cdot \tuple {x_1, x_2, \ldots, x_n} } | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \lambda \cdot \paren {\mu \cdot \mathbf x} | c = Definition of $\mathbf x$ }} {{end-eqn}} So $(3)$ has been shown to hold. $(4): \quad \forall \mathbf x \in \R^n: 1 \cdot \mathbf x = \mathbf x$: {{begin-eqn}} {{eqn | l = 1 \cdot \mathbf x = \mathbf x | r = \tuple {1 \times_\R x_1, 1 \times_\R x_2, \ldots, 1 \times_\R x_n} | c = Definition of $\cdot$ over $\R \times \R^n$ }} {{eqn | r = \tuple {x_1, x_2, \ldots, x_n} | c = [[Real Multiplication Identity is One]] }} {{eqn | r = \mathbf x | c = Definition of $\mathbf x$ }} {{end-eqn}} So $(4)$ has been shown to hold. So the [[Definition:Module on Cartesian Product|$\R$-module $\R^n$]] is a [[Definition:Vector Space|vector space]], as we were to prove. {{qed}}	1
Let $e_2$ be the [[Definition:Elementary Column Operation|elementary column operation]] $\text {ECO} 2$: {{begin-axiom}} {{axiom | n = \text {ECO} 2 | t = For some $\lambda$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Column of Matrix|column]] $j$ to [[Definition:Column of Matrix|column]] $i$ | m = \kappa_i \to \kappa_i + \lambda \kappa_j }} {{end-axiom}} which is to operate on some arbitrary [[Definition:Matrix Space|matrix space]]. Let $\mathbf E_2$ be the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to $e_2$. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E_2$ is: :$\map \det {\mathbf E_2} = 1$	1
Let $A = \left({A_F, \oplus}\right)$ be a [[Definition:Star-Algebra|$*$-algebra]]. Let $A' = \left({A_F, \oplus'}\right)$ be constructed from $A$ using the [[Definition:Cayley-Dickson Construction|Cayley-Dickson construction]]. Then: : $A'$ is an [[Definition:Associative Algebra|associative algebra]] {{iff}}: : $A$ is both a [[Definition:Commutative Algebra|commutative algebra]] and an [[Definition:Associative Algebra|associative algebra]].	1
Let $G$ and $H$ be [[Definition:Module|$R$-modules]]. Let $\phi: G \to H$ be a [[Definition:Linear Transformation|linear transformation]]. Then: :$(1): \quad$ If $M$ is a [[Definition:Submodule|submodule]] of $G$, $\phi \sqbrk M$ is a [[Definition:Submodule|submodule]] of $H$ :$(2): \quad$ If $N$ is a [[Definition:Submodule|submodule]] of $H$, $\phi^{-1} \sqbrk N$ is a [[Definition:Submodule|submodule]] of $G$ :$(3): \quad$ The [[Definition:Codomain of Mapping|codomain]] of $\phi$ is a [[Definition:Submodule|submodule]] of $H$ :$(4): \quad$ The [[Definition:Kernel of Linear Transformation|kernel]] of $\phi$ is a [[Definition:Submodule|submodule]] of $G$.	1
From [[Determinant of Matrix Product]]: : $\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \, \map \det {\mathbf B}$ which is seen to be a [[Definition:Group Homomorphism|group homomorphism]] by definition. {{qed}}	1
Let $\norm {\,\cdot\,}$ be [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. Then by the definition of a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]], for $n \in \N$: {{begin-eqn}} {{eqn | lo= \forall n \in \N_{>0}: | l = \norm {n \cdot 1_R} | r = \norm {1_R + \dots + 1_R} | c = ($n$ summands) }} {{eqn | o = \le | r = \max \set {\norm {1_R}, \ldots, \norm {1_R} } | c = }} {{eqn | r = 1 | c = because $\norm {1_R} = 1$ }} {{end-eqn}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Dimension of Module|$n$-dimensional]] [[Definition:Module|module]] over $R$. Let $\sequence {a_n}$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Then there is an [[Definition:Ordered Basis|ordered basis]] $\sequence {a'_n}$ of $G^*$ satisfying $\forall i, j \in \closedint 1 n: \map {a'_i} {a_j} = \delta_{i j}$. This ordered basis $\sequence {a'_n}$ of $G^*$ is called the '''ordered basis of $G^*$ dual to $\sequence {a_n}$''', or the '''ordered dual basis of $G^*$'''.	1
The [[Definition:P-Norm|$p$-norm]] on the [[Definition:Complex Number|complex numbers]] is a [[Definition:Norm on Vector Space|norm]].	1
As $\struct {G, \cdot}$, being a [[Definition:Group|group]], is a [[Definition:Monoid|monoid]], it follows from [[Matrix Space Semigroup under Hadamard Product]] that $\struct {\map {\MM_G} {m, n}, \circ}$ is also a [[Definition:Monoid|monoid]]. As $\struct {G, \cdot}$ is a [[Definition:Group|group]], it follows from [[Negative Matrix is Inverse for Hadamard Product]] that all [[Definition:Element|elements]] of $\struct {\map {\MM_G} {m, n}, \circ}$ have an [[Definition:Inverse Element|inverse]]. The result follows. {{Qed}}	1
By definition of [[Definition:Hilbert Matrix|Hilbert matrix]], the [[Definition:Element of Matrix|element]] $a_{i j}$ is: :$a_{i j} = \dfrac 1 {i + j - 1}$ For all $i, j \in \Z$ such that $1 \le i \le n$ and $1 \le j \le n$, let: :$x_i = i$ :$y_j = j - 1$ Then: :$a_{i j} = \dfrac 1 {x_i + y_j}$ The result follows by definition of a [[Definition:Cauchy Matrix|Cauchy matrix]]. {{qed}}	1
Let $\mathbf u = \tuple {u_1, u_2, \ldots, u_n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = u_1 u_1 + u_2 u_2 + \cdots + u_n u_n | c = {{Defof|Dot Product}} }} {{eqn | r = u_1^2 + u_2^2 + \cdots + u_n^2 | c = }} {{eqn | r = \paren {\sqrt {\sum_{i \mathop = 1}^n u_i^2} }^2 | c = }} {{eqn | r = \norm {\mathbf u}^2 | c = {{Defof|Vector Length|subdef = Real Vector Space|Vector Length in $\R^n$}} }} {{end-eqn}} {{qed}}	1
Let $z_0 \in \C$ be a [[Definition:Complex Number|complex number]]. Then the [[Definition:Modulus of Complex Number|complex modulus function]] is [[Definition:Continuous Complex Function|continuous]] at $z_0$.	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be an [[Definition:Module|$R$-module]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $\left \langle {x, t'} \right \rangle$ be the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]] from $G$ to $G^{**}$. Then the [[Definition:Mapping|mapping]] $\phi: G \times G^* \to R$ defined as $\forall \left({x, t'}\right) \in G \times G^*: \phi \left({x, t'}\right) = \left \langle {x, t'} \right \rangle$ satisfies the following properties: : $(1): \quad \forall x, y \in G: \forall t' \in G^*: \left \langle {x + y, t'} \right \rangle = \left \langle {x, t'} \right \rangle + \left \langle {y, t'} \right \rangle$ : $(2): \quad \forall x \in G: \forall s', t' \in G^*: \left \langle {x, s' + t'} \right \rangle = \left \langle {x, s'} \right \rangle + \left \langle {x, t'} \right \rangle$ : $(3): \quad \forall x \in G: \forall s', t' \in G^*: \forall \lambda \in R: \left \langle {\lambda x, t'} \right \rangle = \lambda \left \langle {x, t'} \right \rangle = \left \langle {x, \lambda t'} \right \rangle$	1
We have by definition that [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is a specific instance of a [[Definition:Hadamard Product|Hadamard product]]. By definition of a [[Definition:Ring (Abstract Algebra)|ring]], the [[Definition:Algebraic Structure|structure]] $\struct {R, +}$ is a [[Definition:Group|group]]. As $\struct {R, +}$ is [[Definition:A Fortiori|a fortiori]] a [[Definition:Monoid|monoid]], it follows from [[Matrix Space Semigroup under Hadamard Product]] that $\struct {\map {\MM_R} {m, n}, +}$ is also a [[Definition:Monoid|monoid]]. As $\struct {R, +}$ is a [[Definition:Group|group]], it follows from [[Negative Matrix is Inverse for Matrix Entrywise Addition]] that all [[Definition:Element|elements]] of $\struct {\map {\MM_R} {m, n}, +}$ have an [[Definition:Inverse Element|inverse element]]. From [[Matrix Entrywise Addition is Commutative]] it follows that $\struct {\map {\MM_R} {m, n}, +}$ is an [[Definition:Abelian Group|Abelian group]]. The result follows. {{Qed}}	1
Let $x, y \in \displaystyle \bigcap \mathcal C$. Then by definition of [[Definition:Set Intersection|set intersection]], $\forall C \in \mathcal C: x, y \in C$. The [[Definition:Convex Set (Vector Space)|convexity]] of each $C$ yields: :$\forall t \in \left[{0 \,.\,.\, 1}\right]: t x + \left({1 - t}\right) y \in C$ Therefore, these elements are also in $\displaystyle \bigcap \mathcal C$, by definition of [[Definition:Set Intersection|set intersection]]. Hence $\displaystyle \bigcap \mathcal C$ is also [[Definition:Convex Set (Vector Space)|convex]]. {{qed}}	1
Let $\map e {\mathbf A}$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \to r_k + \lambda r_l$ Then $r'_k$ is such that: :$\forall a'_{k i} \in r'_k: a'_{k i} = a_{k i} + \lambda a_{l i}$ Now let $\map {e'} {\mathbf A'}$ be the [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A'$ to $\mathbf A''$: :$e' := r'_k \to r'_k - \lambda r'_l$ Applying $e'$ to $\mathbf A'$ we get: {{begin-eqn}} {{eqn | lo= \forall a''_{k i} \in r''_k: | l = a''_{k i} | r = a'_{k i} - \lambda a'_{l i} | c = }} {{eqn | r = \paren {a_{k i} + \lambda a_{l i} } - \lambda a'_{l i} | c = }} {{eqn | r = \paren {a_{k i} + \lambda a_{l i} } - \lambda a_{l i} | c = as $\lambda a'_{l i} = \lambda a_{l i}$: [[Definition:Row of Matrix|row]] $l$ was not changed by $e$ }} {{eqn | r = a_{k i} | c = }} {{eqn | ll= \leadsto | lo= \forall a''_{k i} \in r''_k: | l = a''_{k i} | r = a_{k i} | c = }} {{eqn | ll= \leadsto | l = r''_{k i} | r = r_{k i} | c = }} {{eqn | ll= \leadsto | l = \mathbf A'' | r = \mathbf A | c = }} {{end-eqn}} It is noted that for $e'$ to be an [[Definition:Elementary Row Operation|elementary row operation]], the only possibility is for it to be as defined.	1
The [[Definition:Complex Plane|complex plane]], along with the [[Definition:Modulus of Complex Number|complex modulus]], forms a [[Definition:Banach Space|Banach space]] over $\C$.	1
From [[General Linear Group to Determinant is Homomorphism]]: :$\det$ is a [[Definition:Group Homomorphism|group homomorphism]]. The [[Definition:Special Linear Group|special linear group]] $\SL {n, \R}$ is the [[Definition:Subset|subset]] of $\GL {n, \R}$ such that: :$\forall \mathbf A \in \SL {n, \R}: \map \det {\mathbf A} = 1$ From [[Real Multiplication Identity is One]]: : $1$ is the [[Definition:Identity Element|identity]] of the [[Definition:Multiplicative Group of Real Numbers|multiplicative group of real numbers]]. It follows by definition that $\SL {n, \R}$ is the [[Definition:Kernel of Group Homomorphism|kernel]] of the $\det$ [[Definition:Mapping|mapping]]. {{qed}}	1
Let $z$ be a point on the $L$. Then: :$z - z_1 = b \paren {z - z_2}$ where $b$ is some [[Definition:Real Number|real number]]. Then: {{begin-eqn}} {{eqn | l = b | r = \frac {z - z_1} {z - z_2} | c = }} {{eqn | ll= \leadsto | l = \map \arg {\frac {z - z_1} {z_2 - z_1} } | r = \arg b | c = }} {{eqn | r = 0 | c = as $b$ is [[Definition:Real Number|real]] }} {{end-eqn}} {{qed}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\mathbf C$ be its [[Definition:Cofactor Matrix|cofactor matrix]]. The '''adjugate matrix''' of $\mathbf A$ is the [[Definition:Transpose of Matrix|transpose]] of $\mathbf C$: :$\adj {\mathbf A} = \mathbf C^\intercal$	1
We will demonstrate this for each of the $3$ types of [[Definition:Elementary Row Operation|elementary row operation]]. In the below: :$e$ denotes a given [[Definition:Elementary Row Operation|elementary row operation]] :$\mathbf E$ denotes the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e$ :$e'$ denotes the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$ :$\mathbf E'$ denotes the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e'$. Let $n$ denote the [[Definition:Order of Matrix|order]] of $\mathbf E$ and $\mathbf E'$. The strategy is to demonstrate that: :$\mathbf E \mathbf E' = \mathbf I$ where $\mathbf I$ denotes the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Matrix|order]] $n$. Let $x_{i, j}$ and $y_{i, j}$ denote the [[Definition:Element of Matrix|elements]] of $\mathbf E$ and $\mathbf E'$ respectively at [[Definition:Index of Matrix Element|indices]] $\tuple {i, j}$. Let $z_{i j}$ denote the [[Definition:Element of Matrix|element]] of $\mathbf E \mathbf E'$ at [[Definition:Index of Matrix Element|indices]] $\tuple {i, j}$. === $\text {ERO} 1$: Scalar Product of Row === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \to \lambda r_k$ where $\lambda \ne 0$. From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E$ is of the form: :$x_{a b} = \begin {cases} \delta_{a b} & : a \ne k \\ \lambda \cdot \delta_{a b} & : a = k \end {cases}$ where: :$\delta_{a b}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: ::$\delta_{a b} = \begin {cases} 1 & : \text {if $a = b$} \\ 0 & : \text {if $a \ne b$} \end {cases}$ From [[Existence of Inverse Elementary Row Operation/Scalar Product of Row|Existence of Inverse Elementary Row Operation: Scalar Product of Row]], $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r_k \to \dfrac 1 \lambda r_k$ From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E'$ is of the form: :$y_{a b} = \begin {cases} \delta_{a b} & : a \ne k \\ \dfrac 1 \lambda \cdot \delta_{a b} & : a = k \end {cases}$ By definition of [[Definition:Matrix Product (Conventional)|matrix product]]: :$\displaystyle \forall a, b \in \set {1, 2, \ldots, n}: z_{a b} = \sum_{p \mathop = 1}^n x_{a p} y_{p b}$ Thus $z_{a b} \ne 0$ {{iff}} $a = p$ and $b = p$. When $a \ne k$: :$x_{a a} = y_{a a} = 1$ and so: :$z_{a a} = 1 \times 1 = 1$ When $a = k$: :$x_{a a} = \lambda$, $y_{l b} = \dfrac 1 \lambda$ and so: :$z_{a a} = \lambda \times \dfrac 1 \lambda = 1$ and for all $z_{a b}$ where $a \ne b$: :$z_{a b} = 0$ That is: :$z_{a b} = \delta_{a b}$ and by definition: :$\mathbf E \mathbf E' = \mathbf I$ {{qed|lemma}} === $\text {ERO} 2$: Add Scalar Product of Row to Another === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_i \to r_i + \lambda r_j$ From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E$ is of the form: :$x_{a b} = \delta_{a b} + \lambda \cdot \delta_{a i} \cdot \delta_{j b}$ From [[Existence of Inverse Elementary Row Operation/Add Scalar Product of Row to Another|Existence of Inverse Elementary Row Operation: Add Scalar Product of Row to Another]], $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r_i \to r_i - \lambda r_j$ From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E'$ is of the form: :$y_{a b} = \delta_{a b} - \lambda \cdot \delta_{a i} \cdot \delta_{j b}$ {{begin-eqn}} {{eqn | lo= \forall a, b \in \set {1, 2, \ldots, n}: | l = z_{a b} | r = \sum_{p \mathop = 1}^n x_{a p} y_{p b} | c = }} {{eqn | r = \sum_{p \mathop = 1}^n \paren {\delta_{a p} + \lambda \cdot \delta_{a i} \cdot \delta_{j p} } \cdot \paren {\delta_{p b} - \lambda \cdot \delta_{p i} \cdot \delta_{j b} } | c = }} {{eqn | r = \sum_{p \mathop = 1}^n \paren {\delta_{a p} \cdot \delta_{p b} + \lambda \cdot \delta_{a i} \cdot \delta_{j p} \cdot \delta_{p b} - \lambda \cdot \delta_{p i} \cdot \delta_{j b} \cdot \delta_{a p} - \lambda \cdot \delta_{a i} \cdot \delta_{j p} \cdot \lambda \cdot \delta_{p i} \cdot \delta_{j b} } | c = }} {{end-eqn}} We have that: {{begin-eqn}} {{eqn | l = \sum_{p \mathop = 1}^n \delta_{a p} \cdot \delta_{p b} | r = \delta_{a b} | c = }} {{eqn | l = \sum_{p \mathop = 1}^n \lambda \cdot \delta_{p i} \cdot \delta_{j b} \cdot \delta_{a p} | r = \lambda \cdot \delta_{a i} \cdot \delta_{j b} | c = }} {{eqn | l = -\sum_{p \mathop = 1}^n \lambda \cdot \delta_{a i} \cdot \delta_{j p} \cdot \delta_{p b} | r = -\lambda \cdot \delta_{j b} \cdot \delta_{a i} | c = }} {{eqn | l = \sum_{p \mathop = 1}^n \lambda \cdot \delta_{a i} \cdot \delta_{j p} \cdot \lambda \cdot \delta_{p i} \cdot \delta_{j b} | r = \lambda^2 \sum_{p \mathop = 1}^n \delta_{a i} \cdot \delta_{j p} \cdot \delta_{p i} \cdot \delta_{j b} | c = }} {{eqn | r = \lambda^2 \delta_{a i} \cdot \delta_{j i} \cdot \delta_{j b} | c = }} {{eqn | r = 0 | c = as $i \ne j$ }} {{end-eqn}} But: :$\lambda \cdot \delta_{j b} \cdot \delta_{a i} - \lambda \cdot \delta_{j b} \cdot \delta_{a i} = 0$ So everything vanishes except $\delta_{a b}$, and so: :$z_{a b} = \delta_{a b}$ and by definition, again: :$\mathbf E \mathbf E' = \mathbf I$ {{qed|lemma}} === $\text {ERO} 3$: Exchange Rows === Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_i \leftrightarrow r_j$ From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E$ is of the form: :$x_{a b} = \begin {cases} \delta_{a b} & : \text {if $a \ne i$ and $a \ne j$} \\ \delta_{j b} & : \text {if $a = i$} \\ \delta_{i b} & : \text {if $a = j$} \end {cases}$ From [[Existence of Inverse Elementary Row Operation/Exchange Rows|Existence of Inverse Elementary Row Operation: Exchange Rows]], $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r_i \leftrightarrow r_j$ From [[Elementary Matrix corresponding to Elementary Row Operation]], $\mathbf E'$ is of the form: :$y_{a b} = \begin {cases} \delta_{a b} & : \text {if $a \ne i$ and $a \ne j$} \\ \delta_{j b} & : \text {if $a = i$} \\ \delta_{i b} & : \text {if $a = j$} \end {cases}$ By definition of [[Definition:Matrix Product (Conventional)|matrix product]]: :$\displaystyle \forall a, b \in \set {1, 2, \ldots, n}: z_{a b} = \sum_{p \mathop = 1}^n x_{a p} y_{p b}$ When $a \ne i$ and $b \ne j$ we have: {{begin-eqn}} {{eqn | l = z_{a b} | r = \sum_{p \mathop = 1}^n \delta_{a p} \delta_{p b} | c = }} {{eqn | r = \delta_{a b} | c = }} {{end-eqn}} When $a = i$ and $b \ne i$ we have: {{begin-eqn}} {{eqn | l = z_{a b} | r = \sum_{p \mathop = 1}^n \delta_{j p} \delta_{p b} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} When $a = j$ and $b \ne j$ we have: {{begin-eqn}} {{eqn | l = z_{a b} | r = \sum_{p \mathop = 1}^n \delta_{i p} \delta_{p b} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} When $a = b = i$ we have: {{begin-eqn}} {{eqn | l = z_{i i} | r = \sum_{p \mathop = 1}^n \delta_{j p} \delta_{p j} | c = }} {{eqn | r = \delta_{j j} | c = }} {{eqn | r = 1 | c = }} {{end-eqn}} When $a = b = j$ we have: {{begin-eqn}} {{eqn | l = z_{j j} | r = \sum_{p \mathop = 1}^n \delta_{i p} \delta_{p i} | c = }} {{eqn | r = \delta_{i i} | c = }} {{eqn | r = 1 | c = }} {{end-eqn}} Hence by definition, again: :$\mathbf E \mathbf E' = \mathbf I$ {{qed|lemma}} Thus in all cases: :$\mathbf E \mathbf E' = \mathbf I$ Hence the result. {{qed}}	1
{{AimForCont}} $\displaystyle \lim_{n \to \infty} x_n = L_1$ and $\displaystyle \lim_{n \mathop \to \infty} x_n = L_2$, where $L_1 \ne L_2$. Let $\displaystyle \epsilon = \frac {\norm {L_1 - L_2} } 3$. From [[Definition:Norm Axioms (Vector Space)|properties of norm]] it follows that $\epsilon > 0$. By [[Definition:Convergent Sequence in Normed Vector Space|definition]]: :$\exists N_1 \in \N : \forall n > N_1 : \norm {x_n - L_1} < \epsilon$ :$\exists N_2 \in \N : \forall n > N_2 : \norm {x_n - L_2} < \epsilon$ Choose $n > N_1 + N_2$. Then: {{begin-eqn}} {{eqn | l = \norm {L_1 - L_2} | r = \norm {L_1 - x_n + x_n - L_2} }} {{eqn | o = \le | r = \norm {L_1 - x_n} + \norm {x_n - L_2} | c = [[Triangle Inequality]] }} {{eqn | o = < | r = \epsilon + \epsilon }} {{eqn | r = \frac 2 3 \norm {L_1 - L_2} }} {{end-eqn}} which implies that: :$\displaystyle 1 < \frac 2 3$ This is a [[Definition:Contradiction|contradiction]]. Hence $L_1 = L_2$. {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Then: :$\struct {R, \norm {\, \cdot \,} }$ has a [[Definition:Completion (Normed Division Ring)|normed division ring completion]] $\struct {R', \norm {\, \cdot \,}' }$	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $P, Q$ be [[Definition:Projection (Hilbert Spaces)|projections]]. Then $P + Q$ is a [[Definition:Projection (Hilbert Spaces)|projection]] {{iff}} $\Rng P \perp \Rng Q$.	1
From [[Leigh.Samphier/Sandbox/Complete Normed Division Ring is Completion of Dense Subring]]: :$\struct {R, \norm {\, \cdot \,} }$ is a [[Definition:Completion (Normed Division Ring)|completion]] of $\struct {S, \norm {\, \cdot \,}}$ where the [[Definition:Inclusion Mapping|inclusion mapping]] $i : S \to R$ is the required [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]]. From [[Leigh.Samphier/Sandbox/Element of Completion is Limit of Sequence in Normed Division Ring]]: :for all $x \in R$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $S$: ::$\quad x = \displaystyle \lim_{n \mathop \to \infty} \map i {x_n}$ That is: :for all $x \in R$, there exists a [[Definition:Sequence|sequence]] $\sequence{x_n}$ in $S$: ::$\quad x = \displaystyle \lim_{n \mathop \to \infty} x_n$ {{qed}} [[Category:Completion of Normed Division Ring]] pcx12a1d0qgl2952fx99gp4wd4wj91j	1
Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{m n}$ and $\mathbf C = \sqbrk c_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A + \mathbf B} + \mathbf C | r = \paren {\sqbrk a_{m n} + \sqbrk b_{m n} } + \sqbrk c_{m n} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{eqn | r = \sqbrk {a + b}_{m n} + \sqbrk c_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {\paren {a + b} + c}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {a + \paren {b + c} }_{m n} | c = {{Ring-axiom|A1}} }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {b + c}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk a_{m n} + \paren {\sqbrk b_{m n} + \sqbrk c_{m n} } | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \mathbf A + \paren {\mathbf B + \mathbf C} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{end-eqn}} {{qed}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {R, +_R, *_R}$ be the [[Definition:Opposite Ring|opposite ring]] of $\struct {R, +_R, \times_R}$. Let $\struct{G, +_G, \circ}$ be a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. Let $\circ’ : R \times G \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: \lambda \circ’ x = x \circ \lambda $ Then $\struct{G, +_G, \circ’}$ is a [[Definition:Left Module|left module]] over $\struct {R, +_R, *_R}$.	1
By definition of [[Definition:Orthonormal Basis of Vector Space|orthonormal basis]]: :$(1): \quad \tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ is an [[Definition:Orthogonal Basis of Vector Space|orthogonal basis of $V$]] :$(2): \quad \norm {\mathbf e_1} = \norm {\mathbf e_2} = \cdots = \norm {\mathbf e_1} = 1$ From $(1)$ we have by definition that $\mathbf e_i$ and $\mathbf e_j$ are [[Definition:Perpendicular|perpendicular]] whenever $i \ne j$ Thus, from [[Dot Product of Perpendicular Vectors]], for all $i$ and $j$, when $i \ne j$: :$\mathbf e_i \cdot \mathbf e_j = 0$ From [[Dot Product of Vector with Itself]], for all $i$ we have: :$\mathbf e_i \cdot \mathbf e_i = \norm {\mathbf e_i}^2$ From $(2)$, we have that: :$\norm {\mathbf e_i} = 1$ and so: :$\mathbf e_i \cdot \mathbf e_i = 1$ Putting this together, we have: :$\forall i, j \in \set {1, 2, \ldots, n}: \mathbf e_i \cdot \mathbf e_j = \begin {cases} 1 & : i = j \\ 0 & : i \ne j \end {cases}$ The result follows by definition of the [[Definition:Kronecker Delta|Kronecker delta]]. {{qed}}	1
Let $\psi' = \phi_2 \circ \phi_1^{-1}:\phi_1 \paren R \to \phi_2 \paren R$ be the composition of $\phi_1^{-1}$ with $\phi_2$. Then $\psi': \struct {\map {\phi_1} R, \norm {\, \cdot \,}_1 } \to \struct {\map {\phi_2} R, \norm {\, \cdot \,}_2 }$ is an [[Definition:Isometry (Metric Spaces)|isometric]] [[Definition:Isomorphism (Abstract Algebra)/Ring Isomorphism|ring isomorphism]].	1
For all $h \in H$, the following inequality holds: :$\left|{Lh}\right| \le \left\|{L}\right\| \left\|{h}\right\|$	1
{{proof wanted|Proceeds by sequence definition of limit point}}	1
Suppose $l \ne 0$. Then: :$\exists k \in \N : \forall n \in \N: x_{k + n} \ne 0$ and the [[Definition:Subsequence|subsequence]] $\sequence { x_{k+n}^{-1} }$ is well-defined and [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} {x_{k + n} }^{-1} = l^{-1}$.	1
Let $m_1, \ldots, m_n$ be a [[Definition:Generator of Module|generating set]] for $M$. Then for each $i$, $\phi \left({m_i}\right) \in \mathfrak a M$, say: :$\displaystyle \phi \left({m_i}\right) = \sum_{j \mathop = 1}^n a_j m_j$ for $i = 1, \ldots, n$. {{explain|Is it significant that $\phi \left({m_i}\right)$ shares the same symbol as $\phi$, or would it be clearer to use a separate symbol?}} Thus for each $i$: :$(1): \quad \displaystyle \sum_{j \mathop = 1}^n \left[{\delta_{ij} \phi - a_{ij} }\right] m_i = 0$ where $\delta_{ij}$ is the [[Definition:Kronecker_Delta|Kronecker delta]]. Now let $\Delta$ be the [[Definition:Matrix|matrix]] defined as: : $\Delta := \left({\phi \delta_{ij} - a_{ij} }\right)$ {{explain|$\phi$ and $\delta_{ij}$ have changed places -- is this significant? Otherwise consistency in presentation is to be aimed for.}} Let $\operatorname{adj} \left({\Delta}\right)$ be the [[Definition:Adjugate Matrix|adjugate matrix]] of $\Delta$. Recall [[Cramer's Rule]]: {{begin-eqn}} {{eqn | l = \operatorname{adj} \left({\Delta}\right) \cdot \Delta | r = \Delta \cdot \operatorname{adj} \left({\Delta}\right) | c = }} {{eqn | r = \det \left({\Delta}\right) \cdot \mathbf I_n | c = }} {{end-eqn}} {{Explain|Explanation needed as to what $\cdot$ means in this context, why it is significant to commute the factors of the expression, and how the final expression arises (probably explained once Cramer's rule is posted up.}} Multiplying through by $\operatorname{adj} \left({\Delta}\right)$ in $(1)$ and applying [[Cramer's Rule]]: :$\displaystyle \sum_{j \mathop = 1}^n \det \left({\Delta}\right) m_i = 0$ Therefore $\det \left({\Delta}\right)$ [[Definition:Annihilator|annihilates]] each $m_i$ and is the [[Definition:Zero Endomorphism|zero endomorphism]] of $M$. But $\det \left({\phi \delta_{ij} - a_{ij}}\right)$ is a [[Definition:Monic Polynomial|monic polynomial]] in $\phi$ with coefficients in $\mathfrak a$. Thus we have an equation of the required form. {{qed}} {{Namedfor|Arthur Cayley|cat = Cayley|name2 = William Rowan Hamilton|cat2 = Hamilton}} [[Category:Commutative Algebra]] 29rl82viqzs2luy06wxty7atbibni97	1
Let $n_1, n_2, \ldots, n_r$ be [[Definition:Positive Integer|positive integers]] such that $n_i \perp n_j$ for all $i \ne j$ (that is, $\gcd \left\{{n_i, n_j}\right\} = 1$). Let $N = n_1 \cdots n_r$. For an integer $k$, let $\Z / k \Z$ denote the [[Definition:Ring of Integers Modulo m|ring of integers modulo $k$]]. Then we have a [[Definition:Ring Isomorphism|ring isomorphism]]: :$\Z / N \Z \simeq \Z / n_1 \Z \times \cdots \times \Z / n_r \Z$	1
=== Proof of $\text M 1$ and $\text M 4$ === Let $x, y \in V$. Then: :$\map d {x, y} = \norm {x - y} \ge 0$ and furthermore: {{begin-eqn}} {{eqn | l = \map d {x, y} | r = 0 }} {{eqn | ll= \leadstoandfrom | l = \norm {x - y} | r = 0 }} {{eqn | ll= \leadstoandfrom | l = x - y | r = \mathbf 0_V | c = [[Definition:Norm Axioms (Vector Space)|Norm Axioms: Axiom $\text N 1$]] }} {{eqn | ll= \leadstoandfrom | l = x | r = y }} {{end-eqn}} {{qed|lemma}} === Proof of $\text M 2$ === Let $x, y, z \in V$. Then: {{begin-eqn}} {{eqn | l = \map d {x, z} | r = \norm {x - z} }} {{eqn | r = \norm {x - y + y - z} | c = }} {{eqn | o = \le | r = \norm {x - y} + \norm {y - z} | c = [[Definition:Norm Axioms (Vector Space)|Norm Axioms: Axiom $\text N 3$]] }} {{eqn | r = \map d {x, y} + \map d {y, z} }} {{end-eqn}} {{qed|lemma}} === Proof of $\text M 3$ === Let $x, y \in V$. Then: {{begin-eqn}} {{eqn | l = \map d {x, y} | r = \norm {x - y} }} {{eqn | r = \norm {-1 \paren {y - x} } }} {{eqn | r = \norm {-1} \times \norm {y - x} | c = [[Definition:Norm Axioms (Vector Space)|Norm Axioms: Axiom $\text N 2$]] }} {{eqn | r = \norm {y - x} }} {{eqn | r = \map d {y, x} }} {{end-eqn}} {{qed|lemma}} As $d$ satisfies the [[Definition:Metric Space Axioms|metric space axioms]], it is a [[Definition:Metric|metric]]. {{qed}} [[Category:Norm Theory]] [[Category:Metric Spaces]] 1ojmpq5wvlf7ez22c3n312r3utnvffc	1
Let: :$l = \displaystyle \lim_{n \mathop \to \infty} \norm {x_n}$ and: :$m = \displaystyle \lim_{n \mathop \to \infty} \norm {y_n}$ By [[Norm Sequence of Cauchy Sequence has Limit]], both of these [[Definition:Limit of Sequence|limits]] exist. Then: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty } \paren {\norm {x_n} - \norm {y_n} } | r = l - m | c = [[Difference Rule for Real Sequences]] }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} \size {\norm {x_n} - \norm {y_n} } | r = \size {l - m} | c = [[Modulus of Limit]] }} {{end-eqn}} By [[Reverse Triangle Inequality/Normed Division Ring|Reverse Triangle Inequality]], for $n \in \N$: :$l = \size {\norm {x_n} - \norm {y_n} } \le \norm {x_n - y_n} = \to 0$ as $n \to \infty$. By the [[Squeeze Theorem]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\size {\norm {x_n} - \norm {y_n} } } = 0$ So: :$\size {l - m} = 0$ and therefore: $l = m$ {{qed}} [[Category:Cauchy Sequences]] [[Category:Normed Division Rings]] o6fk3yrl6ydw69omhl6r33ifdxxbfal	1
We prove the [[Definition:Contrapositive Statement|contrapositive]]. Let $\norm {\, \cdot \,}_1$ be a [[Definition:Nontrivial Division Ring Norm|nontrivial norm]]. Then: :$\exists y \in R: \norm y_1 \ne 0, \norm y_1 \ne 1$. By [[Real Numbers form Ordered Field]] either $\norm y_1 < 1$ or $\norm y_1 > 1$. Suppose $\norm y_1 > 1$. By [[Definition:Norm on Division Ring|Norm axiom $(\text N 1)$: Positive Definiteness]]: :$y \ne 0_R$ By [[Norm of Inverse in Division Ring]]: :$\norm {y^{-1} }_1 = \dfrac 1 {\norm y_1} < 1$ So either $\norm y_1 < 1$ or $\norm {y^{-1} }_1 < 1$ So: :$\exists x \in R, x \ne 0_R:\norm x_1 < 1$ The theorem now follows by the [[Rule of Transposition]]. {{qed}} [[Category:Equivalence of Definitions of Equivalent Division Ring Norms]] m7xz77776pfypxq1omyk1sb2jg57gwo	1
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Module|$R$-module]]. Let $H \subseteq G$ such that $e \in H$. Then $H$ is a [[Definition:Linearly Dependent Set|linearly dependent set]].	1
Let $x \divides y$ denote [[Definition:Divisor of Ring Element|$x$ divides $y$]]. Let $x, y \in A$, with [[Definition:Complete Factorization|complete factorizations]]: :$x = u x_1 \cdots x_r$ :$y = v y_1 \cdots y_s$ where: :$u, v$ are [[Definition:Unit of Ring|units]] :the $x_i$, $y_i$ [[Definition:Irreducible Element of Ring|irreducible]]. We arrange the [[Definition:Complete Factorization|complete factorizations]] as follows: :$x = u \paren {x_1 \cdots x_t} x_{t + 1} \cdots x_r$ :$y = v \paren {y_1 \cdots y_t} y_{t + 1} \cdots y_s$ where: :$t \le \min \set {r, s}$ :For $i = 1, \ldots, t$, $x_i$ and $y_i$ are [[Definition:Associate in Integral Domain|associates]] :For any $i \in \set {t + 1, \ldots, r}$, $j \in \set {t + 1, \ldots, s}$, $x_i$ and $y_j$ are not [[Definition:Associate in Integral Domain|associates]]. Let $d = x_1 \cdots x_t$ (recall that the [[Definition:Vacuous Product|empty product]] is $1$, i.e. $d = 1$ when $t = 0$). We claim that $d$ is a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]] for $x$ and $y$. Certainly $d \divides x$ and $d \divides y$. So, let $f$ be another [[Definition:Common Divisor of Ring Elements|common divisor]] of $x$ and $y$. We can find $w, z \in A$ such that $x = f w$, and $y = f z$. If $f$ is a [[Definition:Unit of Ring|unit]], then $f \divides d$ by definition. {{AimForCont}} $f \nmid d$. Then the [[Definition:Complete Factorization|complete factorization]] of $f$ must contain an [[Definition:Irreducible Element of Ring|irreducible element]] that does not [[Definition:Divisor of Ring Element|divide]] $d$. Call this [[Definition:Irreducible Element of Ring|irreducible element]] $g$. We have that: :$g$ must [[Definition:Divisor of Ring Element|divide]] some $x_j$ where $j > t$ and :$g$ must [[Definition:Divisor of Ring Element|divide]] some $y_k$ where $k > t$. Either: :$g$ is a [[Definition:Unit of Ring|unit]], [[Definition:Contradiction|contradicting]] its [[Definition:Irreducible Element of Ring|irreducibility]] or: :$x_j$ and $y_k$ are not [[Definition:Irreducible Element of Ring|irreducible]], which is a [[Definition:Contradiction|contradiction]] also. Hence by [[Proof by Contradiction]]: :$f \divides d$ and so $x$ and $y$ have a [[Definition:Greatest Common Divisor of Ring Elements|greatest common divisor]]. {{qed}} [[Category:Unique Factorization Domains]] [[Category:Factorization]] k6llamsyc7acqtb6vxxrbunq6tnjqvp	1
By [[Norm Sequence of Cauchy Sequence has Limit]] then: :for each $\eqclass {x_n}{}$ the $\displaystyle \lim_{n \mathop \to \infty} \norm{x_n}$ exists. Suppose $\eqclass {x_n}{} = \eqclass {y_n}{}$. By [[Left Cosets are Equal iff Product with Inverse in Subgroup|Left Cosets are Equal iff Difference in Subgroup]] then: :$\sequence {x_n} - \sequence {y_n} = \sequence {x_n - y_n} \in \mathcal N$ By [[Equivalent Cauchy Sequences have Equal Limits of Norm Sequences]] then: :$\displaystyle \lim_{n \mathop \to \infty} \norm{x_n} = \lim_{n \mathop \to \infty} \norm{y_n}$ Hence: :$\displaystyle \norm { \eqclass {x_n}{} }_1 = \lim_{n \mathop \to \infty} \norm{x_n} = \lim_{n \mathop \to \infty} \norm{y_n} = \norm { \eqclass {x_n}{} }_1$ The result follows. {{qed}}	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$ of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$. Let $f$ be a [[Definition:Symmetric Bilinear Form|symmetric bilinear form]] on $V$. Then there exists an [[Definition:Ordered Basis|ordered basis]] for which the [[Definition:Relative Matrix of Bilinear Form|relative matrix]] of $f$ is [[Definition:Diagonal Matrix|diagonal]].	1
By the definition of [[Definition:Like Vector Quantities|like vector quantities]]: :$\mathbf a$ and $\mathbf b$ are '''like [[Definition:Vector Quantity|vector quantities]]''' {{iff}} they have the same [[Definition:Direction|direction]]. By definition of [[Definition:Unit Vector|unit vector]]: :$\dfrac {\mathbf a} {\size {\mathbf a} } = \dfrac {\mathbf b} {\size {\mathbf b} }$ as both are in the same [[Definition:Direction|direction]], and both have [[Definition:Vector Length|length]] $1$. By definition of [[Definition:Scalar Division on Vector Quantity|scalar division]]: :$\dfrac 1 {\size {\mathbf a} } \mathbf a = \dfrac 1 {\size {\mathbf b} } \mathbf b$ Hence, multiplying by $\size {\mathbf a}$: :$\mathbf a = \dfrac {\size {\mathbf a} } {\size {\mathbf b} } \mathbf b$ {{qed}}	1
Consider the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\map \MM {m, n}, +, \circ}$, where: :$+$ denotes [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] :$\circ$ denotes [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]]. From [[Ring of Square Matrices over Field is Ring with Unity]], $\struct {\map \MM {m, n}, +, \circ}$ is a [[Definition:Ring with Unity|ring with unity]]. Hence [[Definition:A Fortiori|a fortiori]] $\struct {\map \MM {m, n}, +, \circ}$ is a [[Definition:Monoid|monoid]]. The result follows directly from [[Inverse in Monoid is Unique]]. {{qed}}	1
The eigenvectors of a Hermitian [[Definition:Operation|operation]] are [[Definition:Orthogonal (Linear Algebra)|orthogonal]].	1
Suppose $\phi$ is a [[Definition:Vector Space Monomorphism|monomorphism]]. Let $\sequence {a_n}$ be a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. Let: :$\displaystyle \sum_{k \mathop = 1}^n \lambda_k \map \phi {a_k} = 0$ Then: :$\displaystyle \map \phi {\sum_{k \mathop = 1}^n \lambda_k a_k} = 0$ So by hypothesis: :$\displaystyle \sum_{k \mathop = 1}^n \lambda_k a_k = 0$ Hence: :$\forall k \in \closedint 1 n: \lambda_k = 0$ Suppose that for every [[Definition:Linearly Independent Sequence|linearly independent sequence]] $\sequence {a_n}$ of vectors of $G$, $\sequence {\map \phi {a_n} }$ is a linearly independent sequence of vectors of $H$. Let $\map \phi {a_1} = 0$. Then $a_1 = 0$, otherwise the sequence $\sequence {a_1}$ of one term would be linearly independent but $\sequence {\map \phi {a_1} }$ would not. Thus $\map \ker \phi = \set 0$ and by the [[Quotient Theorem for Group Epimorphisms]] $\phi$ is an [[Definition:Vector Space Isomorphism|isomorphism]] and therefore a [[Definition:Vector Space Monomorphism|monomorphism]]. {{Qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over one of the [[Definition:Standard Number System|standard number systems]]. Let $\mathbf 0 = \sqbrk 0_{m n}$ be the [[Definition:Zero Matrix|zero matrix]] of $\map \MM {m, n}$. Then $\mathbf 0$ is the [[Definition:Identity Element|identity element]] for [[Definition:Matrix Entrywise Addition|matrix entrywise addition]].	1
As a counterexample, let $\mathbf A = \mathbf I_n$ be the [[Definition:Unit Matrix|unit matrix]] of order $n > 1$. Let $\mathbf B$ be any [[Definition:Invertible Matrix|invertible matrix]] over $R$ of order $n$ that is different from the [[Definition:Unit Matrix|unit matrix]]. Then [[Definition:Matrix Equivalence|$\mathbf A \equiv \mathbf B$]], as: :$\mathbf I_n^{-1} \mathbf A \mathbf B = \mathbf I_n^{-1} \mathbf I_n \mathbf B = \mathbf B$. If $\mathbf P$ is an invertible [[Definition:Square Matrix|square matrix]] of order $n$, then: :$\mathbf P^{-1} \mathbf A \mathbf P = \mathbf P^{-1} \mathbf P = \mathbf I_n \ne \mathbf B$ Hence, $\mathbf A$ is not [[Definition:Matrix Similarity|similar]] to $\mathbf B$. {{qed}}	1
By definition, $T$ is [[Definition:Separable Space|separable]] {{iff}} there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Let $H \subseteq S$ be [[Definition:Everywhere Dense|everywhere dense]] in $T$. Then by definition of [[Definition:Everywhere Dense|everywhere dense]], $H^- = S$ where $H^-$ denotes the [[Definition:Closure (Topology)|closure]] of $H$. However, as $T$ is a [[Definition:Discrete Space|discrete space]], $H^- = H$ from [[Interior Equals Closure of Subset of Discrete Space]]. So $H^- = S \implies H = S$. But $S$ is [[Definition:Uncountable Set|uncountable]]. So there exists no $H \subseteq S$ such that $H$ is both [[Definition:Countable Set|countable]] and [[Definition:Everywhere Dense|everywhere dense]]. Hence by definition of [[Definition:Separable Space|separable space]], if $T$ is an [[Definition:Uncountable Discrete Topology|uncountable discrete space]] it can not be [[Definition:Separable Space|separable]]. {{qed}}	1
The [[Definition:Kernel of Group Homomorphism|kernel]] of the $\det$ [[Definition:Mapping|mapping]] is the [[Definition:Special Linear Group|special linear group]] $\SL {n, \R}$.	1
From the [[Parallelogram Law]]: :[[File:ParallelogramLaw.png|350px]] {{finish}}	1
This is a special case of [[Direct Product of Modules is Module]]. {{qed}}	1
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $M$ be a [[Definition:Free Module|free]] $R$-[[Definition:Module|module]]. Let $M$ be [[Definition:Finitely Generated Module|finitely generated]]. Let $B$ and $C$ be [[Definition:Basis of Module|bases]] of $M$. Then $B$ and $C$ are [[Definition:Finite|finite]] and have the same [[Definition:Cardinality of Finite Set|cardinality]].	1
By definition of $k$: :if $\mathbf A$ has more [[Definition:Row of Matrix|rows]] than [[Definition:Column of Matrix|columns]], $k$ is the number of [[Definition:Column of Matrix|columns]] of $\mathbf A$. :if $\mathbf A$ has more [[Definition:Column of Matrix|columns]] than [[Definition:Row of Matrix|rows]], $k$ is the number of [[Definition:Row of Matrix|rows]] of $\mathbf A$. Thus let $\mathbf A'$ be the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $k$ consisting of the first $k$ [[Definition:Row of Matrix|rows]] and [[Definition:Column of Matrix|columns]] of $\mathbf A$: :$\mathbf A' = \begin {bmatrix} a_{1 1} & a_{1 2} & a_{1 3} & \cdots & a_{1, k - 1} & a_{1 k} \\ 0 & a_{2 2} & a_{2 3} & \cdots & a_{2, k - 1} & a_{2 k} \\ 0 & 0 & a_{3 3} & \cdots & a_{3, k - 1} & a_{3 k} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{k - 1, k - 1} & a_{k - 1, k} \\ 0 & 0 & 0 & \cdots & 0 & a_{k k} \\ \end {bmatrix}$ $\mathbf A$ can be transformed into [[Definition:Echelon Form|echelon form]] $\mathbf B$ by using the [[Definition:Elementary Row Operation|elementary row operations]]: :$\forall j \in \set {1, 2, \ldots, k}: e_j := r_j \to \dfrac 1 {a_{j j} } r_j$ Again, let $\mathbf B'$ be the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $k$ consisting of the first $k$ [[Definition:Row of Matrix|rows]] and [[Definition:Column of Matrix|columns]] of $\mathbf B$: :$\mathbf B' = \begin {bmatrix} 1 & b_{1 2} & b_{1 3} & \cdots & b_{1, k - 1} & b_{1 k} \\ 0 & 1 & b_{2 3} & \cdots & b_{2, k - 1} & b_{2 k} \\ 0 & 0 & 1 & \cdots & b_{3, k - 1} & b_{3 k} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & 1 & b_{k - 1, k} \\ 0 & 0 & 0 & \cdots & 0 & 1 \\ \end {bmatrix}$ $\mathbf B$ is then transformed into [[Definition:Reduced Echelon Form|reduced echelon form]] $\mathbf C$ by means of the [[Definition:Elementary Row Operation|elementary row operations]]: :$\forall j \in \set {1, 2, \ldots, k - 1}: e_{j k} := r_j \to r_j - b_{j k} r_k$ :$\forall j \in \set {1, 2, \ldots, k - 2}: e_{j, k - 1} := r_j \to r_j - b_{j, k - 1} r_{k - 1}$ and so on, until: :$e_{1 2} := r_1 \to r_1 - b_{1 2} r_2$ Again, let $\mathbf C'$ be the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $k$ consisting of the first $k$ [[Definition:Row of Matrix|rows]] and [[Definition:Column of Matrix|columns]] of $\mathbf C$: :$\mathbf C' = \begin {bmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & 1 & 0 \\ 0 & 0 & 0 & \cdots & 0 & 1 \\ \end {bmatrix}$ By inspection, $\mathbf C$ is seen to be the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order]] $k$. {{qed}}	1
When considering [[Definition:Hilbert Space|Hilbert spaces]], one wants to deal with [[Definition:Orthogonal Projection|projections]] onto subspaces. These projections however require the linear subspace to be [[Definition:Closed Set (Topology)|closed in topological sense]] in order to be well-defined. Therefore, in treatises of [[Definition:Hilbert Space|Hilbert spaces]], one encounters the terminology '''linear manifold''' for the concept of '''vector subspace''' defined above. The adapted definition of '''linear subspace''' is then that it is a [[Definition:Closed Set (Topology)|topologically closed]] '''linear manifold'''.	1
Let the [[Definition:Vector|vectors]] $\mathbf u$, $\mathbf v$ and $\mathbf w$ be embedded in a [[Definition:Cartesian 3-Space|Cartesian $3$-space]]. It is noted that $\mathbf u$, $\mathbf v$ and $\mathbf w$ are not necessarily [[Definition:Coplanar Vectors|coplanar]]. :[[File:Dot-product-distributes-over-addition.png|520px]] Let instances of $\mathbf u$ and $\mathbf w$ be selected so their [[Definition:Initial Point of Vector|initial points]] are at some point $O$. Let an instance of $\mathbf v$ be selected so its [[Definition:Initial Point of Vector|initial point]] is positioned at the [[Definition:Terminal Point of Vector|terminal point]] $U$ of $\mathbf u$. Let the [[Definition:Terminal Point of Vector|terminal point]] $\mathbf v$ be $V$. Let $UA$ be dropped [[Definition:Perpendicular|perpendicular]] to $\mathbf w$. Let $VB$ be dropped [[Definition:Perpendicular|perpendicular]] to $\mathbf w$. Let another instance of $\mathbf w$ be selected so that its [[Definition:Initial Point of Vector|initial point]] is at $U$. Let $VC$ be dropped [[Definition:Perpendicular|perpendicular]] to this second instance of $\mathbf w$. Let $CB$ be dropped from $C$ to the first instance of $\mathbf w$. We have that: :$UA \parallel CB$ and: :$UC \parallel AB$ Thus $\Box ABCU$ is a [[Definition:Parallelogram|parallelogram]]. Hence: :$AB = UC$ Then we have that: {{begin-eqn}} {{eqn | l = OA | r = OU \cos \angle \mathbf u, \mathbf w | c = }} {{eqn | r = \norm {\mathbf u} \cos \angle \mathbf u, \mathbf w | c = }} {{eqn | l = AB | r = UV \cos \angle \mathbf v, \mathbf w | c = }} {{eqn | r = \norm {\mathbf v} \cos \angle \mathbf v, \mathbf w | c = }} {{eqn | l = OB | r = OV \cos \angle \paren {\mathbf u + \mathbf v}, \mathbf w | c = }} {{eqn | r = \norm {\paren {\mathbf u + \mathbf v} } \cos \angle \paren {\mathbf u + \mathbf v}, \mathbf w | c = }} {{eqn | r = \norm {\mathbf u} \cos \angle \mathbf u, \mathbf w + \norm {\mathbf v} \cos \angle \mathbf v, \mathbf w | c = }} {{eqn | ll= \leadsto | l = \norm {\paren {\mathbf u + \mathbf v} } \norm {\mathbf w} \cos \angle \paren {\mathbf u + \mathbf v}, \mathbf w | r = \norm {\mathbf u} \norm {\mathbf w} \cos \angle \mathbf u, \mathbf w + \norm {\mathbf v} \norm {\mathbf w} \cos \angle \mathbf v, \mathbf w | c = }} {{eqn | ll= \leadsto | l = \paren {\mathbf u + \mathbf v} \cdot \mathbf w | r = \mathbf u \cdot \mathbf w + \mathbf v \cdot \mathbf w | c = {{Defof|Dot Product|index = 2}} }} {{end-eqn}} Hence the result. {{qed}}	1
Let $I_G$ be the [[Definition:Identity Mapping|identity]] [[Definition:Linear Operator|linear operator]] on $G$. Let $\left[{I_G; \left \langle {a_n} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ be the [[Definition:Relative Matrix|matrix of $I_G$ relative to $\left \langle {b_n} \right \rangle$ and $\left \langle {a_n} \right \rangle$]]. Then $\left[{I_G; \left \langle {a_n} \right \rangle, \left \langle {b_n} \right \rangle}\right]$ is called the '''matrix corresponding to the change of basis from $\left \langle {a_n} \right \rangle$ to $\left \langle {b_n} \right \rangle$'''.	1
An [[Definition:Identity Matrix|identity matrix]], by definition, has instances of $1$ on the [[Definition:Main Diagonal|main diagonal]] and $0$ elsewhere. Each [[Definition:Diagonal Element|diagonal element]] is by definition on one [[Definition:Row of Matrix|row]] and one [[Definition:Column of Matrix|column]] of the [[Definition:Square Matrix|matrix]]. Also by definition, each [[Definition:Diagonal Element|diagonal element]] is on a different [[Definition:Row of Matrix|row]] and [[Definition:Column of Matrix|column]] from each other [[Definition:Diagonal Element|diagonal element]]. The result follows by definition of [[Definition:Permutation Matrix|permutation matrix]]. {{qed}}	1
Let $\mathbf A$ be a [[Definition:Full Rook Matrix|full rook matrix]]. By definition, $\mathbf A$ is an instance of a [[Definition:Permutation Matrix|permutation matrix]]. By [[Determinant of Permutation Matrix]], it follows that $\det \mathbf A = \pm 1$. By [[Matrix is Invertible iff Determinant has Multiplicative Inverse]]: :$\mathbf A$ is [[Definition:Invertible Matrix|invertible]]. {{qed}}	1
Let $\mathbf X$ and $\mathbf Y$ be two $m \times n$ [[Definition:Matrix|matrices]] that differ by exactly one [[Definition:Elementary Row Operation|elementary row operation]]. Then there exists an [[Definition:Elementary Row Matrix|elementary row matrix]] of [[Definition:Order of Square Matrix|order]] $m$ such that: :$\mathbf {E X} = \mathbf Y$	1
This is a special case of [[Direct Product of Modules is Module]]. {{qed}}	1
:$\displaystyle \paren {\sum \cmod {w_i}^2} \paren {\sum \cmod {z_i}^2} \ge \cmod {\sum w_i z_i}^2$ where all of $w_i, z_i \in \C$.	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $F \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] in $X$ over $F$. Let $J$ be a non-[[Definition:Null Ideal|null]] [[Definition:Ideal of Ring|ideal]] of $F \sqbrk X$. Then there exists [[Definition:Unique|exactly one]] [[Definition:Monic Polynomial|monic polynomial]] $f \in F \sqbrk X$ such that: :$J = \ideal f$ where $\ideal f$ is the [[Definition:Principal Ideal of Ring|principal ideal]] generated by $f$ in $F \sqbrk X$.	1
{{begin-eqn}} {{eqn | l = \mathbf A \mathbf 0 | r = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m 1} & a_{m 2} & \cdots & a_{m n} \\ \end {bmatrix} \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix} }} {{eqn | r = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix} }} {{eqn | r = \mathbf 0 }} {{end-eqn}} The [[Definition:Order of Matrix|order]] is correct [[Definition:By Hypothesis|by hypothesis]]. The result follows by the definition of [[Definition:Null Space|null space]]. {{qed}}	1
Let $\mathcal F$ be the set of [[Definition:Ideal of Ring|ideals]] of $R$ of the form $x R$, with $x$ not a [[Definition:Unit of Ring|unit]] and such that $x$ cannot be [[Definition:Decomposable Element|decomposed]] in the form: :$x = u p_1 \dotsm p_r$ where $u$ is a [[Definition:Unit of Ring|unit]] and $p_1, \dotsc, p_r$ [[Definition:Irreducible Element of Ring|irreducible]]. We show by [[Proof by Contradiction|contradiction]] that $\mathcal F = \O$. {{AimForCont}} $\mathcal F \ne \O$. Since $R$ is [[Definition:Noetherian Ring|noetherian]], we can choose a [[Definition:Maximal Element|maximal element]] $a R \in \mathcal F$. By construction, $a$ is not [[Definition:Irreducible Element of Ring|irreducible]], so we can write: : $a = b c$ with $b, c$ non-[[Definition:Unit of Ring|units]] and not [[Definition:Associate/Commutative and Unitary Ring|associates]]. By the [[Definition:Associate/Commutative and Unitary Ring|definition of associates in a commutative ring]] this means that $b R \subsetneq a R$ and $a R \subsetneq b R$. Since $a R$ is assumed [[Definition:Maximal Element|maximal]], this means that $b R$ and $c R$ do not belong to $\mathcal F$. Therefore there exist [[Definition:Unit of Ring|units]] $u, v$ and [[Definition:Irreducible Element of Ring|irreducible elements]] $p_1, \dotsc, p_r, q_1, \dotsc, q_s$ such that: :$b = u p_1 \dotsm p_r$ and: :$c = v q_1 \dotsm q_s$ But this implies that: :$a = b c = \paren {u v} p_1 \cdots p_r \cdot q_1 \dotsm q_s$ This is a [[Definition:Contradiction|contradiction]], since [[Definition:By Hypothesis|by hypothesis]] $a$ cannot be written in this form. {{Qed}} [[Category:Ring Theory]] [[Category:Factorization]] qtrvz0wcnqlgi83sxmiqiv3zduptpbv	1
Let $\mathbf a, \mathbf b, \mathbf c$ be [[Definition:Vector (Linear Algebra)|vectors]] in $3$ [[Definition:Dimension (Linear Algebra)|dimensional]] [[Definition:Euclidean Space|Euclidean space]]. Let $\times$ denotes the [[Definition:Cross Product|cross product]]. Then: :$\mathbf a \times \paren {\mathbf b \times \mathbf c} + \mathbf b \times \paren {\mathbf c \times \mathbf a} + \mathbf c \times \paren {\mathbf a \times \mathbf b} = \mathbf 0$ That is, the [[Definition:Cross Product|cross product operation]] satisfies the [[Definition:Jacobi Identity|Jacobi identity]].	1
Let $\mathbf A = \left[{a}\right]_{1 n}$ be a [[Definition:Row Matrix|row matrix]] with $n$ [[Definition:Column of Matrix|columns]]. and $\mathbf B = \left[{b}\right]_{n 1}$ be a [[Definition:Column Matrix|column matrix]] with $n$ [[Definition:Row of Matrix|rows]]. Let $\mathbf A \mathbf B$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A$ and $\mathbf B$. Then: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{j \mathop = 1}^n a_j b_j$ where: :$a_j$ is [[Definition:Element of Matrix|element]] $a_{1 j}$ of $\mathbf A$ :$b_j$ is [[Definition:Element of Matrix|element]] $b_{j 1}$ of $\mathbf B$.	1
Let $\ell^\infty$ be the [[Definition:Space of Bounded Sequences|space of bounded sequences]]. Let $\struct {\C, +_\C, \times_\C}$ be the [[Definition:Field of Complex Numbers|field of complex numbers]]. Let $\paren +$ be the [[Definition:Pointwise Addition on Ring of Sequences|pointwise addition on the ring of sequences]]. Let $\paren {\, \cdot \,}$ be the [[Definition:Pointwise Multiplication on Ring of Sequences|pointwise multiplication on the ring of sequences]]. Then $\struct {\ell^\infty, +, \, \cdot \,}_\C$ is a [[Definition:Vector Space|vector space]].	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]] Let $\phi: R \to \CC$ be the [[Definition:Mapping|mapping]] from $R$ to $\CC$ defined as: :$\forall a \in R: \map \phi a = \tuple {a, a, a, \dots}$ where $\tuple {a, a, a, \dots}$ is the [[Definition:Constant Sequence|constant sequence]]. Then $\phi$ is a [[Definition:Ring Monomorphism|ring monomorphism]].	1
Let $z = x + i y$. By definition of [[Definition:Complex Modulus|complex modulus]]: :$\cmod z = \sqrt {x^2 + y^2}$ and so: :$\cmod z \ge 0$ Thus: :$-\cmod z\le 0$ Hence the result. {{qed}}	1
Let $\struct {V, \norm {\,\cdot\,} }$ be a [[Definition:Normed Linear Space|normed linear space]]. Let $x, y \in V$. Then $x$ and $y$ are '''[[Definition:Birkhoff-James Orthogonality|Birkhoff-James orthogonal]]''' {{iff}} either: :$(1): \quad x = 0$ or: :$(2): \quad$ there exists a [[Definition:Continuous Functional|continuous functional]] $ f$ on $\struct {V, \norm {\,\cdot\,} }$ such that: ::::$\norm f = 1$ ::::$\map f x = \norm x$ ::::$\map f y = 0$	1
Let $\mathbf A$ be a [[Definition:Matrix|matrix]] in the [[Definition:Matrix Space|matrix space]] $\map {\MM_\R} {m, n}$ such that: :$\mathbf A \mathbf x = \mathbf 0$ represents a [[Definition:Homogeneous Linear Equations|homogeneous system of linear equations]]. Let $\mathbf H$ be [[Definition:Row Equivalence|row equivalent]] to $\mathbf A$. Then the [[Definition:Solution Set to System of Simultaneous Equations|solution set]] of $\mathbf H \mathbf x = \mathbf 0$ equals the [[Definition:Solution Set to System of Simultaneous Equations|solution set]] of $\mathbf A \mathbf x = \mathbf 0$. That is: :$\mathbf A \sim \mathbf H \implies \set {\mathbf x: \mathbf A \mathbf x = \mathbf 0} = \set {\mathbf x: \mathbf H \mathbf x = \mathbf 0}$ where $\sim$ represents [[Definition:Row Equivalence|row equivalence]].	1
{{ProofWanted}} [[Category:Algebras]] 0cfyiqjazgztxulfzqobfd020aim4vs	1
Let $v \in V$ and $\epsilon \in \R_{>0}$. Denote the [[Definition:Open Ball|open $\epsilon$-ball]] of $v$ as $B_\epsilon \left({v}\right)$. Let $x, y \in B_\epsilon \left({v}\right)$. Then $x + t \left({y - x}\right)$ lies on [[Definition:Convex Set (Vector Space)|line segment]] joining $x$ and $y$ for all $t \in \left[{0 \,.\,.\, 1}\right]$. The distance between $x + t \left({y - x}\right)$ and $v$ is: {{begin-eqn}} {{eqn |l= \left\Vert{x + t \left({y - x}\right) - v}\right\Vert |r= \left\Vert{\left({1 - t}\right) \left({x - v}\right) + t \left({y - v}\right) }\right\Vert }} {{eqn |o= \le |r= \left\Vert{\left({1 - t}\right) \left({x - v}\right) }\right\Vert + \left\Vert{t \left({y - v}\right) }\right\Vert |c= by [[Definition:Metric Space/Triangle Inequality|triangle inequality]] }} {{eqn |r= \left({1 - t}\right) \left\Vert{x - v}\right\Vert + t \left\Vert{y-v}\right\Vert |c= by [[Definition:Norm on Vector Space|norm axiom $(N2)$]] }} {{eqn |o= < |r= \left({1 - t}\right) \epsilon + t \epsilon |c= as $x, y \in B_\epsilon \left({v}\right)$ }} {{eqn |r= \epsilon }} {{end-eqn}} Hence, $x + t \left({y - x}\right) \in B_\epsilon \left({v}\right)$. By [[Definition:Convex Set (Vector Space)|definition of convex set]], $B_\epsilon \left({v}\right)$ is a convex set. {{qed}} [[Category:Vector Spaces]] [[Category:Open Balls]] 8lwxjlnq0kh1w3z7tvrti5n8wiaigx4	1
We first consider the case where $L$ is [[Definition:Finite Set|finite]]. Let $S \subseteq \N$ be the set of all $n \in \N$ such that: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le n$, then $\card L \le \card F$ where: :$L \setminus F$ denotes the [[Definition:Set Difference|set difference]] between $L$ and $F$ :$\card L$ and $\card F$ denote the [[Definition:Cardinality|cardinality]] of $L$ and $F$ respectively. We use the [[Principle of Finite Induction]] to prove that $S = \N$. === Basis of the Induction === Let $\card {L \setminus F} \le 0$. Then from [[Cardinality of Empty Set]]: :$L \setminus F = \O$ By [[Set Difference with Superset is Empty Set]]: :$L \subseteq F$ By [[Cardinality of Subset of Finite Set]]: :$\card L \le \card F$ Hence: :$0 \in S$ This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === It is to be shown that if $k \in S$ where $k \ge 1$, then it follows that $k + 1 \in S$. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le k$, then $\card L \le \card F$ It is to be demonstrated that it follows that: :For every [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] $F$ of $V$, if $\card {L \setminus F} \le k + 1$, then $\card L \le \card F$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: Assume the [[Size of Linearly Independent Subset is at Most Size of Finite Generator/Proof 1#Induction Hypothesis|induction hypothesis]] that $n \in S$. Let $F$ be a [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] of $V$ such that: :$\card {L \setminus F} = n + 1$ Let $v \in L \setminus F$. Let $L' = L \cap \paren {F \cup \set v}$. By [[Intersection is Subset]]: :$L' \subseteq L$ By [[Subset of Linearly Independent Set]], it follows that $L'$ is [[Definition:Linearly Independent Set|linearly independent]] over $R$. Also by [[Intersection is Subset]]: :$L' \subseteq F \cup \set v$ Therefore, by [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]]: :[[Definition:Existential Quantifier|there exists]] a [[Definition:Basis (Linear Algebra)|basis]] $B$ of $V$ such that: ::$L' \subseteq B \subseteq F \cup \set v$ Since $v \notin F$ is a [[Definition:Linear Combination of Subset|linear combination]] of $F$, it follows that $F \cup \set v$ is [[Definition:Linearly Dependent Set|linearly dependent]] over $R$. Therefore: :$B \subsetneq F \cup \set v$ By [[Cardinality of Subset of Finite Set]]: :$\card B < \card {F \cup \set v} = \card F + 1$ Hence: :$\card B \le \card F$ We have that: {{begin-eqn}} {{eqn | l = \card {L \setminus B} | o = \le | r = \card {L \setminus L'} | c = [[Relative Complement inverts Subsets]] and [[Cardinality of Subset of Finite Set]] }} {{eqn | r = \card {L \setminus \paren {F \cup \set v} } | c = [[Set Difference with Intersection is Difference]] }} {{eqn | r = \card {\paren {L \setminus F} \setminus \set v} | c = [[Set Difference with Union]] }} {{eqn | r = n + 1 - 1 | c = [[Cardinality of Set Difference with Subset]] }} {{eqn | r = n }} {{end-eqn}} Since $n \in S$: :$\card L \le \card B \le \card F$ Hence: :$n + 1 \in S$ and so the [[Definition:Induction Step|induction step]] has been completed. By [[Set Difference is Subset]]: :$L \setminus F \subseteq L$ From [[Subset of Finite Set is Finite]]: :$L \setminus F$ is [[Definition:Finite Set|finite]]. Therefore, we can apply the fact that $S = \N$ to conclude that: :$\card L \le \card F$ Let $L$ be [[Definition:Infinite Set|infinite]]. Then by [[Set is Infinite iff exist Subsets of all Finite Cardinalities]], there exists a [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $L' \subseteq L$ such that: :$\card {L'} = \card F + 1$ By [[Subset of Linearly Independent Set]], it follows that $L'$ is [[Definition:Linearly Independent Set|linearly independent]] over $R$. It is proven above that this is impossible. Hence the result. {{qed}}	1
The [[Definition:P-Norm|$p$-norm]] on the [[Definition:P-Sequence Space|$p$-sequence space]] is a [[Definition:Norm on Vector Space|norm]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H, K}\right)$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Let $A^{-1} \in B \left({K, H}\right)$ be an [[Definition:Inverse (Bounded Linear Operator)|inverse]] for $A$. Then the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$, $A^*$, is [[Definition:Invertible Linear Operator|invertible]]. Furthermore, $\left({A^*}\right)^{-1} = \left({A^{-1}}\right)^*$.	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n$]] of the [[Definition:Matrix Space|matrix space]] $\map {\MM_\R} n$. Let $\mathbf I$ be the [[Definition:Unit Matrix|unit matrix]] of order $n$. Suppose there exists a [[Definition:Sequence|sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] that reduces $\mathbf A$ to $\mathbf I$. Then $\mathbf A$ is [[Definition:Invertible Matrix|invertible]]. Futhermore, the same sequence, when performed on $\mathbf I$, results in the [[Definition:Inverse Element|inverse]] of $\mathbf A$.	1
Let $\struct {V, \innerprod \cdot \cdot}$ be an [[Definition:Inner Product Space|inner product space]]. Let $u, v \in V$. Then $u$ and $v$ are '''orthogonal''' {{iff}}: :$\innerprod u v = 0$ === [[Definition:Orthogonal (Linear Algebra)/Set|Orthogonal Set]] === {{:Definition:Orthogonal (Linear Algebra)/Set}} === [[Definition:Orthogonal (Linear Algebra)/Real Vector Space|Vectors in $\R^n$]] === {{:Definition:Orthogonal (Linear Algebra)/Real Vector Space}}	1
{{ProofWanted|Needs a proper explanation of the nature of [[Definition:Projection (Analytic Geometry)]]}}	1
=== 1 implies 2 === Follows from [[Ring Homomorphism Preserves Units]]. {{qed|lemma}} === 2 implies 1 === Let $a$ be a [[Definition:Unit of Ring|unit]] of $B$. Let $P \in A \sqbrk x$ be a [[Definition:Monic Polynomial|monic polynomial]] with $\map P {1 / a} = 0$. Let $n$ be its [[Definition:Degree of Polynomial|degree]] and $\map P x = x^n + \map Q x$. Then $1 + a^n \map Q {1 / a} = 0$. Note that $a^{n - 1} \map Q {1 / a} \in A$. Thus $a$ is a [[Definition:Unit of Ring|unit]] of $A$, with [[Definition:Product Inverse|inverse]] $-a^{n - 1} \map Q {1 / a}$. {{qed}} [[Category:Integral Ring Extensions]] bb33y4znlaafmq9bnk4dlyrjd8b2njg	1
Let $M$ be a [[Definition:Locally Euclidean Space|locally Euclidean space of dimension $d$]]. Let $\mathcal A$ be an [[Definition:Atlas|atlas]] on $M$. Then there exists a [[Definition:Unique|unique]] [[Definition:Differentiable Structure|differentiable structure]] $\mathcal F$ on $M$ with $\mathcal A \in \mathcal F$.	1
Let: :$\map {\mathrm N} {\mathbf A} = \set {\mathbf x \in \R^n : \mathbf A \mathbf x = \mathbf 0}$ be the [[Definition:Null Space|null space]] of $\mathbf A$, where: :$\mathbf A_{m \times n} = \begin {bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}$, $\mathbf x_{n \times 1} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$ and $\mathbf 0_{m \times 1} = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$ are [[Definition:Matrix|matrices]] :the [[Definition:Column Matrix|column matrix]] $\mathbf x_{n \times 1}$ is interpreted as a [[Definition:Vector (Euclidean Space)|vector in $\R^n$]]. Then $\map {\mathrm N} {\mathbf A}$ is [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Vector Sum|vector addition]]: :$\forall \mathbf v, \mathbf w \in \map {\mathrm N} {\mathbf A}: \mathbf v + \mathbf w \in \map {\mathrm N} {\mathbf A}$	1
:[[File:DistanceFromOrigin.png|300px]] By definition of the [[Definition:Cartesian Plane|cartesian plane]], the point $P$ is $x$ units from the [[Definition:Y-Axis|$y$-axis]] and $y$ units from the [[Definition:X-Axis|$x$-axis]]. The [[Definition:Y-Axis|$y$-axis]] and [[Definition:X-Axis|$x$-axis]] are [[Definition:Perpendicular|perpendicular]] to each other, also by definition. Thus $x$, $y$ and $OP$ form a [[Definition:Right-Angled Triangle|right-angled triangle]]. By [[Pythagoras' Theorem]]: :$OP^2 = x^2 + y^2$ Hence the result. {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\Gamma$ be a [[Definition:Row Operation|row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$. Then there exists another [[Definition:Row Operation|row operation]] $\Gamma'$ which transforms $\mathbf B$ back to $\mathbf A$.	1
{{begin-eqn}} {{eqn | l = \mathbf a + \mathbf b | r = \mathbf a | c = }} {{eqn | ll= \leadsto | l = \paren {-\mathbf a} + \paren {\mathbf a + \mathbf b} | r = \paren {-\mathbf a} + \mathbf a | c = }} {{eqn | ll= \leadsto | l = \paren {\paren {-\mathbf a} + \mathbf a} + \mathbf b | r = \paren {-\mathbf a} + \mathbf a | c = {{Vector-space-axiom|2}} }} {{eqn | ll= \leadsto | l = \bszero + \mathbf b | r = \bszero | c = {{Vector-space-axiom|4}} }} {{eqn | ll= \leadsto | l = \mathbf b | r = \bszero | c = {{Vector-space-axiom|3}} }} {{end-eqn}} {{qed}}	1
By definition, an [[Definition:Open Set in Normed Vector Space|open set]] $S \subseteq A$ is one where every point inside it is an element of an [[Definition:Open Ball in Normed Vector Space|open ball]] contained entirely within that set. Let $x \in X$. An [[Definition:Open Ball in Normed Vector Space|open ball]] of $x$ in $M$ is by definition a [[Definition:Subset|subset]] of $X$. Hence the result. {{qed}}	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\,\cdot\,}$ to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$, then: :$\forall \epsilon \in \R_{\gt 0}: \exists N \in \N : \forall n \ge N: \norm {x_n - l} < \epsilon$ Let $n_1$ satisfy: :$\forall n \ge n_1: \norm {x_n - l} < 1$ Then $\forall n \ge n_1$: {{begin-eqn}} {{eqn | l = \norm {x_n} | r = \norm {x_n - l + l} }} {{eqn | o = \le | r = \norm {x_n - l} + \norm l | c = [[Definition:Norm Axioms|Norm axiom (N3)]] (Triangle Inequality) }} {{eqn | o = \le | r = 1 + \norm l | c = as $n \ge n_1$ }} {{end-eqn}} Let $K = \max \set {\norm {x_1}, \norm {x_2}, \dots, \norm {x_{n_1 - 1} }, 1 + \norm l}$. Then: :$\forall n < n_1: \norm {x_n} \le K$ :$\forall n \ge n_1: \norm {x_n} \le 1 + \norm l \le K$ Thus, by definition, $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
Let $\mathbf {A, B}$ be [[Definition:Square Matrix|square matrices of order $n$]] Let $\mathbf I$ be the $n \times n$ [[Definition:Unit Matrix|unit matrix]]. Let $\mathbf A$ and $\mathbf B$ be [[Definition:Invertible Matrix|invertible]]. Then the [[Definition:Matrix Product (Conventional)|matrix product]] $\mathbf {AB}$ is also [[Definition:Invertible Matrix|invertible]], and: :$\paren {\mathbf A \mathbf B}^{-1} = \mathbf B^{-1} \mathbf A^{-1}$	1
:$\paren {-1_R} \circ x = - x$	1
This is a special case of [[Direct Product of Unitary Modules is Unitary Module]].	1
By definition of [[Definition:Inverse Matrix|inverse matrix]]: :$\mathbf A \mathbf A^{-1} = \mathbf I_n$ where $\mathbf I_n$ is the [[Definition:Unit Matrix|unit matrix]]. By [[Determinant of Unit Matrix]]: :$\map \det {\mathbf I_n} = 1_K$ By [[Determinant of Matrix Product]]: :$\map \det {\mathbf A^{-1} } \map \det {\mathbf A} = \map \det {\mathbf A^{-1} \mathbf A}$ Hence the result. {{qed}}	1
$M$ is the '''internal direct sum''' of $(M_i)_{i\in I}$ {{Iff}}: * $\displaystyle\bigcup_{i\in I}M_i$ [[Definition:Generator of Module|generates]] $M$ * For all $i\in I$, $M_i\cap \displaystyle\sum_{j\neq i} M_j = \{0\}$	1
By [[Null Sequences form Maximal Left and Right Ideal]] then $\NN$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] of $\CC$. A [[Definition:Field (Abstract Algebra)|field]] is by definition a [[Definition:Commutative Ring|commutative ring]]. In a [[Definition:Commutative Ring|commutative ring]], a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] is by definition a [[Definition:Maximal Ideal of Ring|maximal ideal]]. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\Bbb F \in \set {\R, \C}$, and let $A \subseteq H$ be a [[Definition:Subset|subset]]. {{TFAE|def = Closed Linear Span|view = closed linear span of $A$}} :$(1): \quad \displaystyle \vee A = \bigcap \Bbb M$, where $\Bbb M$ consists of all [[Definition:Closed Linear Subspace|closed linear subspaces]] $M$ of $H$ with $A \subseteq M$ :$(2): \quad \displaystyle \vee A$ is the smallest [[Definition:Closed Linear Subspace|closed linear subspace]] $M$ of $H$ with $A \subseteq M$ :$(3): \quad \displaystyle \vee A = \map \cl {\set {\sum_{k \mathop = 1}^n \alpha_k f_k: n \in \N_{\ge 1}, \alpha_i \in \Bbb F, f_i \in A} }$, where $\cl$ denotes [[Definition:Closure (Topology)|closure]]	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Then $\mathbf A$ is [[Definition:Non-Invertible Matrix|non-invertible]] if there exists a [[Definition:Vector (Linear Algebra)|vector]] $\mathbf v$ of $n$ such that: :$\mathbf v \ne \mathbf 0$ :$\mathbf A \mathbf v = \mathbf 0$ where $\mathbf 0$ is the [[Definition:Zero Vector|zero vector]].	1
Let $\left({R, +_R, \times_R}\right)$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\left({G, +_G, \circ_G}\right)_R$ be an [[Definition:Module|$R$-module]]. Let $\left({H, +_H, \circ_H}\right)_R$ be an [[Definition:R-Algebraic Structure|$R$-algebraic structure]]. Let $\phi: G \to H$ be a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]]. Then the [[Definition:Homomorphic Image|homomorphic image]] of $\phi$ is an [[Definition:Module|$R$-module]].	1
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]] Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence in Normed Division Ring|null sequences]]. Let $\\CC \,\big / \NN$ be the [[Quotient Ring of Cauchy Sequences is Division Ring|quotient ring of Cauchy sequences]] of $\CC$ by the [[Null Sequences form Maximal Left and Right Ideal|maximal ideal]] $\NN$. Let $\sequence {x_n} \in \CC$. Then $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] in $\struct {R, \norm {\,\cdot\,} }$ {{iff}} :$\exists a \in R: \sequence {x_n} \in \sequence {a, a, a, \dotsc} + \NN$ where $\sequence {a, a, a, \dotsc} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {a, a, a, \dotsc}$.	1
The only [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $\set x$ is the one that goes: $x$. Suppose $\exists \lambda \in K: \lambda \circ x = e$. From [[Zero Vector Space Product iff Factor is Zero]] it follows that $\lambda = 0$. Hence the result from definition of [[Definition:Linearly Independent Set|linearly independent set]]. {{qed}}	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]] expressed in the [[Definition:Complex Plane|complex plane]]. Then the [[Definition:Complex Modulus|modulus]] of $z$ can be interpreted as the [[Definition:Distance (Linear Measure)|distance]] of $z$ from the [[Definition:Origin|origin]].	1
Two [[Definition:Vector Quantity|vector quantities]] are [[Definition:Equality|equal]] {{iff}} they have the same [[Definition:Component of Vector|components]].	1
{{proof wanted}} [[Category:Vandermonde Matrices]] 91falks8quapb23z4vth44nc0obh7vf	1
Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]]. Let $\mathbf B$ be an [[Definition:Matrix|$n \times m$ matrix]]. Let $1 \le j_1, j_2, \ldots, j_m \le n$. Let $\mathbf A_{j_1 j_2 \ldots j_m}$ denote the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. Let $\mathbf B_{j_1 j_2 \ldots j_m}$ denote the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. Then: :$\displaystyle \map \det {\mathbf A \mathbf B} = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \map \det {\mathbf A_{j_1 j_2 \ldots j_m} } \map \det {\mathbf B_{j_1 j_2 \ldots j_m} }$ where $\det$ denotes the [[Definition:Determinant of Matrix|determinant]].	1
We have $u = I_H \circ u \circ I_G$ and $\mathbf Q^{-1} = \left[{I_H; \left \langle {{b_m}'} \right \rangle, \left \langle {b_m} \right \rangle}\right]$. Thus by [[Linear Transformations Isomorphic to Matrix Space]]: {{begin-eqn}} {{eqn | l = \mathbf Q^{-1} \mathbf A \mathbf P | r = \left[{I_H; \left \langle { {b_m}'} \right \rangle, \left \langle {b_m} \right \rangle}\right] \left[{u; \left \langle {b_m} \right \rangle, \left \langle {a_n} \right \rangle}\right] \left[{I_G; \left \langle {a_n} \right \rangle, \left \langle { {a_n}'} \right \rangle}\right] | c = }} {{eqn | r = \left[{I_H \circ u \circ I_G; \left \langle { {b_m}'} \right \rangle, \left \langle { {a_n}'} \right \rangle}\right] | c = }} {{eqn | r = \mathbf B | c = }} {{end-eqn}} {{qed}}	1
:$\quad d' = \tilde d$	1
Suppose that $\forall x \in R: x + x = 0 \implies x = 0$. From [[Determinant with Rows Transposed]], if you exchange two [[Definition:Row of Matrix|rows]] of a [[Definition:Square Matrix|square matrix]], the sign of its [[Definition:Determinant of Matrix|determinant]] changes. If you exchange two identical [[Definition:Row of Matrix|rows]] of a [[Definition:Square Matrix|square matrix]], then the sign of its [[Definition:Determinant of Matrix|determinant]] changes from $D$, say, to $-D$. But the [[Definition:Square Matrix|matrix]] stays the same. So $D = -D$ and so $D = 0$. {{qed}}	1
Let $x, y \in \map {B_1} 0$. Then: {{begin-eqn}} {{eqn | l = \norm {\paren {1 - \alpha} x + \alpha y} | o = \le | r = \norm {\paren {1 - \alpha} x} + \norm {\alpha y} | c = [[Definition:Norm Axioms (Vector Space)|Norm Axiom $(\text N 3)$: Triangle Inequality]] }} {{eqn | r = \size {1 - \alpha} \norm x + \size \alpha \norm y | c = [[Definition:Norm Axioms (Vector Space)|Norm Axiom $(\text N 2)$: Positive Homogeneity]] }} {{eqn | r = \paren {1 - \alpha} \norm x + \alpha \norm y | c = {{Defof|Convex Set (Vector Space)}}: $0 \le \alpha \le 1$ }} {{eqn | o = \le | r = \paren {1 - \alpha} + \alpha | c = $x, y \in \map {B_1^-} 0$ }} {{eqn | r = 1 }} {{end-eqn}} Therefore, $\paren {1 - \alpha}x + \alpha y \in \map {B_1^-} 0$. By definition, $\map {B_1^-} 0$ is [[Definition:Convex Set (Vector Space)|convex]]. {{qed}}	1
The [[Integers form Integral Domain|integers]] $\struct {\Z, +, \times}$ form a [[Definition:Unique Factorization Domain|unique factorization domain]].	1
Suppose that $\forall x \in R: x + x = 0 \implies x = 0$. From [[Determinant with Rows Transposed]], if you exchange two [[Definition:Row of Matrix|rows]] of a [[Definition:Square Matrix|square matrix]], the sign of its [[Definition:Determinant of Matrix|determinant]] changes. If you exchange two identical [[Definition:Row of Matrix|rows]] of a [[Definition:Square Matrix|square matrix]], then the sign of its [[Definition:Determinant of Matrix|determinant]] changes from $D$, say, to $-D$. But the [[Definition:Square Matrix|matrix]] stays the same. So $D = -D$ and so $D = 0$. {{qed}}	1
{{begin-eqn}} {{eqn | l = \norm {\sum_{i \mathop = 1}^n f_i}^2 | r = \innerprod {\sum_{i \mathop = 1}^n f_i} {\sum_{j \mathop = 1}^n f_j} | c = {{Defof|Inner Product Norm}} }} {{eqn | r = \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n \innerprod {f_i} {f_j} | c = Linearity of [[Definition:Inner Product|Inner Product]] }} {{eqn | r = \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n \begin {cases} \innerprod {f_i} {f_i} & i = j \\ 0 & i \ne j \end {cases} | c = The $f_i$ are pairwise [[Definition:Orthogonal (Hilbert Space)|orthogonal]] }} {{eqn | r = \sum_{i \mathop = 1}^n \norm {f_i}^2 | c = {{Defof|Inner Product Norm}} }} {{end-eqn}} {{qed}}	1
To show that $\struct {G^S, +_G', \circ}_R$ is an [[Definition:Module|$R$-module]], we verify the following: $\forall f, g, \in G^S, \forall \lambda, \mu \in R$: :$(1): \quad \lambda \circ \paren {f +_G' g} = \paren {\lambda \circ f} +_G' \paren {\lambda \circ g}$ :$(2): \quad \paren {\lambda +_R \mu} \circ f = \paren {\lambda \circ f} +_G \paren {\mu \circ f}$ :$(3): \quad \paren {\lambda \times_R \mu} \circ f = \lambda \circ \paren {\mu \circ f}$ === Criterion 1 === :$(1): \quad \lambda \circ \paren {f +_G' g} = \paren {\lambda \circ f} +_G' \paren {\lambda \circ g}$ Let $x \in S$. Then: {{begin-eqn}} {{eqn | l = \lambda \circ \paren {\map {\paren {f +_G' g} } x} | r = \lambda \circ \paren {\map f x +_G \map g x} | c = }} {{eqn | r = \paren {\lambda \circ \map f x} +_G \paren {\lambda \circ \map g x} | c = {{Defof|Module}} }} {{eqn | r = \paren {\map {\paren {\lambda \circ f} } x} +_G \paren {\map {\paren {\lambda \circ g} } x} | c = }} {{eqn | r = \map {\paren {\paren {\lambda \circ f} +_G' \paren {\lambda \circ g} } } x | c = }} {{end-eqn}} Thus $(1)$ holds. {{qed|lemma}} === Criterion 2 === : $(2): \quad \paren {\lambda +_R \mu} \circ f = \paren {\lambda \circ f} +_G \paren {\mu \circ f}$ Let $x \in S$. {{begin-eqn}} {{eqn | l = \map {\paren {\paren {\lambda +_R \mu} \circ f} } x | r = \paren {\lambda +_R \mu} \circ \paren {\map f x} | c = }} {{eqn | r = \paren {\lambda \circ \map f x} +_G \paren {\mu \circ \map f x} | c = }} {{eqn | r = \map {\paren {\lambda \circ f} } x +_G \map {\paren {\mu \circ f} } x | c = }} {{end-eqn}} Thus $(2)$ holds. {{qed|lemma}} === Criterion 3 === :$(3): \quad \paren {\lambda \times_R \mu} \circ f = \lambda \circ \paren {\mu \circ f}$ {{begin-eqn}} {{eqn | l = \map {\paren {\paren {\lambda \times_R \mu} \circ f} } x | r = \paren {\lambda \times_R \mu} \circ \paren {\map f x} | c = }} {{eqn | r = \lambda \circ \paren {\mu \circ \map f x} | c = }} {{eqn | r = \lambda \circ \paren {\map {\paren {\mu \circ f} } x} | c = }} {{eqn | r = \map {\paren {\lambda \circ \paren {\mu \circ f} } } x | c = }} {{end-eqn}} Thus $(3)$ holds. {{qed|lemma}} Hence the result. {{qed}}	1
Let $C_n$ be the [[Definition:Cauchy Matrix|Cauchy matrix]] of [[Definition:Order of Square Matrix|order $n$]] given by: :$C_n = \begin{bmatrix} \dfrac 1 {x_1 + y_1} & \dfrac 1 {x_1 + y_2 } & \cdots & \dfrac 1 {x_1 + y_n} \\ \dfrac 1 {x_2 + y_1} & \dfrac 1 {x_2 + y_2 } & \cdots & \dfrac 1 {x_2 + y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_m + y_1} & \dfrac 1 {x_m + y_2 } & \cdots & \dfrac 1 {x_m + y_n} \\ \end{bmatrix}$ Let $C_n^{-1}$ be its [[Definition:Inverse Matrix|inverse]], from [[Inverse of Cauchy Matrix]]: :$b_{i j} = \dfrac {\displaystyle \prod_{k \mathop = 1}^n \paren {x_j + y_k} \paren {x_k + y_i} } {\displaystyle \paren {x_j + y_i} \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne j} } \paren {x_j - x_k} } \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne i} } \paren {y_i - x_k} } }$ The sum of all the [[Definition:Element of Matrix|elements]] of $C_n^{-1}$ is: :$\displaystyle \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} = \sum_{k \mathop = 1}^n x_k + \sum_{k \mathop = 1}^n y_k$	1
Let $S$ be the [[Definition:Set|set]] defined as: :$S = \set {\pi + q: q \in \Q}$ From [[Rational Numbers are Countably Infinite]], $\Q$ is [[Definition:Countable Set|countable]]. Therefore $S$ is also [[Definition:Countable Set|countable]]. From [[Pi is Irrational|$\pi$ is Irrational]]: :$\pi \in \R \setminus \Q$ It follows from [[Rational Addition is Closed]] that: :$\forall q \in \Q: \pi + q \in \R \setminus \Q$ and so: :$S \subseteq \R \setminus \Q$ From [[Rationals plus Irrational are Everywhere Dense in Irrationals]], it follows that $S$ is [[Definition:Everywhere Dense|everywhere dense]] in $\R \setminus \Q$. Thus we have constructed a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] $S$ of $\R \setminus \Q$ which is [[Definition:Everywhere Dense|everywhere dense]] in $\R \setminus \Q$. The result follows by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $S$ be an [[Definition:Orthonormal Subset|orthonormal subset]] of $H$. Then there exists a [[Definition:Basis (Hilbert Space)|basis]] for $H$ that contains $S$ as a subset.	1
=== Prime Modulus === $\struct {\Z_m, +, \times}$ is a [[Definition:Commutative Ring with Unity|commutative ring with unity]] by definition. From [[Reduced Residue System under Multiplication forms Abelian Group]], $\struct {\Z'_m, \times}$ is an [[Definition:Abelian Group|abelian group]]. $\Z'_m$ consists of all the elements of $\Z_m$ [[Definition:Coprime Integers|coprime]] to $m$. Now when $m$ is [[Definition:Prime Number|prime]], we have, from [[Reduced Residue System Modulo Prime]]: :$\Z'_m = \set {\eqclass 1 m, \eqclass 2 m, \ldots, \eqclass {m - 1} m}$ That is: :$\Z'_m = \Z_m \setminus \set {\eqclass 0 m}$ where $\setminus$ denotes [[Definition:Set Difference|set difference]]. Hence the result. {{qed|lemma}} === Composite Modulus === Now suppose $m \in \Z: m \ge 2$ is [[Definition:Composite Number|composite]]. From [[Ring of Integers Modulo Composite is not Integral Domain]], $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]]. From [[Field is Integral Domain]] $\struct {\Z_m, +, \times}$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
:$+ : \struct {R \times R, d_p} \to \struct{R,d}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]].	1
Let $\mathbf a, \mathbf b, \mathbf c, \mathbf d$ be [[Definition:Vector (Linear Algebra)|vectors]] in a [[Definition:Vector Space|vector space]] $\mathbf V$ of [[Definition:Dimension of Vector Space|$3$ dimensions]]: {{begin-eqn}} {{eqn | l = \mathbf a | r = a_1 \mathbf e_1 + a_2 \mathbf e_2 + a_3 \mathbf e_3 }} {{eqn | l = \mathbf b | r = b_1 \mathbf e_1 + b_2 \mathbf e_2 + b_3 \mathbf e_3 }} {{eqn | l = \mathbf c | r = c_1 \mathbf e_1 + c_2 \mathbf e_2 + c_3 \mathbf e_3 }} {{eqn | l = \mathbf d | r = d_1 \mathbf e_1 + d_2 \mathbf e_2 + d_3 \mathbf e_3 }} {{end-eqn}} where $\left({\mathbf e_1, \mathbf e_2, \mathbf e_3}\right)$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\mathbf V$. Let $\mathbf a \times \mathbf b$ denote the [[Definition:Vector Cross Product|vector cross product]] of $\mathbf a$ with $\mathbf b$. Let $\mathbf a \cdot \mathbf b$ denote the [[Definition:Dot Product|dot product]] of $\mathbf a$ with $\mathbf b$. Then: :$\left({\mathbf a \times \mathbf b}\right) \cdot \left({\mathbf c \times \mathbf d}\right) = \left({\mathbf a \cdot \mathbf c}\right) \left({\mathbf b \cdot \mathbf d}\right) - \left({\mathbf a \cdot \mathbf d}\right) \left({\mathbf b \cdot \mathbf c}\right)$	1
Using the operation [[Row Operation to Clear First Column of Matrix]], $\mathbf A$ is converted to $\mathbf B$, which will be in the form: :$\begin{bmatrix} 0 & \cdots & 0 & 1 & b_{1, j + 1} & \cdots & b_{1 n} \\ 0 & \cdots & 0 & 0 & b_{2, j + 1} & \cdots & b_{2 n} \\ \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & 0 & b_{m, j + 1} & \cdots & b_{m n} \\ \end{bmatrix}$ If some [[Definition:Zero Row or Column|zero rows]] have appeared, do some further [[Definition:Elementary Row Operation|elementary row operations]], that is row interchanges, to put them at the bottom. We then address our attention to the [[Definition:Submatrix|submatrix]]: :$\begin{bmatrix} b_{2, j + 1} & b_{2, j + 2} & \cdots & b_{2 n} \\ b_{3, j + 1} & b_{3, j + 2} & \cdots & b_{3 n} \\ \vdots & \vdots & \ddots & \vdots \\ b_{m, j + 1} & b_{m, j + 2} & \cdots & b_{m n} \\ \end{bmatrix}$ and perform the same operation on that. This results in the [[Definition:Submatrix|submatrix]] being transformed into the form: :$\begin{bmatrix} 1 & c_{2, j + 2} & \cdots & c_{2 n} \\ 0 & c_{3, j + 2} & \cdots & c_{3 n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & c_{m, j + 2} & \cdots & c_{m n} \\ \end{bmatrix}$ Again, we process the [[Definition:Submatrix|submatrix]]: :$\begin{bmatrix} c_{3, j + 2} & \cdots & c_{3 n} \\ \vdots & \ddots & \vdots \\ c_{m, j + 2} & \cdots & c_{m n} \\ \end{bmatrix}$ Thus we progress, until the entire [[Definition:Matrix|matrix]] is in [[Definition:Echelon Form|echelon form]]. {{Qed}}	1
{{ProofWanted}} [[Category:Implicit Functions]] 4t3eiy6pzrf811n7rr0487hxp0tgrea	1
Let $m \in \Z: m \ge 2$. Let $\struct {\Z_m, +, \times}$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. Then: :$m$ is [[Definition:Prime Number|prime]] {{iff}}: :$\struct {\Z_m, +, \times}$ is a [[Definition:Field (Abstract Algebra)|field]].	1
{{ProofWanted}} [[Category:Bases of Vector Spaces]] sor4sfol7raf8l3pr4u27v7w22abozr	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $T \in B_0 \left({H}\right)$ be a [[Definition:Compact Operator|compact operator]]. Let $\lambda \in \sigma_p \left({T}\right), \lambda \ne 0$ be a nonzero [[Definition:Eigenvalue|eigenvalue]] of $T$. Then the [[Definition:Eigenspace|eigenspace]] for $\lambda$ has [[Definition:Finite|finite]] [[Definition:Dimension (Hilbert Space)|dimension]].	1
From [[Kernel is G-Module]], $\ker \left({f}\right)$ is a $G$-submodule of $V$. From [[Image is G-Module]], $\operatorname{Im} \left({f}\right)$ is a $G$-submodule of $V'$. By the definition of [[Definition:Reducible Linear Representation|irreducible]]: :$\ker \left({f}\right) = \left\{{0}\right\}$ or: :$\ker \left({f}\right) = V$ {{explain|Link to a result which shows this. While it does indeed follow from the definition, it would be useful to have a page directly demonstrating this.}} If $\ker \left({f}\right) = V$ then by definition: :$f \left({v}\right) = 0$ for all $v \in V$ Let $\ker \left({f}\right) = \left\{{0}\right\}$. Then from [[Linear Transformation is Injective iff Kernel Contains Only Zero]]: :$f$ is [[Definition:Injection|injective]]. {{explain|Establish whether the above result (which discusses linear transformations on $R$-modules, not $G$ modules) can be directly applied. If so, amend its wording so as to make this clear.}} It also follows that: :$\operatorname{Im} \left({f}\right) = V'$ {{Explain|Prove this}} Thus $f$ is [[Definition:Surjection|surjective]] and [[Definition:Injection|injective]]. Thus by definition $f$ is a [[Definition:Bijection|bijection]] and thence an [[Definition:Module Isomorphism|isomorphism]]. {{qed}} {{namedfor|Issai Schur|cat = Schur}} [[Category:Representation Theory]] bythnhbim7dkufkqe8z7ggoqxlq6p70	1
Let $M_a = \struct {X, \norm {\, \cdot \, }_a}$ and $M_b = \struct {X, \norm {\, \cdot \,}_b}$ be [[Definition:Normed Vector Space|normed vector spaces]]. Let $\sequence {x_n}_{n \mathop \in \N}$ be an [[Definition:Convergent Sequence in Normed Vector Space|convergent sequence]] in $M_a$. Suppose, $\norm {\, \cdot \, }_a$ and $\norm {\, \cdot \,}_b$ are [[Definition:Equivalence of Norms|equivalent norms]], i.e. $\norm {\, \cdot \, }_a \sim \norm {\, \cdot \,}_b$. Then $\sequence {x_n}_{n \mathop \in \N}$ is also [[Definition:Convergent Sequence in Normed Vector Space|convergent]] in $M_b$.	1
Let $x_1, x_2 \in A$. Let $a \in A$ be a [[Definition:Star Convex Set/Star Center|star center]] of $A$. By [[Definition:Star Convex Set|definition of star convex set]], it follows that for all $t \in \left[{0 \,.\,.\, 1}\right]$, we have $t x_1 + \left({1 - t}\right) a, t x_2 + \left({1 - t}\right) a \in A$. Define two [[Definition:Path (Topology)|paths]] $\gamma_1, \gamma_2: t \in \left[{0 \,.\,.\, 1}\right] \to A$ by $\gamma_1 \left({t}\right) = t x_1 + \left({1 - t}\right) a$, and $\gamma_2 \left({t}\right) = t a + \left({1 - t}\right) x_2$. As $\gamma_2 \left({t}\right) = \left({1 - t}\right) x_2 + \left({1 - \left({1 - t}\right) }\right) a$, and $\left({1 - t}\right) \in \left[{0 \,.\,.\, 1}\right]$, it follows that $\gamma_2 \left({t}\right) \in A$. Note that $\gamma_1 \left({0}\right) = x_1$, $\gamma_1 \left({1}\right) = \gamma_2 \left({0}\right) = a$, and $\gamma_2 \left({1}\right) = x_2$. Define $\gamma: \left[{0 \,.\,.\, 1}\right] \to A$ as the [[Definition:Concatenation (Topology)|concatenation]] $\gamma_1 * \gamma_2$. Then $\gamma$ is a path in $A$ joining $x_1$ and $x_2$, so $A$ is [[Definition:Path-Connected Set (Topology)|path-connected]]. {{qed}}	1
Let $\sequence {x_n} $ be a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in $\struct {R, \norm {\,\cdot\,} }$. Then: :$\exists K \in \R_{\gt 0} : \forall n : \norm {x_n} \le K$ Then $\forall n, m \in \N$: {{begin-eqn}} {{eqn | l = \map d { x_n , x_m } | r = \norm {x_n - x_m} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | o = \le | r = \norm {x_n} + \norm {x_m} | c = [[Properties of Norm on Division Ring/Norm of Difference|Norm of Difference]] }} {{eqn | o = \le | r = K + K | c = {{Defof|Bounded Sequence in Normed Division Ring}} }} {{eqn | r = 2 K }} {{end-eqn}} Hence the [[Definition:Sequence|sequence]] $\sequence {x_n} $ is [[Definition:Bounded Sequence in Metric Space|bounded]] by $2 K$ in the [[Definition:Metric Space|metric space]] $\struct {R, d}$.	1
Let $S$ be a system of [[Definition:Simultaneous Linear Equations|simultaneous linear equations]]: :$\displaystyle \forall i \in \set {1, 2, \ldots, m}: \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$ Let $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ denote the [[Definition:Augmented Matrix of Simultaneous Linear Equations|augmented matrix]] of $S$. Let $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$ be obtained from $\begin {pmatrix} \mathbf A & \mathbf b \end {pmatrix}$ by means of an [[Definition:Elementary Row Operation|elementary row operation]]. Let $S'$ be the system of [[Definition:Simultaneous Linear Equations|simultaneous linear equations]] of which $\begin {pmatrix} \mathbf A' & \mathbf b' \end {pmatrix}$ is the [[Definition:Augmented Matrix of Simultaneous Linear Equations|augmented matrix]]. Then $S$ and $S'$ are [[Definition:Equivalent Systems of Simultaneous Linear Equations|equivalent]].	1
By the definition of the [[Definition:Metric Induced by Norm on Division Ring|metric induced by a norm]] then: :a sequence $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {R, \norm {\, \cdot \,} }$ {{iff}} $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Metric Space|Cauchy sequence]] in $\struct {R, d}$. So $\mathcal C$ is the set of [[Definition:Cauchy Sequence in Metric Space|Cauchy sequences]] in $\struct {R, d}$. Let $\sim$ be the [[Equivalence Relation on Cauchy Sequences|equivalence relation]] on $\mathcal C$ defined by: :$\displaystyle \sequence {x_n} \sim \sequence {y_n} \iff \lim_{n \mathop \to \infty} \map d {x_n, y_n} = 0$ Let $\tilde {\mathcal C} = \mathcal C / \sim$ denote the set of [[Definition:Equivalence Class|equivalence classes]] under $\sim$. For $\sequence {x_n} \in \mathcal C$, let $\eqclass {x_n} {}$ denote the [[Definition:Equivalence Class|equivalence class]] containing $\sequence {x_n}$. === [[Quotient of Cauchy Sequences is Metric Completion/Lemma 1|Lemma 1]] === {{:Quotient of Cauchy Sequences is Metric Completion/Lemma 1}}{{qed|lemma}} Let $\tilde d: \tilde {\mathcal C} \times \tilde {\mathcal C} \to \R_{\ge 0}$ be the [[Completion Theorem (Metric Space)/Lemma 2|metric]] defined by: :$\displaystyle \map {\tilde d} {\eqclass {x_n} {}, \eqclass {y_n} {} } = \lim_{n \mathop \to \infty} \map d {x_n, y_n}$ By [[Completion Theorem (Metric Space)|Completion of a Metric Space]] then: :$\struct {\tilde {\mathcal C}, \tilde d}$ is the [[Definition:Completion (Metric Space)|metric completion]] of $\struct {R, d}$. === [[Quotient of Cauchy Sequences is Metric Completion/Lemma 2|Lemma 2]] === {{:Quotient of Cauchy Sequences is Metric Completion/Lemma 2}}{{qed|lemma}} Let $\tilde \phi: R \to \tilde {\mathcal C}$ be the [[Definition:Mapping|mapping]] defined by: :$\map {\tilde \phi} a = \eqclass {a, a, a, \dotsc} {}$ where $\eqclass {a, a, a, \dotsc} {}$ denotes the [[Definition:Equivalence Class|equivalence class]] containing the constant [[Definition:Sequence|sequence]] $\sequence {a, a, a, \dotsc}$. By [[Completion Theorem (Metric Space)|Completion of a Metric Space]] then: :$\map {\tilde \phi} R$ is a [[Definition:Everywhere Dense|dense subset]] of $\tilde {\mathcal C}$. By [[Quotient of Cauchy Sequences is Metric Completion/Lemma 1|Lemma 1]] then: :$\forall \sequence {x_n} \in \mathcal C: \sequence {x_n} + \mathcal N = \eqclass {x_n} {}$ In particular: :$\forall a \in R: \sequence {a, a, a, \dotsc} + \mathcal N = \eqclass {a, a, a, \dotsc} {}$ That is: :$\forall a \in R: \map \phi a = \map {\tilde \phi} a$ Hence $\phi = \tilde \phi$. The result follows. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A$ be an [[Definition:Idempotent Operator|idempotent operator]]. Then $\Rng A = \map \ker {I - A}$.	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. From [[Closed Ball in Normed Division Ring is Closed Ball in Induced Metric]], $\map { {B_\epsilon}^-} a$ is the [[Definition:Closed Ball|closed $\epsilon$-ball of $a$]] in the [[Definition:Metric Space|metric space]] $\struct{R,d}$. From [[Center is Element of Closed Ball]]: :$a \in \map { {B_\epsilon}^-} a$ {{qed}} [[Category:Normed Division Rings]] njelynv9t36p3joi480cdvqbcsryu6v	1
Let $\struct {R, \norm{\,\cdot\,}}$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]] with [[Definition:Ring Zero|zero]] $0_R$ and [[Definition:Unity of Ring|unity]] $1_R$. Let $\OO$ be the [[Definition:Valuation Ring Induced by Non-Archimedean Norm|valuation ring induced]] by the [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$, that is: :$\OO = \set {x \in R : \norm{x} \le 1}$ Then $\OO$ is a [[Definition:Subring|subring]] of $R$: :with a [[Definition:Ring with Unity|unity]]: $1_R$ :in which there are no [[Definition:Proper Zero Divisor|(proper) zero divisors]], that is: :::$\forall x, y \in \OO: x \circ y = 0_R \implies x = 0_R \text{ or } y = 0_R$	1
Let $T = \left({S, \tau}\right)$ be a [[Definition:Discrete Space|discrete]] [[Definition:Topological Space|topological space]]. Then: :$T$ is [[Definition:Separable Space|separable]] {{iff}} $S$ is [[Definition:Countable Set|countable]].	1
Let $\tau$ be the [[Definition:Topology Induced by Metric|topology induced by the metric]] $d$. Then: :$\struct {R, \tau}$ is a [[Definition:Topological Division Ring|topological division ring]].	1
:$\norm {\, \cdot \,}_1$ is [[Definition:Well-Defined Mapping|well-defined]]. That is, :$(1): \quad \forall \eqclass {x_n}{}: \lim_{n \mathop \to \infty} \norm{x_n}$ exists. :$(2): \quad \displaystyle \forall \eqclass {x_n}{}, \eqclass {y_n}{} \in \mathcal C \,\big / \mathcal N: \eqclass {x_n}{} = \eqclass {y_n}{} \implies \lim_{n \mathop \to \infty} \norm{x_n} = \lim_{n \mathop \to \infty} \norm{y_n}$	1
Let $\struct {G, +_G}$ be an [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e_G$. Let $\struct {R, +_R, \circ_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}_R$ be the [[Definition:Trivial Module|trivial $R$-module]], such that: :$\forall \lambda \in R: \forall x \in G: \lambda \circ x = e_G$ Then unless $R$ is a [[Definition:Ring with Unity|ring with unity]] and $G$ contains only one element, this is ''not'' a [[Definition:Unitary Module|unitary module]].	1
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] where $S$ is a [[Definition:Countable Set|countable set]]. Then $T$ is a [[Definition:Separable Space|separable space]].	1
For ongoing brevity in this proof, the term '''[[Definition:Non-Unity Variant of Echelon Matrix|non-unity echelon matrix]]''' will be used to refer to this [[Definition:Non-Unity Variant of Echelon Matrix|variant echelon matrix]] in which the [[Definition:Leading Coefficient|leading coefficients]] are not necessarily equal to $1$. === $(1)$ implies $(2)$ === Let $\mathbf A$ be an [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 1|non-unity echelon matrix by definition $1$]]. Then, apart from [[Definition:Zero Row or Column|zero rows]], each [[Definition:Row of Matrix|row]] starts with strictly more [[Definition:Zero (Number)|zeroes]] than the one before it. {{AimForCont}} there exist adjacent [[Definition:Row of Matrix|rows]] in $\mathbf A$ of the form: :$\begin {pmatrix} 0 & 0 & \cdots & 0 & x_1 & x_2 & \cdots \\ 0 & 0 & \cdots & 0 & y_1 & y_2 & \cdots \\ \end {pmatrix}$ where $y_1 \ne 0$. If $x_1 \ne 0$, then the $2$nd of these [[Definition:Row of Matrix|rows]] starts with the same number of [[Definition:Zero (Number)|zeroes]] as the [[Definition:Row of Matrix|row]] before it. If $x_1 = 0$, then the $2$nd of these [[Definition:Row of Matrix|rows]] starts with fewer [[Definition:Zero (Number)|zeroes]] as the [[Definition:Row of Matrix|row]] before it. In both cases, this [[Definition:Contradiction|contradicts]] our definition of a [[Definition:Non-Unity Variant of Echelon Matrix|non-unity echelon matrix]]. That is, there there exist no adjacent [[Definition:Row of Matrix|rows]] in $\mathbf A$ of the form: :$\begin {pmatrix} 0 & 0 & \cdots & 0 & x_1 & x_2 & \cdots \\ 0 & 0 & \cdots & 0 & y_1 & y_2 & \cdots \\ \end {pmatrix}$ where $y_1 \ne 0$. That is $\mathbf A$ is a [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 2|non-unity echelon matrix by definition $2$]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Let $\mathbf A$ be a [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 2|non-unity echelon matrix by definition $2$]]. Then by definition there exist no adjacent [[Definition:Row of Matrix|rows]] in $\mathbf A$ of the form: :$\begin {pmatrix} 0 & 0 & \cdots & 0 & x_1 & x_2 & \cdots \\ 0 & 0 & \cdots & 0 & y_1 & y_2 & \cdots \\ \end {pmatrix}$ where $y_1 \ne 0$. Suppose [[Definition:Row of Matrix|row]] $k$ does not start with a [[Definition:Sequence|sequence]] of [[Definition:Zero (Number)|zeroes]]. Then it cannot start with strictly more [[Definition:Zero (Number)|zeroes]] than the previous [[Definition:Row of Matrix|row]] unless $k = 1$, in which case there is no previous [[Definition:Row of Matrix|row]]. Hence criterion $1$ of [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 1|definition $1$]] is satisfied. Suppose [[Definition:Row of Matrix|row]] $k$, where $k > 1$, is not a [[Definition:Zero Row or Column|zero row]]. Let its [[Definition:Leading Coefficient of Matrix|leading coefficient]] be in [[Definition:Column of Matrix|column]] $r$. Then the [[Definition:Leading Coefficient of Matrix|leading coefficient]] of [[Definition:Row of Matrix|row]] $k - 1$ must be in [[Definition:Column of Matrix|column]] $s$ where $s < r$. That is, [[Definition:Row of Matrix|row]] $k$ starts with strictly more [[Definition:Zero (Number)|zeroes]] than [[Definition:Row of Matrix|row]] $k - 1$. Hence criterion $2$ of [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 1|definition $1$]] is satisfied. Suppose [[Definition:Row of Matrix|row]] $k$ is a [[Definition:Zero Row or Column|zero row]] such that $k < m$. Then by definition [[Definition:Row of Matrix|row]] $k + 1$ cannot have a [[Definition:Leading Coefficient of Matrix|leading coefficient]]. So [[Definition:Row of Matrix|row]] $k + 1$ and all [[Definition:Row of Matrix|rows]] following must be [[Definition:Zero Row or Column|zero rows]]. Hence criterion $3$ of [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 1|definition $1$]] is satisfied. Thus $\mathbf A$ is a [[Definition:Echelon Matrix/Echelon Form/Non-Unity Variant/Definition 1|non-unity echelon matrix by definition $1$]]. {{qed}}	1
Let $T = \struct {S, \tau_{\bar p} }$ be a [[Definition:Countable Excluded Point Topology|countable excluded point space]]. Then $T$ is a [[Definition:Separable Space|separable space]].	1
Let $\left({k, +, \cdot}\right)$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $k$ of [[Definition:Finite Dimensional Module|finite]] [[Definition:Dimension (Linear Algebra)|dimension]]. Let $\left({G, *}\right)$ be a [[Definition:Finite Group|finite group]]. There is a one-to-one correspondence between [[Definition:Linear Group Action|linear group actions]] of $G$ on $V$ and [[Definition:Linear Representation|linear representations]] of $G$ in $V$, as follows: Let $\phi : G \times V \to V$ be a [[Definition:Group Action|group action]]. Let $\rho : G \to \operatorname{Sym}(V)$ be a [[Definition:Permutation Representation|permutation representation]] of $G$ on $V$. The following are [[Definition:Logically Equivalent|equivalent]]: :$(1): \quad$ $\rho$ is the [[Definition:Permutation Representation/Group Action|permutation representation associated to]] $\phi$ :$(2): \quad$ $\phi$ is the [[Definition:Group Action/Permutation Representation|group action associated to]] $\rho$ If this is the case, the following are [[Definition:Logically Equivalent|equivalent]]: :$(1): \quad$ $\rho$ is a [[Definition:Linear Representation|linear representation]] :$(2): \quad$ $\phi$ is a [[Definition:Linear Group Action|linear group action]]	1
Let $\left({G, +_G, \circ}\right)_R$ and $\left({H, +_H, \circ}\right)_R$ be [[Definition:Module|$R$-modules]]. Let $\phi: G \to H$ and $\psi: G \to H$ be [[Definition:Linear Transformation|linear transformations]]. Let $\phi +_H \psi$ be the operation on $H^G$ induced by $+_H$ as defined in [[Definition:Induced Structure|Induced Structure]]. Then $\phi +_H \psi: G \to H$ is a [[Definition:Linear Transformation|linear transformation]].	1
=== [[Cauchy-Bunyakovsky-Schwarz Inequality/Inner Product Spaces|Semi-Inner Product Spaces]] === {{:Cauchy-Bunyakovsky-Schwarz Inequality/Inner Product Spaces}}	1
Let $T = \struct {S, \tau}$ be a [[Definition:Countable Discrete Topology|countable discrete topological space]]. Then $T$ is [[Definition:Separable Space|separable]].	1
Let $z_1$ and $z_2$ be represented by the [[Definition:Point|points]] $A = \tuple {x_1, y_1}$ and $B = \tuple {x_2, y_2}$ respectively in the [[Definition:Complex Plane|complex plane]]. Let $z$ be an arbitrary [[Definition:Point|point]] on $L$ represented by the [[Definition:Point|point]] $P$. :[[File:Line-in-Complex-Plane-through-Two-Points.png|500px]] From [[Geometrical Interpretation of Complex Addition]]: {{begin-eqn}} {{eqn | l = OA + AP | r = OP | c = }} {{eqn | ll= \leadsto | l = z_1 + AP | r = z | c = }} {{eqn | ll= \leadsto | l = AP | r = z - z_1 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = OA + AB | r = OB | c = }} {{eqn | ll= \leadsto | l = z_1 + AB | r = z_2 | c = }} {{eqn | ll= \leadsto | l = AB | r = z_2 - z_1 | c = }} {{end-eqn}} As $AP$ and $AB$ are collinear: :$AP = t AB$ and so: :$z - z_1 = t \paren {z_2 - z_1}$ The given expressions follow after algebra. {{qed}}	1
Let $\mathbb J$ be an [[Definition:Open Real Interval|open interval]] of the [[Definition:Real Number Line|real number line]] $\R$. Let $\map \DD {\mathbb J}$ be the set of all [[Definition:Differentiable on Interval|differentiable real functions]] on $\mathbb J$. Then $\struct {\map \DD {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of the [[Definition:Vector Space|$\R$-vector space]] $\struct {\R^{\mathbb J}, +, \times}_\R$.	1
Then for all $i$, $0 \le i \le n$: :$\norm x^i \le \max \set {\norm x^n , 1}$	1
Let $\mathbf A = \sqbrk a_{m n}$, $\mathbf B = \sqbrk b_{m n}$ and $\mathbf C = \sqbrk c_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $R$. Then: {{begin-eqn}} {{eqn | l = \paren {\mathbf A + \mathbf B} + \mathbf C | r = \paren {\sqbrk a_{m n} + \sqbrk b_{m n} } + \sqbrk c_{m n} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{eqn | r = \sqbrk {a + b}_{m n} + \sqbrk c_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {\paren {a + b} + c}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {a + \paren {b + c} }_{m n} | c = {{Ring-axiom|A1}} }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {b + c}_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk a_{m n} + \paren {\sqbrk b_{m n} + \sqbrk c_{m n} } | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \mathbf A + \paren {\mathbf B + \mathbf C} | c = Definition of $\mathbf A$, $\mathbf B$ and $\mathbf C$ }} {{end-eqn}} {{qed}}	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime]] $p$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Integer|integers]] such that: :$\forall n: x_{n + 1} \equiv x_n \pmod {p^n}$ Then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]] in $\struct {\Q, \norm {\,\cdot\,}_p}$.	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $K$. Let $B$ be a [[Definition:Basis of Vector Space|basis]] of $V$. Let $G$ be a [[Definition:Generator of Vector Space|generator]] of $V$. Then there exists an [[Definition:Injection|injection]] from $B$ to $G$.	1
Let $T = \struct {S, \tau_{\bar p} }$ be an [[Definition:Uncountable Excluded Point Topology|uncountable excluded point space]]. Then $T$ is not [[Definition:Separable Space|separable]].	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Unity of Ring|unity]] $1_R$. Then $\norm {\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] {{iff}}: :$\forall n \in \N_{>0}: \norm {n \cdot 1_R} \le 1$ where: :$n \cdot 1_R = \underbrace {1_R + 1_R + \dotsb + 1_R}_{\text {$n$ times} }$	1
A [[Definition:Full Rook Matrix|full rook matrix]] is [[Definition:Invertible Matrix|invertible]].	1
If $T$ is a [[Definition:Proper Subset|proper subset]] of $S$, then $\struct {T, +_T, \circ_T}_K$ is a '''proper (vector) subspace''' of $\struct {S, +, \circ}_K$.	1
Let: :$V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 2} & x_1^{n - 1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 2} & x_2^{n - 1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 2} & x_n^{n - 1} \end{vmatrix}$ Let $\map f x$ be '''any''' [[Definition:Monic Polynomial|monic polynomial]] of [[Definition:Degree of Polynomial|degree]] $n - 1$: :$\ds \map f x = x^{n - 1} + \sum_{i \mathop = 0}^{n - 2} a_i x^i$ Apply [[Effect of Elementary Row Operations on Determinant|elementary column operations]] to $V_n$ repeatedly to show: :$V_n = W$ where: :$ W = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 2} & \map f {x_1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 2} & \map f {x_2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 2} & \map f {x_n} \end{vmatrix}$ Select a specific degree $n - 1$ monic polynomial: :$\ds \map f x = \prod_{k \mathop = 1}^{n - 1} \paren {x - x_k}$ The selected polynomial is zero at all values $x_1, \ldots, x_{n - 1}$. Then the last column of $W$ is all zeros except the entry $\map f {x_n}$. Expand $\map \det W$ by cofactors along the last column to prove: {{begin-eqn}} {{eqn | n = 1 | l = V_n | r = \map f {x_n} V_{n - 1} | c = [[Expansion Theorem for Determinants]] for columns }} {{eqn | r = V_{n - 1} \prod_{k \mathop = 1}^{n - 1} \paren {x_n - x_k} }} {{end-eqn}} For $n \ge 2$, let $\map P n$ be the statement: :$\ds V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {x_j - x_i}$ [[Definition:Mathematical Induction|Mathematical induction]] will be applied. === Basis for the Induction === By definition, determinant $V_1 = 1$. To prove $\map P 2$ is true, use equation $(1)$ with $n = 2$: :$V_2 = \paren {x_2 - x_1} V_1$ This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Step === This is the [[Definition:Induction Step|induction step]]: Let $\map P n$ is be assumed true. We are to prove that $\map P {n + 1}$ is true. As follows: {{begin-eqn}} {{eqn | l = V_{n + 1} | r = V_n \prod_{k \mathop = 1}^n \paren {x_{n + 1} - x_k} | c = from $(1)$, setting $n \to n + 1$ }} {{eqn | r = \prod_{1 \mathop \le m \mathop < k \mathop \le n} \paren {x_k - x_m} \prod_{k \mathop = 1}^n \paren {x_{n + 1} - x_k} | c = [[Vandermonde Determinant/Proof 4#Induction Hypothesis|Induction hypothesis]] with new indexing symbols: $i \to m, j \to k$ }} {{eqn | r = \prod_{1 \mathop \le i \mathop < j \mathop \le n + 1} \paren {x_j - x_i} | c = simplifying }} {{end-eqn}} Thus $\map P {n + 1}$ has been shown to be true. The induction is complete. {{qed}}	1
Let $I_1\subseteq I_2\subseteq I_3\subseteq \ldots$ be an [[Definition:Ascending Chain Condition|ascending chain of ideals]]. Build $\displaystyle I = \bigcup_{i \mathop = 1}^\infty I_i$. $I$ is an ideal. Since $R$ is a [[Definition:Principal Ideal Domain|principal ideal domain]], $I = \left({a}\right)$ for some $a \in R$. Now, since $a \in I$, there is some $n$ such that $a \in I_n$. Thus $\left({a}\right) \subseteq I_n$. By definition $I_n \subset I = \left({a}\right)$, and so $I_n = I$. Thus: : $\forall m \ge n: I_m = I$ {{MissingLinks|Still some links to be made, I'll get to it later.}} {{qed}} {{Proofread}} [[Category:Ideal Theory]] [[Category:Principal Ideal Domains]] 74ixkrt5agvvk4h1rgv6paap3v38g51	1
We prove the [[Definition:Norm on Vector Space|norm axioms]]. === Positive Definiteness === As $\mathbf 0 = \tuple {0, \ldots, 0}$, it follows that: :$\norm {\mathbf 0} = \sqrt {0^2 + \ldots + 0^2} = 0$ Suppose instead that $\norm {\mathbf v} = 0$ for some $\mathbf v = \tuple {v_1, \ldots, v_n}$ with $v_1, \ldots, v_n \in \R$. As $\displaystyle \sqrt {\sum_{i \mathop = 1}^n v_n^2 } = 0$, it follows from squaring both sides that: :$\displaystyle \sum_{i \mathop = 1}^n v_n^2 = 0$ From [[Even Power is Non-Negative]], it follows that for all $i$: $v_i = 0$. Hence $\mathbf v = \mathbf 0$. {{qed|lemma}} === Positive homogeneity === {{ProofWanted|I refuse to believe this is not covered somewhere, but cannot find it}} === Triangle Inequality === Follows from [[Triangle Inequality for Vectors in Euclidean Space]]. {{qed}} [[Category:Examples of Norms]] [[Category:Euclidean Space]] h0t1y2y2hps3keus5ajty6mep8p8nd5	1
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Normed Division Ring|normed division ring]] Let $x \in \R$. Let $\sequence {x_n}$ be the [[Definition:Real Sequence|sequence in $\R$]] defined as $x_n = x^n$. Then: :$\norm x < 1$ {{iff}} $\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]].	1
{{ProofWanted}} [[Category:Implicit Functions]] s18r12x1y8fezruex4l4o9glbp06lpn	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,}}$. By [[Convergent Sequence is Cauchy Sequence/Normed Division Ring|Convergent Sequence is Cauchy Sequence in Normed Division Ring]], $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {R, \norm {\,\cdot\,}}$. By [[Cauchy Sequence is Bounded/Normed Division Ring|Cauchy Sequence in Normed Division Ring is Bounded]], $\sequence {x_n}$ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in $\struct {R, \norm {\,\cdot\,}}$. {{qed}}	1
Follows directly from [[Scalar Product with Inverse]]. {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]], and denote by $\left\langle{\cdot, \cdot}\right\rangle_H$ and $\left\langle{\cdot, \cdot}\right\rangle_K$ their respective [[Definition:Inner Product|inner products]]. Let $U: H \to K$ be a [[Definition:Surjection|surjection]] such that: :$\forall g,h \in H: \left\langle{g, h}\right\rangle_H = \left\langle{Ug, Uh}\right\rangle_K$ Then $U$ is a [[Definition:Linear Mapping|linear map]], and hence an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]].	1
=== Sufficient Condition === That $\mathbf 0 \in \map \ker T$ follows from [[Kernel of Linear Transformation contains Zero Vector]]. That $\map \ker T$ is a [[Definition:Singleton|singleton]] follows from the definition of [[Definition:Injection|injection]]. {{qed|lemma}} === Necessary Condition === Let $\map \ker T = \set {\mathbf 0}$. Consider: :$\map T {\mathbf x} = \mathbf b$ where $\mathbf b$ is in the [[Definition:Codomain of Mapping|codomain]] of $T$. Let this equation have a solution: :$\mathbf x = \mathbf x_1 \in \mathbf V$ Suppose $\mathbf x = \mathbf x_2 \in \mathbf V$ is also a solution. [[Definition:Clearly|Clearly]]: :$\map T {\mathbf x_1} = \map T {\mathbf x_2}$ Observe that: {{begin-eqn}} {{eqn | l = \map T {\mathbf x_1} | r = \mathbf b }} {{eqn | ll= \land | l = \map T {\mathbf x_2} | r = \mathbf b }} {{eqn | ll= \leadsto | l = \map T {\mathbf x_1} - \map T {\mathbf x_2} | r = \mathbf b - \mathbf b }} {{eqn | r = \mathbf 0' }} {{eqn | ll= \leadsto | l = \map T {\mathbf x_1 - \mathbf x_2} | r = \mathbf 0' | c = {{Defof|Linear Transformation on Vector Space}} }} {{eqn | ll= \leadsto | l = \paren {\mathbf x_1 - \mathbf x_2} | o = \in | r = \map \ker T | c = {{Defof|Kernel of Linear Transformation}} }} {{eqn | ll= \leadsto | l = \mathbf x_1 - \mathbf x_2 | r = \mathbf 0 | c = {{Defof|Set Equality}}: recall $\map \ker T = \set {\mathbf 0}$ }} {{eqn | ll= \leadsto | l = \mathbf x_1 | r = \mathbf x_2 }} {{end-eqn}} As $\mathbf x_1, \mathbf x_2$ were arbitrary: :$\forall \mathbf x_1,\mathbf x_2 \in \mathbf V: \map T {\mathbf x_1} = \map T {\mathbf x_2} \implies \mathbf x_1 = \mathbf x_2$ and the result follows from the definition of [[Definition:Injection|injectivity]]. {{qed}} [[Category:Linear Transformations]] n7es60exp1uazpyjxotyj4ili2emz1h	1
Let $C$ be a [[Definition:Contour (Complex Plane)|contour]]. Let $f: \Img C \to \C$ be a [[Definition:Continuous Complex Function|continuous complex function]], where $\Img C$ denotes the [[Definition:Image of Contour (Complex Plane)|image]] of $C$. Then: :$\displaystyle \size {\int_C \map f z \rd z} \le \max_{z \mathop \in \Img C} \size {\map f z} \map L C$ where $\map L C$ denotes the [[Definition:Length of Contour (Complex Plane)|length]] of $C$.	1
From [[Isomorphism from R^n via n-Term Sequence]], they are both [[Definition:R-Algebraic Structure Isomorphism|isomorphic]] to the [[Definition:Module on Cartesian Product|$R$-module $R^n$]]. {{qed}}	1
By definition of [[Definition:Similar Matrices|similar matrices]] :$\exists \mathbf P: \mathbf P^{-1} \mathbf A \mathbf P = \mathbf B$ where $\mathbf P$ is an [[Definition:Invertible Matrix|invertible matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Thus it remains to show that: :$\map \tr {\mathbf P^{-1} \mathbf A \mathbf P} = \map \tr {\mathbf A}$ {{ProofWanted}}	1
By definition of [[Definition:Vector Cross Product/Definition 2|vector cross product]]: :$\mathbf a \times \mathbf b = \left\vert{\mathbf a}\right\vert \, \left\vert{\mathbf b}\right\vert \sin \theta \hat {\mathbf n}$ where: :$\left\vert{a}\right\vert$ denotes the [[Definition:Vector Length|length]] of $\mathbf a$ :$\theta$ denotes the [[Definition:Angle|angle]] from $\mathbf a$ to $\mathbf b$, measured in the [[Definition:Positive Direction|positive direction]] :$\hat {\mathbf n}$ is the [[Definition:Unit Vector|unit vector]] [[Definition:Perpendicular|perpendicular]] to both $\mathbf a$ and $\mathbf b$ in the direction according to the [[Definition:Right Hand Rule|right hand rule]]. As $\hat {\mathbf n}$ is the [[Definition:Unit Vector|unit vector]]: :$\left\vert{\left({\left\vert{\mathbf a}\right\vert \, \left\vert{\mathbf b}\right\vert \sin \theta \hat {\mathbf n} }\right)}\right\vert = \left\vert{\mathbf a}\right\vert \, \left\vert{\mathbf b}\right\vert \sin \theta$ By [[Area of Parallelogram]], the [[Definition:Area|area]] of the [[Definition:Parallelogram|parallelogram]] equals the product of one of its [[Definition:Base of Parallelogram|bases]] and the associated [[Definition:Altitude of Parallelogram|altitude]]. Let $\mathbf a$ denote the [[Definition:Base of Parallelogram|base]] of the [[Definition:Parallelogram|parallelogram]]. Then its [[Definition:Altitude of Parallelogram|altitude]] is $\left\vert{\mathbf b}\right\vert \sin \theta$. The result follows. {{qed}}	1
Let $\mathbf A = \sqbrk a_n, \mathbf B = \sqbrk b_n$ be [[Definition:Upper Triangular Matrix|upper triangular matrices]] of [[Definition:Order of Square Matrix|order]] $n$. Let $\mathbf C = \mathbf A \mathbf B$. Then :$(1): \quad$ the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf C$ are given by: ::::$\forall j \in \closedint 1 n: c_{j j} = a_{j j} b_{j j}$ :::That is, the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf C$ are those of the factor matrices multiplied together. :$(2): \quad$ The matrix $\mathbf C$ is itself [[Definition:Upper Triangular Matrix|upper triangular]]. The same applies if both $\mathbf A$ and $\mathbf B$ are [[Definition:Lower Triangular Matrix|lower triangular matrices]].	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\struct {G, +_G, \circ}_K$ be a [[Definition:Vector Space|$K$-vector space]]. Let $M$ and $N$ be [[Definition:Finite Dimensional Vector Space|finite-dimensional]] [[Definition:Vector Subspace|subspaces]] of $G$. Then the [[Definition:Sum of Vector Subspaces|sum]] $M + N$ and [[Definition:Intersection|intersection]] $M \cap N$ are [[Definition:Finite Dimensional Vector Space|finite-dimensional]], and: :$\map \dim {M + N} + \map \dim {M \cap N} = \map \dim M + \map \dim N$	1
We have: : [[Compact Complement Topology is Second-Countable]] : [[Second-Countable Space is Separable]] Hence the result. {{qed}}	1
Follows directly from [[Matrix Equivalence is Equivalence Relation]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \paren {\mathbf a + \mathbf b} \times \paren {\mathbf a + \mathbf b} | r = \mathbf a \times \mathbf a + \mathbf a \times \mathbf b + \mathbf b \times \mathbf a + \mathbf b \times \mathbf b | c = [[Vector Cross Product Operator is Bilinear]] }} {{eqn | l = 0 | r = 0 + \mathbf a \times \mathbf b + \mathbf b \times \mathbf a + 0 | c = [[Cross Product of Vector with Itself is Zero]] }} {{eqn | l = \mathbf a \times \mathbf b | r = -\paren {\mathbf b \times \mathbf a} | c = simplifying }} {{end-eqn}} {{qed}}	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $F$. Let $\BB$ be a [[Definition:Generator of Vector Space|generator]] for $V$ containing $m$ [[Definition:Element|elements]]. Then: :$\map {\dim_F} V \le m$ where $\map {\dim_F} V$ is the [[Definition:Dimension of Vector Space|dimension]] of $V$.	1
Let $p, q \ge 1$ be [[Definition:Real Number|real numbers]]. Let $\ell^p$ denote the [[Definition:P-Sequence Space|$p$-sequence space]]. Let $\norm {\mathbf x}_p$ denote the [[Definition:P-Norm|$p$-norm]]. Let $\mathbf x = \sequence {x_n} \in \ell^{p q}$. Suppose further that $\mathbf x^p = \sequence { {x_n}^p} \in \ell^q$. Then: :$\norm {\mathbf x^p}_q = \norm {\mathbf x}_{p q}^p$	1
=== Definition 1 implies Definition 2 === Let $\norm {\,\cdot\,} : R \to \R_{\ge 0}$ be a [[Definition:Norm on Division Ring|norm on a division ring]] satisfying: {{begin-axiom}} {{axiom | n = \text N 4 | lc= Ultrametric Inequality: | q = \forall x, y \in R | ml= \norm {x + y} | mo= \le | mr= \max \set {\norm x, \norm y} }} {{end-axiom}} It remains only to show that $\norm {\,\cdot\,}$ satisfies $(\text N 1)$ and $(\text N 2)$. This follows from the definition of a [[Definition:Norm on Division Ring|norm on a division ring]]. {{qed|lemma}} === Definition 2 implies Definition 1 === Let $\norm{\,\cdot\,} : R \to \R_{\ge 0}$ satisfy the [[Definition:Non-Archimedean Norm Axioms|non-Archimedean norm axioms]]: $(\text N 1)$, $(\text N 2)$ and $(\text N 4)$. To show that $\norm{\,\cdot\,}$ is a [[Definition:Norm on Division Ring|norm on a division ring]] satisfying $(\text N 4)$, it remains to show that $\norm{\,\cdot\,}$ satisfies: {{begin-axiom}} {{axiom | n = \text N 3 | lc= Triangle Inequality: | q = \forall x, y \in R | ml= \norm {x + y} | mo= \le | mr= \norm x + \norm y }} {{end-axiom}} Let $x, y \in R$. {{WLOG}}, suppose $\norm x \le \norm y$. From [[Definition:Non-Archimedean Norm Axioms|non-Archimedean norm axiom $(\text N 1)$ : Positive Definiteness]]: :$0 \le \norm x$ Then: {{begin-eqn}} {{eqn | l = \norm{x + y} | o = \le | r = \max \set {\norm x, \norm y} | c = [[Definition:Non-Archimedean Norm Axioms|Non-Archimedean Norm Axiom $(\text N 4)$ : Ultrametric Inequality]] }} {{eqn | r = \norm y | c = as $\norm x \le \norm y$ by assumption }} {{eqn | o = \le | r = \norm x + \norm y | c = as $0 \le \norm x$ }} {{end-eqn}} The result follows. {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf a \times \mathbf b | r = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ a_i & a_j & a_k \\ b_i & b_j & b_k \\ \end{vmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = -\begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ b_i & b_j & b_k \\ a_i & a_j & a_k \\ \end{vmatrix} | c = [[Determinant with Rows Transposed]] }} {{eqn | r = -\left({\mathbf b \times \mathbf a}\right) | c = {{Defof|Vector Cross Product}} }} {{end-eqn}} {{qed}}	1
Let $\left({R, +_R, \circ_R}\right)$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $G$ be the [[Definition:Trivial Group|trivial group]]. Let $\left({G, +_G, \circ}\right)_R$ be the [[Definition:Null Module|null module]]. Then $\left({G, +_G, \circ}\right)_R$ is a [[Definition:Module|module]].	1
Follows directly from [[Combination Theorem for Cauchy Sequences/Product Rule|Product Rule for Normed Division Ring Sequences]], setting :$\sequence {y_n} := \sequence {x_n}$ and: :$\sequence {x_n} := \tuple {a, a, a, \ldots}$ {{qed}}	1
{{begin-eqn}} {{eqn | l = \left({c \mathbf u}\right) \cdot \mathbf v | r = \sum_{i \mathop = 1}^n \left({c u_i}\right) v_i | c = {{Defof|Dot Product}} }} {{eqn | r = \sum_{i \mathop = 1}^n c \left({ u_i v_i }\right) | c = [[Real Multiplication is Associative]] }} {{eqn | r = c \sum_{i \mathop = 1}^n u_i v_i | c = [[Real Multiplication Distributes over Real Addition]] }} {{eqn | r = c \left({\mathbf u \cdot \mathbf v}\right) | c = {{Defof|Dot Product}} }} {{end-eqn}} {{qed}}	1
We do a case analysis. === $(1): \quad x \ge 0, y \ge 0$ === {{begin-eqn}} {{eqn | l = x | o = \ge | r = 0 | c = }} {{eqn | l = y | o = \ge | r = 0 | c = }} {{eqn | ll= \leadsto | l = \size {x + y} | r = x + y | c = }} {{eqn | r = \size x + \size y | c = }} {{end-eqn}} {{qed|lemma}} === $(2): \quad x \le 0, y \le 0$ === {{begin-eqn}} {{eqn | l = x | o = \le | r = 0 | c = }} {{eqn | l = y | o = \le | r = 0 | c = }} {{eqn | ll= \leadsto | l = \size {x + y} | r = -x - y | c = }} {{eqn | r = \size x + \size y | c = }} {{end-eqn}} {{qed|lemma}} === $(3): \quad x \ge 0, y \le 0$ === We have that $\size x = x$ and $\size y = -y$. In this case we show: :$\size {x + y} \le \max \set {\size x, \size y}$ Let $\size x \le \size y$. Then: {{begin-eqn}} {{eqn | l = x | o = \le | r = -y }} {{eqn | ll= \leadsto | l = y | o = \le | r = y + x | c = as $x \ge 0$ }} {{eqn | o = \le | r = 0 | c = }} {{eqn | ll= \leadsto | l = \size {x + y} | r = -\paren {x + y} }} {{eqn | o = \le | r = -y }} {{eqn | r = \size y }} {{end-eqn}} Let $\size x \ge \size y$. Then: {{begin-eqn}} {{eqn | l = x | r = -y | o = \ge }} {{eqn | ll= \leadsto | l = x | o = \ge | r = x + y | c = as $y \le 0$ }} {{eqn | o = \ge | r = 0 }} {{eqn | ll= \leadsto | l = \size {x + y} = | r = x + y }} {{eqn | o = \le | r = x }} {{eqn | r = \size x }} {{end-eqn}} We have $\max \set {a, b} \le a + b$ for [[Definition:Positive Real Number|positive real numbers]] $a$ and $b$. The result follows by taking $a = \size x$ and $b = \size y$. {{qed|lemma}} === $(4): \quad x \le 0, y \ge 0$ === Follows by symmetry from the case $(3)$. {{qed}}	1
This is a special case of [[Module on Cartesian Product is Module]]: :$\struct {R^n, +, \circ}_R$ is an [[Definition:Module|$R$-module]] where $n = 1$. {{qed}}	1
By [[Isomorphism from R^n via n-Term Sequence]], $V$ is [[Definition:R-Algebraic Structure Isomorphism|isomorphic]] to the [[Definition:Vector Space on Cartesian Product|$K$-vector space $K^{\map \dim V}$]]. Thus: :$\size V = \size {K^{\map \dim V} }$ By [[Cardinality of Cartesian Space]]: :$\size {K^{\map \dim V} } = \size K^{\map \dim V}$ Thus: :$\size V = \size K^{\map \dim V}$ {{qed}} [[Category:Vector Spaces]] 9shqkcdbnuaf4de5rjyxlcmkv8y4r6g	1
The [[Definition:Integer|integers]] $\Z$ form a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
Let $M$ and $N$ be distinct [[Definition:Straight Line|straight lines]] through [[Definition:The Plane|the plane]] through the [[Definition:Origin|origin]]. Let $\operatorname{pr}_{M, N}$ be the [[Definition:Projection (Analytic Geometry)|projection on $M$ along $N$]]. $M$ and $N$ are respectively the [[Definition:Codomain of Mapping|codomain]] and [[Definition:Kernel of Linear Transformation|kernel]] of $\operatorname{pr}_{M, N}$. {{explain|As the kernel is a concept defined in relation to a homomorphism, it needs to be clarified what homomorphism is being considered.}} :$\operatorname{pr}_{M, N} \left({x}\right) = x \iff x \in M$ If $M$ is the [[Definition:X-Axis|$x$-axis]] and $N$ is the [[Definition:Y-Axis|$y$-axis]], then $\operatorname{pr}_{M, N} \left({\lambda_1, \lambda_2}\right) = \left({\lambda_1, 0}\right)$. If $M$ is the [[Definition:Y-Axis|$y$-axis]] and $N$ is the [[Definition:X-Axis|$x$-axis]], then $\operatorname{pr}_{M, N} \left({\lambda_1, \lambda_2}\right) = \left({0, \lambda_2}\right)$. Any such [[Definition:Projection (Analytic Geometry)|projection]] is a [[Definition:Linear Operator|linear operator]].	1
Let $G$ be an [[Definition:Module|$R$-module]]. $b$ is a [[Definition:Linear Combination of Subset|linear combination of $\varnothing$]] {{iff}}: : $b = e_G$	1
This follows by [[Principle of Mathematical Induction|induction]] from {{Module-axiom|2}}, as follows. For all $m \in \N_{>0}$, let $\map P m$ be the [[Definition:Proposition|proposition]]: :$\ds \paren {\sum_{k \mathop = 1}^m \lambda_k} \circ x = \sum_{k \mathop = 1}^m \paren {\lambda_k \circ x}$ === Basis for the Induction === $\map P 1$ is true, as this just says: :$\lambda_1 \circ x = \lambda_1 \circ x$ This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P n$ is true, where $n \ge 1$, then it logically follows that $\map P {n + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\ds \paren {\sum_{k \mathop = 1}^n \lambda_k} \circ x = \sum_{k \mathop = 1}^n \paren {\lambda_k \circ x}$ Then we need to show: :$\ds \paren {\sum_{k \mathop = 1}^{n + 1} \lambda_k} \circ x = \sum_{k \mathop = 1}^{n + 1} \paren {\lambda_k \circ x}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \paren {\sum_{k \mathop = 1}^{n + 1} \lambda_k} \circ x | r = \paren {\sum_{k \mathop = 1}^n \lambda_k + \lambda_{n + 1} } \circ x | c = }} {{eqn | r = \paren {\sum_{k \mathop = 1}^n \lambda_k \circ x} + \lambda_{n + 1} \circ x | c = {{Module-axiom|2}} }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {\lambda_k \circ x} + \lambda_{n + 1} \circ x | c = [[Product with Sum of Scalar#Induction Hypothesis|Induction hypothesis]] }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \paren {\lambda_k \circ x} | c = }} {{end-eqn}} So $\map P n \implies \map P {n + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\ds \forall m \in \N_{>0}: \paren {\sum_{k \mathop = 1}^m \lambda_k} \circ x = \sum_{k \mathop = 1}^m \paren {\lambda_k \circ x}$ {{qed}}	1
Let $\mathbf A$ be a [[Definition:Real Matrix|real]] [[Definition:Symmetric Matrix|symmetric matrix]]. Then we have: {{begin-eqn}} {{eqn | l = \left[{\mathbf A}\right]^\dagger_{ij} | r = \overline{\left[{\mathbf A}\right]_{ji} } | c = {{Defof|Hermitian Conjugate}} }} {{eqn | r = \left[{\mathbf A}\right]_{ji} | c = $\mathbf A$ is real }} {{eqn | r = \left[{\mathbf A}\right]_{ij} | c = $\mathbf A$ is symmetric }} {{end-eqn}} So: :$\mathbf A = \mathbf A^\dagger$ Thus, by definition, $\mathbf A$ is [[Definition:Hermitian Matrix|Hermitian]]. {{qed}} [[Category:Linear Algebra]] [[Category:Hermitian Matrices]] gxilvi28qzxafxfs9ovlunu7gvs8ees	1
Because $\sequence {x_n} $ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], it is [[Definition:Bounded Sequence in Normed Division Ring|bounded]] by [[Cauchy Sequence is Bounded]]. Suppose $\norm {x_n} \le K_1$ for $n = 1, 2, 3, \ldots$. Because $\sequence {y_n} $ is a is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], it is [[Definition:Bounded Sequence in Normed Division Ring|bounded]] by [[Cauchy Sequence is Bounded]]. Suppose $\norm {y_n} \le K_2$ for $n = 1, 2, 3, \ldots$. Let $K = \max \set {K_1, K_2}$. Then both sequences are [[Definition:Bounded Sequence in Normed Division Ring|bounded]] by $K$. Let $\epsilon > 0$ be given. Then $\dfrac \epsilon {2K} > 0$. Since $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], we can find $N_1$ such that: :$\forall n, m > N_1: \norm {x_n - x_m} < \dfrac \epsilon {2 K}$ Similarly, $\sequence {y_n} $ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], we can find $N_2$ such that: :$\forall n, m > N_2: \norm {y_n - y_m} < \dfrac \epsilon {2 K}$ Now let $N = \max \set {N_1, N_2}$. Then if $n, m > N$, both the above inequalities will be true. Thus $\forall n, m > N$: {{begin-eqn}} {{eqn | l = \norm {x_n y_n - x_m y_m} | r = \norm {x_n y_n - x_n y_m + x_n y_m - x_m y_m} | c = }} {{eqn | o = \le | r = \norm {x_n y_n - x_n y_m} + \norm {x_n y_m - x_m y_m} | c = {{NormAxiom|3}} }} {{eqn | r = \norm {x_n \paren {y_n - y_m } } + \norm {\paren {x_n - x_m} y_m} | c = }} {{eqn | o = \le | r = \norm {x_n} \cdot \norm {y_n - y_m} + \norm {x_n - x_m} \cdot \norm {y_m} | c = {{NormAxiom|2}} }} {{eqn | o = \le | r = K \cdot \norm {y_n - y_m} + \norm {x_n - x_m} \cdot K | c = as both [[Definition:Cauchy Sequence in Normed Division Ring|sequences]] are [[Definition:Bounded Sequence in Normed Division Ring|bounded]] by $K$ }} {{eqn | o = \le | r = K \cdot \dfrac \epsilon {2K} + \dfrac \epsilon {2K} \cdot K | c = }} {{eqn | r = \dfrac \epsilon 2 + \dfrac \epsilon 2 | c = }} {{eqn | r = \epsilon | c = }} {{end-eqn}} Hence: :$\sequence {x_n y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. {{qed}}	1
Let $A$ be a [[Definition:Unique Factorization Domain|unique factorization domain (UFD)]]. Then $A$ is [[Definition:Integrally Closed|integrally closed]].	1
From [[Ring is Ideal of Itself]], $R$ is a [[Definition:Right Ideal|right ideal]]. From [[Right Ideal is Right Module over Ring]], $\struct {R, +, \times}$ is a [[Definition:Right Module|right module]] over $\struct {R, +, \times}$. {{qed}}	1
We have the definition of the [[Definition:Power to Real Number|power to a real number]]: :$\paren {n + 1}^{1/n} = \map \exp {\dfrac 1 n \, \map \ln {n + 1} }$ For $n >= 1$ then $n + 1 \le 2 n$. Hence: {{begin-eqn}} {{eqn | l = \frac 1 n \, \map \ln {n + 1} | o = \le | r = \frac 1 n \, \map \ln {2 n} | c = [[Logarithm is Strictly Increasing]] }} {{eqn | o = }} {{eqn | r = \frac 1 n \paren {\ln 2 + \ln n} | c = [[Logarithm on Positive Real Numbers is Group Isomorphism]] }} {{eqn | o =}} {{eqn | r = \frac {\ln 2} n + \frac 1 n \ln n | c = }} {{end-eqn}} By [[Powers Drown Logarithms]]: :$\displaystyle \lim_{n \mathop \to \infty} \frac 1 n \ln n = 0$ By [[Sequence of Reciprocals is Null Sequence]]: :$\displaystyle \lim_{n \mathop \to \infty} \frac 1 n = 0$ By [[Combined Sum Rule for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\frac {\ln 2} n + \frac 1 n \ln n} = \ln 2 \cdot 0 + 0 = 0$ By the [[Squeeze Theorem for Real Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {n + 1}^{1/n} = 0$ Hence: :$\displaystyle \lim_{n \mathop \to \infty} \paren {n + 1}^{1/n} = \exp 0 = 1$ and the result follows. {{qed}} [[Category:Characterisation of Non-Archimedean Division Ring Norms]] 4gtwgr743fe4lo5o5j9ncxmmwnpxg30	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $E = \left\{{e_n: n \in \N}\right\}$ be a [[Definition:Countable|countably infinite]] [[Definition:Orthonormal Subset|orthonormal subset]] of $H$. Then, for all $h \in H$, one has the inequality: :$\displaystyle \sum_{n \mathop = 1}^\infty \left|{\left\langle{h, e_n}\right\rangle}\right|^2 \le \left\|{h}\right\|^2$	1
Let $n_1, n_2, \ldots, n_r$ be [[Definition:Positive Integer|positive integers]] such that $n_i \perp n_j$ for all $i \ne j$ (that is, $\gcd \set {n_i, n_j} = 1$). Then the [[Definition:Simultaneous Linear Congruences|system of linear congruences]]: :$x \equiv b_1 \pmod {n_1}$ :$x \equiv b_2 \pmod {n_2}$ :$\ldots$ :$x \equiv b_r \pmod {n_r}$ has a simultaneous solution which is unique [[Definition:Congruence (Number Theory)|modulo]] $\displaystyle \prod_{i \mathop = 1}^r n_i$.	1
Let $p_1 := \tuple {x_1, y_1}$ and $p_2 := \tuple {x_2, y_2}$ be [[Definition:Point|points]] in a [[Definition:Cartesian Plane|cartesian plane]]. Let $\LL$ be the [[Definition:Straight Line|straight line]] passing through $p_1$ and $p_2$. Then $\LL$ can be described by the equation: :$\dfrac {y - y_1} {x - x_1} = \dfrac {y_2 - y_1} {x_2 - x_1}$ or: :$\dfrac {x - x_1} {x_2 - x_1} = \dfrac {y - y_1} {y_2 - y_1}$	1
Note that: $\left({R, +, \circ}\right)$ is a [[Definition:Ring (Abstract Algebra)|ring]] by assumption. $\left({R, +}\right)$ is an [[Definition:Abelian Group|abelian group]] by the definition of a [[Definition:Ring (Abstract Algebra)|ring]]. Let us verify the [[Definition:Module Axioms|module axioms]]: {{begin-axiom}} {{axiom | n = 1 | q = \forall x, y, z \in R | m = x \circ \left({y + z}\right) = \left({x \circ y}\right) + \left({x \circ z}\right) }} {{axiom | n = 2 | q = \forall x, y, z \in R | m = \left({x + y}\right) \circ z = \left({x \circ z}\right) + \left({y \circ z}\right) }} {{axiom | n = 3 | q = \forall x, y, z \in R | m = \left({x \circ y}\right) \circ z = x \circ \left({y \circ z}\right) }} {{end-axiom}} Axiom $(1)$ and $(2)$ follow from [[Definition:Ring (Abstract Algebra)|distributivity]] of $\circ$. Axiom $(3)$ follows from [[Definition:Ring (Abstract Algebra)|associativity]] of $\circ$. {{qed|lemma}} Assume now that $\left({R, +, \circ}\right)$ has a [[Definition:Unity of Ring|unity]], $1_R$. For $\left({R, +, \circ}\right)_R$ to be [[Definition:Unitary Module|unitary]], it must satisfy the additional axiom: {{begin-axiom}} {{axiom | n = 4 | q = \forall x \in R | m = 1_R \circ x = x }} {{end-axiom}} The axiom follows from the definition of a [[Definition:Unity of Ring|unity]]. {{qed}}	1
{{ProofWanted|Lots of work to be done yet in this area}}	1
Let $w_1, w_2, \ldots, w_n$ and $z_1, z_2, \ldots, z_n$ be arbitrary [[Definition:Complex Number|complex numbers]]. Take the [[Binet-Cauchy Identity]]: :$\displaystyle \paren {\sum_{i \mathop = 1}^n a_i c_i} \paren {\sum_{j \mathop = 1}^n b_j d_j} = \paren {\sum_{i \mathop = 1}^n a_i d_i} \paren {\sum_{j \mathop = 1}^n b_j c_j} + \sum_{1 \mathop \le i \mathop < j \mathop \le n} \paren {a_i b_j - a_j b_i} \paren {c_i d_j - c_j d_i}$ and set $a_i = w_i, b_j = \overline {z_j}, c_i = \overline {w_i}, d_j = z_j $. This gives us: {{begin-eqn}} {{eqn | l = \paren {\sum_{i \mathop = 1}^n w_i \overline {w_i} } \paren {\sum_{j \mathop = 1}^n \overline {z_j} z_j} | r = \paren {\sum_{i \mathop = 1}^n w_i z_i} \paren {\sum_{j \mathop = 1}^n \overline {z_j} \overline {w_j} } | c = }} {{eqn | o = | ro= + | r = \sum_{1 \mathop \le i \mathop < j \mathop \le n} \paren {w_i \overline {z_j} - w_j \overline {z_i} } \paren {\overline {w_i} z_j - \overline {w_j} z_i} | c = }} {{eqn | ll= \leadsto | l = \paren {\sum_{i \mathop = 1}^n w_i \overline {w_i} } \paren {\sum_{j \mathop = 1}^n \overline {z_j} z_j} | r = \paren {\sum_{i \mathop = 1}^n w_i z_i} \overline {\paren {\sum_{i \mathop = 1}^n w_i z_i} } | c = }} {{eqn | o = | ro= + | r = \sum_{1 \mathop \le i \mathop < j \mathop \le n} \paren {w_i \overline {z_j} - w_j \overline {z_i} } \overline {\paren {w_i \overline {z_j} - w_j \overline {z_i} } } | c = }} {{eqn | ll= \leadsto | l = \paren {\sum_{i \mathop = 1}^n \cmod {w_i}^2} \paren {\sum_{j \mathop = 1}^n \cmod {z_j}^2} | r = \cmod {\sum_{i \mathop = 1}^n w_i z_i}^2 + \sum_{1 \mathop \le i \mathop < j \mathop \le n} \cmod {w_i \overline {z_j} - w_j \overline {z_i} }^2 | c = [[Modulus in Terms of Conjugate]] }} {{eqn | ll= \leadsto | l = \paren {\sum_{i \mathop = 1}^n \cmod {w_i}^2} \paren {\sum_{j \mathop = 1}^n \paren {z_j}^2} | o = \ge | r = \paren {\sum_{i \mathop = 1}^n w_i z_i}^2 | c = as $\cmod {w_i \overline {z_j} - w_j \overline {z_i} }^2$ cannot be [[Definition:Negative Real Number|negative]] }} {{end-eqn}} Hence the result. {{qed}} {{Namedfor|Augustin Louis Cauchy|name2 = Karl Hermann Amandus Schwarz}}	1
{{begin-eqn}} {{eqn | l = \mathbf x | r = a \mathbf 1 + b \mathbf i + c \mathbf j + d \mathbf k | c = }} {{eqn | r = a \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + b \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} + c \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} + d \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} a & 0 \\ 0 & a \end{bmatrix} + \begin{bmatrix} b i & 0 \\ 0 & - b i \end{bmatrix} + \begin{bmatrix} 0 & c \\ -c & 0 \end{bmatrix} + \begin{bmatrix} 0 & d i \\ d i & 0 \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} a + b i & c + d i \\ -c + d i & a - b i \end{bmatrix} | c = }} {{end-eqn}} {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $N$ be an $R$-module. Let $\left({M_i}\right)_{i \mathop \in I}$ be a family of $R$-modules. Let $M = \displaystyle \prod_{i \mathop \in I} M_i$ be their [[Definition:Module Direct Product|direct product]]. Let $\left({\psi_i}\right)_{i \mathop \in I}$ be a family of $R$-module morphisms $N \to M_i$. Then there exists a unique morphism: :$\Psi: N \to M$ such that: : $\forall i: \psi_i = \pi_i \circ \Psi$ where $\pi_i: M \to M_i$ is the $i$th canonical projection.	1
For $1 \le k \le n$, let $e_k$ be the [[Definition:Elementary Row Operation|elementary row operation]] that [[Definition:Matrix Scalar Product|multiplies]] [[Definition:Row of Matrix|row]] $k$ of $\mathbf A$ by $\lambda$. By definition of the [[Definition:Matrix Scalar Product|scalar product]], $\lambda \mathbf A$ is obtained by [[Definition:Matrix Scalar Product|multiplying]] every [[Definition:Row of Matrix|row]] of $\mathbf A$ by $\lambda$. That is the same as applying $e_k$ to $\mathbf A$ for each of $k \in \set {1, 2, \ldots, n}$. Let $\mathbf E_k$ denote the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e_k$. By [[Determinant of Elementary Row Matrix/Scale Row|Determinant of Elementary Row Matrix: Scale Row]]: :$\map \det {\mathbf E_k} = \lambda$ Then we have: {{begin-eqn}} {{eqn | l = \lambda \mathbf A | r = \prod_{k \mathop = 1}^n \mathbf E_k \mathbf A | c = }} {{eqn | ll= \leadsto | l = \map \det {\lambda \mathbf A} | r = \map \det {\prod_{k \mathop = 1}^n \mathbf E_k \mathbf A} | c = }} {{eqn | r = \paren {\prod_{k \mathop = 1}^n \map \det {\mathbf E_k} } \map \det {\mathbf A} | c = [[Determinant of Matrix Product]] }} {{eqn | r = \paren {\prod_{k \mathop = 1}^n \lambda} \map \det {\mathbf A} | c = [[Determinant of Elementary Row Matrix/Scale Row|Determinant of Elementary Row Matrix: Scale Row]] }} {{eqn | r = \lambda^n \map \det {\mathbf A} | c = }} {{end-eqn}} {{qed}}	1
:$\forall z_1, z_2 \in \C: z_1 \times z_2 = -\paren {z_2 \times z_1}$	1
{{begin-eqn}} {{eqn | l = \nabla \left({f + g}\right) | r = \sum_{k \mathop = 1}^n \frac {\partial \left({f + g}\right)} {\partial x_k} \mathbf e_k | c = {{Defof|Gradient Operator}} }} {{eqn | r = \sum_{k \mathop = 1}^n \left({\frac {\partial f} {\partial x_k} \mathbf e_k + \frac {\partial g} {\partial x_k} \mathbf e_k}\right) | c = [[Linear Combination of Partial Derivatives]] }} {{eqn | r = \sum_{k \mathop = 1}^n \frac {\partial f} {\partial x_k} \mathbf e_k + \sum_{k \mathop = 1}^n \frac {\partial g} {\partial x_k} \mathbf e_k | c = }} {{eqn | r = \nabla f + \nabla g | c = }} {{end-eqn}} {{qed}}	1
Let $T = \left({S, \tau}\right)$ be a [[Definition:Countable Discrete Topology|countable discrete topological space]]. From [[Countable Discrete Space is Second-Countable]]: :$T$ is [[Definition:Second-Countable Space|second-countable]]. From [[Second-Countable Space is Separable]]: :$T$ is [[Definition:Separable Space|separable]]. {{qed}}	1
By definition of the [[Definition:P-adic Norm|$p$-adic norm]]: :$\norm p_p = \frac 1 p < 1$ By definition of the [[Definition:Absolute Value|absolute value]]: :$\size p = p > 1$ By definition of [[Definition:Equivalent Division Ring Norms/Open Unit Ball Equivalent|open unit ball equivalence]], $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. By [[Equivalence of Definitions of Equivalent Division Ring Norms]] and the definition of [[Definition:Equivalent Division Ring Norms/Topologically Equivalent|topologically equivalent norms]] then the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_p$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\size{\,\cdot\,}$. {{qed}}	1
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \map c {i_1, i_{n + 1} } = \sum_{i_n \mathop = 1}^{d_n} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_{n - 1} } {i_{n - 1}, i_n} \map {a_n} {i_n, i_{n + 1} }$ $\map P 1$ is trivially true, as this just says: :$\map c {i_1, i_2} = \map {a_1} {i_1, i_2}$ === Basis for the Induction === $\map P 2$ is the case: :$\displaystyle \map c {i_1, i_3} = \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3}$ which is the definition of [[Definition:Matrix Product (Conventional)|(conventional) matrix product]]. The [[Definition:Order of Matrix|order]] of $\mathbf C$ is $d_1 \times d_3$, while the [[Definition:Order of Matrix|orders]] of $\mathbf A_1$ and $\mathbf A_2$ are $d_1 \times d_2$ and $d_2 \times d_3$ respectively. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 2$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \map c {i_1, i_{k + 1} } = \sum_{i_k \mathop = 1}^{d_k} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_k} {i_k, i_{k + 1} }$ Then we need to show: :$\displaystyle \map c {i_1, i_{k + 2} } = \sum_{i_{k + 1} \mathop = 1}^{d_{k + 1} } \sum_{i_k \mathop = 1}^{d_k} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_k} {i_k, i_{k + 1} } \, \map {a_{k + 1} } {i_{k + 1}, i_{k + 2} }$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: Let $\displaystyle \mathbf C_1 := \prod_{j \mathop = 1}^k \mathbf A_j$. Let $\displaystyle \mathbf C := \prod_{j \mathop = 1}^{k + 1} \mathbf A_j = \mathbf C_1 \mathbf A_{k + 1}$. {{begin-eqn}} {{eqn | l = \map c {i_1, i_{k + 2} } | r = \sum_{i_{k + 1} \mathop = 1}^{d_{k + 1} } \map {c_1} {i_1, i_{k + 1} } \map {a_{k + 1} } {i_{k + 1}, i_{k + 2} } | c = {{Defof|Matrix Product (Conventional)|Matrix Product}} }} {{eqn | r = \sum_{i_{k + 1} \mathop = 1}^{d_{k + 1} } \paren {\sum_{i_k \mathop = 1}^{d_k} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_k} {i_k, i_{k + 1} } } \map {a_{k + 1} } {i_{k + 1}, i_{k + 2} } | c = [[Product of Finite Sequence of Matrices#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \sum_{i_{k + 1} \mathop = 1}^{d_{k + 1} } \sum_{i_k \mathop = 1}^{d_k} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_k} {i_k, i_{k + 1} } \map {a_{k + 1} } {i_{k + 1}, i_{k + 2} } | c = [[Associative Law of Addition]] }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore for all $n \in \N_{>0}$: :$\displaystyle \map c {i_1, i_{n + 1} } = \sum_{i_n \mathop = 1}^{d_n} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_{n - 1} } {i_{n - 1}, i_n} \map {a_n} {i_n, i_{n + 1} }$ {{qed}} [[Category:Conventional Matrix Multiplication]] k54na6jenrbhsne4qu3bxh3gvy39zz0	1
Let $\left({S, \ast_1, \ast_2, \ldots, \ast_n, \circ}\right)_R$ and $\left({T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}\right)_R$ be [[Definition:R-Algebraic Structure|$R$-algebraic structures]]. Then $\phi: S \to T$ is an '''$R$-algebraic structure epimorphism''' {{iff}}: : $(1): \quad \phi$ is a [[Definition:Surjection|surjection]] : $(2): \quad \forall k: k \in \left[{1 \,.\,.\, n}\right]: \forall x, y \in S: \phi \left({x \ast_k y}\right) = \phi \left({x}\right) \odot_k \phi \left({y}\right)$ : $(3): \quad \forall x \in S: \forall \lambda \in R: \phi \left({\lambda \circ x}\right) = \lambda \otimes \phi \left({x}\right)$ This definition also applies to [[Definition:Module|modules]], and also to [[Definition:Vector Space|vector spaces]].	1
Let $\left \langle {a_n} \right \rangle$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Then $\left \langle {J \left({a_n}\right)} \right \rangle$ is the [[Definition:Ordered Dual Basis|ordered basis of $G^{**}$ dual]] to the [[Definition:Ordered Basis|ordered basis of $G^*$ dual]] to $\left \langle {a_n} \right \rangle$. {{ProofWanted}} From this it follows that $J$ is an [[Definition:Module Isomorphism|isomorphism]].	1
This is a consequence of [[Vector Subspace of Real Vector Space]]. {{qed}}	1
=== [[Definition:Trace (Linear Algebra)/Matrix|Matrix]] === {{:Definition:Trace (Linear Algebra)/Matrix}} === [[Definition:Trace (Linear Algebra)/Linear Operator|Linear Operator]] === {{:Definition:Trace (Linear Algebra)/Linear Operator}} [[Category:Definitions/Linear Algebra]] [[Category:Definitions/Matrix Algebra]] iwd100re67e8wjvyj3pv3ss0y9xwzxv	1
=== Definition 1 equivalent to Definition 3 === {{begin-eqn}} {{eqn | o = | r = \map \Re {\overline {z_1} z_2} | c = {{Defof|Dot Product|subdef = Complex|index = 3}} }} {{eqn | r = \map \Re {\paren {x_1 - i y_1} {x_2 + i y_2} } | c = {{Defof|Complex Conjugate}} }} {{eqn | r = \map \Re {\paren {x_1 x_2 + y_1 y_2} + i \paren {x_1 y_2 - x_2 y_1} } | c = {{Defof|Complex Multiplication}} }} {{eqn | r = x_1 x_2 + y_1 y_2 | c = {{Defof|Real Part}} }} {{end-eqn}} {{qed|lemma}} === Definition 2 equivalent to Definition 3 === {{begin-eqn}} {{eqn | o = | r = \map \Re {\overline {z_1} z_2} | c = {{Defof|Dot Product|subdef = Complex|index = 3}} }} {{eqn | r = r_1 r_2 \map \cos {\theta_2 - \theta_1} | c = [[Complex Dot Product in Exponential Form]] }} {{eqn | r = \cmod {z_1} \, \cmod {z_2} \map \cos {\theta_2 - \theta_1} | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \cmod {z_1} \, \cmod {z_2} \cos \theta | c = where $\theta = \theta_2 - \theta_1$ is the angle between $z_1$ and $z_2$ }} {{end-eqn}} {{qed|lemma}} === Definition 1 equivalent to Definition 4 === {{begin-eqn}} {{eqn | o = | r = \frac {\overline {z_1} z_2 + z_1 \overline {z_2} } 2 | c = {{Defof|Dot Product|subdef = Complex|index = 4}} }} {{eqn | r = \frac {\paren {x_1 - i y_1} \paren {x_2 + i y_2} + \paren {x_1 + i y_1} \paren {x_2 - i y_2} } 2 | c = {{Defof|Complex Conjugate}} }} {{eqn | r = \frac {\paren {\paren {x_1 x_2 + y_1 y_2} + i \paren {x_1 y_2 - x_2 y_1} } + \paren {\paren {x_1 x_2 + y_1 y_2} + i \paren {-x_1 y_2 + x_2 y_1} } } 2 | c = {{Defof|Complex Multiplication}} }} {{eqn | r = x_1 x_2 + y_1 y_2 | c = after algebra }} {{end-eqn}} {{qed}}	1
{{ProofWanted|the definition [[Definition:Completion (Inner Product Space)]] may be justified}}	1
Let $\left({S, \le}\right)$ be a [[Definition:Boolean Algebra|Boolean algebra]]. Let $I$ be an [[Definition:Ideal (Order Theory)|ideal]] in $S$. Let $F$ be a [[Definition:Filter|filter]] on $S$. Let $I \cap F = \varnothing$. Then there exists a [[Definition:Prime Ideal (Order Theory)|prime ideal]] $P$ in $S$ such that: : $I \subseteq P$ and: : $P \cap F = \varnothing$	1
By definition, $\map \cl T$ is [[Definition:Closed Set under Closure Operator|closed]]. Let $C$ be closed. Let $T \subseteq C$. By the definition of [[Definition:Closure Operator|closure operator]], $\cl$ is $\subseteq$-[[Definition:Increasing Mapping|increasing]]. So: :$\map \cl T \subseteq \map \cl C$ Since $C$ is [[Definition:Closed Set under Closure Operator|closed]], $\map \cl C = C$. So: :$\map \cl T \subseteq C$ Thus $\map \cl T$ is the smallest [[Definition:Closed Set under Closure Operator|closed set]] containing $T$ as a [[Definition:Subset|subset]]. {{qed}} [[Category:Closure Operators]] [[Category:Set Closure is Smallest Closed Set]] rgb5hevh3a910j1evzkrqqrzjppg3ey	1
From [[Ring is Commutative iff Opposite Ring is Itself]], $\struct {R, +_R, \times_R}$ is its own [[Definition:Opposite Ring|opposite ring]]. From [[Right Module over Ring Induces Left Module over Opposite Ring]], $\struct{G, +_G, \circ'}$ is a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. {{qed}}	1
Let that $x \divides y$. Then by definition of [[Definition:Divisor of Ring Element|divisor]]: {{begin-eqn}} {{eqn | l = x \divides y | o = \leadsto | r = \exists t \in D: y = t x | c = {{Defof|Divisor of Ring Element}} }} {{eqn | o = \leadsto | r = y \in \ideal x | c = {{Defof|Principal Ideal of Ring}} }} {{eqn | o = \leadsto | r = \ideal y \subseteq \ideal x | c = {{Defof|Principal Ideal of Ring}}: $\ideal y$ is the smallest ideal containing $y$ }} {{end-eqn}} Conversely: {{begin-eqn}} {{eqn | l = \ideal y \subseteq \ideal x | o = \leadsto | r = y \in \ideal x | c = as $y \in \ideal y$ }} {{eqn | o = \leadsto | r = \exists t \in D: y = t x | c = {{Defof|Principal Ideal of Ring}} }} {{eqn | o = \leadsto | r = x \divides y | c = {{Defof|Divisor of Ring Element}} }} {{end-eqn}} So: :$x \divides y \iff \ideal y \subseteq \ideal x$ {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n} \in \map {\MM_G} {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A \circ \paren {-\mathbf A} | r = \sqbrk a_{m n} \circ \paren {-\sqbrk a_{m n} } | c = Definition of $\mathbf A$ }} {{eqn | r = \sqbrk a_{m n} \circ \sqbrk {a^{-1} }_{m n} | c = {{Defof|Negative Matrix|subdef = General Group}} }} {{eqn | r = \sqbrk {a \cdot \paren {a^{-1} } }_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \sqbrk e_{m n} | c = {{Defof|Inverse Element}} }} {{eqn | ll= \leadsto | l = \mathbf A \circ \paren {-\mathbf A} | r = \mathbf e | c = {{Defof|Zero Matrix over General Monoid}} }} {{end-eqn}} The result follows from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A1}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Associative Operation|associative]]. The result then follows directly from [[Associativity of Hadamard Product]]. {{qed}}	1
Define: :$\ds S = \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \paren {T - I}^n$ $S$ converges since $\norm {T - I} < 1$. We have that $\ds \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \norm {T - I}^n$ is the [[Definition:Newton-Mercator Series|Newton-Mercator Series]]. This converges since $\norm {T - I} < 1$. Hence the series for $S$ converges absolutely, and so $S$ is well defined. Using the series definition for the [[Properties of Matrix Exponential|matrix exponential]]: {{begin-eqn}} {{eqn | l = e^S | r = I + S + \frac 1 {2!} S^2 + \frac 1 {3!} S^3 + \cdots }} {{eqn | r = I + \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \paren {T - I}^n + \frac 1 {2!} \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \paren {T - I}^n}^2 + \frac 1 {3!} \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \paren {T - I}^n}^3 + \cdots }} {{eqn | ll= \leadsto | l = e^S | r = I + \paren {T - I} + c_2 \paren {T - I}^2 + c_3 \paren {T - I}^3 + \cdots | c = grouping terms by powers of $T - I$ }} {{eqn | r = T + c_2 \paren {T - I}^2 + c_3 \paren {T - I}^3 + \cdots }} {{end-eqn}} If $c_i = 0$ for $i \ge 2$, then $e^S = T$, and the result is shown. The [[Definition:Newton-Mercator Series|Newton-Mercator Series]] is a Taylor expansion for $\map \ln {1 + x}$. When combined with the [[Power Series Expansion for Exponential Function]], it gives: {{begin-eqn}} {{eqn | l = e^{\map \ln {1 + x} } | r = 1 + \map \ln {1 + x} + \frac 1 {2!} \paren {\map \ln {1 + x} }^2 + \frac 1 {3!} \paren {\map \ln {1 + x} }^3 + \cdots }} {{eqn | r = 1 + \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n x^n + \frac 1 {2!} \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n x^n}^2 + \frac 1 {3!} \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n x^n}^3 + \cdots }} {{eqn | r = 1 + x + c_2 x^2 + c_3 x^3 + \cdots | c = grouping terms by powers of $x$ }} {{end-eqn}} But $e^{\map \ln {1 + x} } = 1 + x$. Thus: :$1 + x = 1 + x + c_2 x^2 + c_3 x^3 + \cdots \implies c_i = 0$ for $i \ge 2$. {{qed}}	1
From the definition of [[Definition:Associate in Integral Domain|associate]], there exist $\map e X$ and $\map {e'} X$ \in $F \sqbrk X$ such that: :$\map d X = \map e X \cdot \map {d'} X$ :$\map {d'} X = \map {e'} X \cdot \map d X$ From [[Field is Integral Domain]], $F$ is an [[Definition:Integral Domain|integral domain]]. From [[Degree of Product of Polynomials over Integral Domain]] it follows that necessarily $\deg e = \deg e' = 0$, as $F$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. Thus for some $c, c' \in F$, it must be that $\map e X = c$ and $\map {e'} X = c'$. From the two equations above it follows that $c \cdot c' = 1_F$, where $1_F$ is the [[Definition:Unity of Field|unity]] of $F$. Hence, it follows that $c \ne 0_F$. The result follows. {{qed}}	1
Let $\left({G, +_G, \circ}\right)_R$ be an [[Definition:Module|$R$-module]]. Then $\left({G, +_G, \circ}\right)_R$ is a [[Definition:Submodule|submodule]] of itself.	1
:$\exists x \in \Z_{>0}: p \nmid x, x \ge \dfrac {p + 1} 2$	1
Hermitian [[Definition:Operation|operations]] have [[Definition:Real Number|real]] eigenvalues.	1
A direct corollary of [[Special Orthogonal Group is Subgroup of Orthogonal Group]]. {{qed}}	1
'''Vector calculus''' is the branch of [[Definition:Linear Algebra|linear algebra]] concerned with the application of the techniques of [[Definition:Calculus|calculus]] to [[Definition:Vector Space|vector spaces]].	1
:$G$ contains a [[Definition:Basis of Vector Space|basis]] for $E$.	1
Let $m\in M$. Let $[m]_{\mathcal A}$ be its [[Definition:Coordinate Vector|coordinate vector]] relative to $\mathcal A$, and similary for $\mathcal B$ and $\mathcal C$. On the one hand: {{begin-eqn}} {{eqn | l = [m]_{\mathcal A} | r = \mathbf M_{\mathcal A,\mathcal C} \cdot [m]_{\mathcal C} | c = [[Change of Coordinate Vector Under Change of Basis]] }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | l = [m]_{\mathcal A} | r = \mathbf M_{\mathcal A,\mathcal B} \cdot [m]_{\mathcal B} | c = [[Change of Coordinate Vector Under Change of Basis]] }} {{eqn | l = | r = \mathbf M_{\mathcal A,\mathcal B} \cdot \mathbf M_{\mathcal B,\mathcal C} \cdot [m]_{\mathcal C} | c = [[Change of Coordinate Vector Under Change of Basis]] }} {{end-eqn}} Thus $(\mathbf M_{\mathcal A,\mathcal C} - \mathbf M_{\mathcal A,\mathcal B} \cdot \mathbf M_{\mathcal B,\mathcal C}) \cdot [m]_{\mathcal C} = 0$ for all $m\in M$. Because $m$ is arbitrary, the result follows. {{explain|find a link for this}} {{qed}}	1
Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then: :$\forall \lambda \in \mathbb F: \lambda \circ \bszero = \bszero$ where $\bszero \in \mathbf V$ is the [[Definition:Zero Vector|zero vector]].	1
From the definition, a [[Definition:Noetherian Ring|Noetherian ring]] is also a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $f = a_n x^n + \cdots + a_1 x + a_0 \in A \sqbrk x$ be a [[Definition:Polynomial (Abstract Algebra)|polynomial]] over $x$. Let $I \subseteq A \sqbrk x$ be an [[Definition:Ideal of Ring|ideal]] of $A \sqbrk x$. We will show that $I$ is [[Definition:Finitely Generated Ideal of Ring|finitely generated]]. Let $f_1$ be an element of least [[Definition:Degree of Polynomial|degree in]] $I$, and let $\ideal {g_1, \ldots, g_r}$ denote the [[Definition:Ideal of Ring|ideal]] [[Definition:Generated Ideal of Ring|generated]] by the polynomials $g_1, \dotsc, g_r$. For $i \ge 1$, if $\ideal {f_1, \ldots, f_i} \ne I$, then choose $f_{i + 1}$ to be an element of minimal degree in $I \setminus \ideal {f_1, \ldots, f_i}$. If $\ideal {f_1, \ldots, f_i} = I$ then stop choosing elements. Let $a_j$ be the [[Definition:Leading Coefficient (Polynomial)|leading coefficient]] of $f_j$. Since $A$ is [[Definition:Noetherian Ring|Noetherian]], the ideal $\ideal {a_1, a_2, \ldots} \subseteq A$ is generated by $a_1, a_2, \ldots, a_m$ for some $m \in \N$. We claim that $f_1, f_2, \ldots, f_m$ generate $I$. {{AimForCont}} not. Then our process chose an element $f_{m + 1}$, and $\displaystyle a_{m + 1} = \sum_{j \mathop = 1}^m u_j a_j$ for some $u_j \in A$. Since the [[Definition:Degree (Polynomial)|degree]] of $f_{m + 1}$ is greater than or equal to the degree of $f_j$ for $j = 1, \ldots, m$, the polynomial: :$\displaystyle g = \sum_{j \mathop = 1}^m u_j f_j x^{\deg f_{m + 1} - \deg f_j} \in \ideal {f_1, \ldots, f_m}$ has the same [[Definition:Leading Coefficient of Polynomial|leading coefficient]] and degree as $f_{m + 1}$. The difference $f_{m + 1} - g$ is not in $\ideal {f_1, \ldots, f_m}$ and has degree strictly less than $f_{m + 1}$, a contradiction of our choice of $f_{m + 1}$. Thus $I = \ideal {f_1, \ldots, f_m}$ is finitely generated, and the proof is complete. {{qed}} {{Namedfor|David Hilbert|cat = Hilbert}} [[Category:Noetherian Rings]] [[Category:Commutative Algebra]] 23a4xpjufa4jjg4kxbrowbcljw7efxy	1
Let $\struct {G, +_G, \circ}_K$ be a [[Definition:Vector Space|vector space]], where: :$\struct {K, +_K, \times_K}$ is a [[Definition:Field (Abstract Algebra)|field]] :$\struct {G, +_G}$ is an [[Definition:Abelian Group|abelian group]] $\struct {G, +_G}$ :$\circ: K \times G \to G$ is a [[Definition:Binary Operation|binary operation]]. Then the [[Definition:Field (Abstract Algebra)|field]] $\struct {K, +_K, \times_K}$ is called the '''scalar field''' of $\struct {G, +_G, \circ}_K$.	1
Follows directly from [[Matrix Equivalence is Equivalence Relation]]. {{qed}}	1
Each of the [[Definition:Norm Axioms|norm axioms]] is examined in turn: === $N1$: Positive Definiteness === This is proved in [[Field Norm of Quaternion is Positive Definite]]. {{qed|lemma}} === $N2$: Multiplicativity === This is proved in [[Field Norm of Quaternion is Multiplicative]]. {{qed|lemma}} === $N3$: Triangle Inequality === For example: :$\map n {1 + 1} = 4 > 2 = \map n 1 + \map n 1$ and so $N3$ is not [[Definition:Satisfaction|satisfied]]. {{qed|lemma}} Not all the [[Definition:Norm Axioms|norm axioms]] are fulfilled. Hence the result. {{qed}} [[Category:Quaternions]] [[Category:Norm Theory]] q26q9bmf8pl0w3600m2bv534c6iiqun	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. By the definition of an [[Definition:Open Ball of Normed Division Ring|open ball]] in $\norm {\,\cdot\,}$: :$\map {B_r} x$ is an [[Definition:Open Ball|open ball]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$. By [[Open Ball of Metric Space is Open Set]] then $\map {B_r} x$ is [[Definition:Open Set of Metric Space|open]] in $\struct {R, d}$. So it remains to show that $\map {B_r} x$ is [[Definition:Closed Set of Metric Space|closed]] in $\struct {R, d}$. Let $\map \cl {\map {B_r} x}$ denote the [[Definition:Closure (Metric Space)|closure]] of $\map {B_r} x$. Let $y \in \map \cl {\map {B_r} x}$. By the definition of the [[Definition:Closure (Metric Space)|closure]] of $\map {B_r} x$ then: :$\forall s > 0: \map {B_s} y \cap \map {B_r} x \ne \O$ In particular: :$\map {B_r} y \cap \map {B_r} x \ne \O$ Let $z \in \map {B_r} y \cap \map {B_r} x$. By [[Topological Properties of Non-Archimedean Division Rings/Centers of Open Balls|Centers of Open Balls]]: :$\map {B_r} y = B_r \paren{z} = \map {B_r} x$ By the definition of an [[Definition:Open Ball|open ball]]: :$y \in \map {B_r} y = \map {B_r} x$. Hence: :$\map \cl {\map {B_r} x} \subseteq \map {B_r} x$ By [[Subset of Metric Space is Subset of its Closure]] then: :$\map {B_r} x \subseteq \map \cl {\map {B_r} x}$ So by definition of [[Definition:Set Equality/Definition 2|set equality]]: :$\map \cl {\map {B_r} x} = \map {B_r} x$ By [[Set is Closed iff Equals Topological Closure]] then $\map {B_r} x$ is [[Definition:Closed Set of Metric Space|closed]]. {{qed}}	1
:$\forall n \in N: \norm n \le n^\alpha$	1
The [[Cauchy-Binet Formula]] gives: :$(1): \quad \displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m}}\right) \det \left({\mathbf B_{j_1 j_2 \ldots j_m}}\right)$ where: :$\mathbf A$ is an [[Definition:Matrix|$m \times n$ matrix]] :$\mathbf B$ is an [[Definition:Matrix|$n \times m$ matrix]]. :For $1 \le j_1, j_2, \ldots, j_m \le n$: ::$\mathbf A_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. ::$\mathbf B_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. But here $m > n$. Therefore the [[Definition:Set|set]] $\left\{ {j_1, j_2, \ldots, j_m}\right\}$ such that: :$1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n$ is the [[Definition:Empty Set|empty set]]. Thus the {{RHS}} of $(1)$ is a [[Definition:Vacuous Summation|vacuous summation]]. Hence the result. {{qed}}	1
Let $A$ be a [[Definition:Noetherian Ring|Noetherian ring]]. Let $A \sqbrk x$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms over $A$ in the single indeterminate $x$]]. Then $A \sqbrk x$ is also a [[Definition:Noetherian Ring|Noetherian ring]].	1
Let $\mathbf v,\mathbf w$ be two non-[[Definition:Zero Vector|zero]] [[Definition:Vector (Euclidean Space)|vectors in $\R^n$]]. The [[Definition:Dot Product|dot product]] of $\mathbf v$ and $\mathbf w$ can be calculated by: :$\mathbf v \cdot \mathbf w = \norm {\mathbf v} \norm {\mathbf w} \cos \theta$ where: :$\norm {\, \cdot \,}$ denotes [[Definition:Vector Length|vector length]] and :$\theta$ is the [[Definition:Angle Between Vectors|angle between $\mathbf v$ and $\mathbf w$]].	1
Let $L/k$ be a [[Definition:Field Extension|field extension]]. Let $L$ be [[Definition:Finitely Generated Algebra|finitely generated]] [[Definition:Algebra Defined by Ring Homomorphism|as an algebra]] over $k$. Then $L/k$ is a [[Definition:Finite Field Extension|finite field extension]].	1
Because $\Omega$ is the first [[Definition:Uncountable Ordinal|uncountable ordinal]], any [[Definition:Ordinal|ordinal]] which [[Definition:Strictly Precede|strictly precedes]] $\Omega$ is [[Definition:Countable Set|countable]]. Let $H \subseteq \hointr 0 \Omega$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $\hointr 0 \Omega$. Let $\sigma$ be the [[Definition:Supremum of Set|supremum]] of $H$. As $H$ by definition [[Definition:Strictly Precede|strictly precedes]] $\Omega$, $H$ itself is [[Definition:Countable Set|countable]]. Thus $\sigma$ [[Definition:Strictly Precede|strictly precedes]] $\Omega$. The [[Definition:Closed Interval|closed interval]] $\closedint 0 \sigma$ is such that $H \subseteq \closedint 0 \sigma$. By definition of the [[Definition:Closure (Topology)|closure]] $H^-$ of $H$ as the smallest [[Definition:Closed Set (Topology)|closed set]] of $\hointr 0 \Omega$ containing $H$, it follows that $H^- \subseteq \closedint 0 \sigma$. Therefore, there exists an [[Definition:Open Interval|open interval]] $\openint \sigma \Omega$ in the [[Definition:Relative Complement|complement]] of $H^-$ in $\hointr 0 \Omega$. Thus the [[Definition:Closure (Topology)|closure]] of $H$ does not equal $\hointr 0 \Omega$. Thus $H$ is not [[Definition:Everywhere Dense|everywhere dense]] in $\hointr 0 \Omega$. Hence, by definition, $\hointr 0 \Omega$ is not [[Definition:Separable Space|separable]]. {{qed}}	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {S, +_S, \times_S}$ be a [[Definition:Subring|subring]] of $R$. Let $\struct {G, +_G, \circ}_R$ be an [[Definition:Module|$R$-module]]. Let $\circ_S$ be the [[Definition:Restriction of Operation|restriction]] of $\circ$ to $S \times G$. Then $\struct {G, +_G, \circ_S}_S$ is an [[Definition:Module|$S$-module]]. The module $\struct {G, +_G, \circ_S}_S$ is called the '''$S$-module obtained from $\struct {G, +_G, \circ}_R$ by restricting scalar multiplication'''. {{refactor|Extract the below into its own page}} If $\struct {G, +_G, \circ}_R$ is a [[Definition:Unitary Module|unitary $R$-module]] and $1_R \in S$, then $\struct{G, +_G, \circ_S}_S$ is also [[Definition:Unitary Module|unitary]].	1
Let $i : A \to A_S$ and $j : A \to A_T$ be the [[Definition:Localization Homomorphism of Ring|localization homomorphisms]]. === 1 implies 2 === Let $h : A_S \to A_T$ be an $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]]. Then by definition, $j = h \circ i$: :$\xymatrix{ A \ar[d]_i \ar[r]^{j} & A_T\\ A_S \ar[ru]_{h} }$ Let $s \in S$. By definition of [[Definition:Localization of Ring|localization]], $i(s)$ is a [[Definition:Unit of Ring|unit]] of $A_S$. By [[Ring Homomorphism Preserves Invertible Elements]], $j(s) = h(i(s))$ is a [[Definition:Unit of Ring|unit]] of $A_T$. Thus $s$ is an [[Definition:Element of Set|element]] of the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $T$. {{qed|lemma}} === 2 implies 1 === Let $S$ be a [[Definition:Subset|subset]] of the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $T$. Then its [[Definition:Image of Subset under Mapping|image]] $j(S) \subseteq A_T^\times$ consists of [[Definition:Unit of Ring|units]] of $A_T$. By definition of [[Definition:Localization of Ring|localization]] at $S$, there exists a [[Definition:Unique|unique]] $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]] $h : A_S \to A_T$. {{qed|lemma}} === 2 implies 3 === Let $S$ be a [[Definition:Subset|subset]] of the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $T$. By definition, its [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] is the [[Definition:Smallest|smallest]] [[Definition:Saturated Multiplicatively Closed Subset of Ring|saturated multiplicatively closed subset]] of $A$ containing $S$. Thus the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $S$ is a subset of the saturation of $T$. {{qed|lemma}} === 3 implies 2 === By definition, $S$ is a [[Definition:Subset|subset]] of its [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]]. {{qed|lemma}} === 3 iff 4 === By definition, the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $S$ is the [[Definition:Relative Complement|complement]] of the [[Definition:Set Union|union]] of [[Definition:Prime Ideal of Ring|prime ideals]] that are [[Definition:Disjoint Sets|disjoint]] from $S$: :$\operatorname{Sat}(S) = A - \displaystyle \bigcup \left\{ \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap S = \varnothing \right\}$ Thus: {{begin-eqn}} {{eqn | l = \operatorname{Sat}(S) \subseteq \operatorname{Sat}(T) | r = A - \bigcup \left\{ \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap S = \varnothing \right\} \subseteq A - \bigcup \left\{ \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap T = \varnothing \right\} | o = \iff }} {{eqn | r = \bigcup \left\{ \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap S = \varnothing \right\} \supseteq \bigcup \left\{ \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap T = \varnothing \right\} | o = \iff | c = [[Subset iff Complement is Superset of Complement]] }} {{eqn | r = \forall \mathfrak p \in \operatorname{Spec} A : \left( \mathfrak p \cap T = \varnothing \implies \mathfrak p \subseteq \bigcup \left\{ \mathfrak q \in \operatorname{Spec} A : \mathfrak q \cap S = \varnothing \right\} \right) | o = \iff | c = [[Sets are Subset iff Union is Subset]] }} {{end-eqn}} To finish, we show that the last statement is equivalent to: :$\forall \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap T = \varnothing \implies \mathfrak p \cap S = \varnothing$ We show that, for $\mathfrak p \in \operatorname{Spec} A$: :$\mathfrak p \subseteq \displaystyle \bigcup \left\{ \mathfrak q \in \operatorname{Spec} A : \mathfrak q \cap S = \varnothing \right\} \iff \mathfrak p \cap S = \varnothing$ Let $\mathfrak p \in \operatorname{Spec} A$. If $\mathfrak p \cap S = \varnothing$, then by [[Set is Subset of Union]]: :$\mathfrak p \subseteq \displaystyle \bigcup \left\{ \mathfrak q \in \operatorname{Spec} A : \mathfrak q \cap S = \varnothing \right\}$ Conversely, let $\mathfrak p \subseteq \displaystyle \bigcup \left\{ \mathfrak q \in \operatorname{Spec} A : \mathfrak q \cap S = \varnothing \right\}$. By: :[[Union of Sets Disjoint with Set]] :[[Subset of Disjoint Set]] we have $\mathfrak p \cap S = \varnothing$. We conclude that $ \operatorname{Sat}(S) \subseteq \operatorname{Sat}(T)$ {{iff}} :$\forall \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap T = \varnothing \implies \mathfrak p \cap S = \varnothing$ That is: :$\forall \mathfrak p \in \operatorname{Spec} A : \mathfrak p \cap S \neq \varnothing \implies \mathfrak p \cap T \neq \varnothing$ {{qed}}	1
:The [[Definition:Open Ball of Normed Division Ring|open $r$-ball of $x$]], $\map {B_r} x$, is both [[Definition:Open Set of Metric Space|open]] and [[Definition:Closed Set of Metric Space|closed]] in the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\norm {\,\cdot\,}$.	1
Let $\sequence {e_i}_{1 \mathop \le i \mathop \le k}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] that compose $\Gamma$. Let $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Column Matrix|elementary column matrices]]. From [[Column Operation is Equivalent to Post-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf A \mathbf K = \mathbf B$ where $\mathbf K$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$: :$\mathbf K = \mathbf E_1 \mathbf E_2 \dotsb \mathbf E_{k - 1} \mathbf E_k$ By [[Elementary Column Matrix is Invertible]], each of $\mathbf E_i$ is [[Definition:Invertible Matrix|invertible]]. By [[Product of Matrices is Invertible iff Matrices are Invertible]], it follows that $\mathbf K$ is likewise [[Definition:Invertible Matrix|invertible]]. Thus $\mathbf K$ has an [[Definition:Inverse Matrix|inverse]] $\mathbf K^{-1}$. Hence: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf K | r = \mathbf B | c = }} {{eqn | ll= \leadsto | l = \mathbf A \mathbf K \mathbf K^{-1} | r = \mathbf B \mathbf K^{-1} | c = }} {{eqn | ll= \leadsto | l = \mathbf A | r = \mathbf B \mathbf K^{-1} | c = }} {{end-eqn}} We have: {{begin-eqn}} {{eqn | l = \mathbf K^{-1} | r = \paren {\mathbf E_1 \mathbf E_2 \dotsb \mathbf E_{k - 1} \mathbf E_k}^{-1} | c = }} {{eqn | r = {\mathbf E_k}^{-1} {\mathbf E_{k - 1} }^{-1} \dotsb {\mathbf E_2}^{-1} {\mathbf E_1}^{-1} | c = [[Inverse of Matrix Product]] }} {{end-eqn}} From [[Elementary Column Matrix for Inverse of Elementary Column Operation is Inverse]], each of ${\mathbf E_i}^{-1}$ is the [[Definition:Elementary Column Matrix|elementary column matrix]] corresponding to the [[Existence of Inverse Elementary Column Operation|inverse]] $e'_i$ of the corresponding [[Definition:Elementary Column Operation|elementary column operation]] $e_i$. Let $\Gamma'$ be the [[Definition:Column Operation|column operation]] composed of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Column Operation|elementary column operations]] $\tuple {e'_k, e'_{k - 1}, \ldots, e'_2, e'_1}$. Thus $\Gamma'$ is a [[Definition:Column Operation|column operation]] which transforms $\mathbf B$ into $\mathbf A$. Hence the result. {{qed}}	1
Let $\mathcal L$ be a [[Definition:Straight Line|straight line]] such that: :the [[Definition:Perpendicular Distance|perpendicular distance]] from $\mathcal L$ to the [[Definition:Origin|origin]] is $p$ :the [[Definition:Angle|angle]] made between that [[Definition:Perpendicular|perpendicular]] and the [[Definition:X-Axis|$x$-axis]] is $\alpha$. Then $\mathcal L$ can be defined by the [[Definition:Equation of Geometric Figure|equation]]: :$x \cos \alpha + y \sin \alpha = p$	1
{{WLOG}}, consider the [[Definition:Right Inverse Element|right inverse]] of $\mathbf D$. Suppose none of the diagonal elements are zero. Then by the definition of inverse, our assertion is that the [[Definition:Matrix Product (Conventional)|matrix product]] of the two matrices in question is the [[Definition:Unit Matrix|unit matrix of order $n$]]. Now, observe that: {{begin-eqn}} {{eqn | l = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & a_{nn} \\ \end{bmatrix} \begin{bmatrix} \frac 1 {a_{11} } & 0 & \cdots & 0 \\ 0 & \frac 1 {a_{22} } & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac 1 {a_{nn} } \\ \end{bmatrix} | r = \begin{bmatrix} \frac{a_{11} }{a_{11} } & 0 & \cdots & 0 \\ 0 & \frac{a_{22} }{a_{22} } & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{a_{nn} }{a_{nn} } \\ \end{bmatrix} | c = }} {{eqn | r = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \\ \end{bmatrix} | c = }} {{eqn | r = \mathbf I | c = }} {{end-eqn}} {{qed|lemma}} Now suppose one of the [[Definition:Diagonal Element|diagonal elements]] is zero. Then $\map \det {\mathbf D} = 0$, where $\det$ indicates the [[Definition:Determinant of Matrix|determinant]] of $\mathbf D$. From [[Determinant of Inverse Matrix]] it would follow that: :$\map \det {\mathbf D^{-1} } = \dfrac 1 {\map \det {\mathbf D} }$ But this equation has no solution, and so $\mathbf D$ cannot admit an [[Definition:Inverse Matrix|inverse]]. {{qed}} [[Category:Diagonal Matrices]] [[Category:Inverse Matrices]] gh1d57u7f1tiwpqjlan4hfxa3atfd4x	1
From [[Ring of Integers Modulo m is Ring]], $\struct {\Z_m, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. It remains to be shown that $\struct {\Z_m, +, \times}$ has no [[Definition:Proper Zero Divisor|proper zero divisors]] {{iff}} $m$ is [[Definition:Prime Number|prime]]. === $m$ Composite === Let $m$ be [[Definition:Composite Number|composite]]. Then: :$m = m_1 m_2$ where $0 < m_1 < m, 0 < m_2 < m$. Then: :$\eqclass {m_1} m \eqclass {m_2} m = \eqclass 0 m$ and so both $\eqclass {m_1} m$ and $\eqclass {m_2} m$ are [[Definition:Proper Zero Divisor|proper zero divisors]]. Hence if $m$ is not [[Definition:Prime Number|prime]] then $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]] by definition. {{qed|lemma}} === $m$ Prime === Let $m$ be [[Definition:Prime Number|prime]]. Let $\eqclass a m \eqclass b m = \eqclass 0 m$. Then by [[Modulo Multiplication is Well-Defined]]: :$\eqclass {a b} m = \eqclass 0 m$ Thus either :$m \divides a$ or $m \divides b$ where $\divides$ denotes the [[Definition:Divisor of Integer|divisibility relation]]. If $m \divides a$ then $\eqclass a m = 0$. If $m \divides b$ then $\eqclass b m = 0$. Thus neither $a$ nor $b$ is a [[Definition:Proper Zero Divisor|proper zero divisor]]. Hence there are no [[Definition:Proper Zero Divisor|proper zero divisors]] of $\struct {\Z_m, +, \times}$. Hence, by definition, $\struct {\Z_m, +, \times}$ is an [[Definition:Integral Domain|integral domain]] {{qed}}	1
A [[Definition:Metric Space|metric space]] is [[Definition:Separable Space|separable]] {{iff}} it is [[Definition:Second-Countable Space|second-countable]].	1
From the definitions of the [[Definition:Transpose of Linear Transformation|transpose]] $u^t$ and the [[Definition:Annihilator|annihilator]] $\left({u \left({G}\right)}\right)^\circ$, it follows that: :$u^t \left({y'}\right) = 0 \iff y' = \left({u \left({G}\right)}\right)^\circ$ Thus: :$\ker \left({u^t}\right) = \left({u \left({G}\right)}\right)^\circ$. Let $x \in \ker \left({u}\right)$. Let $H^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $H$. Let $\left \langle {x, t'} \right \rangle$ be the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]]. Then: :$\forall y' \in H^*: \left \langle {x, u^t \left({y'}\right)} \right \rangle = \left \langle {u \left({x}\right), y'} \right \rangle = \left \langle {0, y'} \right \rangle = 0$ So: :$u^t \left({H^*}\right) \subseteq \left({\ker \left({u}\right)}\right)^\circ$ From [[Rank Plus Nullity Theorem]] and [[Results Concerning Annihilator of Vector Subspace]]: {{begin-eqn}} {{eqn | l = \dim \left({u^t \left({H^*}\right)}\right) | r = n - \dim \left({\ker \left({u^t}\right)}\right) | c = }} {{eqn | r = n - \dim \left({\left({u \left({G}\right)}\right)^\circ}\right) | c = }} {{eqn | r = \dim \left({u \left({G}\right)}\right) | c = }} {{eqn | r = n - \dim \left({\ker \left({u}\right)}\right) | c = }} {{eqn | r = \dim \left({\left({\ker \left({u}\right)}\right)^\circ}\right) | c = }} {{end-eqn}} So it follows that $u$ and $u^t$ have the same [[Definition:Rank of Linear Transformation|rank]] and [[Definition:Nullity of Linear Transformation|nullity]], and that: :$u^t \left({H^*}\right) = \left({\ker \left({u}\right)}\right)^\circ$ {{Qed}}	1
By the [[Definition:Complex Number|definition of a complex number]], we have: :$z = \map \Re z + i \map \Im z$ Then: {{begin-eqn}} {{eqn | l = \cmod z | r = \sqrt {\paren {\map \Re z}^2 + \paren {\map \Im z}^2} | c = {{Defof|Complex Modulus}} }} {{eqn | o = \ge | r = \sqrt {\paren {\map \Re z}^2 } | c = [[Square of Real Number is Non-Negative]], as $\map \Im z$ is [[Definition:Real Number|real]] }} {{eqn | r = \cmod {\map \Re z} | c = [[Square of Real Number is Non-Negative]], as $\map \Re z$ is [[Definition:Real Number|real]] }} {{end-eqn}} {{qed}}	1
Let $\Q$ be the set of [[Definition:Rational Number|rational numbers]]. Then the [[Definition:Module on Cartesian Product|$\Q$-module $\Q^n$]] is a [[Definition:Vector Space|vector space]]. It follows directly, by setting $n = 1$, that the [[Definition:Module on Cartesian Product|$\Q$-module $\Q$]] itself can also be regarded as a [[Definition:Vector Space|vector space]].	1
Follows directly from the fact that the [[Trivial Subgroup is Subgroup|trivial subgroup is a subgroup of $\left({G, +_G}\right)$]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf c \times \left({\mathbf a \times \mathbf b}\right) | r = \left({\mathbf{c \cdot b} }\right) \mathbf a - \left({\mathbf{c \cdot a} }\right) \mathbf b | c = [[Lagrange's Formula]] }} {{eqn | ll= \leadsto | l = \left({\mathbf a \times \mathbf b}\right) \times \mathbf c | r = -\left({\left({\mathbf{c \cdot b} }\right) \mathbf a - \left({\mathbf{c \cdot a} }\right) \mathbf b}\right) | c = [[Vector Cross Product is Anticommutative]] }} {{eqn | r = -\left({\mathbf{b \cdot c} }\right) \mathbf a + \left({\mathbf{a \cdot c} }\right) \mathbf b | c = [[Dot Product Operator is Commutative]] }} {{eqn | r = \left({\mathbf{a \cdot c} }\right) \mathbf b - \left({\mathbf{b \cdot c} }\right) \mathbf a | c = [[Real Addition is Commutative]] }} {{end-eqn}} {{qed}} {{Namedfor|Joseph Louis Lagrange}}	1
:$\mathbf u \cdot \mathbf u = \norm {\mathbf u}^2$	1
Given a [[Definition:Triangle (Geometry)|triangle]] $ABC$, the sum of the [[Definition:Length of Line|lengths]] of any two [[Definition:Side of Polygon|sides]] of the triangle is greater than the [[Definition:Length of Line|length]] of the third [[Definition:Side of Polygon|side]]. {{:Euclid:Proposition/I/20}}	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \left\Vert{\mathbf u}\right\Vert^2 | c = [[Dot Product of Vector with Itself]] }} {{eqn | o = \ge | r = 0 | c = [[Square of Real Number is Non-Negative]] }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $A \in B \left({H}\right)$ be a [[Definition:Normal Operator|normal operator]]. Let $\lambda \in \Bbb F$. Then $\ker \left({A - \lambda}\right)$ is a [[Definition:Reducing Subspace|reducing subspace]] for $A$. Here $\ker$ denotes [[Definition:Kernel of Linear Transformation|kernel]].	1
A '''semi-inner product space''' is a [[Definition:Vector Space|vector space]] together with an associated [[Definition:Semi-Inner Product|semi-inner product]].	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \N_{> 1}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. First note that if $n$ is [[Definition:Prime Number|prime]], the result is immediate. === Basis for the Induction === $\map P 2$ is the case: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. As $2$ itself is a [[Definition:Prime Number|prime number]], and the result is immediate. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P j$ is true, for all $j$ such that $2 \le j \le k$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :For all $j \in \N$ such that $2 \le j \le k$, $j$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. from which it is to be shown that: :$k + 1$ can be expressed as a product of [[Definition:Prime Number|prime numbers]]. === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: If $k + 1$ is [[Definition:Prime Number|prime]], then the result is immediate. Otherwise, $k + 1$ is [[Definition:Composite|composite]] and can be expressed as: :$k + 1 = r s$ where $2 \le r < k + 1$ and $2 \le s < k + 1$ That is, $2 \le r \le k$ and $2 \le s \le k$. Thus by the [[Integer is Expressible as Product of Primes/Proof 3#Induction Hypothesis|induction hypothesis]], both $r$ and $s$ can be expressed as a product of [[Definition:Prime Number|primes]]. So $k + 1 = r s$ can also be expressed as a product of [[Definition:Prime Number|primes]]. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Second Principle of Mathematical Induction]]. Therefore, for all $n \in \N_{> 1}$: :$n$ can be expressed as a product of [[Definition:Prime Number|prime numbers]].	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|subfield]] of $\C$. Let $V$ be a [[Definition:Semi-Inner Product Space|semi-inner product space]] over $\mathbb K$. Let $x, y$ be [[Definition:Vector (Linear Algebra)|vectors]] in $V$. Then: :$\size {\innerprod x y}^2 \le \innerprod x x \innerprod y y$	1
{{begin-eqn}} {{eqn | l = z_1 \circ z_2 | r = \map \Re {\overline {z_1} z_2} | c = {{Defof|Dot Product|subdef = Complex|index = 3}} }} {{eqn | r = \map \Re {r_1 e^{-i \theta_1} r_2 e^{i \theta_2} } | c = [[Exponential Form of Complex Conjugate]] }} {{eqn | r = \map \Re {r_1 r_2 e^{i \paren {\theta_2 - \theta_1} } } | c = [[Product of Complex Numbers in Exponential Form]] }} {{eqn | r = \map \Re {r_1 r_2 \paren {\map \cos {\theta_2 - \theta_1} + i \, \map \sin {\theta_2 - \theta_1} } } | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = r_1 r_2 \map \cos {\theta_2 - \theta_1} | c = {{Defof|Real Part}} }} {{end-eqn}} {{qed}}	1
Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$. Let $\epsilon > 0$ be given. Since $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] then: :$\exists N \in \N: \forall n,m \ge N: \norm {x_n - x_m}_1 < \epsilon^\alpha$ Then: :$\exists N \in \N: \forall n,m \ge N: \norm {x_n - x_m}_2^\alpha < \epsilon^\alpha$ Hence: :$\exists N \in \N: \forall n,m \ge N: \norm {x_n - x_m}_2 < \epsilon$ So $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$ It follows that for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1 \implies \sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$ {{qed|lemma}} Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2$. Let $\epsilon > 0$ be given. Since $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] then: :$\exists N \in \N: \forall n,m \ge N: \norm {x_n - x_m}_2 \lt \epsilon^{1/\alpha}$ Then: :$\exists N \in \N: \forall n, m \ge N: \norm {x_n - x_m}_2^\alpha < \epsilon$ Hence: :$\exists N \in \N: \forall n, m \ge N: \norm {x_n - x_m}_1 < \epsilon$ So $\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$ It follows that for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_2 \implies \sequence {x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\norm {\, \cdot \,}_1$ The result follows. {{qed}}	1
=== [[Definition:Symmetric Mapping (Mapping Theory)|Mapping Theory]] === {{:Definition:Symmetric Mapping (Mapping Theory)}} === [[Definition:Symmetric Mapping (Linear Algebra)|Linear Algebra]] === {{:Definition:Symmetric Mapping (Linear Algebra)}} [[Category:Definitions/Mapping Theory]] [[Category:Definitions/Linear Algebra]] g3wpcxdc1eztpzylzbb1npci2h88ha7	1
By definition of [[Definition:Diagonal Matrix|diagonal matrix]]: :$\forall j, k: j \ne k \implies a_{jk} = 0 = a_{kj}$ So by definition of [[Definition:Transpose of Matrix|transpose]] of $D$: :$D = D^\intercal$ where $D^\intercal$ denotes the [[Definition:Transpose of Matrix|transpose]]. Hence the result, by definition of [[Definition:Symmetric Matrix|symmetric matrix]]. {{qed}} [[Category:Symmetric Matrices]] [[Category:Diagonal Matrices]] dv7n9kzvzryg6tjumxlrly9jp6semgo	1
=== [[Definition:Nullity/Linear Transformation|Linear Transformation]] === {{:Definition:Nullity/Linear Transformation}} === [[Definition:Nullity/Matrix|Matrix]] === {{:Definition:Nullity/Matrix}} [[Category:Definitions/Linear Algebra]] lc760v5s457b9y1705qimrz55m6xrua	1
From: :[[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]] :[[Bases of Finitely Generated Vector Space have Equal Cardinality]] and :[[Sufficient Conditions for Basis of Finite Dimensional Vector Space]] all we need to do is show that every [[Definition:Infinite Set|infinite]] [[Definition:Generator of Vector Space|generator]] $S$ for $E$ contains a [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]]. Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be an [[Definition:Ordered Basis|ordered basis]] of $E$. For each $k \in \closedint 1 n$ there is a [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $S_k$ of $S$ such that $a_k$ is a [[Definition:Linear Combination|linear combination]] of $S_k$. Hence $\displaystyle \bigcup_{k \mathop = 1}^n S_k$ is a [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] of $S$ [[Definition:Generator of Vector Space|generating]] $E$, for the [[Definition:Vector Subspace|subspace]] it generates contains $\set {a_1, \ldots, a_n}$ and hence is $E$. {{qed}}	1
Take the [[Definition:Determinant of Matrix|determinant]] $\det \left({C_n}\right)$: :$\det \left({C_n}\right) = \begin{vmatrix} x + y & y & y & \cdots & y \\ y & x + y & y & \cdots & y \\ y & y & x + y & \cdots & y \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y & y & y & \cdots & x + y \end{vmatrix}$ Subtract [[Definition:Column of Matrix|column]] $1$ from [[Definition:Column of Matrix|column]]s $2$ to $n$. From [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the determinant: :$\det \left({C_n}\right) = \begin{vmatrix} x + y & -x & -x & \cdots & -x \\ y & x & 0 & \cdots & 0 \\ y & 0 & x & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y & 0 & 0 & \cdots & x \end{vmatrix}$ Add [[Definition:Row of Matrix|rows]] $2$ to $n$ to [[Definition:Row of Matrix|row]] $1$. Again, from [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the determinant: :$\det \left({C_n}\right) = \begin{vmatrix} x + n y & 0 & 0 & \cdots & 0 \\ y & x & 0 & \cdots & 0 \\ y & 0 & x & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y & 0 & 0 & \cdots & x \end{vmatrix}$ This is now the [[Definition:Determinant of Matrix|determinant]] of a [[Definition:Lower Triangular Matrix|(lower) triangular matrix]]. From [[Determinant of Triangular Matrix]], it follows immediately that: :$\det \left({C_n}\right) = x^{n-1} \left ({x + n y}\right)$ {{qed}}	1
Let us define $r_{k + 1}, r_{k + 2}, \ldots, r_n$ such that: :$1 \le r_{k + 1} < r_{k + 2} < \cdots < r_n \le n$ :$\rho = \tuple {r_1, r_2, \ldots, r_n}$ is a [[Definition:Permutation on n Letters|permutation on $\N^*_n$]]. Let $\sigma = \tuple {s_1, s_2, \ldots, s_n}$ be a [[Definition:Permutation on n Letters|permutation on $\N^*_n$]]. Then by [[Permutation of Determinant Indices]] we have: {{begin-eqn}} {{eqn | l = D | r = \sum_\sigma \map \sgn \rho \, \map \sgn \sigma \prod_{j \mathop = 1}^n a_{\map \rho j \, \map \sigma j} | c = }} {{eqn | r = \sum_\sigma \paren {-1}^{\sum_{i \mathop = 1}^k \paren {r_i + s_i} } \map \sgn {\map \rho {r_1, \ldots, r_k} } \, \map \sgn {\map \sigma {s_1, \ldots, s_k} } \map \sgn {\map \rho {r_{k + 1}, \ldots, r_n} } \, \map \sgn {\map \sigma {s_{k + 1}, \ldots, s_n} } \prod_{j \mathop = 1}^n a_{\map \rho j \, \map \sigma j} | c = }} {{end-eqn}} We can obtain all the [[Definition:Permutation on n Letters|permutations]] $\sigma$ exactly once by separating the numbers $1, \ldots, n$ in all possible ways into a set of $k$ and $n - k$ numbers. We let $\tuple {s_1, \ldots, s_k}$ vary over the first set and $\tuple {s_{k + 1}, \ldots, s_n}$ over the second set. So the summation over all $\sigma$ can be replaced by: :$\tuple {u_1, \ldots, u_n} = \map \sigma {1, \ldots, n}$ :$u_1 < u_2 < \cdots < u_k, u_{k + 1} < u_{k + 2} < \cdots < u_n$ :$\tuple {s_1, \ldots, s_k} = \map \sigma {u_1, \ldots, u_k}$ :$\tuple {s_{k + 1}, \ldots, s_n} = \map \sigma {u_{k + 1}, \ldots, u_n}$ Thus we get: {{begin-eqn}} {{eqn | l = D | r = \sum_{\map \sigma {u_1, \ldots, u_n} } \paren {-1}^{\sum_{i \mathop = 1}^k \paren {r_i + u_i} } \sum_{\map \sigma {u_1, \ldots, u_k} } \, \map \sgn {\map \rho {r_1, \ldots, r_k} } \, \map \sgn {\map \sigma {s_1, \ldots, s_k} } \prod_{j \mathop = 1}^k a_{\map \rho j \, \map \sigma j} | c = }} {{eqn | o = \times | r = \sum_{\map \sigma {u_{k + 1}, \ldots, u_n} } \map \sgn {\map \rho {r_{k + 1}, \ldots, r_n} } \, \map \sgn {\map \sigma {s_{k + 1}, \ldots, s_n} } \prod_{j \mathop = k + 1}^n a_{\map \rho j \, \map \sigma j} | c = }} {{eqn | r = \sum_{\map \sigma {u_1, \ldots, u_n} } \paren {-1}^{\sum_{i \mathop = 1}^k \paren {r_i + u_i} } \begin {vmatrix} a_{r_1 u_1} & \cdots & a_{r_1 u_k} \\ \vdots & \ddots & \vdots \\ a_{r_k u_1} & \cdots & a_{r_k u_k} \end {vmatrix} \times \begin {vmatrix} a_{r_{k + 1} u_{k + 1} } & \cdots & a_{r_{k + 1} u_n} \\ \vdots & \ddots & \vdots \\ a_{r_n u_{k + 1} } & \cdots & a_{r_n u_n} \end {vmatrix} | c = }} {{eqn | r = \sum_{\map \sigma {u_1, \ldots, u_n} } \paren {-1}^{\sum_{i \mathop = 1}^k \paren {r_i + u_i} } \map D {r_1, \ldots, r_k \mid u_1, \ldots, u_k} \times \map D {r_{k + 1}, \ldots, r_n \mid u_{k + 1}, \ldots, u_n} | c = }} {{eqn | r = \sum_{\map \sigma {u_1, \ldots, u_n} } \map D {r_1, \ldots, r_k \mid u_1, \ldots, u_k} \times \map {\tilde D} {r_1, \ldots, r_k \mid u_1, \ldots, u_k} | c = }} {{eqn | r = \sum_{1 \mathop \le u_1 \mathop < \cdots \mathop < u_k \mathop \le n} \map D {r_1, \ldots, r_k \mid u_1, \ldots, u_k} \, \map {\tilde D} {r_1, \ldots, r_k \mid u_1, \ldots, u_k} \sum_{u_{k + 1}, \ldots, u_n} 1 | c = }} {{end-eqn}} That last inner sum extends over all integers which satisfy: :$\tuple {u_1, \ldots, u_n} = \map \sigma {1, \ldots, n}$ :$u_1 < u_2 < \cdots < u_k, u_{k + 1} < u_{k + 2} < \cdots < u_n$ But for each set of $u_1, \ldots, u_k$, then the integers $u_{k + 1}, \ldots, u_n$ are clearly uniquely determined. So that last inner sum equals 1 and the theorem is proved. {{Explain|I'm not too happy about this, it seems a bit handwavey and imprecise. I'm going to have to revisit it.}} The result for columns follows from [[Determinant of Transpose]]. {{qed}} {{Proofread}}	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ be [[Definition:Norm on Division Ring|norms]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ be [[Definition:Equivalent Division Ring Norms|equivalent norms]]. Then: :$\exists \alpha \in \R_{\gt 0}: \forall n \in \N: \norm n_1 = \norm n_2^\alpha$	1
Let $\struct {R, \norm {\,\cdot\,} } $ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] on $R$ be the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Then: :$\sequence {x_n} $ is a [[Definition:Bounded Sequence in Normed Division Ring|bounded sequence]] in the [[Definition:Normed Division Ring|normed division ring]] $\struct {R, \norm {\,\cdot\,} }$ {{iff}} $\sequence {x_n} $ is a [[Definition:Bounded Sequence in Metric Space|bounded sequence]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$.	1
{{ProofWanted}} [[Category:Bilinear Forms]] 347cej7rct13thjadei7zksp6ejkc16	1
By [[Rational Numbers are Countably Infinite]], the [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] is [[Definition:Countably Infinite Set|countably infinite]]. By [[P-adic Numbers are Uncountable|P-adic Numbers are Uncountable]], the [[Definition:Set|set]] of [[Definition:P-adic Number|$p$-adic numbers $\Q_p$]] is [[Definition:Uncountable Set|uncountably infinite]]. Let $\CC$ be the [[Definition:Ring of Cauchy Sequences|commutative ring of Cauchy sequences]] over $\struct {\Q, \norm {\,\cdot\,}_p}$. Let $\NN$ be the [[Definition:Set|set]] of [[Definition:Null Sequence|null sequences]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. The [[Definition:P-adic Number/Quotient of Cauchy Sequences in P-adic Norm|$p$-adic numbers]] $\Q_p$ is the [[Definition:Quotient Ring|quotient ring]] $\CC \, \big / \NN$ by definition. By [[Embedding Division Ring into Quotient Ring of Cauchy Sequences]], the mapping $\phi: \Q \to \Q_p$ defined by: :$\map \phi r = \sequence {r, r, r, \dotsc} + \NN$ where $\sequence {r, r, r, \dotsc} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {r, r, r, \dotsc}$, is a [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|monomorphism]]. By [[Surjection from Natural Numbers iff Countable/Corollary 2|Corollary to Surjection from Natural Numbers iff Countable]] then $\phi$ is not a [[Definition:Surjection|surjection]]. Hence: :$\exists \sequence {x_n} \in \CC: \sequence {x_n} + \NN \not \in \map \phi \Q$ By [[Cauchy Sequence Converges Iff Equivalent to Constant Sequence]] then $\sequence {x_n}$ is not [[Definition:Convergent Sequence in Normed Division Ring|convergent]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. The result follows. {{qed}}	1
[[Definition:Norm on Vector Space|Norms]] on [[Definition:Finite Dimensional Vector Space|finite-dimensional]] [[Definition:Real Vector Space|real vector space]] are [[Definition:Equivalence of Norms|equivalent]].	1
Let $\mathbf a$ be a [[Definition:Vector Quantity|vector quantity]]. Then: :$\mathbf a = \size {\mathbf a} \mathbf {\hat a}$ where: :$\size {\mathbf a}$ denotes the [[Definition:Magnitude|magnitude]] of $\mathbf a$ :$\mathbf {\hat a}$ denotes the [[Definition:Unit Vector|unit vector]] in the [[Definition:Direction|direction]] $\mathbf a$.	1
The proof proceeds by [[Principle of Mathematical Induction|induction]] over $n$, the [[Definition:Order of Square Matrix|order of the square matrix]]. === Basis for the Induction === Let $n = 2$, which is the smallest [[Definition:Natural Number|natural number]] for which a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]] can have two identical [[Definition:Row of Matrix|rows]]. Let $\mathbf A = \left[{a}\right]_2$ be a [[Definition:Square Matrix|square matrix]] over $R$ with two identical [[Definition:Row of Matrix|rows]]. Then, by definition of [[Definition:Determinant of Matrix|determinant]]: :$\det \left({\mathbf A}\right) = a_{11}a_{22} - a_{12}a_{21} = a_{11}a_{22}-a_{22}a_{11} = 0$ {{qed|lemma}} This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Assume for $n \in \N_{\ge 2}$ that any [[Definition:Square Matrix|square matrices of order $n$]] over $R$ with two identical [[Definition:Row of Matrix|rows]] has [[Definition:Determinant of Matrix|determinant]] equal to $0$. === Induction Step === Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix of order $n+1$]] over $R$ with two identical [[Definition:Row of Matrix|rows]]. Let $i_1, i_2 \in \left\{ {1, \ldots, n+1}\right\}$ be the indices of the identical rows, and let $i_1 < i_2$. Let $i \in \left\{ {1, \ldots, n+1}\right\}$. Let $\mathbf A \left({i ; 1}\right)$ denote the [[Definition:Submatrix|submatrix]] obtained from $\mathbf A$ by removing row $i$ and [[Definition:Column of Matrix|column]] $1$. If $i \ne i_1$ and $i \ne i_2$, then $\mathbf A \left({i ; 1}\right)$ still contains two identical rows. By the [[Square Matrix with Duplicate Rows has Zero Determinant/Proof 1#Induction Hypothesis|induction hypothesis]]: : $\det \left({\mathbf A \left({i ; 1}\right) }\right) = 0$ Now consider the [[Definition:Matrix|matrices]] $\mathbf A \left({i_1 ; 1}\right)$ and $\mathbf A \left({i_2 ; 1}\right)$. Let $r_j$ denote row $j$ of $\mathbf A \left({i_1 ; 1}\right)$. If we perform the following [[Definition:Sequence|sequence]] of $i_2 - i_1 -1$ [[Definition:Elementary Row Operation|elementary row operations]] on $\mathbf A \left({i_1 ; 1}\right)$: :$r_{i_1} \leftrightarrow r_{i_1 +1} \ ; \ r_{i_1 + 1} \leftrightarrow r_{i_1 +2} \ ; \ \ldots \ ; \ r_{i_2 - 1} \leftrightarrow r_{i_2}$ we will transform $\mathbf A \left({i_1 ; 1}\right)$ into $\mathbf A \left({i_2 ; 1}\right)$. From [[Determinant with Rows Transposed]], it follows that $\det \left({\mathbf A \left({i_1 ; 1}\right) }\right) = \left({-1}\right)^{i_2 - i_1 - 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right)$ Then: {{begin-eqn}} {{eqn | l = \det \left({\mathbf A}\right) | r = \sum_{k \mathop = 1 }^{n + 1} a_{k1} \left({-1}\right)^{k + 1} \det \left({\mathbf A \left({k ; 1}\right) }\right) | c = [[Expansion Theorem for Determinants|expanding]] the determinant along column $1$ }} {{eqn | r = a_{i_1 1}\left({-1}\right)^{i_1 + 1} \det \left({\mathbf A \left({i_1 ; 1}\right) }\right) + a_{i_2 1} \left({-1}\right)^{i_2 + 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right) }} {{eqn | r = a_{i_2 1} \left({-1}\right)^{i_1 + 1 + i_2 - i_1 - 1} \det \left({\mathbf A \left({i_2; 1}\right) }\right) + a_{i_2 1} \left({-1}\right)^{i_2 + 1} \det \left({\mathbf A \left({i_2 ; 1}\right) }\right) }} {{eqn | r = 0 }} {{end-eqn}} {{qed}}	1
Let $T$ be [[Definition:Separable Space|separable]]. By [[Space is Separable iff Density not greater than Aleph Zero]]: :$\map d T \le \aleph_0$ where: :$\map d T$ denotes the [[Definition:Density of Topological Space|density]] of $T$ :$\aleph$ denotes the [[Definition:Aleph Mapping|aleph mapping]]. By definition of [[Definition:Density of Topological Space|density]]: :$\exists A \subseteq S: A$ is [[Definition:Everywhere Dense|dense]] $\land \map d T = \card A$ where $\card A$ denotes the [[Definition:Cardinality|cardinality]] of $A$. By definition of [[Definition:Everywhere Dense|dense set]]: :$A^- = S$ where $A^-$ denotes the [[Definition:Closure (Topology)|closure]] of $A$. By [[Set in Discrete Topology is Clopen]]: :$A$ is [[Definition:Closed Set (Topology)|closed]] Then by [[Set is Closed iff Equals Topological Closure]]: :$A^- = A$ Thus by [[Countable iff Cardinality not greater than Aleph Zero]]: :$S$ is [[Definition:Countable Set|countable]] {{qed}}	1
=== Sufficient Condition === Assume that $K$ is [[Definition:Everywhere Dense|everywhere dense]] and $x \in K^\perp$. Then: {{begin-eqn}} {{eqn | o = | r = x \in K^\perp }} {{eqn | ll = \leadsto | lo = \forall y \in K : | o = | r = x \perp y }} {{eqn | ll = \leadsto | lo = \forall y \in H : | o = | r = x \perp y | c = $K$ is everywhere dense, and [[Inner Product is Continuous]] }} {{eqn | ll = \leadsto | lo = | o = | r = x = 0 }} {{end-eqn}} {{qed|lemma}} === Necessary Condition === Assume $K^\perp = 0$. Then by [[Double Orthocomplement is Closed Linear Span]], $\vee K=\paren{0}^\perp=H$. Hence $K$ is [[Definition:Everywhere Dense|everywhere dense]]. {{qed|lemma}} {{qed}}	1
=== Positive definiteness === By definition of [[Definition:Induced Norm|induced norm]]: :$\forall y \in Y : \norm {y}_Y = \norm {y}_X > 0$ Suppose $y \in Y : \norm {y}_Y = 0$. Since $\norm {\, \cdot \,}_Y$ is an [[Definition:Induced Norm|induced norm]] in $\struct {X, \norm {\, \cdot \,}_X}$: :$\norm {y}_X = 0$. Therefore, $y = \mathbf 0 \in X$. By definition of a [[Definition:Vector Subspace|vector subspace]], $Y$ has the [[Definition:Vector Space Axioms|additive identity]]: :$\mathbf 0 \in Y$. Thus: :$y = \mathbf 0 \in Y \subseteq X$. === Positive homogeneity === Let $y \in Y$. Let $\alpha \in \R$. Then we have that: {{begin-eqn}} {{eqn | l = \norm {\alpha \cdot y}_Y | r = \norm {\alpha \cdot y}_X | c = {{defof|Induced Norm}} }} {{eqn | r = \size \alpha \norm {y}_X | c = [[Definition:Norm on Vector Space|Norm axiom (N2)]] }} {{eqn | r = \size \alpha \norm {y}_Y | c = {{defof|Induced Norm}}; $y \in Y$ }} {{end-eqn}} === Triangle inequality === Let $y_1, y_2 \in Y$. By [[Definition:Vector Space Axioms|closure axiom]] of [[Definition:Vector Space|vector space]]: :$y_1 + y_2 \in Y$. Furthermore: {{begin-eqn}} {{eqn | l = \norm {y_1 + y_2}_Y | r = \norm {y_1 + y_2}_X | c = {{defof|Induced Norm}} }} {{eqn | o = \le | r = \norm {y_1}_X + \norm {y_2}_X | c = [[Definition:Norm on Vector Space|Norm axiom (N2)]] }} {{eqn | r = \norm {y_1}_Y + \norm {y_2}_Y | c = {{defof|Induced Norm}}; $y_1, y_2 \in Y$ }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = \phi \left({\log_a y}\right) | r = \phi \left({\frac {\ln y}{\ln a} }\right) | c = [[Change of Base of Logarithm]] }} {{eqn | r = \frac 1 {\ln a} \left({\phi \left({\ln y}\right)}\right) | c = definition of [[Definition:Linear Operator|linear operator]] (as $\frac 1 {\ln a}$ is [[Definition:Constant|constant]]) }} {{end-eqn}} {{qed}} [[Category:Linear Algebra]] [[Category:Logarithms]] 17whhp0hyp3jtcb4rjwxzv7tncm83vy	1
{{begin-eqn}} {{eqn | ll= \forall x \in \R: | l = 0 | o = \le | r = \paren {x \map f t + \map g t}^2 }} {{eqn | l = 0 | o = \le | r = \int_a^b \paren {x \map f t + \map g t}^2 \rd t | c = [[Relative Sizes of Definite Integrals]] }} {{eqn | r = x^2 \int_a^b \paren {\map f t}^2 \rd t + 2 x \int_a^b \map f t \, \map g t \rd t + \int_a^b \paren {\map g t}^2 \rd t | c = [[Linear Combination of Integrals]] }} {{eqn | r = A x^2 + 2 B x + C | c = }} {{end-eqn}} where: {{begin-eqn}} {{eqn | l = A | r = \int_a^b \paren {\map f t}^2 \rd t }} {{eqn | l = B | r = \int_a^b \map f t \map g t \rd t }} {{eqn | l = C | r = \int_a^b \paren {\map g t}^2 \rd t }} {{end-eqn}} The [[Definition:Quadratic Equation|quadratic equation]] $A x^2 + 2 B x + C$ is non-negative for all $x$. It follows (using the same reasoning as in [[Cauchy's Inequality]]) that the [[Definition:Discriminant of Quadratic Equation|discriminant]] $\paren {2 B}^2 - 4 A C$ of this polynomial must be non-positive. Thus: :$B^2 \le A C$ and hence the result. {{qed}}	1
Let $\struct {G, \cdot}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $\map {\MM_G} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\struct {G, \cdot}$. Let $\mathbf A$ be an [[Definition:Element|element]] of $\map {\MM_G} {m, n}$. Let $-\mathbf A$ be the [[Definition:Negative Matrix/General Group|negative]] of $\mathbf A$. Then $-\mathbf A$ is the [[Definition:Inverse Element|inverse]] for the operation $\circ$, where $\circ$ is the [[Definition:Hadamard Product|Hadamard product]].	1
Let $\mathbf a$ and $\mathbf b$ be [[Definition:Like Vector Quantities|like vector quantities]]. Then: :$\mathbf {\hat a} = \mathbf {\hat b}$ where $\mathbf {\hat a}$ and $\mathbf {\hat b}$ denote the [[Definition:Unit Vector|unit vectors]] in the [[Definition:Direction|direction]] of $\mathbf a$ and $\mathbf b$.	1
If $\phi = 0$ then the assertion is clear. Let $\phi$ be a non-zero [[Definition:Linear Transformation on Vector Space|linear transformation]]. By [[Dimension of Proper Subspace is Less Than its Superspace]] and [[Generator of Vector Space Contains Basis]], there is an [[Definition:Ordered Basis|ordered basis]] $\sequence {a_n}$ of $G$ such that: :$\exists r \in \N_n: \set {a_k: r + 1 \le k \le n}$ is a basis of $\map \ker \phi$ As a consequence: :$\map \nu \phi = n - r$ and by [[Unique Linear Transformation Between Vector Spaces]]: :$\map \rho \phi = r$ The result follows. {{qed}}	1
Let $\left({\R^n,+,\cdot}\right)_{\R}$ be a [[Real Vector Space is Vector Space|real vector space]]. Let $\mathbf 0 \in \R^n$ be the [[Definition:Zero Vector|zero vector]]. Let $\left \langle {\mathbf v_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Vector (Euclidean Space)|vectors in $\R^n$]]. Then $\left \langle {\mathbf v_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ is '''linearly dependent''' iff: : $\displaystyle \exists \left \langle {\lambda_k} \right \rangle_{1 \mathop \le k \mathop \le n} \subseteq \R: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0$ where not all $\lambda_k$ are equal to $0$. That is, it is possible to find a [[Definition:Linear Combination|linear combination]] of $\left \langle {\mathbf v_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ which equals $\mathbf 0$.	1
First we check the [[Definition:Ring Axioms|ring axioms]]: :$\text A$: The [[Integers Modulo m under Addition form Abelian Group |Integers Modulo $m$ under Addition form Abelian Group]]. :From [[Modulo Addition has Identity]], $\eqclass 0 m$ is the [[Definition:Identity Element|identity]] of the [[Definition:Additive Group of Ring|additive group]] $\struct {\Z_m, +_m}$. From [[Integers Modulo m under Multiplication form Commutative Monoid]]: :$\text M 0$: $\struct {\Z_m, \times_m}$ is [[Definition:Closed Algebraic Structure|closed]]. :$\text M $: $\struct {\Z_m, \times_m}$ is [[Definition:Associative|associative]]. :$\text M 2$: $\struct {\Z_m, \times_m}$ has an [[Definition:Identity Element|identity]] $\eqclass 1 m$. :$\text C$: $\struct {\Z_m, \times_m}$ is [[Definition:Commutative Operation|commutative]]. Then: :$\text D$: [[Modulo Multiplication Distributes over Modulo Addition|$\times_m$ distributes over $+_m$ in $\Z_m$]]. {{qed}}	1
The [[Definition:Identity Element|identity]] of $\struct {G, +_G}$ is usually denoted $\bszero$, or some variant of this, and called the '''zero vector''': :$\forall \mathbf a \in \struct {G, +_G, \circ}_R: \bszero +_G \mathbf a = \mathbf a = \mathbf a +_G \bszero$ Note that on occasion it is advantageous to denote the '''[[Definition:Zero Vector|zero vector]]''' differently, for example by $e$, or $\bszero_V$ or $\bszero_G$, in order to highlight the fact that the '''[[Definition:Zero Vector|zero vector]]''' is not the same object as the [[Definition:Zero Scalar|zero scalar]].	1
By definition of a [[Definition:Right Ideal|right ideal]] then $\circ$ is [[Definition:Well-Defined|well-defined]]. === $(\text {RM} 1)$ : Scalar Multiplication (Right) Distributes over Module Addition === Follows directly from {{Ring-axiom|D}} {{qed|lemma}} === $(\text {RM} 2)$ : Scalar Multiplication (Left) Distributes over Scalar Addition === Follows directly from {{Ring-axiom|D}} {{qed|lemma}} === $(\text {RM} 3)$ : Associativity of Scalar Multiplication === Follows directly from {{Ring-axiom|M1}} {{qed}}	1
Let $K$ be the [[Definition:Field of Quotients|field of quotients]] of $A$. Let $x \in K$ be [[Definition:Integral Element of Ring Extension|integral]] over $A$. Let: $x = a / b$ for $a, b \in A$ with $\gcd \set {a, b} \in A^\times$. This makes sense because a [[UFD is GCD Domain]]. There is an equation: :$\paren {\dfrac a b}^n + a_{n - 1} \paren {\dfrac a b}^{n - 1} + \dotsb + a_0$ {{explain|This is not actually an equation, it has no equals sign in it.}} with $a_i \in A$, $i = 0, \dotsc, n - 1$. Multiplying by $b^n$, we obtain: :$a^n + b c = 0$ with $c \in A$. Therefore: :$b \divides a^n$ {{AimForCont}} $b$ is not a [[Definition:Unit of Ring|unit]]. Then: :$\gcd \set {a, b} \notin A^\times$ which is a [[Definition:Contradiction|contradiction]]. So $b$ is a [[Definition:Unit of Ring|unit]], and: :$a b^{-1} \in A$ {{explain|Make it clear why this fulfils the conditions for the statement of the result}} {{qed}} [[Category:Algebraic Number Theory]] [[Category:Factorization]] [[Category:Unique Factorization Domains]] hc338lvrefaby5nmas6fbieb7ik5n10	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n, m$ be [[Definition:Positive Integer|positive integers]]. Let $E_{ij}$ denote the $n \times n$ [[Definition:Matrix|matrix]] with only zeroes except a $1$ at the $\tuple {i, j}$th [[Definition:Element of Matrix|element]]. Let $A_1, \ldots, A_m \in R^{n \times n}$. Let $i_k, j_k \in \set {1, \ldots, n}$ for $k \in \set {1, \ldots, m}$. Let $i_0 = i_m$ and $j_0 = j_m$. Then: :$\map \tr {A_1 E_{i_1, j_1} A_2 E_{i_2, j_2} \cdots A_m E_{i_m, j_m} } = \displaystyle \prod_{k \mathop = 1}^m \sqbrk {A_k}_{j_{k - 1} i_k}$	1
Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ECO} 3 | t = Interchange [[Definition:Column of Matrix|columns]] $i$ and $j$ | m = \kappa_i \leftrightarrow \kappa_j }} {{end-axiom}}	1
From [[Matrix Entrywise Addition forms Abelian Group]] we have that $\struct {\map {\MM_R} n, +}$ is an [[Definition:Abelian Group|abelian group]], because $\struct {R, +}$ is itself an [[Definition:Abelian Group|abelian group]]. Similarly, it is clear that $\struct {\map {\MM_R} n, \times}$ is a [[Definition:Semigroup|semigroup]], as [[Matrix Multiplication is Closed]] and [[Matrix Multiplication is Associative]]. Finally, we note that [[Matrix Multiplication Distributes over Matrix Addition]]. {{qed}}	1
Intuitively, when we consider $\BB$ and $\CC$ as row vectors, this is because: :$\CC = \BB \cdot \mathbf M_{\BB, \CC}$ and: :$\BB \cdot \sqbrk m_\BB = \CC \cdot \sqbrk m_\CC$ imply: :$\BB \cdot \sqbrk m_\BB = \BB \cdot \mathbf M_{\BB, \CC} \cdot \sqbrk m_\CC$. Because $\BB$ is a [[Definition:Basis of Module|basis]], this implies $\sqbrk m_\BB = \mathbf M_{\BB, \CC} \cdot \sqbrk m_\CC$. This can be formalized by giving $R \times M$ the structure of a [[Definition:Ring (Abstract Algebra)|ring]]. Alternatively, this can be verified directly, which boils down to re-proving that that [[Definition:Matrix Multiplication|matrix multiplication]] is [[Definition:Associative Operation|associative]]. {{ProofWanted}}	1
Let $\norm {\, \cdot \,}$ be a [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Then $\norm {\, \cdot \,}$ is [[Definition:Equivalent Division Ring Norms|equivalent]] to the [[Definition:P-adic Norm|$p$-adic norm $\norm {\, \cdot \,}_p$]] for some [[Definition:Prime Number|prime]] $p$.	1
:[[File:Vector-Components-in-3-Space.png|480px]] By definition, the [[Definition:Direction Cosine|direction cosines]] are the [[Definition:Cosine|cosines]] of the [[Definition:Angle|angles]] that $\mathbf r$ makes with the [[Definition:Coordinate Axis|coordinate axes]]. By definition of the [[Definition:Component of Vector|components of $\mathbf r$]]: :$\mathbf r = x \mathbf i + y \mathbf j + z \mathbf k$ Thus: :$\mathbf r = r \cos \alpha \mathbf i + r \cos \beta \mathbf j + r \cos \gamma \mathbf k$ and the result follows. {{qed}}	1
Let $x$ and $y$ be elements of either the [[Definition:Real Number|real numbers]] $\R$ or the [[Definition:Complex Number|complex numbers]] $\C$. Then: :$\cmod {x - y} \ge \size {\cmod x - \cmod y}$	1
:$H$ has at most $n$ [[Definition:Element|elements]].	1
First, suppose $k \perp m$. That is: :$\gcd \set {k, m} = 1$ Then, by [[Bézout's Identity]]: :$\exists u, v \in \Z: u k + v m = 1$ Thus: :$\eqclass {u k + v m} m = \eqclass {u k} m = \eqclass u m \eqclass k m = \eqclass 1 m$ Thus $\eqclass u m$ is an [[Definition:Multiplicative Inverse|inverse]] of $\eqclass k m$. Suppose that: :$\exists u \in \Z: \eqclass u m \eqclass k m = \eqclass {u k} m = 1$. Then: :$u k \equiv 1 \pmod m$ and: :$\exists v \in \Z: u k + v m = 1$ Thus from [[Bézout's Identity]]: :$k \perp m$ {{qed}}	1
:$\set {\mathbf x: \mathbf A \mathbf x = \mathbf 0} = \set {\mathbf x: \map {\mathrm {ref} } {\mathbf A} \mathbf x = \mathbf 0}$ where $\map {\mathrm {ref} } {\mathbf A}$ is the [[Definition:Reduced Echelon Form|reduced echelon form]] of $\mathbf A$.	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A0}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Closed Algebraic Structure|closed]]. The result then follows directly from [[Closure of Hadamard Product]]. {{qed}}	1
We note that: :$D = \begin{vmatrix} 1 & b_{12} & \cdots & b_{1n} \\ 0 & b_{22} & \cdots & b_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & b_{n2} & \cdots & b_{nn} \end{vmatrix}$ is the [[Definition:Transpose of Matrix|transpose]] of: :$D^\intercal = \begin{vmatrix} 1 & 0 & \cdots & 0 \\ b_{12} & b_{22} & \cdots & b_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ b_{1n} & b_{2n} & \cdots & b_{nn} \end{vmatrix}$ From [[Determinant with Unit Element in Otherwise Zero Row]]: :$D^\intercal = \begin{vmatrix} b_{22} & \cdots & b_{n2} \\ \vdots & \ddots & \vdots \\ b_{2n} & \cdots & b_{nn} \end{vmatrix}$ The result follows by [[Determinant of Transpose]]. {{Qed}}	1
Let $S$ be a non-zero [[Definition:Vector Subspace|subspace]] of $\left({\R^2, +, \times}\right)_\R$. Then $S$ contains a [[Definition:Zero Vector|non-zero vector]] $\left({\alpha_1, \alpha_2}\right)$. Hence $S$ also contains $\left\{{\lambda \times \left({\alpha_1, \alpha_2}\right), \lambda \in \R}\right\}$. From [[Equation of Straight Line in Plane]], this set may be described as a line through the origin. Suppose $S$ also contains a [[Definition:Zero Vector|non-zero vector]] $\left({\beta_1, \beta_2}\right)$ which is not on that line. Then $\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1 \ne 0$. Otherwise $\left({\beta_1, \beta_2}\right)$ would be $\zeta \times \left({\alpha_1, \alpha_2}\right)$, where either $\zeta = \beta_1 / \alpha_1$ or $\zeta = \beta_2 / \alpha_2$ according to whether $\alpha_1 \ne 0$ or $\alpha_2 \ne 0$. But then $S = \left({\R^2, +, \times}\right)_\R$. Because, if $\left({\gamma_1, \gamma_2}\right)$ is any vector at all, then: : $\left({\gamma_1, \gamma_2}\right) = \lambda \times \left({\alpha_1, \alpha_2}\right) + \mu \times \left({\beta_1, \beta_2}\right)$ where $\lambda = \dfrac {\gamma_1 \times \beta_2 - \gamma_2 \times \beta_1} {\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1}, \mu = \dfrac {\alpha_1 \times \gamma_2 - \alpha_2 \times \gamma_1} {\alpha_1 \times \beta_2 - \alpha_2 \times \beta_1}$ which we get by solving the simultaneous eqns: {{begin-eqn}} {{eqn | l=\alpha_1 \times \lambda + \beta_1 \times \mu | r=0 | c= }} {{eqn | l=\alpha_2 \times \lambda + \beta_2 \times \mu | r=0 | c= }} {{end-eqn}} The result follows. {{qed}}	1
$G$ can be seen as functions: :$\displaystyle f: A \to \bigcup_{a \mathop \in A} G_a$ {{explain|Clarify the above sentence.}} Let $a \in A$. Let $x, y \in G_a$. Let $r \in R$. So both $x + y \in G_a$ and $r x \in G_a$. Let $b \in A$. === Case 1 === {{explain|Explain and clarify the notation}} Let $b = a$. Then: :$\map {\map {\inj_a} {x + y} } b = x + y = \map {\map {\inj_a} x} b + \map {\map {\inj_a} y} b$ and: :$\map {\map {\inj_a} {r x} } b = r x = r \, \map {\map {\inj_a} x} b$ {{qed|lemma}} === Case 2 === Let $b \ne a$. Then: :$\map {\map {\inj_a} {x + y} } b = 0 + 0 = \map {\map {\inj_a} x} b + \map {\map {\inj_a} y} b$ and: :$\map {\map {\inj_a} {r x} } b = 0 = \map r 0 = r \, \map {\map {\inj_a} x} b$ Therefore: :$\map {\inj_a} {x + y} = \map {\inj_a} x + \map {\inj_a} y$ and: :$\map {\inj_a} {r x} = r \, \map {\inj_a} x$ {{qed|lemma}} So $\inj_a$ is a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]]. Combined with [[Canonical Injection is Injection]] gives that $\inj_a$ is a [[Definition:R-Algebraic Structure Monomorphism|monomorphism]]. {{qed}}	1
Let $T = \struct {S, \tau_p}$ be an [[Definition:Uncountable Particular Point Topology|uncountable particular point space]]. Let $H = S \setminus \set p$ where $\setminus$ denotes [[Definition:Set Difference|set difference]]. Then $H$ is not [[Definition:Separable Space|separable]].	1
Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ECO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Column of Matrix|column]] $j$ to [[Definition:Row of Matrix|row]] $i$ | m = \kappa_i \to \kappa_i + \lambda r_j }} {{end-axiom}}	1
Let $S \subseteq F \sqbrk X$ denote the [[Definition:Subset|subset]] of $F \sqbrk X$ defined as: :$S = \set {\mathbf x \in F \sqbrk X: \map \deg {\mathbf x} < d}$ for some $d \in \Z_{>0}$. Then $S$ is an [[Definition:Vector Space|vector space over $F$]].	1
Let $\struct {\ell^2, \norm {\, \cdot \,}_2}$ be the [[Definition:P-Sequence Space|2-sequence space]] equipped with [[Definition:Euclidean Norm|Euclidean norm]]. Let $c_{00}$ be the [[Definition:Space of Almost-Zero Sequences|space of almost-zero sequences]]. Then $c_{00}$ is [[Definition:Everywhere Dense/Normed Vector Space|everywhere dense]] in $\struct {\ell^2, \norm {\, \cdot \,}_2}$	1
We need to show that: $\forall x, y, \in G, \forall \lambda, \mu \in R$: :$(1): \quad \lambda \circ \paren {x + y} = \paren {\lambda \circ x} + \paren {\lambda \circ y}$ :$(2): \quad \paren {\lambda +_R \mu} \circ x = \paren {\lambda \circ x} + \paren {\mu \circ x}$ :$(3): \quad \paren {\lambda \times_R \mu} \circ x = \lambda \circ \paren {\mu \circ x}$ Checking the criteria in order: === Criterion 1 === :$(1): \quad \lambda \circ \paren {x + y} = \paren {\lambda \circ x} + \paren {\lambda \circ y}$ Let $x = \tuple {x_1, x_2, \ldots, x_n}, y = \tuple {y_1, y_2, \ldots, y_n} \in G$. {{begin-eqn}} {{eqn | l = \lambda \circ \paren {x + y} | r = \lambda \circ \paren {\tuple {x_1, x_2, \ldots, x_n} + \tuple {y_1, y_2, \ldots, y_n} } | c = }} {{eqn | r = \lambda \circ \tuple {x_1 +_1 y_1, x_2 +_2 y_2, \ldots, x_n +_n y_n} | c = }} {{eqn | r = \tuple {\lambda \circ_1 \paren {x_1 + y_1}, \lambda \circ_2 \paren {x_2 + y_2}, \ldots, \lambda \circ_n \paren {x_n + y_n} } | c = }} {{eqn | r = \tuple {\lambda \circ_1 x_1, \lambda \circ_2 x_2, \ldots, \lambda \circ_n x_n} + \tuple {\lambda \circ_1 y_1, \lambda \circ_2 y_2, \ldots, \lambda \circ_n y_n} | c = }} {{eqn | r = \paren {\lambda \circ \tuple {x_1, x_2, \ldots, x_n} } + \paren {\lambda \circ \tuple {y_1, y_2, \ldots, y_n} } | c = }} {{eqn | r = \paren {\lambda \circ x} + \paren {\lambda \circ y} | c = }} {{end-eqn}} So $(1)$ holds. {{qed|lemma}} === Criterion 2 === :$(2): \quad \paren {\lambda +_R \mu} \circ x = \paren {\lambda \circ x} + \paren {\mu \circ x}$ Let $x = \tuple {x_1, x_2, \ldots, x_n} \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda +_R \mu} \circ x | r = \paren {\lambda +_R \mu} \circ \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda +_R \mu} \circ_1 x_1, \paren {\lambda +_R \mu} \circ_2 x_2, \ldots, \paren {\lambda +_R \mu} \circ_n x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda \circ_1 x_1} +_1 \paren {\mu \circ_1 x_1}, \paren {\lambda \circ_2 x_2} +_2 \paren {\mu \circ_2 x_2}, \ldots, \paren {\lambda \circ_n x_n} +_n \paren {\mu \circ_n x_n} } | c = }} {{eqn | r = \tuple {\lambda \circ_1 x_1, \lambda \circ_2 x_2, \lambda \circ_n x_n} + \tuple {\mu \circ_1 x_1, \mu \circ_2 x_2, \ldots, \mu \circ_n x_n} | c = }} {{eqn | r = \paren {\lambda \circ_1 \tuple {x_1, x_2, \ldots, x_n} } + \paren {\mu \circ_1 \tuple {x_1, x_2, \ldots, x_n} } | c = }} {{eqn | r = \paren {\lambda \circ x} + \paren {\mu \circ x} | c = }} {{end-eqn}} So $(2)$ holds. {{qed|lemma}} === Criterion 3 === :$(3): \quad \paren {\lambda \times_R \mu} \circ x = \lambda \circ \paren {\mu \circ x}$ Let $x = \tuple {x_1, x_2, \ldots, x_n} \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda \times_R \mu} \circ x | r = \paren {\lambda \times_R \mu} \circ \tuple {x_1, x_2, \ldots, x_n} | c = }} {{eqn | r = \tuple {\paren {\lambda \times_R \mu} \circ_1 x_1, \paren {\lambda \times_R \mu} \circ_2 x_2, \ldots, \paren {\lambda \times_R \mu} \circ_n x_n} | c = }} {{eqn | r = \tuple {\lambda \circ_1 \paren {\mu \circ_1 x_1}, \lambda \circ_2 \paren {\mu \circ_2 x_2}, \ldots, \lambda \circ_n \paren {\mu \circ_n x_n} } | c = }} {{eqn | r = \lambda \circ \tuple {\mu \circ_1 x_1, \mu \circ_2 x_2, \ldots, \mu \circ_n x_n} | c = }} {{eqn | r = \lambda \circ \paren {\mu \circ \tuple {x_1, x_2, \ldots, x_n} } | c = }} {{eqn | r = \lambda \circ \paren {\mu \circ x} | c = }} {{end-eqn}} So $(3)$ holds. {{qed|lemma}} Hence the result. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm|metric induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm {\,\cdot\,}$. The [[Definition:Mapping|mapping]] $\norm {\,\cdot\,} : \struct {R, d} \to \R$ is [[Definition:Continuous Mapping (Metric Space)|continuous]].	1
From [[Equation of Straight Line in Plane]], the equation for a [[Definition:Straight Line|straight line]] is: :$A x + B y + C = 0$ Thus: {{begin-eqn}} {{eqn | l = A x + B y + C | r = 0 | c = }} {{eqn | ll= \leadsto | l = \frac A 2 \paren {z + \overline z} + B y + C | r = 0 | c = [[Sum of Complex Number with Conjugate]] }} {{eqn | ll= \leadsto | l = \frac A 2 \paren {z + \overline z} + \frac B {2 i} \paren {z - \overline z} + C | r = 0 | c = [[Difference of Complex Number with Conjugate]] }} {{eqn | ll= \leadsto | l = \paren {\frac A 2 + \frac B {2 i} } z + \paren {\frac A 2 - \frac B {2 i} } \overline z + C | r = 0 | c = gathering terms }} {{end-eqn}} The result follows by setting $\beta := \dfrac A 2 + \dfrac B {2 i}$ and $\gamma := C$. {{qed}}	1
Let $x, y, z \in R$. {{begin-eqn}} {{eqn | l = \map d {x, y} | r = \norm {x - y} | c = {{Defof|Metric Induced by Norm on Division Ring|Metric Induced by $\norm {\,\cdot\,}$}} }} {{eqn | r = \norm {\paren {x - z} + \paren {z - y} } }} {{eqn | r = \max \set {\norm {x - z}, \norm {z - y} } | o = \le | c = {{Defof|Non-Archimedean Division Ring Norm}} }} {{eqn | r = \max \set {\map d {x, z}, \map d {z, y} } | c = {{Defof|Metric Induced by Norm on Division Ring|Metric Induced by $\norm {\,\cdot\,}$}} }} {{end-eqn}}	1
Let the [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator on $A$ be $*$. Let $\left({a, b}\right), \left({c, d}\right), \left({e, f}\right) \in A'$. In order to streamline notation, let $\oplus$ and $\oplus'$ both be denoted by [[Definition:Multiplicative Notation|product notation]]: :$a \oplus b =: a b$ :$x \oplus' y =: x y$ The context will make it clear which is meant. Suppose $A$ is [[Definition:Commutative Algebra|commutative]] and [[Definition:Associative Algebra|associative]]. Then: {{begin-eqn}} {{eqn | r = \left({\left({a, b}\right) \left({c, d}\right)}\right) \left({e, f}\right) | o = | c = }} {{eqn | r = \left({a c - d b^*, a^* d + c b}\right) \left({e, f}\right) | c = }} {{eqn | r = \left({\left({a c - d b^*}\right) e - f \left({a^* d + c b}\right)^*, \left({a c - d b^*}\right)^* f + e \left({a^* d + c b}\right)}\right) | c = }} {{eqn | r = \left({\left({a c}\right) e - \left({d b^*}\right) e - f \left({a^* d}\right)^* - f \left({c b}\right)^*, \left({a c}\right)^* f - \left({d b^*}\right)^* f + e \left({a^* d}\right) + e \left({c b}\right)}\right) | c = }} {{eqn | r = \left({\left({a c}\right) e - \left({d b^*}\right) e - f \left({d^* a}\right) - f \left({b^* c^*}\right), \left({c^* a^*}\right) f - \left({b d^*}\right) f + e \left({a^* d}\right) + e \left({c b}\right)}\right) | c = Definition of [[Definition:Conjugation (Abstract Algebra)|Conjugation]] }} {{eqn | r = \left({a \left({c e}\right) - a \left({f d^*}\right) - \left({c^* f}\right) b^* - \left({e d}\right) b^*, a^* \left({c^* f}\right) + a^* \left({e d}\right) + \left({c e}\right) b - \left({f d^*}\right) b}\right) | c = $A$ is [[Definition:Commutative Algebra|commutative]] and [[Definition:Associative Algebra|associative]] }} {{eqn | r = \left({a \left({c e - f d^*}\right) - \left({c^* f + e d}\right) b^*, a^* \left({c^* f + e d}\right) + \left({c e - f d^*}\right) b}\right) | c = }} {{eqn | r = \left({a, b}\right) \left({ce - fd^*, c^* f + e d}\right) | c = }} {{eqn | r = \left({a, b}\right) \left({\left({c, d}\right) \left({e, f}\right)}\right) | c = }} {{end-eqn}} {{qed|lemma}} Now suppose $A'$ is an [[Definition:Associative Algebra|associative algebra]]. By picking apart the above series of equations, it is clear that $A$ has to be both [[Definition:Commutative Algebra|commutative]] and [[Definition:Associative Algebra|associative]] in order to assure the [[Definition:Associative Algebra|associativity]] of $A'$. {{qed}}	1
Let $\phi: G \to H$ be a [[Definition:Linear Transformation|linear transformation]] where $G$ and $H$ are [[Definition:Module|$R$-modules]]. Let $e_H$ be the [[Definition:Identity Element|identity of $H$]]. The '''kernel''' of $\phi$ is defined as: :$\map \ker \phi := \phi^{-1} \sqbrk {\set {e_H} }$. where $\phi^{-1} \sqbrk S$ denotes the [[Definition:Preimage of Subset under Mapping|preimage of $S$ under $\phi$]]. === [[Definition:Kernel of Linear Transformation/Vector Space|In Vector Space]] === {{:Definition:Kernel of Linear Transformation/Vector Space}}	1
Let $H_n$ be the [[Definition:Hilbert Matrix|Hilbert matrix]] of [[Definition:Order of Square Matrix|order $n$]]: :$\begin{bmatrix} a_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac 1 {i + j - 1} \end{bmatrix}$ Consider its [[Definition:Inverse Matrix|inverse]] $H_n^{-1}$. All the [[Definition:Element of Matrix|elements]] of $H_n^{-1}$ are [[Definition:Integer|integers]].	1
Let $\mathbf A$ and $\mathbf B$ be [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\mathbf A \mathbf B$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A$ and $\mathbf B$. Then: :$\ds \map \tr {\mathbf A \mathbf B} = \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n a_{i j} b_{j i}$ where $\map \tr {\mathbf A \mathbf B}$ denotes the [[Definition:Trace of Matrix|trace]] of $\mathbf A \mathbf B$. Using the [[Definition:Einstein Summation Convention|Einstein summation convention]], this can be expressed as: :$\map \tr {\mathbf A \mathbf B} = a_{i j} b_{j i}$	1
{{TFAE|def = Associate in Integral Domain|context = Integral Domain|view = Associate}} Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]]. Let $x, y \in D$.	1
Let $K$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $K^K$ be the [[Definition:Module of All Mappings|$K$-module mappings]] $f: K \to K$. Let $P \left({K}\right) \subseteq K^K$ be the [[Definition:Set|set]] of all [[Definition:Polynomial Function (Abstract Algebra)|polynomial functions]] on $K$. Then $P \left({K}\right)$ is a $K$-[[Definition:Submodule|submodule]] of $K^K$.	1
Let: {{begin-eqn}} {{eqn | l = \mathbf A_{m \times n} | r = \begin{bmatrix} \mathbf a_1 & \mathbf a_2 & \cdots & \mathbf a_n \end{bmatrix} }} {{end-eqn}} be a [[Definition:Matrix|matrix]] where: :$\forall i: 1 \le i \le n: \mathbf a_i = \begin{bmatrix} a_{1i} \\ a_{2i} \\ \vdots \\ a_{mi} \end{bmatrix} \in \R^m$ are [[Definition:Vector (Euclidean Space)|vectors]]. Then: :$\set {\mathbf a_1, \mathbf a_2, \cdots, \mathbf a_n}$ is a [[Definition:Linearly Independent Set|linearly independent set]] {{iff}}: :$\map {\mathrm N} {\mathbf A} = \set {\mathbf 0_{n \times 1} }$ where $\map {\mathrm N} {\mathbf A}$ is the [[Definition:Null Space|null space of $\mathbf A$]].	1
{{begin-eqn}} {{eqn | l = \mathbf b \times \mathbf c | r = \begin {bmatrix} b_x \\ b_y \\ b_z \end {bmatrix} \times \begin {bmatrix} c_x \\ c_y \\ c_z \end {bmatrix} }} {{eqn | r = \begin {bmatrix} b_y c_z - b_z c_y \\ b_z c_x - b_x c_z \\ b_x c_y - b_y c_x \end {bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | l = \mathbf a \times \paren {\mathbf b \times \mathbf c} | r = \begin {bmatrix} a_x \\ a_y \\ a_z \end {bmatrix} \times \begin {bmatrix} b_y c_z - b_z c_y \\ b_z c_x - b_x c_z \\ b_x c_y - b_y c_x \end {bmatrix} }} {{eqn | r = \begin {bmatrix} a_y b_x c_y - a_y b_y c_x - a_z b_z c_x + a_z b_x c_z \\ a_z b_y c_z - a_z b_z c_y - a_x b_x c_y + a_x b_y c_x \\ a_x b_z c_x - a_x b_x c_z - a_y b_y c_z + a_y b_z c_y \end {bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = \begin {bmatrix} a_y b_x c_y - a_y b_y c_x - a_z b_z c_x + a_z b_x c_z + a_x b_x c_x - a_x b_x c_x \\ a_z b_y c_z - a_z b_z c_y - a_x b_x c_y + a_x b_y c_x + a_y b_y c_y - a_y b_y c_y \\ a_x b_z c_x - a_x b_x c_z - a_y b_y c_z + a_y b_z c_y + a_z b_z c_z - a_z b_z c_z \end {bmatrix} | c = adding $0 = a_i b_i c_i - a_i b_i c_i$ to each entry }} {{eqn | r = \begin {bmatrix} b_x \paren {a_y c_y + a_z c_z + a_x c_x} - c_x \paren {a_y b_y + a_z b_z + a_x b_x} \\ b_y \paren {a_z c_z + a_x c_x + a_y c_y} - c_y \paren {a_z b_z + a_x b_x + a_y c_y} \\ b_z \paren {a_x c_x + a_y c_y + a_z c_z} - c_z \paren {a_x b_x + a_y b_y + a_z c_z} \end {bmatrix} }} {{eqn | r = \begin {bmatrix} b_x \paren {\mathbf a \cdot \mathbf c} - c_x \paren {\mathbf a \cdot \mathbf b} \\ b_y \paren {\mathbf a \cdot \mathbf c} - c_y \paren {\mathbf a \cdot \mathbf b} \\ b_z \paren {\mathbf a \cdot \mathbf c} - c_z \paren {\mathbf a \cdot \mathbf b} \end {bmatrix} | c = {{Defof|Dot Product}} }} {{eqn | r = \paren {\mathbf a \cdot \mathbf c} \begin {bmatrix} b_x \\ b_y \\ b_z \end {bmatrix} - \paren {\mathbf a \cdot \mathbf b} \begin {bmatrix} c_x \\ c_y \\ c_z \end {bmatrix} }} {{eqn | r = \paren {\mathbf a \cdot \mathbf c} \mathbf b - \paren {\mathbf a \cdot \mathbf b} \mathbf c }} {{end-eqn}} {{qed}} {{Namedfor|Joseph Louis Lagrange|cat = Lagrange}}	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\hointr a {a + 2 \pi}$ be a [[Definition:Half-Open Real Interval|half open interval]] of [[Definition:Length of Real Interval|length]] $2 \pi$. Let $r \in \hointr 0 {+\infty}$ and $\theta \in \hointr a {a + 2 \pi}$. Then: :$r = \cmod z$ and $\theta = \map \arg z$ {{iff}}: :$z = r e^{i \theta}$ where: :$\cmod z$ denotes the [[Definition:Modulus of Complex Number|modulus]] of $z$ :$\map \arg z$ denotes the [[Definition:Argument of Complex Number|argument]] of $z$ :$x \mapsto e^x$ is the [[Definition:Complex Exponential Function|complex exponential function]]. If $z = 0$ or $r = 0$, then $\theta$ may be any number in $\hointr a {a + 2 \pi}$.	1
{{begin-eqn}} {{eqn | l = \lambda \circ \bszero | r = \lambda \circ \paren {\bszero + \bszero} | c = {{Vector-space-axiom|3}} }} {{eqn | r = \lambda \circ \bszero + \lambda \circ \bszero | c = {{Vector-space-axiom|6}} }} {{eqn | ll= \leadsto | l = \lambda \circ \bszero + \paren {-\lambda \circ \bszero} | r = \paren {\lambda \circ \bszero + \lambda \circ \bszero} + \paren {-\lambda \circ \bszero} | c = adding $-\lambda \circ \bszero$ to both sides }} {{eqn | r = \lambda \circ \bszero + \paren {\lambda \circ \bszero + \paren {-\lambda \circ \bszero} } | c = {{Vector-space-axiom|2}} }} {{eqn | ll= \leadsto | l = \bszero | r = \lambda \circ \bszero + \bszero | c = {{Vector-space-axiom|4}} }} {{eqn | r = \lambda \circ \bszero | c = {{Vector-space-axiom|3}} }} {{end-eqn}} {{qed}}	1
Follows from the [[Definition:Arithmetic Mean|definition of arithmetic mean]] and from [[Summation is Linear]]. {{qed}}	1
Then: :$\forall y \in R:\norm y_1 = 1 \iff \norm y_2 = 1$	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $R$. Then: :$\lim_{n \mathop \to \infty} \norm {x_{n + 1} - x_n} = 0$	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Then $\struct {R, +, \circ}$ is a [[Definition:Field (Abstract Algebra)|field]] {{iff}} the only [[Definition:Ideal of Ring|ideals]] of $\struct {R, +, \circ}$ are $\struct {R, +, \circ}$ and $\set {0_R}$.	1
Let $\mathbf A$ be a [[Definition:Matrix|matrix]] in the [[Definition:Matrix Space|matrix space]] $\map {\MM_\R} {m, n}$ such that: :$\mathbf A \mathbf x = \mathbf 0$ represents a [[Definition:Homogeneous Linear Equations|homogeneous system of linear equations]]. The [[Definition:Null Space|null space]] of $\mathbf A$ is the same as that of the null space of the [[Definition:Reduced Echelon Matrix|reduced row echelon form]] of $\mathbf A$: :$\map {\mathrm N} {\mathbf A} = \map {\mathrm N} {\map {\mathrm {rref} } {\mathbf A} }$	1
Let $\R^n$ be a [[Definition:Real Vector Space|real vector space]]. Let $\mathbb W \subseteq \R^n$. Then $\mathbb W$ is a [[Definition:Vector Subspace|linear subspace]] of $\R^n$ {{iff}}: :$(1): \quad \mathbf 0 \in \mathbb W$, where $\mathbf 0$ is the [[Definition:Zero Vector|zero vector]] with $n$ entries :$(2): \quad \mathbb W$ is [[Definition:Closure (Abstract Algebra)|closed]] under [[Definition:Vector Sum|vector addition]] :$(3): \quad \mathbb W$ is [[Definition:Closure (Abstract Algebra)|closed]] under [[Definition:Scalar Multiplication on Vector Space|scalar multiplication]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\MM_R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $R$]]. The [[Definition:Unit Matrix|unit matrix]] over $R$: :$\mathbf I_n = \begin {pmatrix} 1_R & 0_R & 0_R & \cdots & 0_R \\ 0_R & 1_R & 0_R & \cdots & 0_R \\ 0_R & 0_R & 1_R & \cdots & 0_R \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0_R & 0_R & 0_R & \cdots & 1_R \end {pmatrix}$ is the [[Definition:Identity Element|identity element]] of $\struct {\map {\MM_R} n, +, \times}$.	1
Let $\struct {D, +, \circ}$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $a_1, a_2, \dotsc, a_n$ be non-[[Definition:Ring Zero|zero]] [[Definition:Element|elements]] of $D$. Let $J$ be the [[Definition:Set|set]] of all [[Definition:Linear Combination|linear combinations]] in $D$ of $\set {a_1, a_2, \dotsc, a_n}$ Then for some $x \in D$: :$J = \ideal x$ where $\ideal x$ denotes the [[Definition:Principal Ideal of Ring|principal ideal]] [[Definition:Generator of Ideal|generated]] by $x$.	1
Let $\C$ denote the set of [[Definition:Complex Number|complex numbers]]. Then the [[Definition:Complex Vector Space|complex vector space $\C^n$]] is a [[Definition:Vector Space|vector space]].	1
{{begin-eqn}} {{eqn | l = \map {\paren {\theta_{c, d} \circ \theta_{a, b} } } x | r = \map {\theta_{c, d} } {\map {\theta_{a, b} } x} | c = }} {{eqn | r = \map {\theta_{c, d} } {a x + b} | c = }} {{eqn | r = c \paren {a x + b} + d | c = }} {{eqn | r = \paren {a c} x + \paren {b c + d} | c = }} {{eqn | r = \theta_{a c, b c + d} | c = }} {{end-eqn}} {{qed}}	1
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]] whose [[Definition:Topology Induced by Metric|induced topology]] is [[Definition:Separable Space|separable]]. Then $M$ is [[Definition:Homeomorphic Topological Spaces|homeomorphic]] to a [[Definition:Topological Subspace|subspace]] of the [[Definition:Fréchet Space (Functional Analysis)|Fréchet space]] $\struct {\R^\omega, d}$ on the [[Definition:Countable-Dimensional Real Cartesian Space|countable-dimensional real Cartesian space]] $\R^\omega$.	1
Let $B$ be the [[Definition:Set|set]] of all the [[Definition:Identity Mapping|identity functions]] $I^k$ on $F_n \sqbrk X$ where $n \in \Z_{\ge 0}$. By definition, every element of $F_n \sqbrk X$ is a [[Definition:Linear Combination|linear combination]] of $B$. Suppose: :$\displaystyle \sum_{k \mathop = 0}^m \alpha_k I^k = 0, \alpha_m \ne 0$ Then by [[Definition:Differentiation|differentiating]] $m$ times, we obtain from [[Nth Derivative of Nth Power]]: :$m! \alpha_m = 0$ whence $\alpha_m = 0$ which is a contradiction. Hence $B$ is [[Definition:Linearly Independent Set|linearly independent]] and therefore is a [[Definition:Basis of Vector Space|basis]] for $F_n \sqbrk X$. The result follows from [[Linearly Independent Set is Basis iff of Same Cardinality as Dimension]]. {{qed}}	1
Let $z_1 = a_1 + i a_2, z_2 = b_1 + i b_2$. {{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2} | o = \le | r = \cmod {z_1} + \cmod {z_2} | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {\paren {a_1 + b_1}^2 + \paren {a_2 + b_2}^2}^{\frac 1 2} | o = \le | r = \paren { {a_1}^2 + {a_2}^2}^{\frac 1 2} + \paren { {b_1}^2 + {b_2}^2}^{\frac 1 2} | c = {{Defof|Complex Modulus}} }} {{eqn | ll= \leadstoandfrom | l = \paren {a_1 + b_1}^2 + \paren {a_2 + b_2}^2 | o = \le | r = {a_1}^2 + {a_2}^2 + {b_1}^2 + {b_2}^2 + 2 \paren { {a_1}^2 + {a_2}^2}^{\frac 1 2} \paren { {b_1}^2 + {b_2}^2}^{\frac 1 2} | c = squaring both sides }} {{eqn | ll= \leadstoandfrom | l = {a_1}^2 + 2 a_1 b_1 + {b_1}^2 + {a_2}^2 + 2 a_2 b_2 + {b_2}^2 | o = \le | r = {a_1}^2 + {a_2}^2 + {b_1}^2 + {b_2}^2 + 2 \paren { {a_1}^2 + {a_2}^2}^{\frac 1 2} \paren { {b_1}^2 + {b_2}^2}^{\frac 1 2} | c = multiplying out }} {{eqn | ll= \leadstoandfrom | l = a_1 b_1 + a_2 b_2 | o = \le | r = \paren { {a_1}^2 + {a_2}^2}^{\frac 1 2} \paren { {b_1}^2 + {b_2}^2}^{\frac 1 2} | c = simplifying }} {{eqn | ll= \leadstoandfrom | l = \paren {a_1 b_1 + a_2 b_2}^2 | o = \le | r = \paren { {a_1}^2 + {a_2}^2} \paren { {b_1}^2 + {b_2}^2} | c = [[Cauchy's Inequality]] }} {{eqn | ll= \leadstoandfrom | l = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 | o = \le | r = {a_1}^2 {b_1}^2 + {a_2}^2 {b_2}^2 + {a_1}^2 {b_2}^2 + {a_2}^2 {b_1}^2 | c = }} {{eqn | ll= \leadstoandfrom | l = 2 a_1 b_1 a_2 b_2 | o = \le | r = {a_1}^2 {b_2}^2 + {a_2}^2 {b_1}^2 | c = }} {{eqn | ll= \leadstoandfrom | l = 0 | o = \le | r = \paren {a_1 b_2}^2 - 2 \paren {a_1 b_2} \paren {a_2 b_1} + \paren {a_2 b_1}^2 | c = }} {{eqn | ll= \leadstoandfrom | l = 0 | o = \le | r = \paren {a_1 b_2 - a_2 b_1}^2 | c = }} {{end-eqn}} The final statement is a [[Definition:Tautology|tautology]], and all implications are reversible. Hence the result. {{qed}}	1
For each $r \in R$, we have $r$ is a zero of $a - r = 0$. {{MissingLinks|zero}} Hence $R \subseteq C$, and in particular, $C$ is [[Definition:Non-Empty Set|non-empty]]. By [[Subring Test]] it is sufficient to prove that if $x, y \in C$ then $x + \paren {-y}, x \circ y \in C$. Let $x, y \in C$. By [[Equivalent Definitions of Integral Dependence|Equivalent Definitions of Integral Dependence: $(1) \implies (2)$]], $R \sqbrk x$ and $R \sqbrk y$ are [[Definition:Finitely Generated Module|finitely generated]] over $R$. Because $R \subseteq R \sqbrk x$, $R \sqbrk x \sqbrk y$ is also [[Definition:Finitely Generated Module|finitely generated]] over $R \sqbrk x$. Therefore, we have [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Ring Extension|extensions]]: :$R \hookrightarrow R \sqbrk x \hookrightarrow R \sqbrk x \sqbrk y = R \sqbrk {x, y}$ Hence, by [[Transitivity of Finite Generation]], $R \sqbrk {x, y}$ is [[Definition:Finitely Generated Module|finitely generated]] over $R$. So by [[Equivalent Definitions of Integral Dependence|Equivalent Definition of Integral Dependence: $(2) \implies (1)$]], every element of $R \sqbrk {x, y}$ is [[Definition:Integral Element of Ring Extension|integral]] over $R$. In particular, $x + \paren {-y}$ and $x \circ y$ are [[Definition:Integral Element of Ring Extension|integral]] over $R$. {{qed}} [[Category:Commutative Algebra]] [[Category:Algebraic Number Theory]] a4tpxtnv2uundr184ymd3rw6xox1tln	1
Let $\mathbf A = \sqbrk a_{m n}, \mathbf B = \sqbrk b_{n p}$ {{begin-eqn}} {{eqn | lo= \forall i \in \closedint 1 m, j \in \closedint 1 p: | l = \mathbf A \paren {\lambda \mathbf B} | r = \lambda \sum_{k \mathop = 1}^n a_{i k} b_{k j} | c = {{Defof|Matrix Product (Conventional)}} and {{Defof|Matrix Scalar Product}} }} {{eqn | r = \sum_{k \mathop = 1}^n a_{i k} \paren {\lambda b_{k j} } }} {{eqn | r = \mathbf A \paren {\lambda \mathbf B} | c = {{Defof|Matrix Product (Conventional)}} and {{Defof|Matrix Scalar Product}} }} {{end-eqn}} {{qed}} {{proofread}} {{expand|proof literally carries over for any [[Definition:Commutative Ring|commutative ring]] in place of $\Bbb F$}} [[Category:Conventional Matrix Multiplication]] b2tvzdjy8an71tgdj8k0a3t4rceaq58	1
Let $\mathbf u$ and $\mathbf v$ be [[Definition:Vector (Linear Algebra)|vectors]]. Consider a [[Definition:Parallelogram|parallelogram]], two of whose adjacent sides represent $\mathbf y$ and $\mathbf v$ (in [[Definition:Magnitude|magnitude]] and [[Definition:Direction|direction]]). :[[File:ParallelogramLaw-Difference.png|400px]] Then the [[Definition:Diagonal of Quadrilateral|diagonal]] of the [[Definition:Parallelogram|parallelogram]] connecting the [[Definition:Terminal Point of Vector|terminal points]] of $\mathbf u$ and $\mathbf v$ represents the [[Definition:Magnitude|magnitude]] and [[Definition:Direction|direction]] of $\mathbf u - \mathbf v$, the [[Definition:Vector Difference|difference]] of $\mathbf u$ and $\mathbf v$.	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]] over $K$. Then the [[Definition:Rank of Matrix|rank]] of $\mathbf A$ is the [[Definition:Dimension of Vector Space|dimension]] of the [[Definition:Vector Subspace|subspace]] of $K^n$ [[Definition:Generator of Module|generated]] by the [[Definition:Row Matrix|rows]] of $\mathbf A$.	1
Let $\alpha \in \R_{\gt 0}$. Let $\norm{\,\cdot\,}:\Q \to \R$ be the [[Definition:Mapping|mapping]] defined by: :$\forall x \in \Q: \norm{x} = \size {x}^\alpha$ where $\size {x}$ is the [[Definition:Absolute Value|absolute value]] of $x$ in $\Q$. Then: :$\norm{\,\cdot\,}$ is a [[Definition:Norm/Division Ring|norm]] on $\Q$ {{iff}} $\,\,\alpha \le 1$	1
Remember that $\det$ can be interpreted as an alternating multilinear map with respect to the columns. This property is sufficient to prove the theorem as follows. Let $\mathbf A, \mathbf B$ be two $n \times n$ matrices (with coefficients in a commutative field $\mathbb K$ like $\mathbb R$ or $\mathbb C$). Let us denote the vectors of the canonical basis of $\mathbb K^n$ by $\mathbf e_1, \ldots, \mathbf e_n$ (where $\mathbf e_i$ is a column with $1$ at $i$th row, zero elsewhere). Now, we are able to write the matrix $\mathbf B$ as a column block matrix : :$\mathbf B = \begin {pmatrix} \displaystyle \sum_{s_1 \mathop = 1}^n \mathbf B_{s_1, 1} \mathbf e_{s_1} & \cdots & \displaystyle \sum_{s_n \mathop = 1}^n \mathbf B_{s_n, n} \mathbf e_{s_n} \end {pmatrix}$ We can rewrite the product $\mathbf A \mathbf B$ as a column-block matrix : :$\mathbf A \mathbf B = \begin {pmatrix} \displaystyle \sum_{s_1 \mathop = 1}^n \mathbf B_{s_1, 1} \mathbf A \mathbf e_{s_1} & \cdots & \displaystyle \sum_{s_n \mathop = 1}^n B_{s_n, n} \mathbf A \mathbf e_{s_n} \end {pmatrix} $ Using linearity with respect to each columns, we get: :$\map \det {\mathbf A \mathbf B} = \displaystyle \sum_{1 \mathop \leqslant s_1, \ldots, s_n \mathop \leqslant n} \paren {\prod_{i \mathop = 1}^n \mathbf B_{s_i, i} } \det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end {pmatrix}$ Now notice that $\det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end{pmatrix}$ is zero once two entries are the same (since $\det$ is an alternating map), it means that if for some $k \ne \ell$ we have $\mathbf A \mathbf e_{s_k} = \mathbf A \mathbf e_{s_\ell}$, then $\det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end {pmatrix} = 0$. Therefore the only nonzero summands are those one the $s_1, \ldots, s_n$ are all distinct. In other words, the "selector" $s$ represents some permutation of the numbers $1, \ldots, n$. As a result, the determinant of the product can now be expressed as a sum of precisely $n!$ terms using permutations: :$\map \det {\mathbf A \mathbf B} = \displaystyle \sum_{\sigma \in S_n} \paren {\prod_{i \mathop = 1}^n B_{\map \sigma i, i} } \det \begin {pmatrix} \mathbf A \mathbf e_{\map \sigma 1} & \cdots & \mathbf A \mathbf e_{\map \sigma n} \end {pmatrix}$ where $S_n$ denotes the set of the permutations of numbers $1, \ldots, n$. However, the {{RHS}} determinant of the above equality corresponds to the determinant of permutated columns of $\mathbf A$. Whenever we transpose two columns, the determinant is modified by a factor $-1$. Indeed, let us apply some transposition $\tau_{i j}$ to a column-block matrix $\begin {pmatrix} \mathbf C_1 & \cdots & \mathbf C_n \end {pmatrix}$. By linearity it follows that for $i, j$ entries equal to $\mathbf C_i + \mathbf C_j$: {{begin-eqn}} {{eqn | l = 0 | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i + \mathbf C_j \cdots \mathbf C_j + \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | o = | ro= + | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{end-eqn}} Hence, transpose two columns reverse determinant sign: :$\det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_n \end {pmatrix} = -\det \begin {pmatrix} \mathbf C_{\map {\tau_{i j} } 1} \cdots \mathbf C_{\map {\tau_{i j} } n} \end {pmatrix}$ Since every permutation $\sigma \in S_n$ can be written as a product of transpositions, that is: :$\sigma = \tau_m \cdots \tau_1$ for some transpositions $\tau_1, \ldots, \tau_m$, it follows that: {{begin-eqn}} {{eqn | l = \map \det {\mathbf C_1 \cdots \mathbf C_n} | r = -\map \det {\mathbf C_{\map {\tau_1} 1} \cdots \mathbf C_{\map {\tau_1} n} } | c = }} {{eqn | r = \map \det {\mathbf C_{\map {\tau_2 \tau_1} 1} \cdots \mathbf C_{\map {\tau_2 \tau_1} n} } | c = }} {{eqn | r = \cdots | c = }} {{eqn | r = \paren {-1}^m \map \det {\mathbf C_{\map \sigma 1} \cdots \mathbf C_{\map \sigma n} } | c = }} {{end-eqn}} The number $\paren {-1}^m$ is the signature of the permutation $\sigma$ (see article [[Definition:Sign of Permutation|about the signature of permutations]]) and denoted by $\map \sgn \sigma$. It remains to apply several transpositions of columns to $\mathbf A$ to get for any permutation $\sigma$ the equality : :$\det \begin {pmatrix} \mathbf A \mathbf e_{\map \sigma 1} & \cdots & \mathbf A \mathbf e_{\map \sigma n} \end {pmatrix} = \map \sgn \sigma \det \begin {pmatrix} \mathbf A \mathbf e_1 & \cdots & \mathbf A \mathbf e_n \end {pmatrix} = \map \sgn \sigma \map \det {\mathbf A}$ Since $\map \det {\mathbf A}$ is a constant quantity, we can go this factor out of the sum, then write: :$\displaystyle \map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \sum_{\sigma \mathop \in S_n} \map \sgn \sigma \prod_{i \mathop = 1}^n \mathbf B_{\map \sigma i, i}$ But the above sum is exactly the definition of $\map \det {\mathbf B}$ using the Leibniz formula, and so: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \map \det {\mathbf B}$ Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\map {\MM_R} n$ denote the [[Definition:Matrix Space|metric space]] of [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]] over $R$. Let $\mathbf I_n$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]]: Then: :$\forall \mathbf A \in \map {\MM_R} n: \mathbf A \mathbf I_n = \mathbf A = \mathbf I_n \mathbf A$ That is, the [[Definition:Unit Matrix|unit matrix]] $\mathbf I_n$ is the [[Definition:Identity Element|identity element]] for [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]] over $\map {\MM_R} n$.	1
Let $z = a + i b$ be a [[Definition:Complex Number|complex number]]. Let $\cmod z$ be the [[Definition:Complex Modulus|modulus]] of $z$. Then: :$\cmod z = 0 \iff z = 0$	1
Let $G$ be a [[Definition:Finite Group|finite group]]. Let $\chi$ be the [[Definition:Character (Representation Theory)|character]] of any [[Definition:G-Module|$\C \left[{G}\right]$-module]] $\left({V, \rho}\right)$. Then for all $g \in G$, it follows that $\chi \left({g}\right)$ is an [[Definition:Algebraic Integer|algebraic integer]].	1
{{begin-eqn}} {{eqn|l = B + iC |r = \frac 1 2 \left({A + A^*}\right) + i \frac 1 {2i} \left({A - A^*}\right) |c = Definitions of $B, C$ }} {{eqn|r = \frac 1 2 A + \frac 1 2 A^* + \frac 1 2 A - \frac 1 2 A^* }} {{eqn|r = A }} {{end-eqn}} {{qed}}	1
Let $G$ be an [[Definition:Module|$R$-module]]. Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be a [[Definition:Sequence|sequence of elements]] of $G$. Let $b$ be an [[Definition:Element|element]] of $G$. Then: :$b$ is a [[Definition:Linear Combination of Sequence|linear combination]] of the [[Definition:Finite Sequence|sequence]] $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ {{iff}}: :$b$ is a [[Definition:Linear Combination of Subset|linear combination]] of the [[Definition:Set|set]] $\set {a_k: 1 \mathop \le k \mathop \le n}$	1
==== Reduction to $\tuple {a, b} = \tuple {0, 0}$ ==== We may assume $\tuple {a, b} = \tuple {0, 0}$. {{explain|why}} Define: :$F : \Omega \to \R^k: \map F {x, y} = y - \map {D_2 \map f {a, b}^{-1} } {\map f {x, y} }$ By [[Linear Function is Continuous]], $D_2 \map f {a, b}^{-1}$ is [[Definition:Continuous Real Function|continuous]] on $\R^k$. Thus $F$ is [[Definition:Continuous Real Function|continuous]] on $\Omega$. ==== $F$ is locally a uniform contraction ==== Let $r_1 > 0$ such that the [[Definition:Open Ball|open ball]] $\map B {0, r_1} \subseteq \Omega$. We have, for $\tuple {x, y_1}, \tuple {x, y_2} \in \map B {0, r_1}$, by the [[Mean Value Inequality]]: :$\norm {\map F {x, y_2} - \map F {x, y_1} } \le \displaystyle \sup_{y \mathop \in \closedint {y_1} {y_2} } \norm {D_2 \map F {x, y} } \cdot \norm {y_2 - y_1}$ We have, for $\tuple {x, y} \in \Omega$: :$D_2 \map F {x, y} = I - \paren {D_2 \map F {0, 0} }^{-1} \circ D_2 \map F {x, y}$ where $I$ is the [[Definition:Identity Mapping|identity mapping]]. Thus $D_2 F$ is [[Definition:Continuous Mapping|continuous]] in $\Omega$. By definition, $D_2 \map F {0, 0} = 0$. By continuity, there exists $r_2 > 0$ such that: :$\norm {D_2 \map F {x, y} } \le \dfrac 1 2$ for $\norm {\tuple {x, y} } \le r_2$ From the above inequality, $F$ is a [[Definition:Uniform Contraction Mapping|uniform contraction mapping]] on the [[Definition:Open Ball|open ball]] $\map B {0, r_2}$ for the [[Definition:Lipschitz Constant|Lipschitz Constant]] $\dfrac 1 2$. ==== Constructing a stable neighborhood ==== We have, for $x, y \in \map {\overline B} {0, r_2}$: {{begin-eqn}} {{eqn | l = \norm {\map F {x, y} } | o = \le | r = \norm {\map F {x, y} - \map F {x, 0} } + \norm {\map F {x, 0} } | c = [[Triangle Inequality]] }} {{eqn | o = \le | r = \frac 1 2 \norm y + \norm {\map F {x, 0} } | c = Definition of $r_2$ }} {{end-eqn}} Because $\map F {0, 0} = 0$ and $F$ is [[Definition:Continuous Mapping|continuous]], there exists $r_3>0$ such that $\norm {\map F {x, 0} } \le \dfrac {r_2} 2$. Then: :$\norm {\map F {x, y} } \le r_2$ for $\norm y \le r_2$ and: :$\norm x \le r_3$ Thus the [[Definition:Restriction of Mapping|restriction]] of $F$ to $\map B {0, r_3} \times \map {\overline B} {0, r_2}$ is a [[Definition:Uniform Contraction Mapping|uniform contraction]]: :$F: \map B {0, r_3} \times \map {\overline B} {0, r_2} \to \map {\overline B} {0, r_2}$ ==== Applying the Uniform Contraction Theorem ==== By [[Subspace of Complete Metric Space is Closed iff Complete]], $\map {\overline B} {0, r_2}$ is [[Definition:Complete Metric Space|complete]]. By the [[Uniform Contraction Mapping Theorem]], there exists a [[Definition:Unique|unique]] [[Definition:Mapping|mapping]] $g: \map B {0, r_3} \to \map {\overline B} {0, r_2}$ such that $\map F {x, \map g x} = \map g x$. Moreover, $g$ is [[Definition:Continuous Mapping|continuous]]. A [[Definition:Mapping|mapping]] $h: \map B {0, r_3} \to \map {\overline B} {0, r_2}$ satisfies $\map F {x, \map h x} = \map h x$ {{iff}} it satisfies $\map {D_2 \map f {a, b}^{-1} } {\map f {x, \map h x} } = 0$ {{iff}} it satisfies $\map f {x, \map h x} = 0$, because $D_2 \map f {a, b}^{-1}$ is [[Definition:Invertible Linear Mapping|invertible]]. Thus $g$ is the unique [[Definition:Mapping|mapping]] $g: \map B {0, r_3} \to \map {\overline B} {0, r_2}$ such that $\map f {x, \map g x} = 0$. {{qed}}	1
Consider two cases: :$(1): \quad \mathbf A$ is not [[Definition:Invertible Matrix|invertible]]. :$(2): \quad \mathbf A$ is [[Definition:Invertible Matrix|invertible]]. === Proof of case $1$ === Assume $\mathbf A$ is not [[Definition:Invertible Matrix|invertible]]. Then: :$\map \det {\mathbf A} = 0$ Also if $\mathbf A$ is not [[Definition:Invertible Matrix|invertible]] then neither is $\mathbf A \mathbf B$. Indeed, if $\mathbf A \mathbf B$ has an inverse $\mathbf C$, then $\mathbf A \mathbf B \mathbf C = \mathbf I$, whereby $\mathbf B \mathbf C$ is a right inverse of $\mathbf A$. It follows by [[Left or Right Inverse of Matrix is Inverse]] that in that case $\mathbf B \mathbf C$ is the inverse of $A$. It follows that: :$\map \det {\mathbf A \mathbf B} = 0$ Thus: :$0 = 0 \cdot \map \det {\mathbf B}$ :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \cdot \map \det {\mathbf B}$ {{qed|lemma}} === Proof of case $2$ === Assume $\mathbf A$ is [[Definition:Invertible Matrix|invertible]]. Then $\mathbf A$ is a product of [[[[Definition:Elementary Row Matrix|elementary row matrices]], $\mathbf E$. Let $\mathbf A = \mathbf E_k \mathbf E_{k - 1} \cdots \mathbf E_1$. So: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf E_k \mathbf E_{k - 1} \cdots \mathbf E_1 \mathbf B}$ It remains to be shown that for any [[Definition:Square Matrix|square matrix]] $\mathbf D$ of [[Definition:Order of Square Matrix|order]] $n$: :$\map \det {\mathbf E \mathbf D} = \map \det {\mathbf E} \cdot \map \det {\mathbf D}$ Let $e_i \paren {\mathbf I} = \mathbf E_i$ for all $i \in \closedint 1 k$, then using [[Elementary Row Operations as Matrix Multiplications]] and [[Effect of Sequence of Elementary Row Operations on Determinant]] yields: :$\map \det {\mathbf E \mathbf D} = \map \det {\mathbf E_k \mathbf E_{k - 1} \dotsm \mathbf {E_1} \mathbf D} = \map \det {e_k e_{k - 1} \cdots e_1 \paren {\mathbf D} } = \alpha \map \det {\mathbf D}$ Using [[Elementary Row Operations as Matrix Multiplications]] and [[Effect of Sequence of Elementary Row Operations on Determinant]], '''and''' [[Unit Matrix is Unity of Ring of Square Matrices]]: :$\map \det {\mathbf E} = \map \det {\mathbf E_k \mathbf E_{k - 1} \cdots \mathbf {E_1} \mathbf I} = \map \det {e_k e_{k - 1} \cdots e_1 \paren {\mathbf I} } = \alpha \map \det {\mathbf I}$ From [[Determinant of Unit Matrix]]: :$\map \det {\mathbf E} = \alpha$ And so $\map \det {\mathbf E \mathbf D} = \map \det {\mathbf E} \cdot \map \det {\mathbf D}$ {{qed|lemma}} Therefore: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \map \det {\mathbf B}$ as required. {{qed}}	1
Since $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0$, by [[Cauchy Sequence Is Eventually Bounded Away From Non-Limit]] then: :$\exists K \in \N$ and $C \in \R_{>0}: \forall n > K: C < \norm {x_n}$ or equivalently: :$\exists K \in \N$ and $C \in \R_{>0}: \forall n > K: 1 < \dfrac {\norm {x_n} } C$ By {{NormAxiom|1}}: :$\forall n > K : x_n \ne 0$ Let $\sequence {y_n}$ be the [[Definition:Subsequence|subsequence]] of $\sequence {x_n}$ defined as: :$y_n = x_{K + n}$ By [[Subsequence of Cauchy Sequence in Normed Division Ring is Cauchy Sequence]]: :$\sequence {y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]]. So $\sequence { {y_n}^{-1} }$ is well-defined and $\sequence { {y_n}^{-1} } = \sequence {\paren {x_{K + n} }^{-1} }_{n \mathop \in \N}$. Let $\epsilon > 0$ be given. Let $\epsilon' = \epsilon C^2$, then $ \epsilon' > 0$. Similarly, $\sequence {y_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]], we can find $N$ such that: :$\forall n, m > N_2: \norm {y_n - y_m} < \epsilon'$ Thus $\forall n, m > N$: :$(1): \quad 1 < \dfrac {\norm {y_n} } C, \dfrac {\norm {y_m} } C$ :$(2): \quad \norm {y_n - y_m} < \epsilon'$ Hence: {{begin-eqn}} {{eqn | l = \norm { {y_n}^{-1} - {y_m}^{-1} } | o = < | r = \dfrac {\norm {y_n} } C \norm { {y_n }^{-1} - {y_m}^{-1} } \dfrac {\norm {y_m} } C | c = $(1)$ above }} {{eqn | r = \dfrac 1 {C^2} \paren {\norm {y_n} \norm { {y_n }^{-1} - {y_m}^{-1 } } \norm {y_m} } }} {{eqn | r = \dfrac 1 {C^2} \norm {y_n \paren { {y_n }^{-1 } - {y_m }^{-1} } y_m} | c = {{NormAxiom|2}} }} {{eqn | r = \dfrac 1 {C^2} \norm {\paren {y_n {y_n }^{-1 } - y_n {y_m }^{-1} } y_m } | c = [[Definition:Ring (Abstract Algebra)|Ring Axiom $(\text D)$]]: Distributivity }} {{eqn | r = \dfrac 1 {C^2} \norm {y_n {y_n}^{-1} y_m - y_n {y_m}^{-1} y_m} | c = [[Definition:Ring (Abstract Algebra)|Ring Axiom $(\text D)$]]: Distributivity }} {{eqn | r = \dfrac 1 {C^2} \norm {y_m - y_n} | c = [[Definition:Division Ring|Inverse Property of a Division Ring]] }} {{eqn | r = \dfrac 1 {C^2} \epsilon' | o = < | c = $(2)$ above }} {{eqn | r = \dfrac 1 {C^2} \paren {\epsilon C^2} | c = Definition of $\epsilon'$ }} {{eqn | r = \epsilon }} {{end-eqn}} So: :$\sequence { { {y_n}^{-1} } }$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. {{qed}}	1
=== [[Definition:Necessary Condition|Necessary Condition]] === Suppose $P + Q$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. Then: {{begin-eqn}} {{eqn | l = P + Q | r = \paren {P + Q}^2 | c = $P + Q$ is an [[Definition:Idempotent Operator|idempotent]] }} {{eqn | r = P^2 + P Q + Q P + Q^2 }} {{eqn | r = P + P Q + Q P + Q | c = $P$ and $Q$ are [[Definition:Idempotent Operator|idempotents]] }} {{eqn | ll= \leadstoandfrom | l = PQ + QP | r = 0 }} {{end-eqn}} Now suppose that $h \in \Rng Q$; say $h = Q q$ for $q \in H$. Then it follows that $Q h = Q Q q = Q q = h$ as $Q$ is [[Definition:Idempotent Operator|idempotent]]. It follows that: :$0 = P Q h + Q P h = P h + Q P h$ From [[Characterization of Projections]], statement $(6)$, $\innerprod {Q P h} {P h}_H \ge 0$. Next, observe: :$0 = \innerprod {P h + Q p h} {P h}_H = \innerprod {P h} {P h}_H + \innerprod {Q P h} {P h}_H$ As the second term is [[Definition:Positive|non-negative]], the first is [[Definition:Negative|non-positive]]; it follows that $P h = \mathbf 0_H$ from the definition of the [[Definition:Inner Product|inner product]]. Hence: :$h \in \Ker P = \paren {\Rng P}^\perp$ It follows that $\Rng Q \perp \Rng P$, as asserted. {{qed|lemma}} === [[Definition:Sufficient Condition|Sufficient Condition]] === Suppose that $\Rng P \perp \Rng Q$. Then as $P, Q$ are [[Definition:Projection (Hilbert Spaces)|projections]], have: :$\Rng P \subseteq \Ker Q$ :$\Rng Q \subseteq \Ker P$ That is, for all $h \in H$: :$Q P h = P Q h = \mathbf 0_H$ Hence: {{begin-eqn}} {{eqn | l = \paren {P + Q}^2 | r = P^2 + P Q + Q P = Q^2 }} {{eqn | r = P + Q | c = $P$ and $Q$ are [[Definition:Idempotent Operator|idempotents]] }} {{end-eqn}} That is, $P + Q$ is an [[Definition:Idempotent Operator|idempotent]]. Furthermore, by [[Adjoining is Linear]], have: :$\paren {P + Q}^* = P^* + Q^* = P + Q$ where the latter follows from [[Characterization of Projections]], statement $(4)$. This same statement implies that $P + Q$ is also a [[Definition:Projection (Hilbert Spaces)|projection]]. {{qed}}	1
Let: : $z_1 = r_1 \left({\cos \theta_1 + i \sin \theta_1}\right)$ : $z_2 = r_2 \left({\cos \theta_2 + i \sin \theta_2}\right)$ Then: {{begin-eqn}} {{eqn | l = \left\vert{z_1 z_2}\right\vert | r = \left\vert{r_1 \left({\cos \theta_1 + i \sin \theta_1}\right) r_2 \left({\cos \theta_2 + i \sin \theta_2}\right)}\right\vert | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \left\vert{r_1 r_2 \left({\cos \left({\theta_1 + \theta_2}\right) + i \sin \left({\theta_1 + \theta_2}\right)}\right)}\right\vert | c = [[Product of Complex Numbers in Polar Form]] }} {{eqn | r = r_1 r_2 | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = \left\vert{z_1}\right\vert \left\vert{z_2}\right\vert | c = {{Defof|Polar Form of Complex Number}} }} {{end-eqn}} {{qed}}	1
Let $\struct {G, *}$ be an [[Definition:Abelian Group|abelian group]] with [[Definition:Identity Element|identity]] $e$. Let $\struct {G, *, \circ}_\Z$ be the [[Definition:Z-Module Associated with Abelian Group|$Z$-module associated with $G$]]. Then $\struct {G, *, \circ}_\Z$ is a [[Definition:Unitary Module|unitary $Z$-module]].	1
Let $\mathbb K \subset \C$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\struct {V, \innerprod {\, \cdot \,} {\, \cdot \,} }$ be an [[Definition:Inner Product Space|inner product space]] over $\mathbb K$ of [[Definition:Dimension of Vector Space|dimension]] $n$. Let $\tuple {e_1, \ldots, e_n}$ be an [[Definition:Orthonormal Basis|orthonormal basis]] of $V$. Let $f: V \to V$ be a [[Definition:Linear Operator|linear operator]]. Then its [[Definition:Trace of Linear Operator|trace]] equals: :$\map \tr f = \displaystyle \sum_{i \mathop = 1}^n \innerprod {\map f {e_i} } {e_i}$	1
Let $\struct {V, \innerprod \cdot \cdot}$ be an [[Definition:Inner Product Space|inner product space]]. Let $S \subseteq V$ be a [[Definition:Subset|subset]] of $V$. Then $S$ is an '''orthonormal subset''' {{iff}}: :$(1): \quad \forall u \in S: \norm u = 1$ where $\norm {\, \cdot \,}$ is the [[Definition:Inner Product Norm|inner product norm]]. :$(2): \quad S$ is an [[Definition:Orthogonal|orthogonal set]]: ::$\forall u, v \in S: u \ne v \implies \innerprod u v = 0$	1
First we note that the [[P-adic Norm is Norm|$p$-adic norm is a norm]]. Let $\nu_p$ denote the [[Definition:P-adic Valuation|$p$-adic valuation]] on the [[Definition:Rational Number|rational numbers]]. Recall the definition of the [[Definition:P-adic Norm|$p$-adic norm]]: :$\forall q \in \Q: \norm q_p := \begin{cases} 0 & : q = 0 \\ p^{-\map {\nu_p} q} & : q \ne 0 \end{cases}$ We must show the following holds for all $x, y \in \Q$: :$\norm {x + y}_p \le \max \set {\norm x_p, \norm y_p}$ If $x = 0$ or $y = 0$, or $x + y = 0$, the result is trivial, as follows: Let $x = 0$. Then: {{begin-eqn}} {{eqn | l = x | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm x_p | r = 0 | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | ll= \leadsto | l = \max \set {\norm x_p, \norm y_p} | r = \norm y_p | c = as $\norm y_p \ge 0 = \norm x_p$ from [[Definition:Norm Axioms|Norm Axioms: Axiom $(\text N 1)$]] }} {{eqn | r = \norm {x + y}_p | c = }} {{end-eqn}} and so $\norm {x + y}_p \le \max \left( \norm x_p, \norm y_p \right)$ The same argument holds for $y = 0$. Let $x + y = 0$. {{begin-eqn}} {{eqn | l = x + y | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm {x + y}_p | r = 0 | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | o = \le | r = \max \set {\norm x_p, \norm y_p} | c = as $\norm x_p \ge 0$ and $\norm y_p \ge 0$ from [[Definition:Norm Axioms|Norm Axioms: Axiom $(\text N 1)$]] }} {{end-eqn}} Let $x, y, x + y \in \Q_{\ne 0}$. From [[P-adic Valuation is Valuation|$p$-adic Valuation is Valuation]]: :$\map {\nu_p} {x + y} \ge \min \set {\map {\nu_p} x, \map {\nu_p} y}$ Then: {{begin-eqn}} {{eqn | l = \norm {x + y}_p | r = p^{-\map {\nu_p} {x + y} } | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{eqn | o = \le | r = \max \set {p^{- \map {\nu_p} x}, p^{-\map {\nu_p} y} } }} {{eqn | r = \max \set {\norm x_p, \norm y_p} | c = {{Defof|P-adic Norm|$p$-adic Norm}} }} {{end-eqn}} {{qed}}	1
Let $\alpha, \beta \in \C$ be [[Definition:Complex Number|complex numbers]]. Then: :$\left\lvert{\alpha + \beta}\right\rvert^2 + \left\lvert{\alpha - \beta}\right\rvert^2 = 2 \left\lvert{\alpha}\right\rvert^2 + 2 \left\lvert{\beta}\right\rvert^2$	1
:$\eta: \struct {R, d} \to \struct {R, d}: \map \eta x = -x$ is [[Definition:Continuous Mapping (Metric Space)|continuous]].	1
=== [[Matrix Entrywise Addition over Ring is Closed]] === {{:Matrix Entrywise Addition over Ring is Closed}} === [[Matrix Entrywise Addition over Ring is Associative]] === {{:Matrix Entrywise Addition over Ring is Associative}} === [[Matrix Entrywise Addition over Ring is Commutative]] === {{:Matrix Entrywise Addition over Ring is Commutative}} [[Category:Matrix Entrywise Addition]] ofmyntkjczgnuup663ekqa4e6fcfv7v	1
Combine [[Free Module is Isomorphic to Free Module Indexed by Set]] and [[Universal Property of Free Module on Set]]. {{handwaving}} [[Category:Module Theory]] [[Category:Universal Properties]] 7gxxsstllxvoymdm4wgtb13rifn7q8y	1
As $R$ is a [[Definition:Ring with Unity|ring with unity]], we have that: {{begin-eqn}} {{eqn | l = 0_R | o = \ne | r = 1_R }} {{eqn | l = 0_R \times 0_R | r = 0_R | c = }} {{eqn | l = 0_R \times 1_R | r = 0_R = 1_R \times 0_R | c = }} {{eqn | l = 1_R \times 1_R | r = 1_R | c = }} {{end-eqn}} Now let: {{begin-eqn}} {{eqn | l = \mathbf A | r = \begin {pmatrix} 0_R & 1_R \\ 0_R & 0_R \end {pmatrix} }} {{eqn | l = \mathbf B | r = \begin {pmatrix} 0_R & 0_R \\ 1_R & 0_R \end {pmatrix} }} {{end-eqn}} By definition, both $\mathbf A$ and $\mathbf B$ are [[Definition:Element|elements]] of $\map {\MM_R} 2$. It will be demonstrated that $\mathbf A$ and $\mathbf B$ do not [[Definition:Commuting Elements|commute]]. We have: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf B | r = \begin {pmatrix} 0_R & 1_R \\ 0_R & 0_R \end {pmatrix} \begin {pmatrix} 0_R & 0_R \\ 1_R & 0_R \end {pmatrix} | c = }} {{eqn | r = \begin {pmatrix} 0_R \times 0_R + 1_R \times 1_R & 0_R \times 0_R + 1_R \times 0_R \\ 0_R \times 0_R + 0_R \times 1_R & 0_R \times 0_R + 0_R \times 0_R \end{pmatrix} | c = }} {{eqn | r = \begin {pmatrix} 1_R & 0_R \\ 0_R & 0_R \end{pmatrix} | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \mathbf B \mathbf A | r = \begin {pmatrix} 0_R & 0_R \\ 1_R & 0_R \end {pmatrix} \begin {pmatrix} 0_R & 1_R \\ 0_R & 0_R \end {pmatrix} | c = }} {{eqn | r = \begin {pmatrix} 0_R \times 0_R + 0_R \times 0_R & 0_R \times 1_R + 0_R \times 0_R \\ 1_R \times 0_R + 0_R \times 0_R & 1_R \times 1_R + 0_R \times 0_R \end{pmatrix} | c = }} {{eqn | r = \begin {pmatrix} 0_R & 0_R \\ 0_R & 1_R \end{pmatrix} | c = }} {{end-eqn}} and it is seen that: :$\mathbf A \mathbf B \ne \mathbf B \mathbf A$ Thus, whatever the nature of the [[Definition:Ring with Unity|ring with unity]] $R$, it is never the case that [[Definition:Matrix Product (Conventional)|matrix multiplication]] is [[Definition:Commutative Operation|commutative]] over $\map {\MM_R} 2$. {{qed}} [[Category:Matrix Multiplication is not Commutative]] rui1jhhw7alanyt33ovxir9qxs8daf5	1
By definition, $M \ominus N = M \cap N^\perp$. By [[Orthocomplement is Closed Linear Subspace]], $N^\perp$ is a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$. Hence the result, by [[Closed Linear Subspaces Closed under Intersection]]. {{qed}} [[Category:Hilbert Spaces]] 2lj3gg359w9cwzc1kxa4wbfnzjwhsak	1
We have that: :[[P-Sequence Space with Pointwise Addition and Pointwise Scalar Multiplication on Ring of Sequences form Vector Space|$P$-sequence space is a vector space]] :[[P-Norm is Norm/P-Sequence Space|$P$-norm on the $p$-sequence space is a norm]] By definition, $\struct {\ell^p, \norm {\, \cdot \,}_p}$ is a [[Definition:Normed Vector Space|normed vector space]]. {{qed}}	1
Let $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $l$ in $\norm {\, \cdot \,}_1$. Let $\epsilon \in \R_{> 0}$ be given. Let $\map {B_\epsilon^2} i$ denote the [[Definition:Open Ball|open ball]] [[Definition:Center of Open Ball|centered]] on $l$ of [[Definition:Radius of Open Ball|radius]] $\epsilon$ in $\struct {R, \norm {\, \cdot \,}_2}$. By [[Open Ball of Metric Space is Open Set]] then $\map {B_\epsilon^2} l$ is [[Definition:Open Set of Metric Space|open set]] in $\struct {R, d_2}$. Since $d_1$ and $d_2$ are [[Definition:Topologically Equivalent Metrics|topologically equivalent metrics]] then $\map {B_\epsilon^2} l$ is [[Definition:Open Set of Metric Space|open set]] in $\struct {R, d_1}$. By the definition of an [[Definition:Open Set of Metric Space|open set in a metric space]] then: :$\exists \delta \in \R_{> 0}: \map {B_\delta^1} l \subseteq \map {B_\epsilon^2} l$ Hence: :$\forall x \in R: \norm {x - l}_1 < \delta \implies \norm {x - l}_2 < \epsilon$ Since $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_1$ then: :$\exists N \in \N: \forall n \ge N: \norm {x_n - l}_1 < \delta$ Hence: :$\exists N \in \N: \forall n \ge N: \norm {x_n - l}_2 < \epsilon$ Since $\sequence {x_n}$ and $\epsilon > 0$ were arbitrary then it has been shown that for all [[Definition:Sequence|sequences]] $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_1 \implies \sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_2$. By a similar argument it is shown that for all [[Definition:Sequence|sequences]] $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_2 \implies \sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_1$. The result follows. {{qed}}	1
The proof proceeds by [[Principle of Mathematical Induction|induction]]. By definition, $\Gamma$ is a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] on $\mathbf A$. Let $\sequence e_k$ denote a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\tuple {e_1, e_2, \ldots, e_k}$ applied on $\mathbf A$ in order: first $e_1$, then $e_2$, then $\ldots$, then $e_k$. Let $\Gamma_k$ be the [[Definition:Row Operation|row operation]] which consists of $\sequence e_k$. Let $\mathbf E_k$ denote the [[Definition:Elementary Row Matrix|elementary row matrix]] of [[Definition:Order of Square Matrix|order]] $m$ formed by applying $e_k$ to the [[Definition:Unit Matrix|unit matrix]] $I_m$. For all $r \in \Z_{>0}$, let $\map P r$ be the [[Definition:Proposition|proposition]]: :For all $\Gamma_r$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R_r$ of [[Definition:Order of Square Matrix|order $m$]] such that: ::$\mathbf R_r \mathbf A = \mathbf B_r$ :where: ::$\Gamma_r$ is a [[Definition:Row Operation|row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B_r \in \map \MM {m, n}$. ::$\mathbf R_r$ is the [[Definition:Matrix Product (Conventional)|product]] of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Matrix|elementary row matrices]]: :::$\mathbf R_r = \mathbf E_r \mathbf E_{r - 1} \dotsb \mathbf E_2 \mathbf E_1$ === Basis for the Induction === $\map P 1$ is the case where $\Gamma_1$ is a single-[[Definition:Term of Sequence|term]] [[Definition:Finite Sequence|sequence]] consisting of one [[Definition:Elementary Row Operation|elementary row operation]] $e_1$. Let $e_1$ be an [[Definition:Elementary Row Operation|elementary row operation]] operating on $\mathbf A$, which transforms $\mathbf A$ into $\mathbf B_1$. By definition, there exists [[Definition:Unique|exactly one]] [[Definition:Elementary Row Matrix|elementary row matrix]] $\mathbf E_1$ of [[Definition:Order of Square Matrix|order $m$]] such that $\mathbf E_1$ is the result of applying $e_1$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$ of [[Definition:Order of Square Matrix|order $m$]]. From the [[Elementary Row Operations as Matrix Multiplications/Corollary|corollary to Elementary Row Operations as Matrix Multiplications]]: :$\mathbf E_1 \mathbf A = \mathbf B_1$ By [[Elementary Row Matrix is Invertible]], $E_1$ is [[Definition:Invertible Matrix|invertible]]. Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :For all $\Gamma_k$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R_k$ of [[Definition:Order of Square Matrix|order $m$]] such that: ::$\mathbf R_k \mathbf A = \mathbf B_k$ from which it is to be shown that: :For all $\Gamma_{k + 1}$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R_{k + 1}$ of [[Definition:Order of Square Matrix|order $m$]] such that: ::$\mathbf R_{k + 1} \mathbf A = \mathbf B_{k + 1}$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: By definition, $\Gamma_{k + 1}$ is a [[Definition:Row Operation|row operation]] consisting of a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\tuple {e_1, e_2, \ldots, e_k, e_{k + 1} }$ applied on $\mathbf A$ in order. Thus $\Gamma_{k + 1}$ consists of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\tuple {e_1, e_2, \ldots, e_k}$ applied on $\mathbf A$ in order, followed by a further [[Definition:Elementary Row Operation|elementary row operation]] $e_{k + 1}$. By the [[Row Operation is Equivalent to Pre-Multiplication by Product of Elementary Matrices#Induction Hypothesis|induction hypothesis]], there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R_k$ of [[Definition:Order of Square Matrix|order $m$]] such that: :$\mathbf R_k \mathbf A = \mathbf B_k$ where $\mathbf B_k \in \map \MM {m, n}$ is the result of applying $\sequence e_k$ to $\mathbf A$ in order. Let $e_{k + 1}$ be applied to $\mathbf B_k$. By definition, there exists [[Definition:Unique|exactly one]] [[Definition:Elementary Row Matrix|elementary row matrix]] $\mathbf E_{k + 1}$ of [[Definition:Order of Square Matrix|order $m$]] such that $\mathbf E_{k + 1}$ is the result of applying $e_{k + 1}$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$ of [[Definition:Order of Square Matrix|order $m$]]. Then: {{begin-eqn}} {{eqn | l = \mathbf B_{k + 1} | r = \mathbf E_{k + 1} \mathbf B_k | c = [[Elementary Row Operations as Matrix Multiplications/Corollary|Corollary to Elementary Row Operations as Matrix Multiplications]] }} {{eqn | r = \mathbf E_{k + 1} \paren {\mathbf R_k \mathbf A} | c = }} {{eqn | r = \paren {\mathbf E_{k + 1} \mathbf R_k} \mathbf A | c = [[Matrix Multiplication is Associative]] }} {{end-eqn}} By [[Product of Matrices is Invertible iff Matrices are Invertible]], $\mathbf E_{k + 1} \mathbf R_k$ is [[Definition:Invertible Matrix|invertible]]. We have that $\mathbf R_k$ is the [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] resulting from the application of $\sequence e_k$ on $\mathbf I_m$. Thus $\mathbf E_{k + 1} \mathbf R_k$ is the [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] resulting from the application of $\sequence e_{k + 1}$ on $\mathbf I_m$. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore, for every [[Definition:Row Operation|row operation]] $\Gamma$ which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$, there exists a [[Definition:Unique|unique]] [[Definition:Invertible Matrix|invertible]] [[Definition:Square Matrix|square matrix]] $\mathbf R$ of [[Definition:Order of Square Matrix|order $m$]] such that: :$\mathbf R \mathbf A = \mathbf B$ where: :$\mathbf R$ is the [[Definition:Matrix Product (Conventional)|product]] of a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Matrix|elementary row matrices]]. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\struct {R, +, \circ}$. Then $\struct {\map {\MM_R} {m, n}, +}$, where $+$ is [[Definition:Matrix Entrywise Addition|matrix entrywise addition]], is a [[Definition:Group|group]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $M, N$ be [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$. Then the [[Definition:Orthogonal Difference|orthogonal difference]] $M \ominus N$ is also a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$.	1
$\forall x, y \in S$: {{begin-eqn}} {{eqn | l = \norm {x + y}_S | r = \norm {x + y} | c = Definition of $\norm {\,\cdot\,}_S$ }} {{eqn | o = \le | r = \max \set {\norm x, \norm y} | c = [[Definition:Non-Archimedean Division Ring Norm|$(\text N 4)$: Ultrametric Inequality]] }} {{eqn | r = \max \set {\norm x_S, \norm y_S} | c = Definition of $\norm {\, \cdot \,}_S$ }} {{end-eqn}} {{qed}} [[Category:Normed Division Rings]] mnmsno5i821o90pknrspo4twwhbguqt	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $m, n \ge 1$ be [[Definition:Positive Integer|positive integers]]. Let $i, j \in \set {1, \ldots, m} \times \set {1, \ldots, n}$. The $\tuple {i, j}$th '''standard basis matrix''' is the $m \times n$ [[Definition:Matrix|matrix]] which is $0$ everywhere except a $1$ at the $\tuple {i, j}$th [[Definition:Index of Matrix Element|indices]].	1
From the definition of [[Definition:Matrix Product (Conventional)|matrix multiplication]], the product of two [[Definition:Matrix|matrices]] is another [[Definition:Matrix|matrix]]. The [[Definition:Order of Matrix|order]] of an $m \times n$ [[Definition:Matrix Product (Conventional)|multiplied]] by an $n \times p$ [[Definition:Matrix|matrix]] is $m \times p$. The [[Definition:Matrix Entry|entries]] of that [[Definition:Matrix Product (Conventional)|product]] [[Definition:Matrix|matrix]] are [[Definition:Element|elements]] of the [[Definition:Ring (Abstract Algebra)|ring]] over which the [[Definition:Matrix|matrix]] is formed. Thus an $n \times n$ [[Definition:Matrix|matrix]] over $R$ [[Definition:Matrix Product (Conventional)|multiplied]] by an $n \times n$ [[Definition:Matrix|matrix]] over $R$ gives another $n \times n$ [[Definition:Matrix|matrix]] over $R$. Hence the result. {{qed}} [[Category:Conventional Matrix Multiplication]] [[Category:Algebraic Closure]] 9qwfqajxemqtzbxb0nsxfztyliq2lvf	1
=== Prime Modulus === $\struct {\Z_m, +, \times}$ is a [[Definition:Commutative Ring with Unity|commutative ring with unity]] by definition. From [[Reduced Residue System under Multiplication forms Abelian Group]], $\struct {\Z'_m, \times}$ is an [[Definition:Abelian Group|abelian group]]. $\Z'_m$ consists of all the elements of $\Z_m$ [[Definition:Coprime Integers|coprime]] to $m$. Now when $m$ is [[Definition:Prime Number|prime]], we have, from [[Reduced Residue System Modulo Prime]]: :$\Z'_m = \set {\eqclass 1 m, \eqclass 2 m, \ldots, \eqclass {m - 1} m}$ That is: :$\Z'_m = \Z_m \setminus \set {\eqclass 0 m}$ where $\setminus$ denotes [[Definition:Set Difference|set difference]]. Hence the result. {{qed|lemma}} === Composite Modulus === Now suppose $m \in \Z: m \ge 2$ is [[Definition:Composite Number|composite]]. From [[Ring of Integers Modulo Composite is not Integral Domain]], $\struct {\Z_m, +, \times}$ is not an [[Definition:Integral Domain|integral domain]]. From [[Field is Integral Domain]] $\struct {\Z_m, +, \times}$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Follows directly from [[Product of Negative with Product Inverse]] and the definition of [[Definition:Division Product|division product]]. {{qed}}	1
Let $X$ be a [[Definition:Banach Space|Banach space]]. Let $Y$ be a [[Definition:Normed Vector Space|normed vector space]] with norm $\norm {\,\cdot\,}_Y$. Let $\family {T_\alpha: X \to Y}_{\alpha \mathop \in A}$ be an [[Definition:Indexed Family|$A$-indexed family]] of [[Definition:Bounded Linear Transformation|bounded linear transformations]] from $X$ to $Y$. Suppose that: :$\displaystyle \forall x \in X: \sup_{\alpha \mathop \in A} \norm {T_\alpha x}_Y < \infty$ Then: :$\displaystyle \sup_{\alpha \mathop \in A} \norm {T_\alpha} < \infty$ where $\norm {T_\alpha}$ denotes the [[Definition:Norm on Bounded Linear Transformation|norm of the linear transformation $T_\alpha$]]. {{explain|We may need a definitive explanation for the intuitively understood $< \infty$.}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Complete Normed Division Ring|complete normed division ring]]. Let $\struct {S, \norm {\, \cdot \,}}$ be a [[Definition:Everywhere Dense|dense]] [[Definition:Normed Division Subring|normed division subring]] of $\struct {R, \norm {\, \cdot \,}}$. Then: :$\struct {R, \norm {\, \cdot \,} }$ is a [[Definition:Completion (Normed Division Ring)|completion]] of $\struct {S, \norm {\, \cdot \,}}$ where the [[Definition:Inclusion Mapping|inclusion mapping]] $i : S \to R$ is the required [[Definition:Distance-Preserving Mapping|distance-preserving]] [[Definition:Ring Monomorphism|ring monomorphism]].	1
{{WLOG}}, we will only prove $OB$ [[Definition:Angle Bisector|bisects]] $\angle AOC$. Let the [[Definition:Vector (Euclidean Space)|position vector]] of $A$, $B$ and $C$ with respect to $O$ be $\mathbf a$, $\mathbf b$ and $\mathbf c$ respectively. By definition of [[Definition:Rhombus|rhombus]], we have: {{begin-eqn}} {{eqn | n = a | l = \mathbf a + \mathbf c | r = \mathbf b | c = [[Parallelogram Law]] }} {{eqn | n = b | l = \norm {\mathbf a} | r = \norm {\mathbf c} | c = }} {{end-eqn}} From the above we have: {{begin-eqn}} {{eqn | l = \cos \angle \mathbf a, \mathbf b | r = \frac {\mathbf a \cdot \mathbf b} {\norm {\mathbf a} \norm {\mathbf b} } | c = {{Defof|Dot Product|index = 2}} }} {{eqn | r = \frac {\mathbf a \cdot \paren {\mathbf a + \mathbf c} } {\norm {\mathbf a} \norm {\mathbf b} } | c = from $(a)$ above: $\mathbf b = \mathbf a + \mathbf c$ }} {{eqn | r = \frac {\mathbf a \cdot \mathbf a + \mathbf a \cdot \mathbf c} {\norm {\mathbf a} \norm {\mathbf b} } | c = [[Dot Product Distributes over Addition]] }} {{eqn | r = \frac { {\norm {\mathbf a} }^2 + \mathbf a \cdot \mathbf c} {\norm {\mathbf a} \norm {\mathbf b} } | c = [[Dot Product of Vector with Itself]] }} {{eqn | r = \frac { {\norm {\mathbf c} }^2 + \mathbf a \cdot \mathbf c} {\norm {\mathbf c} \norm {\mathbf b} } | c = from $(b)$ above: $\norm {\mathbf a} = \norm {\mathbf c}$ }} {{eqn | r = \frac {\mathbf c \cdot \mathbf c + \mathbf a \cdot \mathbf c} {\norm {\mathbf c} \norm {\mathbf b} } | c = [[Dot Product of Vector with Itself]] }} {{eqn | r = \frac {\mathbf c \cdot \left({\mathbf a + \mathbf c}\right)} {\norm {\mathbf c} \norm {\mathbf b} } | c = [[Dot Product Distributes over Addition]] }} {{eqn | r = \frac {\mathbf c \cdot \mathbf b} {\norm {\mathbf c} \norm {\mathbf b} } | c = from $(a)$ above: $\mathbf b = \mathbf a + \mathbf c$ }} {{eqn | r = \cos \angle \mathbf c, \mathbf b | c = {{Defof|Dot Product|index = 2}} }} {{end-eqn}} By definition of [[Definition:Dot Product/Definition 2|dot product]], the angle between the vectors is between $0$ and $\pi$. From [[Shape of Cosine Function]], [[Definition:Cosine|cosine]] is [[Definition:Injection|injective]] on this interval. Hence: :$\angle \mathbf a, \mathbf b = \angle \mathbf c, \mathbf b$ The result follows. {{qed}}	1
{{ProofWanted}} [[Category:Unique Factorization Domains]] [[Category:Factorization]] 2y7r9urx0npae77nocb10s6j20xxxss	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :$\forall x \in R: \norm x_1 < 1 \iff \norm x_2 < 1$ Then: :$\exists \alpha \in \R_{> 0}: \forall x \in R: \norm x_1 = \norm x_2^\alpha$	1
Let $D$ be the [[Definition:Set|set]] of all [[Definition:Finite Set|finitely]] [[Definition:Support of Mapping to Algebraic Structure|supported]] [[Definition:Sequence|sequences]] with [[Definition:Rational Number|rational]] terms: :$\displaystyle D = \set {\sequence {q_i}_{i \mathop \in \N} : n \in \N : i \le n : q_i \in \Q}$ We have that: :[[Rational Numbers are Countably Infinite]] :A [[Definition:Finite Set|finite set]] is [[Definition:Countable Set/Definition 3|countable]] :$D$ is a union of [[Definition:Finite Set|finite sets]] indexed by $n$, which is [[Definition:Countable|countable]] By [[Countable Union of Countable Sets is Countable]], $D$ is [[Definition:Countable Set|countable]]. Let $\mathbf x := \tuple {x_n}_{n \mathop \in \N} \in \ell^1$ By [[Definition:P-Sequence Space|definition]] of $\ell^1$: :$\displaystyle \sum_{n \mathop = 1}^\infty \size {x_n} < \infty$ Let $\displaystyle s_n := \sum_{i \mathop = 0}^n \size {x_i}$ be a [[Definition:Sequence|sequence]] of [[Definition:Partial Sum|partial sums]] of $\displaystyle s = \sum_{i \mathop = 0}^\infty \size {x_i}$. Then $s$ is a [[Definition:Convergent Sequence in Normed Vector Space|convergent sequence]]: :$\forall \epsilon' \in \R_{>0}: \exists N \in \N: \forall n \in \N: n > N \implies \size {s_n - s} < \epsilon'$ Note that: {{begin-eqn}} {{eqn | l = \size {s_n - s} | r = \size {\sum_{i \mathop = 0}^n \size {x_i} - \sum_{i \mathop = 0}^\infty \size {x_i} } }} {{eqn | r = \size {\sum_{i \mathop = n \mathop + 1}^\infty \size {x_i} } }} {{eqn | r = \sum_{i \mathop = n \mathop + 1}^\infty \size {x_i} | c = {{defof|Absolute Value}} }} {{end-eqn}} Let $\displaystyle \epsilon' = \frac \epsilon 2$. Let $\displaystyle N \in \N : \sum_{n \mathop = N + 1}^\infty \size {x_n} < \frac \epsilon 2$. We have that [[Rationals are Everywhere Dense in Reals/Normed Vector Space|$\Q$ is dense in $\R$]]. :$\forall x \in \R : \exists \epsilon'' \in \R_{\mathop > 0} : \exists q \in \Q : \size {x - q} < \epsilon''$ Then: :$\displaystyle \forall i \in \N : 1 \le i \le N : q_i \in \Q : \sum_{n \mathop = 1}^N \size {x_n - q_n} < \frac \epsilon 2$ where :$\displaystyle \frac \epsilon 2 = \sum_{i \mathop = 1}^N \epsilon_i''$ Let $\mathbf x' := \tuple {q_1, \ldots, q_N, 0, \ldots} \in D$. Then: :$\displaystyle \norm {\mathbf x - \mathbf x '} = \sum_{n \mathop = 1}^N \size {x_n - q_n} + \sum_{n \mathop = N + 1}^\infty \size {x_n} < \epsilon$ {{qed}}	1
A '''vector quantity''' is a is a [[Definition:Real-World|real-world]] concept that needs for its [[Definition:Mathematical Model|model]] a mathematical [[Definition:Object|object]] with more than one [[Definition:Component of Vector|component]] to specify it. Formally, a '''[[Definition:Vector Quantity|vector quantity]]''' is an element of a [[Definition:Vector Space|vector space]], often the [[Definition:Real Vector Space|real vector space $\R^n$]]. The usual intellectual frame of reference is to interpret a '''[[Definition:Vector Quantity|vector quantity]]''' as having: :A [[Definition:Magnitude|magnitude]] :A [[Definition:Direction|direction]].	1
By the [[Triangle Inequality]]: :$\cmod {x + y} - \cmod y \le \cmod x$ Let $z = x + y$. Then $x = z - y$ and so: :$\cmod z - \cmod y \le \cmod {z - y}$ Renaming variables as appropriate gives: :$\cmod {x - y} \ge \cmod x - \cmod y$ {{qed}}	1
Let $\hat o_1, \ldots, \hat o_m$ be a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]]. Here, $\hat o_i$ denotes an [[Definition:Elementary Row Operation|elementary row operation]] on a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]] over a [[Definition:Commutative Ring|commutative]] [[Definition:Ring with Unity|ring with unity]] $\struct {R, +, \circ}$. Here, $i \in \set {1, \ldots, m}$. Then there exists $c \in R$ such that for all [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]] $\mathbf A$ over $R$: :$\map \det {\mathbf A} = c \map \det {\mathbf A'}$ where $\mathbf A'$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]] that results from applying the [[Definition:Elementary Row Operation|elementary row operations]] $\hat o_1, \ldots, \hat o_m$ on $\mathbf A$.	1
=== Proof of Existence === Follows from the [[Definition:Vector Space Axioms|vector space axioms]]. {{qed|lemma}} === Proof of Uniqueness === Let $\mathbf 0$, $\mathbf 0'$ be [[Definition:Zero Vector|zero vectors]]. Utilizing the [[Definition:Vector Space Axioms|vector space axioms]]: {{begin-eqn}} {{eqn | l = \mathbf 0 | r = \mathbf 0 + \mathbf 0' }} {{eqn | r = \mathbf 0' + \mathbf 0 }} {{eqn | r = \mathbf 0' }} {{end-eqn}} {{qed}}	1
Let $e$ be the [[Definition:Elementary Column Operation|elementary column operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ECO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Column of Matrix|column]] $k$ of $\mathbf I$ by $\lambda$ | m = \kappa_k \to \lambda \kappa_k }} {{end-axiom}}	1
From [[Module of All Mappings is Module]], we have that $\struct {G^S, +_G', \circ}_R$ is an [[Definition:Module|$R$-module]]. To show that $\struct {G^S, +_G', \circ}_R$ is a [[Definition:Unitary Module|unitary $R$-module]], we verify the following: :$\forall f \in G^S: 1_R \circ f = f$ Let $\struct {G, +_G, \circ}_R$ be a [[Definition:Unitary Module|unitary $R$-module]]. Then: :$\forall x \in G: 1_R \circ x = x$ Thus: {{begin-eqn}} {{eqn | l = \map {\paren {1_R \circ f} } x | r = 1_R \circ \paren {\map f x} | c = }} {{eqn | r = \map f x | c = }} {{end-eqn}} Hence the result. {{qed}}	1
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]] whose [[Definition:Unity of Ring|unity]] is $1_D$. Let $a \in D$ be a [[Definition:Proper Element of Ring|proper element]] of $D$. Then: :$\forall n \in \Z_{\ge 0}: \ideal {a^{n + 1} } \subsetneq \ideal {a_n}$ where $\ideal x$ denotes the [[Definition:Principal Ideal of Ring|principal ideal of $D$ generated by $x$]].	1
By definition, the [[Definition:Multiplication of Algebra|multiplication]] of $\struct {S_R, *}$ is the [[Definition:Ring Product|ring product]] of $S$. Thus it follows immediately from the fact that $S$ is a [[Definition:Ring with Unity|ring with unity]], that $\struct {S_R, *}$ is a [[Definition:Unitary Algebra|unitary algebra]]. {{qed}} [[Category:Algebras]] cpt3ip7bivyes7cyh5urvhe82e5sjko	1
Let $\mathbf u, \mathbf v \in \R^n$. We will check the four defining properties of an [[Definition:Inner Product|inner product]] in turn. === Conjugate Symmetry === {{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf v | r = \mathbf v \cdot \mathbf u | c = [[Dot Product Operator is Commutative]] }} {{eqn | r = \overline{\mathbf v \cdot \mathbf u} | c = [[Complex Number equals Conjugate iff Wholly Real]] }} {{end-eqn}} Thus the [[Definition:Dot Product|dot product]] possesses [[Definition:Conjugate Symmetric Mapping|conjugate symmetry]]. {{qed|lemma}} === Bilinearity === From [[Dot Product Operator is Bilinear]], the [[Definition:Dot Product|dot product]] possesses [[Definition:Bilinear Mapping|bilinearity]]. {{qed|lemma}} === Non-Negative Definiteness === From [[Dot Product with Self is Non-Negative]], the [[Definition:Dot Product|dot product]] possesses [[Definition:Non-Negative Definite Mapping|non-negative definiteness]]. {{qed|lemma}} === Positiveness === From [[Dot Product with Self is Zero iff Zero Vector]], the [[Definition:Dot Product|dot product]] possesses [[Definition:Positiveness|positiveness]]. {{qed|lemma}} Hence the [[Definition:Dot Product|dot product]] satisfies the definition of an [[Definition:Inner Product|inner product]]. {{Qed}} [[Category:Dot Product]] bptif9dwnicrdc3izmxfl5ljtv2g5fq	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ be [[Definition:Norm on Division Ring|norms]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. Then $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ are [[Definition:Equivalent Division Ring Norms|equivalent]] {{iff}}: :$\exists \alpha \in \R_{\gt 0}: \forall n \in \N: \norm n_1 = \norm n_2^\alpha$	1
Let $\struct {S, \circ}_R$ be an [[Definition:R-Algebraic Structure|$R$-algebraic structure]] over a [[Definition:Ring (Abstract Algebra)|ring]] $R$. Let $T \subseteq S$ such that $\forall \lambda \in R: \forall x \in T: \lambda \circ x \in T$. Then $T$ is '''closed for scalar product'''. If $T$ is also [[Definition:Closed Algebraic Structure|closed for operations]] on $S$, then it is called a '''closed subset''' of $S$.	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\struct {G, +_G, \circ}_R$ be a [[Definition:Vector Space|vector space]] over $K$. === [[Definition:Basis of Vector Space/Definition 1|Definition 1]] === {{:Definition:Basis of Vector Space/Definition 1}} === [[Definition:Basis of Vector Space/Definition 2|Definition 2]] === {{:Definition:Basis of Vector Space/Definition 2}}	1
Let $y \in \map { {B_r}^-} x$. Let $a \in \map { {B_r}^-} y$. By the definition of an [[Definition:Closed Ball of Normed Division Ring|closed ball]], then: :$\norm {a - y} \le r$ :$\norm {y - x} \le r$ Hence: {{begin-eqn}} {{eqn | l = \norm {a - x} | r = \norm {a - y + y - x} }} {{eqn | r = \max \set {\norm {a - y}, \norm {y - x} } | o = \le | c = {{Defof|Non-Archimedean Division Ring Norm}} }} {{eqn | r = r | o = \le | c = }} {{end-eqn}} By the definition of a [[Definition:Closed Ball of Normed Division Ring|closed ball]], then: :$a \in \map { {B_r}^-} x$. Hence: :$\map { {B_r}^-} y \subseteq \map { {B_r}^-} x$ By [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] then: :$\norm {x - y} \le r$ By the definition of a [[Definition:Closed Ball of Normed Division Ring|closed ball]], then: :$x \in \map { {B_r}^-} y$ Similarly it follows that: :$\map { {B_r}^-} x \subseteq \map { {B_r}^-} y$ By [[Definition:Set Equality/Definition 2|set equality]]: :$\map { {B_r}^-} x = \map { {B_r}^-} y$ {{qed}}	1
Let $\displaystyle \lim_{n \mathop \to \infty} x_n = 0$ By the definition of the [[Definition:Ring of Cauchy Sequences|product on the ring of Cauchy sequences]] then: :$\sequence {x_n} \sequence {y_n} = \sequence {x_n y_n}$ :$\sequence {y_n} \sequence {x_n} = \sequence {y_n x_n}$ By [[Product of Sequence Converges to Zero with Cauchy Sequence Converges to Zero|product of sequence converges to zero with Cauchy sequence]] then: :$\displaystyle \lim_{n \mathop \to \infty} x_n y_n = 0$ :$\displaystyle \lim_{n \mathop \to \infty} y_n x_n = 0$ The result follows. {{qed}}	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :for all sequences $\sequence {x_n}$ in $R:\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_1 \iff \sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_2$ Then $\forall x \in R$: :$\norm x_1 < 1 \iff \norm x_2 < 1$	1
Remember that $\det$ can be interpreted as an alternating multilinear map with respect to the columns. This property is sufficient to prove the theorem as follows. Let $\mathbf A, \mathbf B$ be two $n \times n$ matrices (with coefficients in a commutative field $\mathbb K$ like $\mathbb R$ or $\mathbb C$). Let us denote the vectors of the canonical basis of $\mathbb K^n$ by $\mathbf e_1, \ldots, \mathbf e_n$ (where $\mathbf e_i$ is a column with $1$ at $i$th row, zero elsewhere). Now, we are able to write the matrix $\mathbf B$ as a column block matrix : :$\mathbf B = \begin {pmatrix} \displaystyle \sum_{s_1 \mathop = 1}^n \mathbf B_{s_1, 1} \mathbf e_{s_1} & \cdots & \displaystyle \sum_{s_n \mathop = 1}^n \mathbf B_{s_n, n} \mathbf e_{s_n} \end {pmatrix}$ We can rewrite the product $\mathbf A \mathbf B$ as a column-block matrix : :$\mathbf A \mathbf B = \begin {pmatrix} \displaystyle \sum_{s_1 \mathop = 1}^n \mathbf B_{s_1, 1} \mathbf A \mathbf e_{s_1} & \cdots & \displaystyle \sum_{s_n \mathop = 1}^n B_{s_n, n} \mathbf A \mathbf e_{s_n} \end {pmatrix} $ Using linearity with respect to each columns, we get: :$\map \det {\mathbf A \mathbf B} = \displaystyle \sum_{1 \mathop \leqslant s_1, \ldots, s_n \mathop \leqslant n} \paren {\prod_{i \mathop = 1}^n \mathbf B_{s_i, i} } \det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end {pmatrix}$ Now notice that $\det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end{pmatrix}$ is zero once two entries are the same (since $\det$ is an alternating map), it means that if for some $k \ne \ell$ we have $\mathbf A \mathbf e_{s_k} = \mathbf A \mathbf e_{s_\ell}$, then $\det \begin {pmatrix} \mathbf A \mathbf e_{s_1} & \cdots & \mathbf A \mathbf e_{s_n} \end {pmatrix} = 0$. Therefore the only nonzero summands are those one the $s_1, \ldots, s_n$ are all distinct. In other words, the "selector" $s$ represents some permutation of the numbers $1, \ldots, n$. As a result, the determinant of the product can now be expressed as a sum of precisely $n!$ terms using permutations: :$\map \det {\mathbf A \mathbf B} = \displaystyle \sum_{\sigma \in S_n} \paren {\prod_{i \mathop = 1}^n B_{\map \sigma i, i} } \det \begin {pmatrix} \mathbf A \mathbf e_{\map \sigma 1} & \cdots & \mathbf A \mathbf e_{\map \sigma n} \end {pmatrix}$ where $S_n$ denotes the set of the permutations of numbers $1, \ldots, n$. However, the {{RHS}} determinant of the above equality corresponds to the determinant of permutated columns of $\mathbf A$. Whenever we transpose two columns, the determinant is modified by a factor $-1$. Indeed, let us apply some transposition $\tau_{i j}$ to a column-block matrix $\begin {pmatrix} \mathbf C_1 & \cdots & \mathbf C_n \end {pmatrix}$. By linearity it follows that for $i, j$ entries equal to $\mathbf C_i + \mathbf C_j$: {{begin-eqn}} {{eqn | l = 0 | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i + \mathbf C_j \cdots \mathbf C_j + \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | o = | ro= + | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{eqn | r = \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_i \cdots \mathbf C_j \cdots \mathbf C_n \end {pmatrix} + \det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_j \cdots \mathbf C_i \cdots \mathbf C_n \end {pmatrix} | c = }} {{end-eqn}} Hence, transpose two columns reverse determinant sign: :$\det \begin {pmatrix} \mathbf C_1 \cdots \mathbf C_n \end {pmatrix} = -\det \begin {pmatrix} \mathbf C_{\map {\tau_{i j} } 1} \cdots \mathbf C_{\map {\tau_{i j} } n} \end {pmatrix}$ Since every permutation $\sigma \in S_n$ can be written as a product of transpositions, that is: :$\sigma = \tau_m \cdots \tau_1$ for some transpositions $\tau_1, \ldots, \tau_m$, it follows that: {{begin-eqn}} {{eqn | l = \map \det {\mathbf C_1 \cdots \mathbf C_n} | r = -\map \det {\mathbf C_{\map {\tau_1} 1} \cdots \mathbf C_{\map {\tau_1} n} } | c = }} {{eqn | r = \map \det {\mathbf C_{\map {\tau_2 \tau_1} 1} \cdots \mathbf C_{\map {\tau_2 \tau_1} n} } | c = }} {{eqn | r = \cdots | c = }} {{eqn | r = \paren {-1}^m \map \det {\mathbf C_{\map \sigma 1} \cdots \mathbf C_{\map \sigma n} } | c = }} {{end-eqn}} The number $\paren {-1}^m$ is the signature of the permutation $\sigma$ (see article [[Definition:Sign of Permutation|about the signature of permutations]]) and denoted by $\map \sgn \sigma$. It remains to apply several transpositions of columns to $\mathbf A$ to get for any permutation $\sigma$ the equality : :$\det \begin {pmatrix} \mathbf A \mathbf e_{\map \sigma 1} & \cdots & \mathbf A \mathbf e_{\map \sigma n} \end {pmatrix} = \map \sgn \sigma \det \begin {pmatrix} \mathbf A \mathbf e_1 & \cdots & \mathbf A \mathbf e_n \end {pmatrix} = \map \sgn \sigma \map \det {\mathbf A}$ Since $\map \det {\mathbf A}$ is a constant quantity, we can go this factor out of the sum, then write: :$\displaystyle \map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \sum_{\sigma \mathop \in S_n} \map \sgn \sigma \prod_{i \mathop = 1}^n \mathbf B_{\map \sigma i, i}$ But the above sum is exactly the definition of $\map \det {\mathbf B}$ using the Leibniz formula, and so: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \map \det {\mathbf B}$ Hence the result. {{qed}}	1
From [[Group equals Center iff Abelian]], the [[Definition:Center of Ring|center]] of a [[Definition:Commutative Ring|commutative ring]] is the entire ring. The result follows from [[Linear Transformation from Center of Scalar Ring]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \size {z_2 \times z_1} | r = \size {-z_1 \times z_2} | c = [[Complex Cross Product is Anticommutative]] }} {{eqn | r = \size {z_1 \times z_2} | c = {{Defof|Absolute Value}} }} {{end-eqn}} Hence the result. {{qed}}	1
Let $V$ and $W$ be [[Definition:Finite Dimensional Vector Space|finite dimensional vector spaces]]. Let $\phi: V \to W$ be a [[Definition:Linear Transformation|linear transformation]] from $V$ to $W$. Let $\left({e_1, \ldots, e_n}\right)$ and $\left({f_1, \ldots, f_m}\right)$ be [[Definition:Ordered Basis|ordered bases]] of $V$ and $W$ respectively. Let $A$ be the [[Definition:Relative Matrix|matrix of $\phi$ in these bases]]. Define $f: V \to \R^n$ by: :$\displaystyle \sum_{i \mathop = 1}^n a_i e_i \mapsto \left({a_1, \ldots, a_n}\right)$ and $g : W \to \R^m$ by: :$\displaystyle \sum_{i \mathop = 1}^m b_i f_i \mapsto \left({b_1, \ldots, b_m}\right)$ Let $N \left({A}\right) = \left\{ {x \in \R^n: A x = 0}\right\}$ be the [[Definition:Null Space|null space]] of $A$. Let $\ker \phi = \left\{ {x \in V: \phi x = 0}\right\}$ be the [[Definition:Kernel of Linear Transformation|kernel]] of $\phi$. Then: :$f \left[{\ker \phi}\right] = N \left({A}\right)$ and :$f^{-1} \left[{N \left({A}\right)}\right] = \ker \phi$ where $f \left[{X}\right]$ denotes the [[Definition:Image of Subset under Mapping|image set]] of a [[Definition:Subset|subset]] $X$ of the [[Definition:Domain of Mapping|domain]] of $f$.	1
We have that: :[[Space of Bounded Sequences with Pointwise Addition and Pointwise Scalar Multiplication on Ring of Sequences forms Vector Space|Space of bounded sequences is a vector space]] :[[Supremum Norm is Norm/Space of Bounded Sequences|Supremum norm on the space of bounded sequences is a norm]] By definition, $\struct {\ell^\infty, \norm {\, \cdot \,}_\infty}$ is a [[Definition:Normed Vector Space|normed vector space]]. {{qed}}	1
Let $\mathbf A$ be a [[Definition:Matrix|matrix]]. Let $\mathbf A^\intercal$ be the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Then: :$\paren {\mathbf A^\intercal}^\intercal = \mathbf A$	1
We have that: : [[Rationals are Dense in Compact Complement Topology]] : [[Rational Numbers are Countably Infinite]] Hence the result by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $t > 0$ be [[Definition:Wholly Real|wholly real]]. Let $t^z$ be [[Definition:Power (Algebra)/Complex Number/Principal Branch/Positive Real Base|$t$ to the power of $z$ defined on its principal branch]]. Then: :$\cmod {t^z} = t^{\map \Re z}$	1
:$\OO$ is a [[Definition:Local Ring/Noncommutative|local ring]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $G$ be a [[Definition:Dimension (Linear Algebra)|finite-dimensional]] [[Definition:Free Module|free $R$-module]]. Let $A = \left \langle {a_n} \right \rangle$ and $B = \left \langle {b_n} \right \rangle$ be [[Definition:Ordered Basis|ordered bases]] of $G$. === [[Definition:Change of Basis Matrix/Definition 1|Definition 1]] === {{:Definition:Change of Basis Matrix/Definition 1}} === [[Definition:Change of Basis Matrix/Definition 2|Definition 2]] === {{:Definition:Change of Basis Matrix/Definition 2}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ECO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Column of Matrix|column]] $k$ by $\lambda$ | m = \kappa_k \to \lambda \kappa_k }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Column Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Column Operation|elementary column operation]]: :$e' := \kappa_k \to \dfrac 1 \lambda \kappa_k$	1
Let $\mathbf r$ be a [[Definition:Vector Quantity|vector quantity]] embedded in [[Definition:Ordinary Space|space]]. Let $\mathbf a$, $\mathbf b$ and $\mathbf c$ be [[Definition:Non-Coplanar Vectors|non-coplanar]]. Then $\mathbf r$ can be expressed [[Definition:Unique|uniquely]] as the [[Definition:Resultant|resultant]] of $3$ [[Definition:Vector Quantity|vector quantities]] which are each [[Definition:Parallel Lines|parallel]] to one of $\mathbf a$, $\mathbf b$ and $\mathbf c$.	1
{{:Definition:Vector Space/Definition 1}}	1
The proof proceeds by first showing that $(1)$ is equivalent to $(2)$. Then, these are combined and shown equivalent to $(3)$. === $(1)$ implies $(2)$ === Suppose $PQ$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. Then by [[Characterization of Projections]], statement $(4)$, one has: :$PQ = \left({PQ}\right)^* = Q^* P^* = QP$ where the penultimate equality follows from [[Adjoint of Composition]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Suppose that $PQ = QP$. Then $\left({PQ}\right)^2 = PQPQ = P^2 Q^2 = PQ$ as $P, Q$ are [[Definition:Projection (Hilbert Spaces)|projections]]. Hence $PQ$ is an [[Definition:Idempotent Operator|idempotent]]. Also, note that $\left({PQ}\right)^* = Q^* P^* = QP = PQ$. Hence, by [[Characterization of Projections]], statement $(4)$, $PQ$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. {{qed|lemma}} === $(1), (2)$ imply $(3)$ === The above establishes that assuming either of $(1)$ and $(2)$ yields both to hold. So assuming $(1)$, $P, Q$ and $PQ$ are all [[Definition:Projection (Hilbert Spaces)|projections]], and $PQ = QP$. Now compute: {{begin-eqn}} {{eqn | l = \left({P + Q - PQ}\right)^2 | r = P^2 + Q^2 + \left({PQ}\right)^2 + PQ + QP - PPQ - PQP - QPQ - PQQ }} {{eqn | r = P + Q - PQ + PQ + PQ - PQ - PQ - PQ - PQ | c = $P, Q, PQ$ [[Definition:Projection (Hilbert Spaces)|projections]], $PQ = QP$ }} {{eqn | r = P + Q - PQ }} {{end-eqn}} It follows that $P + Q - PQ$ is an [[Definition:Idempotent Operator|idempotent]]. Observe from [[Adjoining is Linear]] and [[Adjoint of Composition]]: :$\left({P + Q - PQ}\right)^* = P^* + Q^* - Q^* P^* = P + Q - QP = P + Q - PQ$ Now applying [[Characterization of Projections]], statement $(4)$, conclude that $P + Q - PQ$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. {{qed|lemma}} === $(3)$ implies $(2)$ === Let $P + Q - PQ$ be a [[Definition:Projection (Hilbert Spaces)|projection]]. Then by [[Characterization of Projections]], statement $(4)$, compute: {{begin-eqn}} {{eqn | l = P + Q - PQ | r = \left({P + Q - PQ}\right)^* }} {{eqn | r = P^* + Q^* - Q^* P^* | c = [[Adjoining is Linear]], [[Adjoint of Composition]] }} {{eqn | r = P + Q - QP }} {{end-eqn}} Hence necessarily $PQ = QP$. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $T \in B_{00} \left({H}\right)$ be a [[Definition:Bounded Linear Operator|bounded]] [[Definition:Finite Rank Operator|finite rank operator]]. Let $n = \operatorname{dim} \left({\operatorname{ran} T}\right)$ be the [[Definition:Rank|rank]] of $T$. Then there are [[Definition:Orthonormal|orthonormal]] [[Definition:Vector (Linear Algebra)|vectors]] $e_1, \ldots, e_n$ and [[Definition:Vector (Linear Algebra)|vectors]] $g_1, \ldots, g_n$ of $H$ such that: :$\forall h \in H: Th = \displaystyle \sum_{i=1}^n \left\langle{h, e_i}\right\rangle_H g_i$	1
Let $R$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\mathbf A \in R^{n \times n}$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order]] $n$. Let the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$ be [[Definition:Unit of Ring|invertible]] in $R$. Then $\mathbf A$ is an [[Definition:Invertible Matrix|invertible matrix]].	1
=== [[Equivalence of Definitions of Associate in Integral Domain/Definition 1 Equivalent to Definition 2|$(1)$ is Equivalent to $(2)$]] === {{:Equivalence of Definitions of Associate in Integral Domain/Definition 1 Equivalent to Definition 2}} === [[Equivalence of Definitions of Associate in Integral Domain/Definition 1 Equivalent to Definition 3|$(1)$ is Equivalent to $(3)$]] === {{:Equivalence of Definitions of Associate in Integral Domain/Definition 1 Equivalent to Definition 3}} [[Category:Integral Domains]] [[Category:Associates]] [[Category:Equivalence of Definitions of Associate in Integral Domain]] lbb4s0ifmkn6vtxm5u7br1j16jo7mv3	1
=== Necessary Condition === Let $x \perp_B y$. Let $V' \subset V$ be the subspace spanned by $x$ and $y$. Define $\overline f$ on $V'$ as: :$\map {\overline f} {a x + b y} = a \norm x$ for $a$ and $b$ scalars. Clearly, $\overline f$ is linear and: :$\map {\overline f} x = \norm x$ :$\map {\overline f} y = 0$ Further: {{begin-eqn}} {{eqn | l = \norm {a x + b y} | r = \size a \norm {x + \dfrac b a y} | c = }} {{eqn | o = \ge | r = \size a \norm x | c = }} {{eqn | r = \size {\map {\overline f} {a x + b y} } | c = }} {{end-eqn}} proving that $\overline f$ is a bounded functional of norm $1$. Now by [[Hahn-Banach Theorem]], $\overline f$ can be extended to a functional $f$ on $V$ such that $\norm f = \norm {\overline f} = 1$ This proves the necessity. {{qed|lemma}} === Sufficient Condition === Let such a functional $f$ on $V$ exist. Then for any scalar $\lambda$: :$\norm {x + \lambda y} \ge \size {\map f {x + \lambda y} } = \norm x$ establishing the sufficiency. {{qed}}	1
By definition of [[Definition:Vector Cross Product/Complex/Definition 2|complex cross product]]: :$z_1 \times z_2 = \cmod {z_1} \, \cmod {z_2} \sin \theta$ :$\cmod {z_1}$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z_1$ :$\theta$ denotes the [[Definition:Angle|angle]] from $z_1$ to $z_2$, measured in the [[Definition:Positive Direction|positive direction]]. === Necessary Condition === Let $z_1$ and $z_2$ be [[Definition:Parallel Vectors|parallel]]. Then either $\theta = 0^\circ$ or $\theta = 180^\circ$. Either way, from [[Sine of Zero is Zero]] or [[Sine of Straight Angle]]: :$\sin \theta = 0$ and so: :$\cmod {z_1} \, \cmod {z_2} \sin \theta = 0$ Hence by definition: :$z_1 \times z_2 = 0$ {{qed|lemma}} === Sufficient Condition === Let $z_1 \times z_2 = 0$. Then by definition: :$\cmod {z_1} \, \cmod {z_2} \sin \theta = 0$ As neither $z_1 = 0$ or $z_2 = 0$ it follows that $\sin \theta = 0$. From [[Sine of Multiple of Pi]] it follows that either: :$\theta = 0^\circ$ :$\theta = 180^\circ$ or: :$\theta$ is either of the above plus a full circle. That is, $z_1$ and $z_2$ are [[Definition:Parallel Vectors|parallel]]. {{qed}}	1
Let $D$ be the [[Definition:Determinant of Matrix|determinant of order $n$]]. Let $r_1, r_2, \ldots, r_k$ be [[Definition:Integer|integers]] such that: :$1 \le k < n$ :$1 \le r_1 < r_2 < \cdots < r_k \le n$ Let $\map D {r_1, r_2, \ldots, r_k \mid u_1, u_2, \ldots, u_k}$ be an [[Definition:Minor of Determinant|order-$k$ minor of $D$]]. Let $\map {\tilde D} {r_1, r_2, \ldots, r_k \mid u_1, u_2, \ldots, u_k}$ be the [[Definition:Cofactor of Minor|cofactor]] of $\map D {r_1, r_2, \ldots, r_k \mid u_1, u_2, \ldots, u_k}$. Then: :$\displaystyle D = \sum_{1 \mathop \le u_1 \mathop < \cdots \mathop < u_k \mathop \le n} \map D {r_1, r_2, \ldots, r_k \mid u_1, u_2, \ldots, u_k} \, \map {\tilde D} {r_1, r_2, \ldots, r_k \mid u_1, u_2, \ldots, u_k}$ A similar result applies for columns.	1
Let $E$ be a [[Definition:Vector Space|vector space]] of $n$ [[Definition:Dimension of Vector Space|dimensions]]. Let $G$ be a [[Definition:Generator of Module|generator]] for $E$. Then $G$ has the following properties:	1
Let $M$ and $N$ be [[Definition:Metric Space|metric spaces]]. Let $M$ be [[Definition:Complete Metric Space|complete]]. Let $f : M \times N \to M$ be a [[Definition:Continuous Mapping|continuous]] [[Definition:Uniform Contraction Mapping|uniform contraction]]. Then for all $t \in N$ there exists a [[Definition:Unique|unique]] $g \left({t}\right) \in M$ such that $f(g \left({t}\right), t) = g \left({t}\right)$, and the [[Definition:Mapping|mapping]] $g: N \to M$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]].	1
Let $T = \struct {\Z_{\ge 0}, \tau}$ denote the [[Definition:Topological Space|topological space]] consisting of the [[Definition:Positive Integer|set of positive integers]] $\Z_{\ge 0}$ under the [[Definition:Discrete Topology|discrete topology]]. Let $I$ be an [[Definition:Indexing Set|indexing set]] with [[Definition:Uncountable Set|uncountable cardinality]]. Let $T' = \struct {\displaystyle \prod_{\alpha \mathop \in I} \struct {\Z_{\ge 0}, \tau}_\alpha, \tau'}$ be the [[Definition:Uncountable Cartesian Product|uncountable Cartesian product]] of $\struct {\Z_{\ge 0}, \tau}$ [[Definition:Indexed Family|indexed]] by $I$ with the [[Definition:Tychonoff Topology|Tychonoff topology]] $\tau'$. From [[Countable Discrete Space is Separable]], $T$ is a [[Definition:Separable Space|separable space]]. But from [[Uncountable Cartesian Product of Discrete Topology on Positive Integers is not Separable]], $T$ is not a [[Definition:Separable Space|separable space]]. Hence the result. {{qed}}	1
Let $n \in \Z_{> 0}$ be a [[Definition:Strictly Positive Integer|strictly positive integer]]. Let $S$ be the [[Definition:Set|set]] of [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order]] $n$ of [[Definition:Real Numbers|real numbers]] whose [[Definition:Determinant of Matrix|determinant]] is either $1$ or $-1$. Let $\struct {S, \times}$ denote the [[Definition:Algebraic Structure|algebraic structure]] formed by $S$ whose [[Definition:Binary Operation|operation]] is [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]]. Then $\struct {S, \times}$ is a [[Definition:Group|group]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free]] [[Definition:Module|$R$-module]] of [[Definition:Dimension of Module|dimension]] $n$. Let $B = \left \langle {b_k} \right \rangle_{1 \mathop \le k \mathop \le n}$ be an [[Definition:Ordered Basis|ordered basis]] of $M$. Let $x\in M$. If $\lambda_1,\ldots,\lambda_n\in R$ are such that $x = \displaystyle\sum_{i=1}^n \lambda_i b_i$, then $(\lambda_1,\ldots,\lambda_n)^\intercal \in R^n$ is the '''coordinate vector''' of $x$ with respect to $B$. This can be denoted: $[x]_B$.	1
:[[File:TangentToRadiusPolar.png|400px]] {{ProofWanted}}	1
Proved in [[Linear Combination of Derivatives]]. {{qed}}	1
$L$ can be expressed by the equation: :$z = z_1 + t \paren {z_2 - z_1}$ or: :$z = \paren {1 - t} z_1 + t z_2$ This form of $L$ is known as the '''parametric form''', where $t$ is the '''parameter'''.	1
Let $\epsilon>0$. Let $z \in \C$ be a [[Definition:Complex Number|complex number]] satisfying $\left\vert{z - z_0}\right\vert < \epsilon$. By the [[Reverse Triangle Inequality/Real and Complex Fields|Reverse Triangle Inequality]]: : $\left\vert{ \left\vert{z}\right\vert - \left\vert{z_0}\right\vert }\right\vert \le \left\vert{z - z_0}\right\vert < \epsilon$ Hence the result, by the [[Definition:Continuous Complex Function#Epsilon-Delta Definition|$\epsilon$-$\delta$ definition of continuity]] (taking $\delta = \epsilon$). {{qed}} [[Category:Complex Modulus]] [[Category:Continuous Functions]] c5b85ilzan02knmwpgg6iyhjvy62lag	1
'''Matrix theory''' is the field of [[Definition:Mathematics|mathematics]] which studies [[Definition:Matrix|matrices]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ and $H$ be [[Definition:Module|$R$-modules]]. Let $G^*$ and $H^*$ be the [[Definition:Algebraic Dual|algebraic duals]] of $G$ and $H$ respectively. Let $\mathcal L_R \left({G, H}\right)$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Let $u \in \mathcal L_R \left({G, H}\right)$. The '''transpose''' of $u$ is the [[Definition:Mapping|mapping]] $u^t: H^* \to G^*$ defined as: :$\forall y' \in H^*: u^t \left({y'}\right) = y' \circ u$ where $y' \circ u$ is the [[Definition:Composition of Mappings|composition]] of $y'$ and $u$.	1
The notation $*^n x$ can be written as $x^n$. Let us verify that $\struct {G, *, \circ}_\Z$ is a [[Definition:Unitary Module|unitary $\Z$-module]] by verifying the axioms in turn. === Axiom $(\text M 1)$ === We need to show that $n \circ \paren {x * y} = \paren {n \circ x} * \paren {n \circ y}$. From the definition, $n \circ x = x^n$ and so $n \circ \paren {x * y} = \paren {x * y}^n$ From [[Power of Product in Abelian Group]]: :$\paren {x * y}^n = x^n * y^n = \paren {n \circ x} * \paren {n \circ y}$ {{qed|lemma}} === Axiom $(\text M 2)$ === We need to show that $\paren {n + m} \circ x = \paren {n \circ x} * \paren {m \circ x}$. That is, that $x^{n + m} = x^n * x^m$. This is an instance of [[Powers of Group Elements/Sum of Indices|Powers of Group Elements: Sum of Indices]]. {{qed|lemma}} === Axiom $(\text M 3)$ === We need to show that $\paren {n \times m} \circ x = n \circ \paren {m \circ x}$. That is, that $x^{n m} = \paren {x^m}^n$. This follows directly from [[Powers of Group Elements/Product of Indices|Powers of Group Elements: Product of Indices]]. {{qed|lemma}} === Axiom $(\text M 4)$ === We need to show that $\forall x \in G: 1 \circ x = x$. That is, that $x^1 = x$. This follows from the definition of [[Definition:Power of Group Element|Power of Group Element]]. {{qed|lemma}} Having verified all four axioms, we have shown that $\struct {G, *, \circ}_\Z$ is a [[Definition:Unitary Module|unitary $\Z$-module]]. {{qed}}	1
Let $\mathbf U = \sqbrk a_{m n}$ be an [[Definition:Upper Triangular Matrix|upper triangular matrix]]. By definition: :$\forall a_{i j} \in \mathbf U: i > j \implies a_{i j} = 0$ Let $\mathbf U^\intercal = \sqbrk b_{n m}$ be the [[Definition:Transpose of Matrix|transpose]] of $\mathbf U$. That is: :$\mathbf U^\intercal = \sqbrk b_{n m}: \forall i \in \closedint 1 n, j \in \closedint 1 n: b_{i j} = a_{j i}$ Thus: :$\forall b_{j i} \in \mathbf U^\intercal: i > j \implies b_{j i} = 0$ By exchanging $i$ and $j$ in the notation of the above: :$\forall b_{i j} \in \mathbf U^\intercal: i < j \implies b_{i j} = 0$ Thus by definition it is seen that $\mathbf U^\intercal$ is a [[Definition:Lower Triangular Matrix|lower triangular matrix]]. {{qed}} [[Category:Triangular Matrices]] [[Category:Transposes of Matrices]] 8brtoc9pbbp6vnhh6u25d3syuohyc91	1
Every [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$ is [[Definition:Equivalent Division Ring Norms|equivalent]] to either: :the [[Definition:P-adic Norm|$p$-adic norm $\norm {\, \cdot \,}_p$]] for some [[Definition:Prime Number|prime]] $p$ or: :the [[Definition:Absolute Value|absolute value]], $\size {\, \cdot \,}$.	1
=== [[Definition:Rank/Linear Transformation|Linear Transformation]] === {{:Definition:Rank/Linear Transformation}} === [[Definition:Rank/Matrix|Matrix]] === {{:Definition:Rank/Matrix}} {{expand|These definitions can be proved compatible (by viewing a matrix as a lin. transform., and maybe vice versa); such is a typical PW entry. Also, cf. [[Definition:Finite Rank Operator]]}} [[Category:Definitions/Linear Algebra|{{SUBPAGENAME}}]] e3nadl00pqmmtky0opygemoaw128urn	1
=== [[Closure of Hadamard Product]] === {{:Closure of Hadamard Product}} === [[Associativity of Hadamard Product]] === {{:Associativity of Hadamard Product}} === [[Commutativity of Hadamard Product]] === {{:Commutativity of Hadamard Product}} [[Category:Hadamard Product]] 9o8hi3biogmwh2i4ezufn49x1df57f7	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence{x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]] in $\struct {R, \norm {\,\cdot\,} }$. Let $x \in R$. Then $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$ {{iff}} $\sequence {x_n}$ has a [[Definition:Subsequence|subsequence]] that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$.	1
Let $R$ be a [[Definition:Division Ring|division ring]]. Let $\norm {\, \cdot \,}_1: R \to \R_{\ge 0}$ and $\norm {\, \cdot \,}_2: R \to \R_{\ge 0}$ be [[Definition:Norm on Division Ring|norms]] on $R$. Let $d_1$ and $d_2$ be the [[Definition:Metric Induced by Norm|metrics induced]] by the [[Definition:Norm on Division Ring|norms]] $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ respectively. {{TFAE|def = Equivalent Division Ring Norms}}	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\struct {G, +_G}$ be an [[Definition:Abelian Group|abelian group]]. A [[Definition:Unitary Module|unitary module over $R$]] is an [[Definition:R-Algebraic Structure|$R$-algebraic structure]] with one [[Definition:Binary Operation|operation]] $\struct {G, +_G, \circ}_R$ which satisfies the [[Definition:Unitary Module Axioms|unitary module axioms]]: {{:Definition:Unitary Module Axioms}}	1
Let $k$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\map {\operatorname O} {n, k}$ be the $n$th [[Definition:Orthogonal Group|orthogonal group]] on $k$. Let $\map {\operatorname {SO} } {n, k}$ be the $n$th [[Definition:Special Orthogonal Group|special orthogonal group]] on $k$. Then $\map {\operatorname {SO} } {n, k}$ is a [[Definition:Subgroup|subgroup]] of $\map {\operatorname O} {n, k}$.	1
:$\forall n \in \N_{\gt 0}: \norm {x^n} = 1 \implies \norm x = 1$	1
For any $d \in F \sqbrk X$, let $\ideal d$ denote the [[Definition:Principal Ideal of Ring|principal ideal of $F \sqbrk X$ generated by $d$]]. Let $J$ be any [[Definition:Ideal of Ring|ideal]] of $F \sqbrk X$. What we need to prove is that $J$ is a [[Definition:Principal Ideal of Ring|principal ideal]]. Let us first [[Definition:Distinguish|distinguish]] the following two cases for $J$: :If $J = \set {0_F}$, then by [[Zero Element Generates Null Ideal]] $J = \ideal {0_F}$, and hence is a [[Definition:Principal Ideal of Ring|principal ideal]]. :If $J = F \sqbrk X$, then by [[Ideal of Unit is Whole Ring/Corollary|Ideal of Unit is Whole Ring: Corollary]] $J = \ideal {1_F}$, and hence is a [[Definition:Principal Ideal of Ring|principal ideal]]. Now suppose $J \ne \set {0_F}$ and $J \ne F \sqbrk X$. Then $J$ necessarily contains a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]]. By the [[Well-Ordering Principle]], we can introduce the lowest [[Definition:Degree of Polynomial over Field|degree]] of a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] of $J$. Denote this [[Definition:Degree of Polynomial over Field|degree]] by $n$. If $n = 0$, then $J$ contains a [[Definition:Polynomial over Ring in One Variable|polynomial]] of [[Definition:Degree of Polynomial over Field|degree]] $0$. This is a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] of $F$. As $F$ is a [[Definition:Field (Abstract Algebra)|field]], this is therefore a [[Definition:Unit of Ring|unit]] of $F$, and thus by [[Ideal of Unit is Whole Ring]], $J = F \sqbrk X$. Because the [[Definition:Degree of Polynomial over Field|degree]] of a non-[[Definition:Field Zero|zero]] [[Definition:Element|element]] is a [[Definition:Natural Numbers|natural number]], we conclude that $n \ge 1$. Now let $d$ be a [[Definition:Polynomial over Field|polynomial]] of degree $n$ in $J$, and let $f \in J$. By [[Division Theorem for Polynomial Forms over Field]], $f = q \circ d + r$ for some $q, r \in F \sqbrk X$ where either: :$r = 0_F$ or: :$r$ is a [[Definition:Polynomial over Field|polynomial]] of [[Definition:Degree of Polynomial over Field|degree]] smaller than $n$. Because $J$ is an [[Definition:Ideal of Ring|ideal]] and $d \in J$, it follows that: :$q \circ d \in J$ Since $f \in J$, we also conclude: :$r = f - q \circ d \in J$ From the construction of $d$, it follows that we must have $r = 0_F$. Therefore: :$f = q \circ d$ and thus: :$f \in \ideal d$. This reasoning shows that: :$J \subseteq \ideal d$ From property $(3)$ of the [[Definition:Principal Ideal of Ring|principal ideal]] $\ideal d$, we conclude that: :$\ideal d \subseteq J$ as $d \in J$. Hence $J = \ideal d$. These $2$ [[Definition:Distinguish|distinguished cases]] cover all of the possible [[Definition:Ideal of Ring|ideals]] of $F \sqbrk X$. Hence $F \sqbrk X$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. {{qed}}	1
Let $\sequence {e_i}_{1 \mathop \le i \mathop \le k}$ be the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] that compose $\Gamma$. Let $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$ be the corresponding [[Definition:Finite Sequence|finite sequence]] of the [[Definition:Elementary Row Matrix|elementary row matrices]]. From [[Row Operation is Equivalent to Pre-Multiplication by Product of Elementary Matrices]], we have: :$\mathbf R \mathbf A = \mathbf B$ where $\mathbf R$ is the [[Definition:Matrix Product (Conventional)|product]] of $\sequence {\mathbf E_i}_{1 \mathop \le i \mathop \le k}$: :$\mathbf R = \mathbf E_k \mathbf E_{k - 1} \dotsb \mathbf E_2 \mathbf E_1$ By [[Elementary Row Matrix is Invertible]], each of $\mathbf E_i$ is [[Definition:Invertible Matrix|invertible]]. By [[Product of Matrices is Invertible iff Matrices are Invertible]], it follows that $\mathbf R$ is likewise [[Definition:Invertible Matrix|invertible]]. Thus $\mathbf R$ has an [[Definition:Inverse Matrix|inverse]] $\mathbf R^{-1}$. Hence: {{begin-eqn}} {{eqn | l = \mathbf R \mathbf A | r = \mathbf B | c = }} {{eqn | ll= \leadsto | l = \mathbf R^{-1} \mathbf R \mathbf A | r = \mathbf R^{-1} \mathbf B | c = }} {{eqn | ll= \leadsto | l = \mathbf A | r = \mathbf R^{-1} \mathbf B | c = }} {{end-eqn}} We have: {{begin-eqn}} {{eqn | l = \mathbf R^{-1} | r = \paren {\mathbf E_k \mathbf E_{k - 1} \dotsb \mathbf E_2 \mathbf E_1}^{-1} | c = }} {{eqn | r = {\mathbf E_1}^{-1} {\mathbf E_2}^{-1} \dotsb {\mathbf E_{k - 1} }^{-1} {\mathbf E_k}^{-1} | c = [[Inverse of Matrix Product]] }} {{end-eqn}} From [[Elementary Row Matrix for Inverse of Elementary Row Operation is Inverse]], each of ${\mathbf E_i}^{-1}$ is the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to the [[Existence of Inverse Elementary Row Operation|inverse]] $e'_i$ of the corresponding [[Definition:Elementary Row Operation|elementary row operation]] $e_i$. Let $\Gamma'$ be the [[Definition:Row Operation|row operation]] composed of the [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $\tuple {e'_k, e'_{k - 1}, \ldots, e'_2, e'_1}$. Thus $\Gamma'$ is a [[Definition:Row Operation|row operation]] which transforms $\mathbf B$ into $\mathbf A$. Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] with [[Definition:Basis (Linear Algebra)|basis]] $\{e_i\mid i\in I\}$. Let $N$ be an $R$-module. Let $\{n_i\mid i\in I\}$ be a family of elements of $N$. Then there exists a unique $R$-module homomorphism that maps $e_i$ to $n_i$ for all $i\in I$.	1
This is a special case of [[Direct Product of Modules is Module]].	1
{{begin-eqn}} {{eqn | l = \mathbf r \left({t}\right) \times \mathbf q \left({t}\right) | r = \begin{bmatrix} x \\ y \\ z \end{bmatrix} \times \begin{bmatrix} \chi \\ \gamma \\ \zeta \end{bmatrix} }} {{eqn | r = \begin{bmatrix} y \zeta - z \gamma \\ z \chi - x \zeta \\ x\gamma -y \chi \end{bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | ll= \leadsto | l = D_t \left({\mathbf r \left({t}\right) \times \mathbf q \left({t}\right)}\right) | r = \begin {bmatrix} D_t\left({y \zeta - z \gamma}\right) \\ D_t\left({z \chi - x \zeta}\right) \\ D_t\left({x\gamma -y \chi}\right) \end{bmatrix} | c = [[Differentiation of Vector-Valued Function Componentwise]] }} {{eqn | r = \begin {bmatrix} \dfrac{\d y} {\d t}\zeta + y \dfrac {\d \zeta} {\d t} - \dfrac {\d z} {\d t} \gamma - z \dfrac {\d \gamma} {\d t} \\ \dfrac {\d z} {\d t} \chi + z \dfrac {\d \chi} {\d t} - \dfrac {\d x} {\d t} \zeta - x \dfrac {\d \zeta} {\d t} \\ \dfrac {\d x} {\d t} \gamma + x \dfrac {\d \gamma} {\d t} - \dfrac {\d y} {\d t} \chi - y \dfrac {\d \chi} {\d t} \end{bmatrix} | c = [[Product Rule for Derivatives]], [[Linear Combination of Derivatives]] }} {{eqn | l = \mathbf r' \left({x}\right) \times \mathbf q \left({x}\right) + \mathbf r \left({x}\right) \times \mathbf q'\left({x}\right) | r = \begin {bmatrix} \dfrac {\d x} {\d t} \\ \dfrac {\d y} {\d t} \\ \dfrac {\d z} {\d t} \end {bmatrix} \times \begin {bmatrix} \chi \\ \gamma \\ \zeta \end {bmatrix} + \begin {bmatrix} x \\ y \\ z \end {bmatrix} \times \begin {bmatrix} \dfrac {\d \chi} {\d t} \\ \dfrac {\d \gamma} {\d t} \\ \dfrac {\d \zeta} {\d t} \end{bmatrix} }} {{eqn | r = \begin {bmatrix} \dfrac {\d y} {\d t} \zeta - \dfrac {\d z} {\d t} \gamma \\ \dfrac {\d z} {\d t} \chi - \dfrac {\d x} {\d t} \zeta \\ \dfrac {\d x} {\d t} \gamma - \dfrac {\d y} {\d t} \chi \end {bmatrix} + \begin {bmatrix} y \dfrac {\d \zeta} {\d t} - z \dfrac {\d \gamma} {\d t} \\ z \dfrac {\d \chi} {\d t} - x \dfrac {\d \zeta} {\d t} \\ x \dfrac {\d \gamma} {\d t} - y \dfrac {\d \chi} {\d t} \end {bmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = \begin {bmatrix} \dfrac {\d y} {\d t} \zeta - \dfrac {\d z} {\d t} \gamma + y \dfrac {\d \zeta} {\d t} - z \dfrac {\d \gamma} {\d t} \\ \dfrac {\d z} {\d t} \chi - \dfrac {\d x} {\d t} \zeta + z \dfrac {\d \chi} {\d t} - x \dfrac {\d \zeta} {\d t} \\ \dfrac {\d x} {\d t} \gamma - \dfrac {\d y} {\d t} \chi + x \dfrac {\d \gamma} {\d t} - y \dfrac {\d \chi} {\d t} \end {bmatrix} }} {{eqn | r = \begin {bmatrix} \dfrac {\d y} {\d t} \zeta + y \dfrac {\d \zeta} {\d t} - \dfrac {\d z} {\d t} \gamma - z \dfrac {\d \gamma} {\d t} \\ \dfrac {\d z} {\d t} \chi + z \dfrac {\d \chi} {\d t} - \dfrac {\d x} {\d t} \zeta - x \dfrac {\d \zeta} {\d t} \\ \dfrac {\d x} {\d t} \gamma + x \dfrac {\d \gamma} {\d t} - \dfrac {\d y} {\d t} \chi - y \dfrac {\d \chi} {\d t} \end {bmatrix} }} {{eqn | r = D_t \left({\mathbf r \left({t}\right) \times \mathbf q \left({t}\right)}\right) }} {{end-eqn}} {{qed}}	1
Follows directly from the definition of the [[Definition:Transpose of Matrix|transpose of a matrix]]. {{Qed}}	1
Then: :$\forall y \in R: \norm y_1 > 1 \iff \norm y_2 > 1$	1
First it is shown that $\PP$ is an [[Definition:Ideal of Ring|ideal]] of $\OO$ by applying [[Test for Ideal]]. That is, it is shown that: :$(1): \quad \PP \ne \O$ :$(2): \quad \forall x, y \in \PP: x + \paren {-y} \in \PP$ :$(3): \quad \forall x \in \PP, y \in \OO: x y \in \PP$ '''(1)''' By {{NormAxiom|1}}: :$\norm {0_R} = 0$ Hence: :$0_R \in \PP \ne \O$ {{qed|lemma}} '''(2)''' Let $x, y \in \PP$. Then: {{begin-eqn}} {{eqn | l = \norm {x + \paren{-y} } | o = \le | r = \max \set {\norm x, \norm{-y} } | c = {{NormAxiom|4}} }} {{eqn | r = \max \set {\norm x, \norm y} | c = [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] }} {{eqn | o = < | r = 1 | c = Since $x, y \in \PP$ }} {{end-eqn}} Hence: :$x + \paren {-y} \in \PP$ {{qed|lemma}} '''(3)''' Let $x \in \PP, y \in \OO$. Then: {{begin-eqn}} {{eqn | l = \norm{x y} | o = \le | r = \norm x \norm y | c = {{NormAxiom|2}} }} {{eqn | o = < | r = 1 | c = Since $x \in \PP, y \in \OO$ }} {{end-eqn}} Hence: :$x y \in \PP$ {{qed|lemma}} By [[Test for Ideal]] it follows that $\PP$ is an [[Definition:Ideal of Ring|ideal]] of $\OO$. By [[Maximal Left and Right Ideal iff Quotient Ring is Division Ring]] the statements '''(a)''', '''(b)''' and '''(c)''' above are [[Definition:Equivalent|equivalent]]. It is now shown that statement '''(a)''' holds. Let $J$ be a [[Definition:Left Ideal of Ring|left ideal]] of $\OO$: :$\PP \subsetneq J \subset \OO$ Let $x \in J \setminus \PP$, then: :$\norm x = 1$ By [[Properties of Norm on Division Ring/Norm of Inverse|Norm of Inverse]] then: :$\norm {x^{-1} } = 1 / \norm x = 1 / 1 = 1$ Hence: :$x^{-1} \in \OO$ Since $J$ is a [[Definition:Left Ideal of Ring|left ideal]] then: :$x^{-1} x = 1_R \in J$ Thus: :$\forall y \in \OO: y \cdot 1_R = y \in J$ That is, $J = \OO$ Hence $\PP$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]]. The result follows. {{qed}}	1
From [[Ring is Ideal of Itself]] and [[Ideal is Bimodule over Ring]], $\struct {R, +, \times, \times}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +, \times}$. {{qed}}	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $G$ be a [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Vector Space|$K$-vector space]]. Let $H$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $G$. Let $F$ be a [[Definition:Finite Set|finite]] [[Definition:Generator of Vector Space|generator]] for $G$ such that $H \subseteq F$. Then there is a [[Definition:Basis of Vector Space|basis]] $B$ for $G$ such that $H \subseteq B \subseteq F$.	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number as Vector|complex numbers expressed as vectors]]. Let $ABCD$ be the [[Definition:Parallelogram|parallelogram]] formed by letting $AD = z_1$ and $AB = z_2$. Then the [[Definition:Area|area]] $\AA$ of $ABCD$ is given by: :$\AA = z_1 \times z_2$ where $z_1 \times z_2$ denotes the [[Definition:Complex Cross Product|cross product]] of $z_1$ and $z_2$.	1
:$\cmod z \ge \size {\map \Re z}$	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be an [[Definition:Module|$R$-module]]. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $G^{**}$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G^*$. For each $x \in G$, we define the [[Definition:Mapping|mapping]] $x^\wedge: G^* \to R$ as: :$\forall t' \in G^*: x^\wedge \left({t'}\right) = t' \left({x}\right)$ Then $x^\wedge \in G^{**}$. The mapping $J: G \to G^{**}$ defined as: :$\forall x \in G: J \left({x}\right) = x^\wedge$ is called the '''evaluation linear transformation from $G$ into $G^{**}$'''. It is usual to denote the mapping $t': G^* \to G$ as follows: :$\forall x \in G, t' \in G^*: \left \langle {x, t'} \right \rangle := t' \left({x}\right)$	1
This is a special case of [[Direct Product of Modules is Module]].	1
Let $\struct {R, \norm { \, \cdot \, } }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$. Let $N \in \N$ Let $\sequence {y_n}$ be the [[Definition:Sequence|sequence]] defined by: :$\forall n, y_n = x_{N+n}$ Let $\sequence {y_n}$ be a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence]] in $R$ with limit $l$. Then: :$\sequence {x_n}$ is a [[Definition:Convergent Sequence in Normed Division Ring|convergent sequence]] in $R$ with limit $l$.	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm{\,\cdot\,}$, Let $x, y \in R$. Then: :* $\norm {x + y} \lt \norm y \implies \norm x = \norm y$ :* $\norm {x - y} \lt \norm y \implies \norm x = \norm y$ :* $\norm {y - x} \lt \norm y \implies \norm x = \norm y$	1
Let $\Bbb F$ denote one of the [[Definition:Standard Number System|standard number systems]]. Let $\map \MM {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\Bbb F$. For $\mathbf A \in \map \MM {m, n}$ and $\lambda$ \in $\Bbb F$, let $\lambda \mathbf A$ be defined as the [[Definition:Matrix Scalar Product|matrix scalar product]] of $\lambda$ and $\mathbf A$. The [[Definition:Matrix Scalar Product|matrix scalar product]] is [[Definition:Associative Operation|associative]] on $\map \MM {m, n}$, in the following sense: For all $\mathbf A$ in $\map \MM {m, n}$ and $\lambda, \mu \in \Bbb F$: :$\lambda \paren {\mu \mathbf A} = \paren {\lambda \mu} \mathbf A$	1
Let $C_n$ be the [[Definition:Combinatorial Matrix|combinatorial matrix]] of [[Definition:Order of Square Matrix|order $n$]] given by: :$C_n = \begin{bmatrix} x + y & y & \cdots & y \\ y & x + y & \cdots & y \\ \vdots & \vdots & \ddots & \vdots \\ y & y & \cdots & x + y \end{bmatrix}$ Let $C_n^{-1}$ be its [[Definition:Inverse Matrix|inverse]], from [[Inverse of Combinatorial Matrix]]: :$b_{i j} = \dfrac {-y + \delta_{i j} \left({x + n y}\right)} {x \left({x + n y}\right)}$ where $\delta_{i j}$ is the [[Definition:Kronecker Delta|Kronecker delta]]. The sum of all the [[Definition:Element of Matrix|elements]] of $C_n^{-1}$ is: :$\displaystyle \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} = \dfrac n {x + n y}$	1
:There is no [[Definition:Left Ideal of Ring|left ideal]] $\JJ$ of $\CC$ such that $\NN \subsetneq \JJ \subsetneq \CC$	1
Let $H \subseteq S$ such that $H$ is [[Definition:Countable Set|countable]]. Then $H \ne S$ as $S$ is [[Definition:Uncountable Set|uncountable]] by hypothesis. From [[Limit Points in Excluded Point Space]], the only [[Definition:Limit Point of Set|limit point]] of $H$ is $p$. So, by definition, the [[Definition:Closure (Topology)|closure]] of $H$ is $H \cup \set p$. From [[Countable Union of Countable Sets is Countable]] we have that $H \cup \set p$ is [[Definition:Countable Set|countable]]. So $H \cup \set p \ne S$. So $H$ is not [[Definition:Everywhere Dense|everywhere dense]] in $T$. Thus $T$ can have no [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Hence the result by definition of [[Definition:Separable Space|separable space]]. {{qed}}	1
=== [[P-adic Norm not Complete on Rational Numbers/Proof 1/Case 1|Case: $p \gt 3$]] === {{:P-adic Norm not Complete on Rational Numbers/Proof 1/Case 1}}{{qed|lemma}} === [[P-adic Norm not Complete on Rational Numbers/Proof 1/Case 2|Case: $p = 2$ or $3$]] === {{:P-adic Norm not Complete on Rational Numbers/Proof 1/Case 2}}{{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ERO} 3 | t = Exchange [[Definition:Row of Matrix|rows]] $k$ and $l$ | m = r_k \leftrightarrow r_l }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r_k \leftrightarrow r_l$ That is: :$e' = e$	1
{{ProofWanted}} [[Category:Generators of Modules]] rqov6fml3sxtmb9wvaaymxl33wl25iu	1
Let $I$ be an [[Definition:Indexing Set|indexing set]] with [[Definition:Countable Set|countable cardinality]]. Let $\family {\struct {S_\alpha, \tau_\alpha} }_{\alpha \mathop \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Topological Space|topological spaces]] [[Definition:Indexed Family|indexed]] by $I$. Let $\displaystyle \struct {S, \tau} = \prod_{\alpha \mathop \in I} \struct {S_\alpha, \tau_\alpha}$ be the [[Definition:Product Space of Topological Spaces|product space]] of $\family {\struct {S_\alpha, \tau_\alpha} }_{\alpha \mathop \in I}$. Let each of $\struct {S_\alpha, \tau_\alpha}$ be [[Definition:Separable Space|separable]]. Then $\struct {S, \tau}$ is also [[Definition:Separable Space|separable]].	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $a \in R$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map {B_\epsilon} a$ be the [[Definition:Open Ball of Normed Division Ring|open $\epsilon$-ball of $a$]] in $\struct{R, \norm {\,\cdot\,} }$. Then: :$a \in \map {B_\epsilon} a$	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime number]] $p$. Let $\size {\,\cdot\,}$ be the [[Definition:Absolute Value|absolute value]] on the [[Definition:Rational Numbers|rationals $\Q$]]. Then $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. That is, the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\norm {\,\cdot\,}_p$ does not equal the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by $\size {\,\cdot\,}$.	1
:$\norm {1_R} = 1$.	1
{{Proofread}} {{wtd|I think some of the partial results are up on their own pages now, and thus can be replaced by a reference (or this should become the case). Furthermore, this is *so* long, transclusions and/or foldable templates are a must}} === $H$ is a Vector Space === From the definition of [[Definition:Hilbert Space Direct Sum|Hilbert space direct sum]], we see that $H$ is a [[Definition:Empty Set|nonempty]] [[Definition:Subset|subset]] of a [[Definition:Vector Space|vector space]] (namely, the [[Definition:Vector Space Direct Sum|direct sum]] of the $H_i$ as [[Definition:Vector Space|vector spaces]]). From the [[Two-Step Vector Subspace Test]] it follows that it is to be shown that: :$(1): \qquad \forall h_1, h_2 \in H: \displaystyle \sum \left\{{ \left\Vert{ \left({h_1 + h_2}\right) \left({i}\right) }\right\Vert^2_{H_i}: i \in I }\right\} < \infty$ :$(2): \qquad \forall \lambda \in \Bbb F, h \in H: \displaystyle \sum \left\{{ \left\Vert{ \left({\lambda h}\right) \left({i}\right) }\right\Vert^2_{H_i}: i \in I }\right\} < \infty$ Considering $(1)$, have the following: {{begin-eqn}} {{eqn|l = \left\Vert{ \left({h_1 + h_2}\right) \left({i}\right) }\right\Vert^2_{H_i} |r = \left\Vert{ h_1 \left({i}\right) + h_2 \left({i}\right) }\right\Vert^2_{H_i} }} {{eqn|o = \le |r = \left({ \left\Vert{h_1 \left({i}\right)}\right\Vert_{H_i} + \left\Vert{h_2 \left({i}\right)}\right\Vert_{H_i} }\right)^2 |c = [[Definition:Norm on Vector Space|Triangle inequality]] for $\left\Vert{\cdot}\right\Vert_{H_i}$ }} {{eqn|ll= \implies |l = \sum \left\{ { \left\Vert{ \left({h_1 + h_2}\right) \left({i}\right) }\right\Vert^2_{H_i}: i \in I }\right\} |o = \le |r = \sum \left\{ { \left({ \left\Vert{h_1 \left({i}\right)}\right\Vert_{H_i} + \left\Vert{h_2 \left({i}\right)}\right\Vert_{H_i} }\right)^2: i \in I }\right\} |c = [[Generalized Sum Preserves Inequality]] }} {{eqn|o = < |r = \infty |c = $h_1, h_2 \in H$, [[Square-Summable Indexed Sets Closed Under Addition]] }} {{end-eqn}} For $(2)$, observe that: {{begin-eqn}} {{eqn|l = \left\Vert{ \left({\lambda h}\right) \left({i}\right) }\right\Vert^2_{H_i} |r = \left\vert{\lambda}\right\vert^2 \left\Vert{h \left({i}\right)}\right\Vert^2_{H_i} |c = $\left\Vert{\cdot}\right\Vert_{H_i}$ is a [[Definition:Norm on Vector Space|norm]] }} {{eqn|ll= \implies |l = \sum \left\{ { \left\Vert{ \left({\lambda h}\right) \left({i}\right) }\right\Vert^2_{H_i}: i \in I }\right\} |o = \le |r = \left\vert{\lambda}\right\vert^2 \sum \left\{ { \left\Vert{h \left({i}\right)}\right\Vert^2_{H_i}: i \in I }\right\} |c = [[Generalized Sum is Linear]] }} {{eqn|o = < |r = \infty |c = As $h \in H$ }} {{end-eqn}} Thus, by the [[Two-Step Vector Subspace Test]], $H$ is a [[Definition:Vector Space|vector space]]. {{qed|lemma}} === $\left\langle{\cdot, \cdot}\right\rangle$ is an Inner Product === It suffices to check well-definedness of $\left\langle{\cdot, \cdot}\right\rangle$, and subsequently the five properties of an [[Definition:Inner Product|inner product]]. ==== Well-definedness ==== It is necessary to verify that for $g, h \in H$, in fact $\left\langle{g, h}\right\rangle \in \Bbb F$. That is, it is required to show that $\left\langle{g, h}\right\rangle = \displaystyle \sum \left\{{ \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\}$ [[Definition:Generalized Sum|converges]] in $\Bbb F$. [[Absolutely Convergent Generalized Sum Converges]] applies to the [[Definition:Banach Space|Banach space]] $\Bbb F$ and the [[Definition:Indexed Set|$I$-indexed]] [[Definition:Subset|subset]] $\left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}$ of $\Bbb F$. Hence it will suffice to show that $\displaystyle \sum \left\{{ \left\vert{ \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i} }\right\vert: i \in I}\right\}$ [[Definition:Generalized Sum|converges]] in $\R$. For brevity, denote already $\left\Vert{h}\right\Vert^2$ for the expression $\displaystyle \sum \left\{{\left\Vert{h \left({i}\right)}\right\Vert_{H_i}^2: i \in I}\right\}$. Define $g' \in H$ by $g' \left({i}\right) = \begin{cases} g \left({i}\right) & \text{if } \left\Vert{g \left({i}\right)}\right\Vert_{H_i} \ge \left\Vert{h \left({i}\right)}\right\Vert_{H_i} \\ \mathbf{0}_{H_i} & \text{otherwise} \end{cases}$. Note that $\left\Vert{g'}\right\Vert^2 \le \left\Vert{g}\right\Vert^2$ by [[Generalized Sum Preserves Inequality]]. Similarly, let $h' \in H$ be defined by $h' \left({i}\right) = \begin{cases} h \left({i}\right) & \text{if } \left\Vert{h \left({i}\right)}\right\Vert_{H_i} \ge \left\Vert{g \left({i}\right)}\right\Vert_{H_i} \\ \mathbf{0}_{H_i} & \text{otherwise} \end{cases}$. By [[Generalized Sum Preserves Inequality]] again, have $\left\Vert{h'}\right\Vert^2 \le \left\Vert{h}\right\Vert^2$. More significantly, by construction of $g', h'$: :$(3): \qquad \left\Vert{g \left({i}\right)}\right\Vert_{H_i}, \left\Vert{h \left({i}\right)}\right\Vert_{H_i} \le \left\Vert{\left({g' + h'}\right) \left({i}\right)}\right\Vert_{H_i}$ As $H$ is a [[Definition:Vector Space|vector space]], $g' + h' \in H$, and we can establish: {{begin-eqn}} {{eqn|l = \left\vert{ \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i} }\right\vert |o = \le |r = \left\Vert{g \left({i}\right)}\right\Vert_{H_i} \left\Vert{h \left({i}\right)}\right\Vert_{H_i} |c = [[Cauchy-Bunyakovsky-Schwarz Inequality/Inner Product Spaces|Cauchy-Bunyakovsky-Schwarz Inequality]] }} {{eqn|o = \le |r = \left\Vert{\left({g' + h'}\right) \left({i}\right)}\right\Vert_{H_i}^2 |c = Equation $(1)$ }} {{eqn|ll= \implies |l = \sum \left\{ { \left\vert{ \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i} }\right\vert: i \in I}\right\} |o = \le |r = \sum \left\{ { \left\Vert{\left({g' + h'}\right) \left({i}\right)}\right\Vert_{H_i}^2: i \in I}\right\} |c = [[Generalized Sum Preserves Inequality]] }} {{end-eqn}} Hence, for all $g, h \in H$, $\left\langle{g, h}\right\rangle \in \Bbb F$ by the comment on [[Generalized Sum Preserves Inequality]]. {{qed|lemma}} ==== Property 1: $\left\langle{g, h}\right\rangle = \overline{\left\langle{h, g}\right\rangle}$ ==== {{begin-eqn}} {{eqn|l = \left\langle{g, h}\right\rangle |r = \sum \left\{ { \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn|r = \sum \left\{ { \overline{\left\langle{ h \left({i}\right), g \left({i}\right) }\right\rangle_{H_i} }: i \in I}\right\} |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]] }} {{eqn|r = \overline{ \sum \left\{ { \left\langle{ h \left({i}\right), g \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} } |c = [[Convergence of Generalized Sum of Complex Numbers/Corollary|Convergence of Generalized Sum of Complex Numbers: Corollary]] }} {{eqn|r = \overline{ \left\langle{h, g}\right\rangle } |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{end-eqn}} {{qed|lemma}} ==== Property 2: $\left\langle{\lambda g, h}\right\rangle = \lambda \left\langle{g, h}\right\rangle$ ==== {{begin-eqn}} {{eqn|l = \left\langle{\lambda g, h}\right\rangle |r = \sum \left\{ { \left\langle{\left({\lambda g}\right) \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn|r = \sum \left\{ { \lambda \left\langle{ g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]] }} {{eqn|r = \lambda \sum \left\{ { \left\langle{g \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Generalized Sum is Linear]] }} {{eqn|r = \lambda \left\langle{h, g}\right\rangle |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{end-eqn}} {{qed|lemma}} ==== Property 3: $\left\langle{g_1 + g_2, h}\right\rangle = \left\langle{g_1, h}\right\rangle + \left\langle{g_2, h}\right\rangle$ ==== {{begin-eqn}} {{eqn|l = \left\langle{g_1 + g_2, h}\right\rangle |r = \sum \left\{ { \left\langle{\left({g_1 + g_2}\right) \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn|r = \sum \left\{ { \left\langle{g_1 \left({i}\right), h \left({i}\right)}\right\rangle_{H_i} + \left\langle{g_2 \left({i}\right), h \left({i}\right)}\right\rangle_{H_i}: i \in I}\right\} |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]] }} {{eqn|r = \sum \left\{ { \left\langle{ g_1 \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} + \sum \left\{ { \left\langle{ g_2 \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Generalized Sum is Linear]] }} {{eqn|r = \left\langle{g_1, h}\right\rangle + \left\langle{g_2, h}\right\rangle |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{end-eqn}} {{qed|lemma}} ==== Property 4: $\left\langle{h, h}\right\rangle \ge 0$ ==== {{begin-eqn}} {{eqn|l = \left\langle{h, h}\right\rangle |r = \sum \left\{ { \left\langle{ h \left({i}\right), h \left({i}\right) }\right\rangle_{H_i}: i \in I}\right\} |c = [[Definition:Hilbert Space Direct Sum|Definition]] of $\left\langle{\cdot, \cdot}\right\rangle$ }} {{eqn|o = \ge |r = \sum \left\{ { 0: i \in I}\right\} |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]], [[Generalized Sum Preserves Inequality]] }} {{eqn|r = 0 }} {{end-eqn}} {{qed|lemma}} ==== Property 5: $\left\langle{h, h}\right\rangle = 0$ [[Definition:Iff|iff]] $h = \mathbf{0}_H$ ==== {{begin-eqn}} {{eqn|l = \left\langle{h, h}\right\rangle |r = 0 }} {{eqn|ll= \iff \forall i \in I: |l = \left\langle{ h \left({i}\right), h \left({i}\right) }\right\rangle_{H_i} |r = 0 |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]], [[Generalized Sum is Monotone]] }} {{eqn|ll= \iff \forall i \in I: |l = h \left({i}\right) |r = \mathbf{0}_{H_i} |c = $\left\langle{\cdot, \cdot}\right\rangle_{H_i}$ is an [[Definition:Inner Product|inner product]] }} {{eqn|ll= \iff |l = h |r = \mathbf{0}_H }} {{end-eqn}} {{qed|lemma}} ==== Conclusion ==== $\left\langle{\cdot, \cdot}\right\rangle$ is checked to be a [[Definition:Mapping|mapping]] from $H \times H$ to $\Bbb F$, satisfying the five conditions for an [[Definition:Inner Product|inner product]]. That is, $\left\langle{\cdot, \cdot}\right\rangle$ is an [[Definition:Inner Product|inner product]] on $H$. {{qed|lemma}} === $H$ is complete === A [[Definition:Hilbert Space|Hilbert space]] is a [[Definition:Complete Metric Space|complete]] [[Definition:Inner Product Space|inner product space]]. Thus, it remains to verify that $H$ is [[Definition:Complete Metric Space|complete]]. Suppose $\left({h_n}\right)_{n\in\N}$ is a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] in $H$. Let $N \in \N$ such that $n, m \ge N \implies \left\Vert{h_n - h_m}\right\Vert < \epsilon$. That is, $\displaystyle \sum \left\{{ \left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2: i \in I}\right\} < \epsilon^2$. From [[Generalized Sum is Monotone]] obtain that, for all $i \in I$: :$\left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2 < \epsilon^2$ It follows that $\left({h_n \left({i}\right) }\right)_{n\in\N}$ is a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] in $H_i$. $H_i$ is a [[Definition:Hilbert Space|Hilbert space]], hence [[Definition:Complete Metric Space|complete]]. Hence there is some $h_i \in H_i$ such that $\displaystyle \lim_{n\to\infty} h_n \left({i}\right) = h_i$. Now let $h$ be defined by $h \left({i}\right) = h_i$; it is the only candidate for $\displaystyle \lim_{n\to\infty} h_n = h$. It remains to be shown that indeed $\displaystyle \lim_{n\to\infty} h_n = h$, and then that $h \in H$. So, for any $\epsilon > 0$, an $N \in \N$ is to be found such that for all $n \ge N$: :$(4): \qquad \displaystyle \sum \left\{{ \left\Vert{ \left({h_n - h}\right) \left({i}\right) }\right\Vert_{H_i}^2: i \in I}\right\} < \epsilon^2$ To this end, let $N \in \N$ be such that: :$(5): \qquad n, m \ge N \implies \left\Vert{h_n - h_m}\right\Vert^2 < \frac {\epsilon^2} 2$ Such an $N$ exists as $\left({h_n}\right)_{n\in\N}$ is a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]]. Now observe that, for any [[Definition:Finite|finite]] $G \subseteq I$ and $n \ge N$: {{begin-eqn}} {{eqn | ll= \forall i \in I: | l = \left\Vert{ \left({h_n - h}\right) \left({i}\right) }\right\Vert_{H_i}^2 | r = \lim_{m \to \infty} \left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2 | c = Definition of $h \left({i}\right)$ }} {{eqn | ll= \implies | l = \sum_{i \in G} \left\Vert{ \left({h_n - h}\right) \left({i}\right) }\right\Vert_{H_i}^2 | r = \sum_{i \in G} \ \lim_{m \to \infty} \left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2 }} {{eqn | r = \lim_{m \to \infty} \ \sum_{i \in G} \left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2 | c = [[Sum Rule for Sequences]] }} {{eqn | o = \le | r = \lim_{m \to \infty} \ \sum \left\{ {\left\Vert{ \left({h_n - h_m}\right) \left({i}\right) }\right\Vert_{H_i}^2 : i \in I}\right\} | c = [[Generalized Sum is Monotone]] }} {{eqn | r = \lim_{m \to \infty} \left\Vert{h_n - h_m}\right\Vert^2 | c = Definition of $\left\Vert{\cdot}\right\Vert$ }} {{eqn | o = \le | r = \frac {\epsilon^2} 2 | c = [[Upper and Lower Bounds of Sequences]] }} {{end-eqn}} The last inequality follows from $(5)$, as $m \ge N$ eventually when $m \to \infty$. From [[Bounded Generalized Sum Converges]], it now follows that: :$\displaystyle \sum \left\{ {\left\Vert{ \left({h_n - h}\right) \left({i}\right) }\right\Vert_{H_i}^2 : i \in I}\right\} \le \frac {\epsilon^2} 2 < \epsilon^2$ This precisely establishes the inequality desired in $(4)$ for $n \ge N$. It follows that $\displaystyle \lim_{n \to \infty} h_n = h$. To show that $h \in H$, it is to be shown that $\left\Vert{h}\right\Vert^2 < \infty$. This is done as follows: {{begin-eqn}} {{eqn | l = \left\Vert{h}\right\Vert^2 | r = \sum \left\{ {\left\Vert{ h \left({i}\right) }\right\Vert_{H_i}^2 : i \in I}\right\} | c = Definition of $\left\Vert{\cdot}\right\Vert$ }} {{eqn | o = \le | r = \sum \left\{ {\left({ \left\Vert{ \left({h - h_n}\right) \left({i}\right) }\right\Vert_{H_i} + \left\Vert{ h_n \left({i}\right) }\right\Vert_{H_i} }\right)^2 : i \in I}\right\} | c = [[Definition:Norm on Vector Space|Triangle Inequality]] for all $\left\Vert{\cdot}\right\Vert_{H_i}$, [[Generalized Sum Preserves Inequality]] }} {{end-eqn}} The latter sum converges by [[Square-Summable Indexed Sets Closed Under Addition]], yielding convergence of $\left\Vert{h}\right\Vert^2$. Therefore, $h \in H$. That is, every [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] in $H$ [[Definition:Convergent Sequence (Metric Space)|converges]] to a limit in $H$, hence $H$ is [[Definition:Complete Metric Space|complete]]. By definition, $H$ is a [[Definition:Hilbert Space|Hilbert space]]. {{qed}}	1
A [[Definition:Banach Space|Banach space]] is a [[Definition:Normed Vector Space|normed vector space]], where a [[Definition:Cauchy Sequence|Cauchy sequence]] [[Definition:Convergent Sequence in Normed Vector Space|converges]] {{WRT}} the supplied [[Definition:Norm on Vector Space|norm]]. To prove the theorem, we need to show that a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] in $\struct {\ell^\infty, \norm {\,\cdot\,}_\infty}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]]. We take a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]] $\sequence {x_n}_{n \mathop \in \N}$ in $\struct {\ell^\infty, \norm {\,\cdot\,}_\infty}$. Then we consider the $k$th component and show, that a [[Definition:Real Cauchy Sequence|real Cauchy sequence]] $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ [[Definition:Convergent Real Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$ with the [[Definition:Limit of Real Sequence|limit]] $x^{\paren k}$ and denote the entire set as $\mathbf x$. Finally, we show that $\sequence {\mathbf x_n}_{n \in \N}$, composed of components $x_n^{\paren k},$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] in $\struct {\ell^\infty, \norm {\,\cdot\,}_\infty}$ with the [[Definition:Limit of Sequence in Normed Vector Space|limit]] $\mathbf x$. Let $\sequence {\mathbf x_n}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\ell^\infty, \norm{\, \cdot \,}_\infty}$. Denote the $k$th component of $\mathbf x_n$ by $x_n^{\paren k}$. === $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ converges in $\struct {\R, \size {\, \cdot \,}}$=== Let $\epsilon >0$. Then: :$\displaystyle \exists N \in \N : \forall m,n \in \N : m,n > N : \norm {\mathbf x_n - \mathbf x_m}_\infty < \epsilon$ For same $N, m, n$ consider $\size {x_n^{\paren k} - x_m^{\paren k} } $: {{begin-eqn}} {{eqn | l = \size {x_n^{\paren k} - x_m^{\paren k} } | o = \le | r = \sup_{k \mathop \in \N} \size {x_n^{\paren k} - x_m^{\paren k} } }} {{eqn | r = \norm {\mathbf x_n - \mathbf x_m}_\infty | c = {{defof|Supremum Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Hence, $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\R, \size {\, \cdot \,}}$. From [[Real Number Line is Complete Metric Space]], $\R$ is a [[Definition:Complete Metric Space|complete metric space]]. Consequently, $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ [[Definition:Convergent Sequence|converges]] in $\struct {\R, \size {\, \cdot \,}}$. {{qed|lemma}} Denote the [[Definition:Limit of Real Sequence|limit]] $\displaystyle \lim_{n \mathop \to \infty} \sequence {x_n^{\paren k}}_{n \mathop \in \N} = x^{\paren k}$. Denote $\sequence {x^{\paren k}}_{k \mathop \in \N} = \mathbf x$. === $\mathbf x$ belongs to $\ell^\infty$ === From previous lemma, $\sequence {x_n^{\paren k}}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence|Cauchy sequence]] in $\struct {\R, \size {\, \cdot \,}}$. Fix any $n > N$ and $k \in \N$. Then: :$\displaystyle \forall m > N : \size {x_n^{\paren k} - x_m^{\paren k}} < \epsilon$ Take the limit $m \to \infty$: :$\size {x_n^{\paren k} - x^{\paren k}} < \epsilon$ Since $k$ was arbitrary: {{begin-eqn}} {{eqn | ll = \forall k \in \N : | l = \size {x_n^{\paren k} - x^{\paren k} } | o = \le | r = \sup_{k \mathop \in \N} \size {x_n^{\paren k} - x^{\paren k} } | c = {{defof|Supremum of Real Sequence}} }} {{eqn | r = \norm {\mathbf x_n - \mathbf x}_\infty | c = {{defof|Supremum Norm}} }} {{eqn | o = < | r = \epsilon }} {{eqn | o = < | r = \infty }} {{end-eqn}} By [[Definition:Space of Bounded Sequences|definition]], $\mathbf x_n - \mathbf x \in \ell^{\infty}$. By [[Definition:Assumption|assumption]], $\mathbf x_n \in \ell^\infty$. Hence, $\mathbf x \in \ell^\infty$. {{qed|lemma}} === $\sequence {\mathbf x_n}_{n \mathop \in \N}$ converges in $\struct {\ell^\infty, \norm {\, \cdot \,}_\infty}$ to $\mathbf x$=== From previous lemma we have that for some $\epsilon > 0$, $N \in \N$ and fixed $n > N$: :$\displaystyle \norm {\mathbf x_n - \mathbf x}_\infty < \epsilon$ Repeat the same argument for all $\epsilon \in \R_{>0}$ and note that $n$ was arbitrary: :$\displaystyle \forall \epsilon \in \R_{>0} : \exists N \in \N : \forall n \in \N : n > N \implies \norm {\mathbf x_n - \mathbf x}_\infty < \epsilon$ Therefore, $\sequence {\mathbf x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] in $\struct {\ell^\infty, \norm{\, \cdot \,}_\infty}$. {{qed}}	1
Let $A = \struct {A_F, \oplus}$ be a [[Definition:Normed Division Algebra|normed divison algebra]] over a [[Definition:Field (Abstract Algebra)|field]] $F$. Let the [[Definition:Unit of Algebra|unit]] of $A$ be $1_A$, and the [[Definition:Zero Vector|zero]] of $A$ be $0_A$. Then $A$ is a [[Definition:Unitary Division Algebra|unitary division algebra]]. Also: :$\norm {1_A} = 1$ where $\norm {1_A}$ denotes the [[Definition:Norm on Vector Space|norm]] of $1_A$.	1
{{begin-eqn}} {{eqn | l = \nabla \times \paren {\mathbf f + \mathbf g} | r = \begin {vmatrix} \mathbf i & \mathbf j & \mathbf k \\ \dfrac \partial {\partial x} & \dfrac \partial {\partial y} & \dfrac \partial {\partial z} \\ f_x + g_x & f_y + g_y & f_z + g_x \end {vmatrix} | c = {{Defof|Curl Operator}} }} {{eqn | r = \begin {vmatrix} \mathbf i & \mathbf j & \mathbf k \\ \dfrac \partial {\partial x} & \dfrac \partial {\partial y} & \dfrac \partial {\partial z} \\ f_x & f_y & f_z \end {vmatrix} + \begin {vmatrix} \mathbf i & \mathbf j & \mathbf k \\ \dfrac \partial {\partial x} & \dfrac \partial {\partial y} & \dfrac \partial {\partial z} \\ g_x & g_y & g_x \end {vmatrix} | c = [[Determinant as Sum of Determinants]] }} {{eqn | r = \nabla \times \mathbf f + \nabla \times \mathbf g | c = }} {{end-eqn}} {{qed}}	1
By definition, for a [[Definition:Trivial Module|trivial module]] to be [[Definition:Unitary Module|unitary]], $R$ needs to be a [[Definition:Ring with Unity|ring with unity]]. For {{Module-axiom|4}} to apply, we require that: :$\forall x \in G: 1_R \circ x = x$ But for the trivial module: :$\forall x \in G: 1_R \circ x = e_G$ So {{Module-axiom|4}} can apply only when: :$\forall x \in G: x = e_G$ Thus for the trivial module to be [[Definition:Unitary Module|unitary]], it is necessary that $G$ be the [[Definition:Trivial Group|trivial group]], and thus to contain one element. {{qed}}	1
{{AimForCont}} the supposition false. That is, suppose there is at least one [[Definition:Positive Integer|positive integer]] that can be expressed in more than one way as a product of [[Definition:Prime Number|primes]]. Let the smallest of these be $m$. Thus: :$m = p_1 p_2 \cdots p_r = q_1 q_2 \cdots q_s$ where all of $p_1, \ldots p_r, q_1, \ldots q_s$ are [[Definition:Prime Number|prime]]. By definition, $m$ is not itself [[Definition:Prime Number|prime]]. Therefore: :$r, s \ge 2$ Let us arrange that the [[Definition:Prime Number|primes]] which compose $m$ are in order of size: :$p_1 \le p_2 \le \dots \le p_r$ and: :$q_1 \le q_2 \le \dots \le q_s$ Let us arrange that $p_1 \le q_1$. Suppose $p_1 = q_1$. Then: :$\dfrac m {p_1} = p_2 p_3 \cdots p_r = q_2 q_3 \cdots q_s = \dfrac m {q_1}$ But then we have the [[Definition:Positive Integer|positive integer]] $\dfrac m {p_1}$ being expressible in two different ways. This contradicts the fact that $m$ is the smallest [[Definition:Positive Integer|positive integer]] that can be so expressed. Therefore: :$p_1 \ne q_1 \implies p_1 < q_1 \implies p_1 < q_2, q_3, \ldots, q_s$ as we arranged them in order. From [[Prime not Divisor implies Coprime]]: :$1 < p_1 < q_j: 1 < j < s \implies p_1 \nmid q_j$ But: :$p_1 \divides m \implies p_1 \divides q_1 q_2 \ldots q_s$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Thus from [[Euclid's Lemma for Prime Divisors]]: :$\exists j: 1 \le j \le s: p_1 \divides q_j$ But $q_j$ was supposed to be a [[Definition:Prime Number|prime]]. This is a [[Definition:Contradiction|contradiction]]. Hence, by [[Proof by Contradiction]], the supposition was false. {{Qed}}	1
From [[Change of Basis is Invertible]], if $\left \langle {b_n} \right \rangle$ is an [[Definition:Ordered Basis|ordered basis]] of $G$ then $\mathbf P$ is [[Definition:Invertible Matrix|invertible]]. Now let $\mathbf P$ be [[Definition:Invertible Matrix|invertible]]. Then by [[Linear Transformations Isomorphic to Matrix Space/Corollary|the corollary to Linear Transformations Isomorphic to Matrix Space]], there is an [[Definition:Module Automorphism|automorphism]] $u$ of $G$ which satisfies $\mathbf P = \left[{u; \left \langle {a_n} \right \rangle}\right]$. Therefore, as $\forall j \in \left[{1 \,.\,.\, n}\right]: b_j = u \left({a_j}\right)$, it follows that $\left \langle {b_n} \right \rangle$ is also an ordered basis of $G$. {{Qed}}	1
Consider $p \ne m$. Then $\mathbf {AB}$ is defined, but $\mathbf {BA}$ is not. So: :$p \ne m \implies \mathbf {AB} \ne \mathbf {BA}$ Now consider $p = m$, and $m \ne n$. Then $\mathbf {A B}$ is an $m \times m$ matrix, while $\mathbf {BA}$ is an $n \times n$ matrix. $p \ne m \implies \mathbf {AB} \ne \mathbf {BA}$ regardless of whether $m = n$ is true or not. Hence: :$p \ne m \lor m \ne n \implies \mathbf {AB} \ne \mathbf {BA}$ Now consider $p = m$, $m = n$, and $n \ne 1$. It remains to be proved that: :$\mathbf {AB} \ne \mathbf {BA}$ This is demonstrated in [[Matrix Multiplication is not Commutative]]. Hence we have that $p \ne m \lor m \ne n \implies \mathbf {AB} \ne \mathbf {BA}$ regardless of whether $n = 1$ is true or not, so it is easily seen that: :$p \ne m \lor m \ne n \lor n \ne 1 \implies \exists \mathbf A \in \MM_{m \times n}, \mathbf B \in \MM_{n \times p}: \mathbf {AB} \ne \mathbf {BA} \dashv \vdash \\ \left(\forall \mathbf A \in \MM_{m \times n}, \mathbf B \in \MM_{n \times p}: \mathbf {AB} = \mathbf {BA}\right) \implies \lnot \left({p \ne m \lor m \ne n \lor n \ne 1}\right)$ by [[Rule of Transposition]]. The [[Definition:Existential Quantifier|existential quantifier]] is needed here because there exist [[Definition:Square Matrix|square matrices]] that commute under [[Definition:Matrix Product (Conventional)|matrix multiplication]], for example the [[Definition:Unit Matrix|unit matrix]]. :$\lnot \left({p \ne m \lor m \ne n \lor n \ne 1}\right) \dashv \vdash p = m \land m = n \land n = 1$ by a generalization of [[De Morgan's Laws (Logic)|De Morgan's Law]]. Finally, by the properties of an [[Definition:Equivalence Relation|equivalence relation]]: :$(1): \quad \paren {\forall \mathbf A \in \MM_{m \times n}, \mathbf B \in \MM_{n \times p}: \mathbf {AB} = \mathbf {BA} } \implies p = m = n = 1$ Now to prove the converse: Let $\mathbf A \in \MM_{m \times n}$, $\mathbf B \in \MM_{n \times p}$, and suppose that $p = m = n = 1$, then because $R$ is a [[Definition:Commutative Ring|commutative ring]]: :$\mathbf {AB} = A_{11} B_{11} = B_{11} A_{11} = \mathbf {BA}$ and so: :$(2): \quad p = m = n = 1 \implies \paren {\forall \mathbf A \in \MM_{m \times n}, \mathbf B \in \MM_{n \times p}: \mathbf {AB} = \mathbf {BA} }$ Combining $(1)$ and $(2)$ yields the result. {{qed}} [[Category:Conventional Matrix Multiplication]] [[Category:Commutativity]] [[Category:Proofs by Induction]] 55e8727xfy0op1ydlh1pgqwl9tzoc1f	1
We begin with the [[Definition:Natural Logarithm|natural logarithm]] of $\norm {\mathbf x}_p$: {{begin-eqn}} {{eqn | l = \dfrac \d {\d p} \map \ln {\norm {\bf x}_p} | r = \frac {\dfrac \d {\d p} \norm {\bf x}_p} {\norm {\bf x}_p} }} {{eqn | r = \map {\dfrac \d {\d p} } {\frac 1 p \map \ln {\sum_{n \mathop = 0}^\infty \size {x_n}^p} } }} {{eqn | r = -\frac 1 {p^2} \map \ln {\sum_{n \mathop = 0}^\infty \size {x_n}^p} + \frac 1 p \frac {\sum_{n \mathop = 0}^\infty \size {x_n}^p \map \ln {\size {x_n} } } {\sum_{n \mathop = 0}^\infty \size {x_n}^p} | c = [[Derivative of Power of Constant]] }} {{eqn | r = -\frac 1 {p^2} \map \ln {\norm {\bf x}_p^p} + \frac 1 p \frac {\sum_{n \mathop = 0}^\infty \size {x_n}^p \map \ln {\size {x_n} } } {\norm {\bf x}_p^p} }} {{eqn | r = \frac 1 p \paren {\frac {\sum_{n \mathop = 0}^\infty \size {x_n}^p \map {\ln} {\size {x_n} } } {\norm {\bf x}_p^p} - \map \ln {\norm {\bf x}_p} } }} {{end-eqn}} [[Definition:Real Multiplication|Multiplication]] by $\norm {\mathbf x}_p$ completes the [[Definition:Proof|proof]]. {{qed}} {{NoSources}} [[Category:P-Norms]] 7awcc9mqw5r3j7przrsv2wt2atlinh7	1
The proof goes by [[Principle of Mathematical Induction|induction]] on $b$. === Basis for the Induction === Let $b < a$. Then all [[Definition:Indexed Summation|indexed summations]] are [[Definition:Zero of Standard Number System|zero]]. Because $|0| \leq |0|$ by definition of the [[Definition:Standard Absolute Value|standard absolute value]], the result follows. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Step === Let $b \geq a$. We have: {{begin-eqn}} {{eqn | l = \left\vert \sum_{i \mathop = a}^{b} f(i) \right\vert | r = \left\vert \sum_{i \mathop = a}^{b-1} f(i) + f(b) \right\vert | c = {{Defof|Indexed Summation}} }} {{eqn | r = \left\vert \sum_{i \mathop = a}^{b-1} f(i) \right\vert + \left\vert f(b) \right\vert | c = [[Triangle Inequality]] | o = \leq }} {{eqn | r = \sum_{i \mathop = a}^{b-1} \left\vert f(i) \right\vert + \left\vert f(b) \right\vert | c = [[Principle of Mathematical Induction#Induction Hypothesis|Induction Hypothesis]], {{defof|Relation Compatible with Operation}} | o = \leq }} {{eqn | r = \sum_{i \mathop = a}^{b} \left\vert f(i) \right\vert | c = {{Defof|Indexed Summation}} }} {{end-eqn}} By the [[Principle of Mathematical Induction]], the proof is complete. {{qed}}	1
Let $M$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $S = \left\langle{m_i}\right\rangle_{i \mathop \in I}$ be a [[Definition:Indexed Family|family]] of [[Definition:Element|elements]] of $M$. Let $\Psi: R^{\left({I}\right)} \to M$ be the morphism given by [[Universal Property of Free Module Indexed by Set]]. Then $S$ is a [[Definition:Spanning Set|spanning set]] of $M$ {{iff}} $\Psi$ is [[Definition:Surjection|surjective]].	1
From [[Sum Rule for Sequences in Normed Division Ring]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {x_n + y_n} = l + m$ From [[Multiple Rule for Sequences in Normed Division Ring]]: :$\displaystyle \lim_{n \mathop \to \infty} \paren {-y_n} = -m$ Hence: :$\displaystyle \lim_{n \mathop \to \infty} \paren {x_n + \paren {-y_n} } = l + \paren {-m}$ The result follows. {{qed}}	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $M$ be an [[Definition:Module|$R$-module]]. Let $S\subset M$ be a [[Definition:Subset|subset]]. {{TFAE|def = Generated Submodule}}	1
Let $\mathbb K \in \left\{ {\C, \R}\right\}$. Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $\mathfrak{sp}_{2 n} \left({\mathbb K}\right)$ be the [[Definition:Lie Algebra|Lie algebra]] of the [[Definition:Symplectic Group|symplectic group]] $\operatorname{Sp} \left({2 n, \mathbb K}\right)$. Then its [[Definition:Killing Form|Killing form]] is $B: \left({X, Y}\right) \mapsto \left({2 n + 2}\right) \operatorname{tr} \left({X Y}\right)$.	1
Let $\struct {G, +_G}$ and $\struct {H, +_H}$ be [[Definition:Group|groups]]. Let $\struct {R, +_R, \times_R}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {G, +_G, \circ}_R$ and $\struct {H, +_H, \circ}_R$ be [[Definition:Module|$R$-modules]]. Let $\map {\mathcal L_R} {G, H}$ be [[Definition:Set of All Linear Transformations|the set of all linear transformations]] from $G$ to $H$. Let $\oplus_H$ be the operation on $H^G$ as defined in [[Addition of Linear Transformations]]. Then $\struct {\map {\LL_R} {G, H}, \oplus_H}$ is an [[Definition:Abelian Group|abelian group]].	1
Let $D$ be the [[Definition:Determinant of Matrix|determinant]]: :$D = \begin{vmatrix} 1 & b_{12} & \cdots & b_{1n} \\ 0 & b_{22} & \cdots & b_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & b_{n2} & \cdots & b_{nn} \end{vmatrix}$ Then: :$D = \begin{vmatrix} b_{22} & \cdots & b_{2n} \\ \vdots & \ddots & \vdots \\ b_{n2} & \cdots & b_{nn} \end{vmatrix}$	1
The [[Equation of Plane|plane]] $P = \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma$ is [[Definition:Homogeneous (Analytic Geometry)|homogeneous]] iff $\gamma = 0$.	1
Let $A$ be a [[Definition:Star Convex Set|star convex]] [[Definition:Subset|subset]] of a [[Definition:Vector Space|vector space]] $V$ over $\R$ or $\C$. Then $A$ is [[Definition:Path-Connected Set (Topology)|path-connected]].	1
Let $G$ be a [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Vector Space|$K$-vector space]]. Let $S$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $G$. Let $M$ be the [[Definition:Vector Subspace|subspace]] of $G$ [[Definition:Generator of Module|generated]] by $S$. If $M \ne G$, then $\forall b \in G: b \notin M$, the set $S \cup \set b$ is [[Definition:Linearly Independent Set|linearly independent]].	1
{{AimForCont}}: :$\exists x, y \in {R^*} : x \circ y = 0_R$ By [[Definition:Multiplicative Norm on Ring|positive definiteness]]: :$x, y \ne 0_R \iff \norm x, \norm y \ne 0$ Thus: :$\norm x \norm y \ne 0$ But we also have: {{begin-eqn}} {{eqn | l = \norm x \norm y | r = \norm {x \circ y} | c = [[Definition:Multiplicative Norm on Ring|Multiplicativity]] }} {{eqn | r = \norm {0_R} | c = by assumption }} {{eqn | r = 0 | c = [[Definition:Multiplicative Norm on Ring|Positive Definiteness]] }} {{end-eqn}} a contradiction. {{qed}} [[Category:Norm Theory]] [[Category:Ring Theory]] azlqrazgbpgo5vzoy5kkmcuepobfreg	1
Let $G$ be an [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e$. Let $R$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\left({G, +_G, \circ}\right)_R$ be a [[Definition:Unitary Module|unitary $R$-module]]. === [[Definition:Linearly Dependent/Sequence|Sequence]] === {{:Definition:Linearly Dependent/Sequence}} === [[Definition:Linearly Dependent/Set|Set]] === {{:Definition:Linearly Dependent/Set}}	1
Let $\alpha \in \R_{\gt 0}$. Let $\norm{\,\cdot\,}:\Q \to \R$ be the [[Definition:Mapping|mapping]] defined by: :$\forall x \in \Q: \norm{x} = \size {x}^\alpha$ where $\size {x}$ is the [[Definition:Absolute Value|absolute value]] of $x$ in $\Q$. Then: :$\norm{\,\cdot\,}$ is a [[Definition:Norm/Division Ring|norm]] on $\Q \implies \,\,\alpha \le 1$	1
Let $\mathbf 0_H$ be the [[Definition:Zero Vector|zero]] of $H$. Since for every $k \in K$, we have: :$\map d {h, k} = \norm {h - k} = \map d {\mathbf 0_H, k - h}$ it follows that: :$\map d {h, K} = \map d {\mathbf 0_H, K - h}$ {{WLOG}}, we may therefore assume that $h = \mathbf 0_H$. The problem has therefore reduced to finding $k_0 \in K$ such that: :$\norm {k_0} = \map d {\mathbf 0_H, K} = \inf \set {\norm k : k \in K}$ Let $d = \map d {\mathbf 0_H, K}$. By definition of [[Definition:Infimum of Subset of Real Numbers|infimum]], there exists a [[Definition:Sequence|sequence]] $\sequence {k_n}_{n \mathop \in \N}$ such that: :$\displaystyle \lim_{n \mathop \to \infty} \norm {k_n} = d$ By the [[Parallelogram Law (Hilbert Space)|Parallelogram Law]], we have that for all $m, n \in \N$: :$(1): \quad \norm {\dfrac {k_n - k_m} 2 } = \dfrac 1 2 \paren {\norm {k_n}^2 + \norm {k_m}^2} - \norm {\dfrac {k_n + k_m} 2 }^2$ Since $K$ is [[Definition:Convex Set (Vector Space)|convex]], $\dfrac {k_n + k_m} 2 \in K$. Hence: :$\norm {\dfrac {k_n + k_m} 2 }^2 \ge d^2$ Now given $\epsilon > 0$, choose $N$ such that for all $n \ge N$: :$\norm {k_n}^2 < d^2 + \epsilon$ From $(1)$, it follows that: :$\norm {\dfrac {k_n - k_m} 2} < d^2 + \epsilon - d^2 = \epsilon$ and hence that $\sequence {k_n}_{n \mathop \in \N}$ is a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]]. Since $H$ is a [[Definition:Hilbert Space|Hilbert space]] and $K$ is [[Definition:Closed Set (Metric Space)|closed]], it follows that there is a $k_0 \in K$ such that: :$\displaystyle \lim_{n \mathop \to \infty} k_n = k_0$ From [[Norm is Continuous]], we infer that $\norm {k_0} = d$. This demonstrates existence of $k_0$. For uniqueness, suppose that $h_0 \in K$ has $\norm {h_0} = d$. Since $K$ is [[Definition:Convex Set (Vector Space)|convex]], it follows that $\dfrac {h_0 + k_0} 2 \in K$. This implies that $\norm {\dfrac {h_0 + k_0} 2} \ge d$. Now from the [[Triangle Inequality]]: :$\norm {\dfrac {h_0 + k_0} 2} \le \dfrac {\norm {h_0} + \norm {k_0} } 2 = d$ meaning that $\norm {\dfrac {h_0 + k_0} 2} = d$. Thus, the [[Parallelogram Law (Hilbert Space)|Parallelogram Law]] implies that: :$d^2 = \norm {\dfrac {h_0 + k_0} 2}^2 = d^2 - \norm {\dfrac {h_0 - k_0} 2}^2$ from which we conclude that $h_0 = k_0$. {{qed}}	1
First let $n = 0$. The assertion follows directly from [[Scalar Product with Identity]]. Next, let $n > 0$. The assertion follows directly from [[Scalar Product with Sum]] and [[Product with Sum of Scalar]], by letting $m = n$ and making all the $\lambda$'s and $x$'s the same. Finally, let $n < 0$. The assertion follows from [[Scalar Product with Product]] for positive $n$, [[Scalar Product with Inverse]], and from [[Index Laws for Monoids/Negative Index|Negative Index Law for Monoids]]. {{qed}}	1
By definition of [[Definition:Evaluation Linear Transformation|evaluation linear transformation]]: :$\forall x \in G: y' \in H^*: \sequence {x, \map {u^t} {y'} } = \sequence {\map u x, y'}$ Since we have: {{begin-eqn}} {{eqn | l = \sequence {x, \map {u^t} {y' + z'} } | r = \sequence {\map u x, y' + z'} | c = }} {{eqn | r = \sequence {\map u x, y'} + \sequence {\map u x, z'} | c = }} {{eqn | r = \sequence {x, \map {u^t} {y'} } + \sequence {x, \map {u^t} {z'} } | c = }} {{eqn | r = \sequence {x, \map {u^t} {y'} + \map {u^t} {z'} } | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \sequence {x, \map {u^t} {\lambda y'} } | r = \sequence {\map u x, \lambda y'} | c = }} {{eqn | r = \lambda \sequence {\map u x, y'} | c = }} {{eqn | r = \lambda \sequence {x, \map {u^t} {y'} } | c = }} {{eqn | r = \sequence {x, \lambda \map {u^t} {y'} } | c = }} {{end-eqn}} it follows that $u^t: H^* \to G^*$ is a [[Definition:Linear Transformation|linear transformation]]. {{qed}}	1
Recall the definition of [[Definition:Jacobson Radical|Jacobson radical]] of $A$: :$\map {\operatorname {Jac} } A$ is the [[Definition:Set Intersection|intersection]] of all [[Definition:Maximal Ideal of Ring|maximal ideals]] of $R$. === $\subset$ Direction === {{AimForCont}} that $x \in \map {\operatorname {Jac} } A$ such that there exists $y \in A$ such that $1_A - x y \notin A^\times$. By definition $x$ is contained in all [[Definition:Maximal Ideal of Ring|maximal ideals]] of $A$. Let $\mathfrak m \subseteq A$ be one particular such [[Definition:Maximal Ideal of Ring|maximal ideal]] of $A$. Then $x \in \map {\operatorname {Jac} } A \subseteq \mathfrak m$ implies that $xy \in \mathfrak m$ and therefore $1_A \in \mathfrak m$. But if $1_A \in \mathfrak m$ then from [[Ideal of Unit is Whole Ring/Corollary|Ideal of Unit is Whole Ring: Corollary]]: :$\mathfrak m = R$ This [[Definition:Contradiction|contradicts]] the definition of $\mathfrak m$ as a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $A$. Hence by [[Proof by Contradiction]]: :$\map {\operatorname {Jac} } A \subseteq \set {a \in A: 1_A - a x \in A^\times \text{ for all } x \in A}$ {{qed|lemma}} === $\supset$ Direction === Now suppose that $x \notin \map {\operatorname {Jac} } A$. Then $x \notin \mathfrak m$ for some [[Definition:Maximal Ideal of Ring|maximal ideal]] $\mathfrak m$ of $A$. Because $\mathfrak m$ is [[Definition:Maximal Ideal of Ring|maximal]], $x$ and $\mathfrak m$ [[Definition:Generator of Ring|generate]] $A$. Therefore there exist $w \in \mathfrak m$ and $y \in A$ such that $w + x y = 1_A$. Thus $1_A - x y \in \mathfrak m$, and $1_A - x y \notin A^\times$. Hence: :$\map {\operatorname {Jac} } A \supseteq \set {a \in A: 1_A - a x \in A^\times \text{ for all } x \in A}$ {{qed|lemma}} The result follows by definition of [[Definition:Set Equality|set equality]]. {{qed}} [[Category:Commutative Algebra]] sr2jx31q78xi1evsr3ym5z0zl65mp6a	1
{{begin-eqn}} {{eqn | l = \dfrac 1 2 \paren {\mathbf c + \mathbf d} | r = \dfrac 1 2 \paren {\paren {\mathbf a + \mathbf b} + \paren {\mathbf a - \mathbf b} } | c = }} {{eqn | r = \dfrac 1 2 \paren {\mathbf a + \mathbf b + \mathbf a - \mathbf b} | c = }} {{eqn | r = \dfrac 1 2 \paren {2 \mathbf a} | c = }} {{eqn | r = \mathbf a | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \dfrac 1 2 \paren {\mathbf c - \mathbf d} | r = \dfrac 1 2 \paren {\paren {\mathbf a + \mathbf b} - \paren {\mathbf a - \mathbf b} } | c = }} {{eqn | r = \dfrac 1 2 \paren {\mathbf a + \mathbf b - \mathbf a + \mathbf b} | c = }} {{eqn | r = \dfrac 1 2 \paren {2 \mathbf b} | c = }} {{eqn | r = \mathbf b | c = }} {{end-eqn}} {{qed}}	1
Since a [[UFD is Noetherian]], and a [[Noetherian Domain is UFD if every irreducible element is prime]], it is sufficient to prove that every [[Definition:Irreducible Polynomial|irreducible]] element of $R \sqbrk X$ is [[Definition:Prime Number|prime]]. etc, {{ProofWanted|this possibly works}} {{Namedfor|Carl Friedrich Gauss|cat = Gauss}} [[Category:Factorization]] [[Category:Unique Factorization Domains]] bljlmlki1jz1fkjcafneorlm61c8zzm	1
From {{Module-axiom|1}}, $y \to \lambda \circ y$ is an [[Definition:Endomorphism|endomorphism]] of $\struct {G, +_G}$. From {{Module-axiom|2}}, $\mu \to \mu \circ x$ is a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]] from $\struct {R, +_R}$ to $\struct {G, +_G}$. The result follows from [[Homomorphism with Cancellable Codomain Preserves Identity]]. {{qed}}	1
By [[Normed Division Ring Completions are Isometric and Isomorphic/Lemma 4|lemma 4]], $\psi$ is an [[Definition:Isometry|isometry]]. By the definition of an [[Definition:Isometry|isometry]], $\psi$ is a [[Definition:Bijective|bijection]]. By the definition of a [[Definition:Ring Isomorphism|ring isomorphism]], all that remains is to show that $\psi$ is a [[Definition:Ring Homomorphism|ring homomorphism]]. That is: :$(1): \quad \forall x, y \in S_1: \map \psi {x + y} = \map \psi x + \map \psi y$ :$(2): \quad \forall x, y \in S_1: \map \psi {x y} = \map \psi x \map \psi y$ Let $x, y \in S_1$. Let $x = \displaystyle \lim_{n \mathop \to \infty} x_n$ for some [[Definition:Sequence|sequence]] $\sequence {x_n} \subseteq R_1$. Let $y = \displaystyle \lim_{n \mathop \to \infty} y_n$ for some [[Definition:Sequence|sequence]] $\sequence {y_n} \subseteq R_1$. By [[Sum Rule for Sequences in Normed Division Ring]] then: :$x + y = \displaystyle \lim_{n \mathop \to \infty} \paren {x_n + y_n}$ By [[Product Rule for Sequences in Normed Division Ring]] then: :$x y = \displaystyle \lim_{n \mathop \to \infty} \paren {x_n y_n}$ Then: {{begin-eqn}} {{eqn | l = \map \psi {x + y} | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n + y_n} | c = Definition of $\psi$ }} {{eqn | r = \lim_{n \mathop \to \infty} \paren {\map {\psi'} {x_n} + \map {\psi'} {y_n} } | c = $\psi'$ is a [[Definition:Ring Isomorphism|ring isomorphism]] }} {{eqn | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n} + \lim_{n \mathop \to \infty} \map {\psi'} {y_n} | c = [[Sum Rule for Sequences in Normed Division Ring]] }} {{eqn | r = \map \psi x + \map \psi y | c = Definition of $\psi$ }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \map \psi {x y} | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n y_n} | c = Definition of $\psi$ }} {{eqn | r = \lim_{n \mathop \to \infty} \paren {\map {\psi'} {x_n} \map {\psi'} {y_n} } | c = $\psi'$ is a [[Definition:Ring Isomorphism|ring isomorphism]] }} {{eqn | r = \lim_{n \mathop \to \infty} \map {\psi'} {x_n} \lim_{n \mathop \to \infty} \map {\psi'} {y_n} | c = [[Product Rule for Sequences in Normed Division Ring]] }} {{eqn | r = \map \psi x \map \psi y | c = Definition of $\psi$ }} {{end-eqn}} {{qed}} [[Category:Completion of Normed Division Ring]] 9x58937p8qd7tqgcynrn16eb87pn045	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]] [[Definition:Ring with Unity|with unity]]. Let $G$ be an [[Definition:Dimension (Linear Algebra)|$n$-dimensional]] [[Definition:Unitary Module|unitary $R$-module]]. Let $\left \langle {a_n} \right \rangle$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Let $\mathbf P = \left[{\alpha}\right]_{n}$ be a [[Definition:Square Matrix|square matrix]] of order $n$ over $R$. Let $\displaystyle \forall j \in \left[{1 \,.\,.\, n}\right]: b_j = \sum_{i \mathop = 1}^n \alpha_{i j} a_i$. Then $\left \langle {b_n} \right \rangle$ is an [[Definition:Ordered Basis|ordered basis]] of $G$ iff $\mathbf P$ is [[Definition:Invertible Matrix|invertible]].	1
Let $S \subseteq G$. Then $S$ is a '''linearly dependent set''' if there exists a [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ which is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]]. That is, such that: :$\displaystyle \exists \set {\lambda_k: 1 \le k \le n} \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e$ where $a_1, a_2, \ldots, a_n$ are distinct elements of $S$, and where at least one of $\lambda_k$ is not equal to $0_R$.	1
Let $\sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Let $\sqbrk b_{m n} = \sqbrk a_{m n} \mathbf I_n$. Then: {{begin-eqn}} {{eqn | ll= \forall i \in \closedint 1 m, j \in \closedint 1 n: | l = b_{i j} | r = \sum_{k \mathop = 1}^n a_{i k} \delta_{k j} | c = where $\delta_{k j}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: $\delta_{k j} = 1_R$ when $k = j$ otherwise $0_R$ }} {{eqn | r = a_{i j} | c = }} {{end-eqn}} Thus $\sqbrk b_{m n} = \sqbrk a_{m n}$ and $\mathbf I_n$ is shown to be a [[Definition:Right Identity|right identity]]. {{qed}} [[Category:Unit Matrix is Identity for Matrix Multiplication]] 8pmjc10owrj8op15gsw82jetlcmq0k4	1
{{ProofWanted|Need to consider which definition you start from}}	1
:$\map {B_r} x \cap \map {B_s} y \ne \O \iff \map {B_r} x \subseteq \map {B_s} y$ or $\map {B_s} y \subseteq \map {B_r} x$	1
Let $\mathbf Q$ be an [[Definition:Orthogonal Matrix|orthogonal matrix]]. Then: :$\mathbf Q \mathbf Q^\intercal = \mathbf I = \mathbf Q^\intercal \mathbf Q$ where: :$\mathbf Q^\intercal$ is the [[Definition:Transpose of Matrix|transpose]] of $\mathbf Q$ :$\mathbf I$ is a [[Definition:Unit Matrix|unit (identity) matrix]]	1
Let $T = \struct {S, \tau}$ be the [[Definition:Either-Or Topology|either-or space]]. Then $T$ is not a [[Definition:Separable Space|separable space]].	1
Let: :$\mathbf a = \begin{bmatrix} a_x \\ a_y \\ a_z \end{bmatrix}$, $\mathbf b = \begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix}$, $\mathbf c = \begin{bmatrix} c_x \\ c_y \\ c_z \end{bmatrix}$ be [[Definition:Vector (Euclidean Space)|vectors]] in a [[Definition:Vector Space|vector space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]]. Then: :$\mathbf a \times \paren {\mathbf b \times \mathbf c} = \paren {\mathbf a \cdot \mathbf c} \mathbf b - \paren {\mathbf a \cdot \mathbf b} \mathbf c$	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $G$ be an [[Definition:Dimension (Linear Algebra)|$n$-dimensional]] [[Definition:Module|$R$-module]]. Let $\map {\mathcal M_R} n$ be the [[Definition:Matrix Space|$n \times n$ matrix space]] over $R$. Let $\map {\mathcal L_R} G$ be [[Definition:Set of All Linear Transformations|the set of all linear operators]] on $G$. Then the [[Definition:Invertible Matrix|invertible]] elements of the [[Definition:Ring of Square Matrices|ring of square matrices]] $\struct {\map {\mathcal M_R} n, +, \times}$ correspond directly to automorphisms of $\map {\mathcal L_R} G$.	1
Let the plane $P = \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = \gamma$ be homogeneous. Then the [[Definition:Origin|origin]] $\left({0, 0, 0}\right)$ lies on the plane $P$. That is, $\alpha_1 0 + \alpha_2 0 + \alpha_3 0= \gamma \implies \gamma = 0$. Let the [[Equation of Plane|equation]] of $P$ be $P = \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = 0$. Then $0 = \alpha_1 0 + \alpha_2 0 + \alpha_3 0 \in P$ and so $\left({0, 0, 0}\right)$ lies on the plane $P$. Hence $P$ is [[Definition:Homogeneous (Analytic Geometry)|homogeneous]]. {{qed}}	1
Let $D$ be the [[Definition:Determinant of Matrix|determinant]]: :$D = \begin {vmatrix} 1 & 0 & \cdots & 0 \\ b_{2 1} & b_{2 2} & \cdots & b_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n 1} & b_{n 2} & \cdots & b_{n n} \end {vmatrix}$ Then: :$D = \begin {vmatrix} b_{2 2} & \cdots & b_{2 n} \\ \vdots & \ddots & \vdots \\ b_{n 2} & \cdots & b_{n n} \end {vmatrix}$	1
Let $d$ denote the [[Definition:Metric Induced by Norm|metric induced by $\norm {\, \cdot \,}$]], that is, :$d \tuple {x, y} = \norm {x - y}$ By definition of [[Definition:Convergent Sequence in Normed Division Ring|convergence in a normed division ring]]: :$\sequence {x_n}$ converges to $l$ in $\struct {R, \norm {\, \cdot \,} }$ {{iff}} $\sequence {x_n}$ [[Definition:Convergent Sequence in Metric Space|converges to $l$ in the metric space]] $\struct {R, d}$. We can then apply the result of [[Limit of Subsequence equals Limit of Sequence/Metric Space|Limit of Subsequence equals Limit of Sequence]] for [[Definition:Metric Space|metric spaces]] to get the result for [[Definition:Normed Division Ring|normed division rings]]. {{qed}} [[Category:Sequences]] [[Category:Normed Division Rings]] nja981douhnr2aoylkfzgebvaz9xo90	1
We need to verify the [[Definition:Vector Space Axioms|vector space axioms]] for $U$. We start with observing that the properties for a [[Definition:Unitary Module|unitary module]] are true for all [[Definition:Element|elements]] of $V$. Hence, since $U \subseteq V$, they hold for all [[Definition:Element|elements]] of $U$ as well. The same holds for the axioms $(G1)$ and $(C)$. From [[Vector Inverse is Negative Vector]], we have for all $u \in U$: :$u + \paren {- 1_K} u = 0_V$ which by assumption is an [[Definition:Element|element]] of $U$. Since $U$ is [[Definition:Non-Empty Set|non-empty]], this means $0_V \in U$. Hence it is seen that axioms $(G2)$ and $(G3)$ are satisfied. The last axiom that remains is $(G0)$. To this end we employ the knowledge that for all $v \in U$, we have: :$1_K v = v$ and hence: :$u + 1_K v = u + v \in U$ Having verified all the [[Definition:Vector Space Axioms|vector space axioms]], we conclude that $U$ is a [[Definition:Vector Subspace|subspace]] of $V$. {{qed}}	1
In the below: :$\kappa_i$ denotes the initial state of [[Definition:Column of Matrix|column]] $i$ :$\kappa_j$ denotes the initial state of [[Definition:Column of Matrix|column]] $j$ :$\kappa_i'$ denotes the state of [[Definition:Column of Matrix|column]] $i$ after having had the latest [[Definition:Elementary Column Operation|elementary column operation]] applied :$\kappa_j'$ denotes the state of [[Definition:Column of Matrix|column]] $j$ after having had the latest [[Definition:Elementary Column Operation|elementary column operation]] applied. $(1)$: Apply [[Definition:Elementary Column Operation|$\text {ECO} 2$]] to [[Definition:Column of Matrix|column]] $j$ for $\lambda = 1$: :$\kappa_j \to \kappa_j + \kappa_i$ After this operation: {{begin-eqn}} {{eqn | l = \kappa_i' | r = \kappa_i }} {{eqn | l = \kappa_j' | r = \kappa_i + \kappa_j }} {{end-eqn}} {{qed|lemma}} $(2)$: Apply [[Definition:Elementary Column Operation|$\text {ECO} 2$]] to [[Definition:Column of Matrix|column]] $i$ for $\lambda = -1$: :$\kappa_i \to \kappa_i + \paren {-\kappa_j}$ After this operation: {{begin-eqn}} {{eqn | l = \kappa_i' | r = \kappa_i - \paren {\kappa_i + \kappa_j} }} {{eqn | r = -\kappa_j }} {{eqn | l = \kappa_j' | r = \kappa_i + \kappa_j }} {{end-eqn}} {{qed|lemma}} $(3)$: Apply [[Definition:Elementary Column Operation|$\text {ECO} 2$]] to [[Definition:Column of Matrix|column]] $j$ for $\lambda = 1$: :$\kappa_j \to \kappa_j + \kappa_i$ After this operation: {{begin-eqn}} {{eqn | l = \kappa_i' | r = -\kappa_j }} {{eqn | l = \kappa_j' | r = \kappa_i + \kappa_j - \kappa_j }} {{eqn | r = \kappa_i }} {{end-eqn}} {{qed|lemma}} $(4)$: Apply [[Definition:Elementary Column Operation|$\text {ECO} 1$]] to [[Definition:Column of Matrix|column]] $i$ for $\lambda = -1$: :$\kappa_i \to -\kappa_i$ After this operation: {{begin-eqn}} {{eqn | l = \kappa_i' | r = -\paren {-\kappa_j} }} {{eqn | r = \kappa_j }} {{eqn | l = \kappa_j' | r = \kappa_i }} {{end-eqn}} {{qed|lemma}} Thus, after all the $4$ [[Definition:Elementary Column Operation|elementary column operations]] have been applied, we have: {{begin-eqn}} {{eqn | l = \kappa_i' | r = \kappa_j }} {{eqn | l = \kappa_j' | r = \kappa_i }} {{end-eqn}} Hence the result. {{qed}}	1
Let $M$ be a [[Definition:Topological Space|topological space]]. {{TFAE}} :$(1):\quad$ $M$ is [[Definition:Locally Euclidean Space|locally euclidean]]. :$(2):\quad$ There exists a [[Definition:Atlas|$C^0$-atlas]] on $M$.	1
Let $\struct{R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. Let $a \in R$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map {B_\epsilon} {a; \norm {\,\cdot\,} }$ denote the [[Definition:Open Ball of Normed Division Ring|open ball]] in the [[Definition:Normed Division Ring|normed division ring]] $\struct {R, \norm {\,\cdot\,} }$. Let $\map {B_\epsilon} {a; d }$ denote the [[Definition:Open Ball|open ball]] in the [[Definition:Metric Space|metric space]] $\struct {R, d}$. Then: :$\map {B_\epsilon} {a; \norm {\,\cdot\,} }$ = $\map {B_\epsilon} {a; d }$	1
Consider the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\map \MM {m, n}, +, \circ}$, where: :$+$ denotes [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] :$\circ$ denotes [[Definition:Matrix Product (Conventional)|(conventional) matrix multiplication]]. From [[Ring of Square Matrices over Field is Ring with Unity]], $\struct {\map \MM {m, n}, +, \circ}$ is a [[Definition:Ring with Unity|ring with unity]]. Hence [[Definition:A Fortiori|a fortiori]] $\struct {\map \MM {m, n}, +, \circ}$ is a [[Definition:Monoid|monoid]]. The result follows directly from [[Left Inverse and Right Inverse is Inverse]]. {{qed}} {{mistake|That's not what it actually says. What the above link says is that ''if'' $\mathbf A$ has both a [[Definition:Right Inverse Matrix|right inverse matrix]] ''and'' a [[Definition:Left Inverse Matrix|left inverse matrix]], then those are equal and can be called an [[Definition:Inverse Matrix|inverse matrix]]. It does not say that if $\mathbf B$ is a [[Definition:Left Inverse Matrix|left inverse matrix]] then it is automatically a [[Definition:Right Inverse Matrix|right inverse matrix]]. I'll sort this out when I get to exercise $1.15$.}}	1
Let $\Bbb F$ be a [[Definition:Field (Abstract Algebra)|field]], usually one of the [[Definition:Standard Number Field|standard number fields]] $\Q$, $\R$ or $\C$. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\map \MM n$ denote the [[Definition:Matrix Space|matrix space]] of [[Definition:Order of Square Matrix|order]] $n$ [[Definition:Square Matrix|square matrices]] over $\Bbb F$. Let $\mathbf B$ be a [[Definition:Left Inverse Matrix|left inverse matrix]] of $\mathbf A$. Then $\mathbf B$ is also a [[Definition:Right Inverse Matrix|right inverse matrix]] of $\mathbf A$. Similarly, let $\mathbf B$ be a [[Definition:Right Inverse Matrix|right inverse matrix]] of $\mathbf A$. Then $\mathbf B$ is also a [[Definition:Right Inverse Matrix|right inverse matrix]] of $\mathbf A$.	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\theta_1$ and $\theta_2$ be [[Definition:Argument of Complex Number|arguments]] of $z_1$ and $z_2$, respectively. Then: :$\cmod {z_1 - z_2}^2 = \cmod {z_1}^2 + \cmod {z_2}^2 - 2 \cmod {z_1} \cmod {z_2} \, \map \cos {\theta_1 - \theta_2}$	1
Let $z_1 = x_1 + i y_1$ and $z_2 = x_2 + i y_2$, where $x_1, y_1, x_2, y_2 \in \R$. {{begin-eqn}} {{eqn | l = \cmod {z_1 z_2} | r = \sqrt {\paren {x_1 x_2 - y_1 y_2}^2 + \paren {x_1 y_2 + x_2 y_1}^2} | c = {{Defof|Complex Modulus}}, {{Defof|Complex Multiplication}} }} {{eqn | r = \sqrt {\paren {x_1^2 x_2^2 + y_1^2 y_2^2 - 2 x_1 x_2 y_1 y_2} + \paren {x_1^2 y_2^2 + x_2^2 y_1^2 + 2 x_1 x_2 y_1 y_2} } | c = }} {{eqn | r = \sqrt {x_1^2 x_2^2 + y_1^2 y_2^2 + x_1^2 y_2^2 + x_2^2 y_1^2} | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \cmod {z_1} \cdot \cmod {z_2} | r = \sqrt {x_1^2 + y_1^2} \sqrt {x_2^2 + y_2^2} | c = }} {{eqn | r = \sqrt {\paren {x_1^2 + y_1^2} \paren {x_2^2 + y_2^2} } | c = }} {{eqn | r = \sqrt {x_1^2 x_2^2 + y_1^2 y_2^2 + x_1^2 y_2^2 + x_2^2 y_1^2} | c = }} {{end-eqn}} {{qed}}	1
Since a [[Definition:Linear Transformation|linear transformation]] $\phi: G \to H$ is, in particular, a [[Definition:Group Homomorphism|homomorphism]] from the [[Definition:Group|group]] $G$ to the [[Definition:Group|group]] $H$, it follows that: :$(1): \quad$ By [[Homomorphism with Cancellable Codomain Preserves Identity]], $\map \phi {e_G} = e_H$ :$(2): \quad$ By [[Homomorphism with Identity Preserves Inverses]], $\map \phi {-x} = -\map \phi x$. From [[Epimorphism preserves Modules]] and definition of [[Definition:Surjection|surjection]], it follows that as $M$ is a [[Definition:Submodule|submodule]] of $G$, then $\phi \sqbrk M$ is a [[Definition:Submodule|submodule]] of $H$. The result follows ... {{stub}}	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\cmod z$ be the [[Definition:Modulus of Complex Number|modulus]] of $z$. Then: :$\cmod {z_1 + z_2} \ge \cmod {\cmod {z_1} - \cmod {z_2} }$	1
Let $V = \struct {X, \norm {\,\cdot\,} }$ be a [[Definition:Normed Vector Space|normed vector space]]. Every [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence in $X$]] is [[Definition:Bounded Sequence in Normed Vector Space|bounded]].	1
Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by the norm]] $\norm {\,\cdot\,}$. By [[Non-Archimedean Norm iff Non-Archimedean Metric]] then $d$ is a [[Definition:Non-Archimedean Metric|non-Archimedean metric]] and $\struct {R, d}$ is an [[Definition:Ultrametric Space|ultrametric space]]. Let $x, y \in R$ and $\norm x \ne \norm y$. By the definition of the [[Definition:Non-Archimedean Metric|non-Archimedean metric]] $d$ then: :$\norm x = \norm {x - 0} = \map d {x, 0}$ and similarly: :$\norm y = \map d {y, 0}$ By assumption then: :$\map d {x, 0} \ne \map d {y, 0}$ By [[Three Points in Ultrametric Space have Two Equal Distances]] then: :$\norm {x - y} = \map d {x, y} = \max \set {\map d {x, 0}, \map d {y, 0} } = \max \set {\norm x, \norm y}$ By [[Properties of Norm on Division Ring/Norm of Negative|Norm of Negative]] then: :$\norm {y - x} = \norm {x - y} = \max \set {\norm x, \norm y}$ By the definition of the [[Definition:Non-Archimedean Metric|non-Archimedean metric]] $d$ then: :$\norm y = \norm {0 - \paren {-y} } = \map d {0, -y} = \map d {-y, 0}$ By assumption then: :$\map d {x, 0} \ne \map d {-y, 0}$ By [[Three Points in Ultrametric Space have Two Equal Distances]] then: :$\map d {x, -y} = \max \set {\map d {x, 0}, \map d {-y, 0} } = \max \set {\norm x, \norm y}$ By the definition of the [[Definition:Non-Archimedean Metric|non-Archimedean metric]] $d$ then: :$\map d {x, -y} = \norm {x - \paren {-y} } = \norm {x + y}$ So $\norm {x + y} = \max \set {\norm x, \norm y}$. The result follows. {{qed}}	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $G$ be a [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Vector Space|$K$-vector space]]. Then any two [[Definition:Basis of Vector Space|bases]] of $G$ are [[Definition:Finite Set|finite]] and [[Definition:Set Equivalence|equivalent]]. {{improve|I still think there's a simpler way of saying "they have the same number of elements" than bringing in all that top-heavy technical set theoretic language of "set equivalence" and "cardinality" and so on. Since we are talking about a finite set, the complexities which arise with regard to transfinites do not arise, so there is no direct need to go into such depth.}}	1
Let $\alpha \in \R_{\gt 0}$. Let $\norm{\,\cdot\,}:\Q \to \R$ be the [[Definition:Mapping|mapping]] defined by: :$\forall x \in \Q: \norm{x} = \size {x}^\alpha$ where $\size {x}$ is the [[Definition:Absolute Value|absolute value]] of $x$ in $\Q$. Then: :$\alpha \le 1 \implies \norm{\,\cdot\,}$ is a [[Definition:Norm/Division Ring|norm]] on $\Q$	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] that [[Definition:Matrix Scalar Product|multiplies]] [[Definition:Row of Matrix|rows]] $i$ by the [[Definition:Scalar (Matrix Theory)|scalar]]$c$. Let $\mathbf B = \map e {\mathbf A}$. Let $\mathbf E$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e$. From [[Elementary Row Operations as Matrix Multiplications]]: :$\mathbf B = \mathbf E \mathbf A$ From [[Determinant of Elementary Row Matrix/Exchange Rows|Determinant of Elementary Row Matrix: Exchange Rows]]: :$\map \det {\mathbf E} = c$ Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf B} | r = \map \det {\mathbf E \mathbf A} | c = [[Determinant of Matrix Product]] }} {{eqn | r = c \map \det {\mathbf A} | c = as $\map \det {\mathbf E} = c$ }} {{end-eqn}} Hence the result. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}, \mathbf B = \sqbrk b_{n p}, \mathbf C = \sqbrk c_{p q}$ be [[Definition:Matrix|matrices]]. From inspection of the subscripts, we can see that both $\paren {\mathbf A \mathbf B} \mathbf C$ and $\mathbf A \paren {\mathbf B \mathbf C}$ are defined: $\mathbf A$ has $n$ [[Definition:Column of Matrix|columns]] and $\mathbf B$ has $n$ [[Definition:Row of Matrix|rows]], while $\mathbf B$ has $p$ [[Definition:Column of Matrix|columns]] and $\mathbf C$ has $p$ [[Definition:Row of Matrix|rows]]. Consider $\paren {\mathbf A \mathbf B} \mathbf C$. Let $\mathbf R = \sqbrk r_{m p} = \mathbf A \mathbf B, \mathbf S = \sqbrk s_{m q} = \mathbf A \paren {\mathbf B \mathbf C}$. Then: {{begin-eqn}} {{eqn | l = s_{i j} | r = \sum_{k \mathop = 1}^p r_{i k} \circ c_{k j} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | l = r_{i k} | r = \sum_{l \mathop = 1}^n a_{i l} \circ b_{l k} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | ll= \leadsto | l = s_{i j} | r = \sum_{k \mathop = 1}^p \paren {\sum_{l \mathop = 1}^n a_{i l} \circ b_{l k} } \circ c_{k j} }} {{eqn | r = \sum_{k \mathop = 1}^p \sum_{l \mathop = 1}^n \paren {a_{i l} \circ b_{l k} } \circ c_{k j} | c = {{Ring-axiom|D}} }} {{end-eqn}} Now consider $\mathbf A \paren {\mathbf B \mathbf C}$. Let $\mathbf R = \sqbrk r_{n q} = \mathbf B \mathbf C, \mathbf S = \sqbrk s_{m q} = \mathbf A \paren {\mathbf B \mathbf C}$. Then: {{begin-eqn}} {{eqn | l = s_{i j} | r = \sum_{l \mathop = 1}^n a_{i l} \circ r_{l j} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | l = r_{l j} | r = \sum_{k \mathop = 1}^p b_{l k} \circ c_{k j} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | ll= \leadsto | l = s_{i j} | r = \sum_{l \mathop = 1}^n a_{i l} \circ \paren {\sum_{k \mathop = 1}^p b_{l k} \circ c_{k j} } }} {{eqn | r = \sum_{l \mathop = 1}^n \sum_{k \mathop = 1}^p a_{i l} \circ \paren {b_{l k} \circ c_{k j} } | c = {{Ring-axiom|D}} }} {{end-eqn}} Using {{Ring-axiom|M1}}: :$\displaystyle s_{i j} = \sum_{k \mathop = 1}^p \sum_{l \mathop = 1}^n \paren {a_{i l} \circ b_{l k} } \circ c_{k j} = \sum_{l \mathop = 1}^n \sum_{k \mathop = 1}^p a_{i l} \circ \paren {b_{l k} \circ c_{k j} } = s'_{i j}$ It is concluded that: :$\paren {\mathbf A \mathbf B} \mathbf C = \mathbf A \paren {\mathbf B \mathbf C}$ {{qed}}	1
Let $V$ be a [[Definition:Topological Vector Space|topological vector space]] over $\R$ or $\C$. Let $A\subset V$ be a [[Definition:Convex Set (Vector Space)|convex subset]]. Then $A$ is [[Definition:Contractible Space|contractible]].	1
Let $M$ and $N$ be [[Definition:Metric Space|metric spaces]]. Let $M$ be [[Definition:Complete Metric Space|complete]]. Let $f : M \times N \to M$ be a [[Definition:Lipschitz Continuous|Lipschitz continuous]] [[Definition:Uniform Contraction Mapping|uniform contraction]]. Then for all $t\in N$ there exists a [[Definition:Unique|unique]] $g(t) \in M$ such that $f(g(t), t) = g(t)$, and the [[Definition:Mapping|mapping]] $g : N \to M$ is [[Definition:Lipschitz Continuous|Lipschitz continuous]].	1
{{begin-eqn}} {{eqn | l = \cmod {\dfrac {z_1} {z_2} } | r = \cmod {z_1 \times z_2^{-1} } | c = {{Defof|Complex Division}} }} {{eqn | r = \cmod {z_1} \times \cmod {z_2^{-1} } | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | r = \cmod {z_1} \times \cmod {z_2}^{-1} | c = [[Complex Modulus of Reciprocal of Complex Number]] }} {{eqn | r = \dfrac {\cmod {z_1} } {\cmod {z_2} } | c = {{Defof|Real Division}} }} {{end-eqn}} {{qed}}	1
By definition, $\map \CC {\mathbb J} \subseteq \R^{\mathbb J}$. Let $f, g \in \map \CC {\mathbb J}$. By [[Two-Step Vector Subspace Test]], it needs to be shown that: :$\paren 1: \quad f + g \in \map \CC {\mathbb J}$ :$\paren 2: \quad \lambda f \in \map \CC {\mathbb J}$ for any $\lambda \in \R$ $\paren 1$ follows by [[Sum Rule for Continuous Functions]]. $\paren 2$ follows by [[Multiple Rule for Continuous Functions]]. Hence $\struct {\map \CC {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of the [[Definition:Vector Space|$\R$-vector space]] $\struct {\R^{\mathbb J}, +, \times}_\R$. {{qed}}	1
Let $G$ and $H$ be [[Definition:Unitary Module|unitary $R$-modules]]. Let $\left \langle {a_n} \right \rangle$ be an [[Definition:Ordered Basis|ordered basis]] of $G$. Let $\left \langle {b_n} \right \rangle$ be a [[Definition:Sequence|sequence]] of elements of $H$. Then there is a unique [[Definition:Linear Transformation|linear transformation]] $\phi: G \to H$ satisfying $\forall k \in \left[{1 \,.\,.\, n}\right]: \phi \left({a_k}\right) = b_k$	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A$ be an [[Definition:Idempotent Operator|idempotent operator]]. Then $\ker A$ and $\Rng A$ are [[Definition:Algebraically Complementary|algebraically complementary]], that is: :$\ker A \cap \Rng A = \left({0}\right)$, the [[Definition:Zero Subspace|zero subspace]] :$\ker A + \Rng A = H$, where $+$ signifies [[Definition:Setwise Addition|setwise addition]]. {{explain|Determine whether range means image or codomain, and replace appropriately}}	1
We have: {{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2}^2 | r = \paren {z_1 + z_2} \paren {\overline {z_1} + \overline {z_2} } | c = [[Modulus in Terms of Conjugate]] and [[Sum of Complex Conjugates]] }} {{eqn | r = z_1 \overline {z_1} + z_2 \overline {z_2} + z_1\overline {z_2} + \overline {z_1} z_2 | c = }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \, \map \Re {z_1 \overline {z_2} } | c = [[Modulus in Terms of Conjugate]] and [[Sum of Complex Number with Conjugate]] }} {{eqn | r = \cmod {z_1}^2 + \cmod {z_2}^2 + 2 \, \cmod{ z_1 } \cmod{ z_2 } \map \cos {\theta_1 - \theta_2} | c = [[Product of Complex Numbers in Polar Form]] and [[Argument of Conjugate of Complex Number]] }} {{end-eqn}} {{qed}}	1
Let $\mathbf A$ be a [[Definition:Hermitian Matrix|Hermitian matrix]]. Then, by definition: : $\mathbf A = \mathbf A^*$ where $\mathbf A^*$ denotes the [[Definition:Conjugate Transpose of Matrix|conjugate transpose]] of $\mathbf A$. Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $\mathbf A$. Let $\mathbf v$ be an [[Definition:Eigenvector|eigenvector]] corresponding to the [[Definition:Eigenvalue|eigenvalue]] $\lambda$. By definition of [[Definition:Eigenvector|eigenvector]]: : $\mathbf{A v} = \lambda \mathbf v$ [[Definition:Matrix Product (Conventional)|Left-multiplying]] both sides by $\mathbf v^*$, we obtain: :$(1):\quad \mathbf v^* \mathbf{A v} = \mathbf v^* \lambda \mathbf v = \lambda \mathbf v^* \mathbf v$ Firstly, note that both $\mathbf v^* \mathbf{A v}$ and $\mathbf v^* \mathbf v$ are $1 \times 1$-matrices. {{explain|Links to the proof of this fact}} Now observe that, using [[Conjugate Transpose of Matrix Product/General Case|Conjugate Transpose of Matrix Product: General Case]]: :$\left({\mathbf v^* \mathbf{A v}}\right)^* = \mathbf v^* \mathbf A^* \left({\mathbf v^*}\right)^*$ As $\mathbf A$ is [[Definition:Hermitian Matrix|Hermitian]], and $\left({\mathbf v^*}\right)^* = \mathbf v$ by [[Conjugate Transpose is Involution]], it follows that: :$\mathbf v^* \mathbf A^* \left({\mathbf v^*}\right)^* = \mathbf v^* \mathbf{A v}$ That is, $\mathbf v^* \mathbf{A v}$ is also [[Definition:Hermitian Matrix|Hermitian]]. By [[Product with Conjugate Transpose Matrix is Hermitian]], $\mathbf v^* \mathbf v$ is [[Definition:Hermitian Matrix|Hermitian]]. So both $\mathbf v^* \mathbf{A v}$ and $\mathbf v^* \mathbf v$ are [[Definition:Hermitian Matrix|Hermitian]] $1 \times 1$ matrices. Now suppose that we have for some $a,b \in \C$: :$\mathbf v^* \mathbf{A v} = \begin{bmatrix}a\end{bmatrix}$ :$\mathbf v^* \mathbf v = \begin{bmatrix}b\end{bmatrix}$ Note that $b \ne 0$ as an [[Definition:Eigenvector|eigenvector]] is by definition [[Definition:Zero Vector|non-zero]]. By definition of a [[Definition:Hermitian Matrix|Hermitian matrix]]: : $a = \bar a$ and $b = \bar b$ where $\bar a$ denotes the [[Definition:Complex Conjugate|complex conjugate]] of $a$. By [[Complex Number equals Conjugate iff Wholly Real]], it follows that $a, b \in \R$, that is, are [[Definition:Real Number|real]]. From equation $(1)$, it follows that $\begin{bmatrix}a\end{bmatrix} = \lambda \begin{bmatrix}b\end{bmatrix}$. Thus, $a = \lambda b$, i.e. $\lambda = \dfrac a b$ (recall that $b \ne 0$). Hence $\lambda$, being a [[Definition:Quotient (Algebra)|quotient]] of [[Definition:Real Number|real numbers]], is [[Definition:Real Number|real]]. {{qed}}	1
Let $A$ be a [[Definition:Square Matrix|square matrix]]. Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $A$ and $\mathbf v$ be the corresponding [[Definition:Eigenvector|eigenvector]]. Then: :$A^n \mathbf v = \lambda^n \mathbf v$ holds for each [[Definition:Positive Integer|positive integer]] $n$.	1
Let $M$ be a [[Definition:Infinite Line|straight line]] in [[Definition:The Plane|the plane]] passing through the [[Definition:Origin|origin]]. Then the '''reflection''' $s_M$ of $\R^2$ in $M$ is the [[Rotation of Plane about Origin is Linear Operator|rotation]] of [[Definition:The Plane|the plane]] in [[Definition:Ordinary Space|space]] through one half turn about $M$ as an [[Definition:Axis|axis]]. :$s_M \circ s_M = I_{\R^2}$ and hence: :$s_M = s_M^{-1}$ If $M$ is the [[Definition:X-Axis|$x$-axis]] then $\map {s_M} {\lambda_1, \lambda_2} = \tuple {\lambda_1, -\lambda_2}$. If $M$ is the [[Definition:Y-Axis|$y$-axis]] then $\map {s_M} {\lambda_1, \lambda_2} = \tuple {-\lambda_1, \lambda_2}$. In general, $s_M$ is a [[Definition:Linear Operator|linear operator]] for every [[Definition:Straight Line|straight line]] $M$ through the [[Definition:Origin|origin]].	1
Let $T$ be a [[Definition:Countable Complement Topology|countable complement topology]] on an [[Definition:Uncountable Set|uncountable set]]. From [[Countable Complement Space Satisfies Countable Chain Condition]], $T$ satisfies the [[Definition:Countable Chain Condition|countable chain condition]]. From [[Countable Complement Space is not Separable]], $T$ is not a [[Definition:Separable Space|separable space]]. Hence the result. {{qed}}	1
{{begin-eqn}} {{eqn | l = e^{i x} | r = \cos x + i \sin x | c = [[Euler's Formula]] }} {{eqn | ll= \leadsto | l = \cmod {e^{i x} } | r = \cmod {\cos x + i \sin x} }} {{eqn | r = \sqrt {\paren {\map \Re {\cos x + i \sin x} }^2 + \paren {\map \Im {\cos x + i \sin x} }^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \sqrt {\cos^2 x + \sin^2 x} | c = as $x$ is [[Definition:Wholly Real|wholly real]] }} {{eqn | r = 1 | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} {{qed}} [[Category:Complex Modulus]] [[Category:Modulus of Exponential of Imaginary Number is One]] 4e54obxfjkhpr31zu5htm7dromjp3mj	1
As $f$ is [[Definition:Additive Function (Conventional)|additive]], we have: {{begin-eqn}} {{eqn | l = f \paren 1 | r = f \paren {0 + 1} | c = }} {{eqn | r = f \paren 0 + f \paren 1 | c = }} {{end-eqn}} that is: :$f \paren 0 = 0$ {{qed}} [[Category:Additive Functions]] b7tdfjld57tam4s0nxbe9x7k62jlq6j	1
Let $\lambda \in \lambda \in \map Z R$. Then: {{begin-eqn}} {{eqn | l = \map {\paren {\lambda \circ \phi} } {x +_G y} | r = \lambda \circ \map \phi {x +_G y} | c = }} {{eqn | r = \lambda \circ \paren {\map \phi x +_H \map \phi y} | c = }} {{eqn | r = \lambda \circ \map \phi x +_H \lambda \circ \map \phi y | c = }} {{eqn | r = \map {\paren {\lambda \circ \phi} } x +_H \map {\paren {\lambda \circ \phi} } y | c = }} {{end-eqn}} Because $\lambda \in \map Z R$, $\lambda$ [[Definition:Commute|commutes]] with all elements of $R$. So $\forall \mu \in R: \lambda \circ \mu = \mu \circ \lambda$. Thus: {{begin-eqn}} {{eqn | l = \map {\paren {\lambda \circ \phi} } {\mu \circ x} | r = \lambda \circ \map \phi {\mu \circ x} | c = }} {{eqn | r = \lambda \circ \mu \circ \map \phi x | c = }} {{eqn | r = \mu \circ \lambda \circ \map \phi x | c = as $\lambda \in \map Z R$ }} {{eqn | r = \mu \circ \map {\paren {\lambda \circ \phi} } x | c = }} {{end-eqn}} {{qed}}	1
Let $T: \R^n \to \R^m, \mathbf x \mapsto \map T {\mathbf x}$ be a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Then: :$\map T {\mathbf x} = \mathbf A_T \mathbf x$ where $\mathbf A_T$ is the [[Definition:Matrix|$m \times n$ matrix]] defined as: :$\mathbf A_T = \begin {bmatrix} \map T {\mathbf e_1} & \map T {\mathbf e_2} & \cdots & \map T {\mathbf e_n} \end {bmatrix}$ where $\tuple {\mathbf e_1, \mathbf e_2, \cdots, \mathbf e_n}$ is the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis]] of $\R^n$.	1
Let $V$ be a [[Definition:Vector Space|vector space]]. Let $\BB = \tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ be a [[Definition:Basis of Vector Space|basis of $V$]]. Then $\BB$ is an '''orthogonal basis''' {{iff}} $\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n$ are [[Definition:Pairwise Perpendicular|pairwise perpendicular]].	1
{{begin-eqn}} {{eqn | l = \norm {x + y} | o = \le | r = \max \set {\norm x, \norm y} }} {{eqn | ll= \leadstoandfrom | l = \norm {x + y} \norm {y^{-1} } | o = \le | r = \max \set {\norm x \norm {y^{-1} }, \norm y \norm {y^{-1} } } | c = Multiply through by $\norm{y^{-1} }$ }} {{eqn | ll= \leadstoandfrom | l = \norm {\paren {x + y} y^{-1} } | o = \le | r = \max \set {\norm {x y^{-1} }, \norm {y y^{-1} } } | c = {{NormAxiom|2}} }} {{eqn | ll= \leadstoandfrom | l = \norm {\paren {x y^{-1} + y y^{-1} } } | o = \le | r = \max \set {\norm {x y^{-1} }, \norm {y y^{-1} } } | c = [[Definition:Ring (Abstract Algebra)|Ring axiom $\text D$: Product is Distributive over Addition]] }} {{eqn | ll= \leadstoandfrom | l = \norm {\paren {x y^{-1} + 1_R } } | o = \le | r = \max \set {\norm {x y^{-1} }, \norm {1_R } } | c = [[Definition:Division Ring|Product with Inverse is Unit]] }} {{eqn | ll= \leadstoandfrom | l = \norm {\paren {x y^{-1} + 1_R } } | o = \le | r = \max \set {\norm {x y^{-1} }, 1 } | c = [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity]] }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = \int_\mathcal C \nabla f \cdot \rd \mathbf r | r = \int_a^b \nabla f \cdot \mathbf r' \left({t}\right) \rd t }} {{eqn | r = \int_a^b \frac {\partial f} {\partial x} \frac {\rd x} {\rd t} + \frac {\partial f} {\partial y} \frac {\rd y} {\rd t} + \frac {\partial f} {\partial z} \frac {\rd z} {\rd t} \rd t}} {{eqn | r = \int_a^b \frac \rd {\rd t} f \left({\mathbf r \left({t}\right)}\right) \rd t | c = [[Chain Rule for Real-Valued Functions]] }} {{eqn | r = f \left({\mathbf r \left({b}\right)}\right) - f \left({\mathbf r \left({a}\right)}\right) | c = [[Fundamental Theorem of Calculus]] }} {{end-eqn}} {{qed}} [[Category:Vector Calculus]] iefbddltrzh0ku75we12ousbhq7ujp1	1
A [[Definition:Vector Space|vector space]] is a [[Definition:Module|module]], so all results about modules also apply to vector spaces. So from [[Scalar Product with Identity]] it follows directly that: :$\lambda = 0_F \lor \mathbf v = e \implies \lambda \circ \mathbf v = \bszero$ Next, suppose $\lambda \circ \mathbf v = \bszero$ but $\lambda \ne 0_F$. Then: {{begin-eqn}} {{eqn | l = \bszero | r = \lambda^{-1} \circ \bszero | c = [[Zero Vector Scaled is Zero Vector]] }} {{eqn | r = \lambda^{-1} \circ \paren {\lambda \circ \mathbf v} | c = as $\lambda \circ \mathbf v = \bszero$ }} {{eqn | r = \paren {\lambda^{-1} \circ \lambda} \circ \mathbf v | c = {{Vector-space-axiom|7}} }} {{eqn | r = 1 \circ \mathbf v | c = {{Field-axiom|M4}} }} {{eqn | r = \mathbf v | c = {{Vector-space-axiom|8}} }} {{end-eqn}} {{Qed}}	1
From [[Negative of Triangular Matrix]], if $\mathbf B \in \map {U_R} n$ then $-\mathbf B \in \map {U_R} n$. Then from [[Sum of Triangular Matrices]], if $\mathbf A, -\mathbf B \in \map {U_R} n$ then $\mathbf A + \paren {-\mathbf B} \in \map {U_R} n$. From [[Product of Triangular Matrices]], if $\mathbf A, \mathbf B \in \map {U_R} n$ then $\mathbf A \mathbf B \in \map {U_R} n$. The result follows from the [[Subring Test]]. The same argument can be applied to matrices in $\map {L_R} n$. {{qed}}	1
The [[Definition:Ring of Gaussian Integers|ring of Gaussian integers]]: :$\struct {\Z \sqbrk i, +, \times}$ forms a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free]] $R$-[[Definition:Module|module]] of [[Definition:Dimension of Module|dimension]] $n$. Let $\tuple {e_1, \ldots, e_n}$ be a [[Definition:Basis of Module|basis]] of $M$. Let $\tuple {e_1^*,\ldots, e_n^*}$ be its [[Definition:Ordered Dual Basis|dual basis]] Let $f: M \to M$ be a [[Definition:Linear Operator|linear operator]]. Then its [[Definition:Trace of Linear Operator|trace]] equals: :$\map \tr f = \displaystyle \sum_{i \mathop = 1}^n e_i^* \paren {\map f {e_i} }$	1
:$\lambda \circ \struct {-x} = \struct {-\lambda} \circ x = -\struct {\lambda \circ x}$	1
Let $\map e {\mathbf A}$ be the [[Definition:Elementary Row Operation|elementary row operation]]: :$e := r_k \to \lambda r_k$ where $\lambda \ne 0$. Then $r'_k$ is such that: :$\forall a'_{k i} \in r'_k: a'_{k i} = \lambda a_{k i}$ Now let $\map {e'} {\mathbf A'}$ be the [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A'$ to $\mathbf A''$: :$e' := r_k \to \dfrac 1 \lambda r_k$ Because it is stipulated in the definition of an [[Definition:Elementary Row Operation|elementary row operation]] that $\lambda \ne 0$, it follows by definition of a [[Definition:Field (Abstract Algebra)|field]] that $\dfrac 1 \lambda$ exists. Hence $e'$ is defined. So applying $e'$ to $\mathbf A'$ we get: {{begin-eqn}} {{eqn | lo= \forall a''_{k i} \in r''_k: | l = a''_{k i} | r = \dfrac 1 \lambda a'_{k i} | c = }} {{eqn | r = \dfrac 1 \lambda \paren {\lambda a_{k i} } | c = }} {{eqn | r = a_{k i} | c = }} {{eqn | ll= \leadsto | lo= \forall a''_{k i} \in r''_k: | l = a''_{k i} | r = a_{k i} | c = }} {{eqn | ll= \leadsto | l = r''_k | r = r_k | c = }} {{eqn | ll= \leadsto | l = \mathbf A'' | r = \mathbf A | c = }} {{end-eqn}} It is noted that for $e'$ to be an [[Definition:Elementary Row Operation|elementary row operation]], the only possibility is for it to be as defined.	1
The [[Definition:Dimension of Vector Space|two-dimensional]] [[Definition:Vector Subspace|subspaces]] of $\R^3$ are precisely the [[Definition:Homogeneous (Analytic Geometry)|homogeneous planes]] of [[Definition:Solid Analytic Geometry|solid analytic geometry]].	1
Let $\mathbf A_1, \mathbf A_2, \ldots, \mathbf A_m$ be [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\mathbf A_1 \mathbf A_2 \cdots \mathbf A_m$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A_1, \mathbf A_2, \ldots, \mathbf A_m$. Then: :$(1): \quad \ds \map \tr {\mathbf A_1 \mathbf A_2 \cdots \mathbf A_m} = \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \cdots \map {a_{m - 1} } {i_{m - 1}, i_m} \map {a_m} {i_m, i_1}$ where: :$\map {a_1} {i_1, i_2}$ (for example) denotes the [[Definition:Element of Matrix|element]] of $\mathbf A_1$ whose [[Definition:Index of Matrix Element|indices]] are $i_1$ and $i_2$ :$\map \tr {\mathbf A_1 \mathbf A_2 \cdots \mathbf A_m}$ denotes the [[Definition:Trace of Matrix|trace]] of $\mathbf A_1 \mathbf A_2 \cdots \mathbf A_m$. In $(1)$, the [[Definition:Einstein Summation Convention|Einstein summation convention]] is used, with the implicit understanding that a [[Definition:Summation|summation]] is performed over each of the [[Definition:Index of Matrix Element|indices]] $i_1$ to $i_m$.	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct{G, +_G, \circ}$ be a [[Definition:Left Module|left module]] over $\struct {R, +_R, \times_R}$. Let $\circ’ : G \times R \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: x \circ’ \lambda = \lambda \circ x$ Then $\struct{G, +_G, \circ’}$ is a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$.	1
:$\quad \mathcal C \,\big / \mathcal N = \tilde {\mathcal C}$	1
=== Necessary Condition === Let the [[Definition:Binary Operation|operation]] $\cdot$ be [[Definition:Commutative Operation|commutative]] on $\struct {S, \cdot}$. Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of the [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. Then: {{begin-eqn}} {{eqn | l = \mathbf A \circ \mathbf B | r = \sqbrk a_{m n} \circ \sqbrk b_{m n} | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{eqn | r = \sqbrk {a \cdot b}_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \sqbrk {b \cdot a}_{m n} | c = as $\cdot$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | r = \sqbrk b_{m n} \circ \sqbrk a_{m n} | c = {{Defof|Hadamard Product}} }} {{eqn | r = \mathbf B \circ \mathbf A | c = Definition of $\mathbf A$ and $\mathbf B$ }} {{end-eqn}} That is, $\circ$ is [[Definition:Commutative Operation|commutative]] on $\map {\MM_S} {m, n}$. {{qed|lemma}} === Sufficient Condition === Suppose $\struct {S, \cdot}$ is such that $\cdot$ is not [[Definition:Commutative Operation|commutative]]. Then there exists $a$ and $b$ such that: :$a \cdot b \ne b \cdot a$ Let $\mathbf A$ and $\mathbf B$ be [[Definition:Element|elements]] of $\map {\MM_S} {m, n}$ such that: :$a_{i j} = a$, $b_{i j} = b$ where: :$a_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf A$ :$b_{i j}$ is the $\tuple {i, j}$th [[Definition:Element of Matrix|element]] of $\mathbf B$ Then: :$a_{i j} \cdot b_{i j} \ne b_{i j} \cdot a_{i j}$ That is: :$\mathbf A \circ \mathbf B \ne \mathbf B \circ \mathbf A$ because they differ (at least) at [[Definition:Index of Matrix Element|indices]] $\tuple {i, j}$. That is, $\circ$ is not [[Definition:Commutative Operation|commutative]] on $\map {\MM_S} {m, n}$. {{qed}}	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\map e {\mathbf A}$ be the [[Definition:Elementary Row Operation|elementary row operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf A' \in \map \MM {m, n}$. {{begin-axiom}} {{axiom | n = \text {ERO} 2 | t = For some $\lambda \in K$, add $\lambda$ [[Definition:Matrix Scalar Product|times]] [[Definition:Row of Matrix|row]] $k$ to [[Definition:Row of Matrix|row]] $l$ | m = r_k \to r_k + \lambda r_l }} {{end-axiom}} Let $\map {e'} {\mathbf A'}$ be the [[Definition:Inverse of Elementary Row Operation|inverse]] of $e$. Then $e'$ is the [[Definition:Elementary Row Operation|elementary row operation]]: :$e' := r'_k \to r'_k - \lambda r'_l$	1
The [[Definition:Quaternion|quaternions]] $\Bbb H$ are formed by the [[Definition:Quaternion/Construction from Cayley-Dickson Construction|Cayley-Dickson Construction]] from the [[Definition:Complex Number|complex numbers]] $\C$. From [[Complex Numbers form Algebra]], we have that $\C$ forms: :$(1): \quad$ An [[Definition:Associative Algebra|associative algebra]] :$(2): \quad$ A [[Definition:Commutative Algebra|commutative algebra]] :$(3): \quad$ A [[Definition:Normed Division Algebra|normed division algebra]] :$(4): \quad$ A [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. From [[Cayley-Dickson Construction forms Star-Algebra]], $\Bbb H$ is a [[Definition:Star-Algebra|$*$-algebra]]. From [[Cayley-Dickson Construction from Nicely Normed Algebra is Nicely Normed]], $\Bbb H$ is a [[Definition:Nicely Normed Star-Algebra|nicely normed $*$-algebra]]. From [[Cayley-Dickson Construction from Commutative Associative Algebra is Associative]], $\Bbb H$ is an [[Definition:Associative Algebra|associative algebra]]. Now suppose $\Bbb H$ formed a [[Definition:Commutative Algebra|commutative algebra]]. Then from [[Cayley-Dickson Construction from Real Algebra is Commutative]], that would mean $\C$ is a [[Definition:Real Algebra|real algebra]]. But from [[Complex Numbers form Algebra]] it is explicitly demonstrated that $\C$ is '''not''' a [[Definition:Real Algebra|real algebra]]. So $\Bbb H$ can not be a [[Definition:Commutative Algebra|commutative algebra]]. === Proof of Normed Division Algebra === Consider the element $\left({1, 0}\right)$ of $\C^2$. We have: {{begin-eqn}} {{eqn | o = | r = \left({x_1, x_2}\right) \times \left({1, 0}\right) | c = }} {{eqn | r = \left({x_1 \times 1 - 0 \times x_2, x_1 \times 0 + x_2 \times 1}\right) | c = }} {{eqn | r = \left({x_1, x_2}\right) | c = }} {{end-eqn}} As $\times$ is [[Definition:Commutative Operation|commutative]] on $\C$, it follows that $\left({1, 0}\right) \times \left({x_1, x_2}\right) = \left({x_1, x_2}\right)$. So $\left({1, 0}\right) \in \C^2$ functions as a [[Definition:Unit of Algebra|unit]]. That is, $\left({\C^2, \times}\right)$ is a [[Definition:Unitary Algebra|unitary algebra]]. <!-- From [[Inverses for Complex Multiplication]], every element of $\left({\C, \times}\right)$ except $0$ has a [[Definition:Multiplicative Inverse|multiplicative inverse]]. So $\left({\C, \times}\right)$ is a [[Definition:Division Algebra|division algebra]] and hence a [[Definition:Unitary Division Algebra|unitary division algebra]]. --> We define a [[Definition:Norm on Vector Space|norm]] on $\left({\C^2, \times}\right)$ by: :$\forall \mathbf a = \left({a_1, a_2}\right) \in \C^2: \left \Vert {\mathbf a} \right \Vert = \sqrt {a_1^2 + a_2^2}$ This is a [[Definition:Norm on Vector Space|norm]] because: : $(1): \quad \left \Vert \mathbf x \right \Vert = 0 \iff \mathbf x = \mathbf 0$ : $(2): \quad \left \Vert \lambda \mathbf x \right \Vert = \left \vert \lambda \right \vert \left \Vert x \right \Vert$ : $(3): \quad \left \Vert x - y \right \Vert \le \left \Vert x - z \right \Vert + \left \Vert z - y \right \Vert$ It also follows that: : $\left \Vert x \times y \right \Vert = \left \vert x \times y \right \vert = \left \vert x \right \vert \times \left \vert y \right \vert = \left \Vert x \right \Vert \times \left \Vert y \right \Vert$ and so $\left({\C^2, \times}\right)$ is a [[Definition:Normed Division Algebra|normed division algebra]]. {{Proofread|Make sure the above are all proved properly.}} {{qed}}	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $U \in B \struct {H, K}$ be a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. Then the following are equivalent: :$(1): \quad U$ is an [[Definition:Isomorphism (Hilbert Spaces)|isomorphism]] :$(2): \quad U$ is [[Definition:Invertible Mapping|invertible]] and $U^{-1} = U^*$, where $U^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $U$.	1
Let $M$ be a [[Definition:Riemannian Manifold|Riemannian manifold]] of [[Definition:Dimension of Riemannian Manifold|dimension]] $2$. The [[Definition:Gaussian Curvature|Gaussian curvature]] on $M$ is zero {{iff}} the [[Definition:Riemannian Metric|Riemannian metric]] on $\mathcal M$ is the same as the [[Definition:Euclidean Metric|Euclidean metric]].	1
Let $\PP$ be the [[Definition:Valuation Ideal Induced by Non-Archimedean Norm|valuation ideal induced]] by the [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$, that is: :$\PP = \set{x \in R : \norm{x} \lt 1}$ By [[Valuation Ideal is Maximal Ideal of Induced Valuation Ring|Valuation Ideal is Maximal Ideal of Induced Valuation Ring]] then: :$\PP$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] of $\OO$. Let $J \subsetneq \OO$ be any [[Definition:Maximal Left Ideal of Ring|maximal left ideal]] of $\OO$. Let $x \in \OO \setminus \PP$. {{AimForCont}} $x \in J$. By [[Properties of Norm on Division Ring/Norm of Inverse|Norm of Inverse]] then: :$\norm {x^{-1}} = 1 / \norm x = 1 / 1 = 1$ Hence: :$x^{-1} \in \OO$ Since $J$ is a [[Definition:Left Ideal|left ideal]] then: :$x^{-1} x = 1_R \in J$ Thus: :$\forall y \in \OO: y \cdot 1_R = y \in J$ That is: :$J = \OO$ This [[Definition:Contradiction|contradicts]] the assumption that $J \ne \OO$. So: :$x \notin J$ Hence: :$\paren {\OO \setminus \PP} \cap J = \O$ That is: :$J \subseteq \PP$ Since $J$ and $\PP$ are both [[Definition:Maximal Left Ideal of Ring|maximal left ideals]] then: :$J = \PP$ The result follows. {{qed}}	1
Let $z = a + i b \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\cmod z$ be the [[Definition:Complex Modulus|modulus]] of $z$. Then: :$\cmod z \ge 0$	1
Let $\mathbf B = \sqbrk b_n$ denote the [[Definition:Inverse Matrix|inverse]] of a [[Definition:Square Matrix|square matrix]] $\mathbf A$ of [[Definition:Order of Square Matrix|order]] $n$. Let $\mathbf A$ be such that it has a row or column of all ones. Then the sum of elements in $\mathbf B$ is one: :$\displaystyle \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n b_{ij} = 1$	1
By definition of [[Definition:Dot Product/Complex/Definition 2|complex dot product]]: :$z_1 \circ z_2 = \cmod {z_1} \, \cmod {z_2} \cos \theta$ :$\cmod {z_1}$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z_1$ :$\theta$ denotes the [[Definition:Angle|angle]] from $z_1$ to $z_2$, measured in the [[Definition:Positive Direction|positive direction]]. === Necessary Condition === Let $z_1$ and $z_2$ be [[Definition:Perpendicular|perpendicular]]. Then either $\theta = 90^\circ$ or $\theta = 270^\circ$. Either way, from [[Cosine of Right Angle]] or [[Cosine of Three Right Angles]]: :$\cos \theta = 0$ and so: :$\cmod {z_1} \, \cmod {z_2} \cos \theta = 0$ Hence by definition: :$z_1 \circ z_2 = 0$ {{qed|lemma}} === Sufficient Condition === Let $z_1 \circ z_2 = 0$. Then by definition: :$\cmod {z_1} \, \cmod {z_2} \cos \theta = 0$ As neither $z_1 = 0$ or $z_2 = 0$ it follows that $\cos \theta = 0$. From [[Cosine of Half-Integer Multiple of Pi]] it follows that either: :$\theta = 90^\circ$ :$\theta = 270^\circ$ or: :$\theta$ is either of the above plus a full circle. That is, $z_1$ and $z_2$ are [[Definition:Perpendicular|perpendicular]]. {{qed}}	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$. Let $\mathcal A$, $\mathcal B$ and $\mathcal C$ be [[Definition:Ordered Basis|ordered bases]] of $M$. Let $\mathbf M_{\mathcal A,\mathcal B}$, $\mathbf M_{\mathcal B,\mathcal C}$ and $\mathbf M_{\mathcal A,\mathcal C}$ be the [[Definition:Change of Basis Matrix|change of basis matrices]] from $\mathcal A$ to $\mathcal B$, $\mathcal B$ to $\mathcal C$ and $\mathcal A$ to $\mathcal C$ respectively. Then $\mathbf M_{\mathcal A,\mathcal C} = \mathbf M_{\mathcal A,\mathcal B} \cdot \mathbf M_{\mathcal B,\mathcal C}$	1
From {{Module-axiom|1}}, $y \to \lambda \circ y$ is an [[Definition:Endomorphism|endomorphism]] of $\struct {G, +_G}$. From {{Module-axiom|2}}, $\mu \to \mu \circ x$ is a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]] from $\struct {R, +_R}$ to $\struct {G, +_G}$. The result follows from [[Homomorphism with Identity Preserves Inverses]]. {{qed}}	1
$\struct {S, \cdot}$ is a [[Definition:Semigroup|semigroup]] and is therefore [[Definition:Closed Algebraic Structure|closed]] and [[Definition:Associative|associative]]. As $\struct {S, \cdot}$ is [[Definition:Closed Algebraic Structure|closed]], then so is $\struct {\map {\MM_S} {m, n}, \circ}$ from [[Closure of Hadamard Product]]. As $\struct {S, \cdot}$ is [[Definition:Associative|associative]], then so is $\struct {\map {\MM_S} {m, n}, \circ}$ from [[Associativity of Hadamard Product]]. Thus if $\struct {S, \cdot}$ is a [[Definition:Semigroup|semigroup]] then so is $\struct {\map {\MM_S} {m, n}, \circ}$. If $\struct {S, \cdot}$ is [[Definition:Commutative Algebraic Structure|commutative]], then so is $\struct {\map {\MM_S} {m, n}, \circ}$ from [[Commutativity of Hadamard Product]]. Thus if $\struct {S, \cdot}$ is a [[Definition:Commutative Semigroup|commutative semigroup]] then so is $\struct {\map {\MM_S} {m, n}, \circ}$. Let $\struct {S, \cdot}$ be a [[Definition:Monoid|monoid]], with [[Definition:Identity Element|identity]] $e$. Then from [[Zero Matrix is Identity for Hadamard Product]], $\struct {\map {\MM_S} {m, n}, \circ}$ also has an [[Definition:Identity Element|identity]] and is therefore also a [[Definition:Monoid|monoid]]. {{Qed}} [[Category:Hadamard Product]] 5bd25x7uey0z2g4kzye6z130c57p57z	1
Let $a_1, a_2 \in \R^2$ such that $\left\{{a_1, a_2}\right\}$ forms a [[Definition:Linearly Independent Set|linearly independent set]]. Then $\left({a_1, a_2}\right)$ is an [[Definition:Ordered Basis|ordered basis]] for the [[Definition:Vector Space|$\R$-vector space]] $\R^2$. Hence the points on the [[Definition:Plane|plane]] can be uniquely identified by means of [[Definition:Linear Combination|linear combinations]] of $a_1$ and $a_2$.	1
=== Norm Axiom $(\text N 1)$ === Let $x \in \ell^p$ with $1 \le p < \infty$. By definition of [[Definition:P-Norm|$p$-norm]]: :$\ds \norm {\mathbf x}_p = \paren {\sum_{n \mathop = 0}^\infty \size {x_n}^p}^{1/p}$ The [[Definition:Complex Modulus|complex modulus]] of $x_n$ is [[Definition:Real Number|real]] and [[Complex Modulus is Non-Negative|non-negative]]. We have the results: :[[Sum of Non-Negative Reals is Non-Negative]] :[[Power of Positive Real Number is Positive]] :[[Definition:Power of Zero|Zero Raised to Positive Power is Zero]] Hence, $\norm {\mathbf x}_p \ge 0$. Suppose $\norm {\mathbf x}_p = 0$. Then: {{begin-eqn}} {{eqn | l = \norm {\mathbf x}_p | r = 0 }} {{eqn | ll= \leadsto | l = \sum_{n \mathop = 0}^\infty \size {x_n}^p | r = 0 | c = raising to power $p > 0$ }} {{eqn | ll= \leadsto | l = \size {x_n} | r = 0 | c = [[Sum of Non-Negatives vanishes iff Summands vanish]] }} {{eqn | ll= \leadsto | l = x_n | r = 0 | c = [[Complex Modulus equals Zero iff Zero]] }} {{eqn | ll= \leadsto | l = \bf x | r = \sequence 0_{n \mathop \in \N} }} {{end-eqn}} Thus [[Definition:Norm Axioms|norm axiom $(\text N 1)$]] is satisfied. {{qed|lemma}} === Norm Axiom $(\text N 2)$ === Suppose $\alpha \in \set {\R, \C}$. {{begin-eqn}} {{eqn| l = \norm {\alpha \cdot \mathbf x}_p | r = \paren {\sum_{n \mathop = 0}^\infty \size {\alpha x_n}^p }^{\frac 1 p} }} {{eqn| r = \paren {\sum_{n \mathop = 0}^\infty \size \alpha^p \size {x_n}^p }^{\frac 1 p} }} {{eqn| r = \size {\alpha} \paren {\sum_{n \mathop = 0}^\infty \size {x_n}^p }^{\frac 1 p} }} {{eqn| r = \size {\alpha} \norm {\mathbf x}_p }} {{end-eqn}} {{qed|lemma}} === Norm Axiom $(\text N 3)$ === From [[P-Norm is Norm]] we have that: :$\ds \paren {\sum_{n \mathop = 0}^d \size {x_n + y_n}^p}^{\frac 1 p} \le \paren {\sum_{n \mathop = 0}^d \size {x_n}^p }^{\frac 1 p} + \paren {\sum_{n \mathop = 0}^d \size {y_n}^p }^{\frac 1 p}$ $\map f z = z^{\frac 1 p}$ is a [[Definition:Continuous Function|continuous function]] for $z \ge 0$ and $p > 0$. For $\mathbf x \in \ell^p$, changing $d$ is equivalent to changing $z$ in the interval $0 \le z < \infty$. Take the [[Limit of Composite Function|composite limit]] $d \to \infty$. Then: :$\norm {\mathbf x + \mathbf y}_p \le \norm {\mathbf x}_p + \norm {\mathbf y}_p$ {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Division Ring|division ring]] with [[Definition:Ring Zero|zero]] $0_R$ and [[Definition:Unity of Ring|unity]] $1_R$. Let $\norm {\,\cdot\,}$ be a [[Definition:Norm on Division Ring|norm]] on $R$. Let $x, y \in R$. Then the following hold:	1
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $M_1,\ldots,M_n$ be [[Definition:Free Module|free $R$-modules]]. Let $B_1,\ldots,B_n$ be [[Definition:Basis_(Linear_Algebra)|bases]] of $M_1,\ldots,M_n$. Let $N$ be an $R$-module. Let $f:B_1\times\cdots\times B_n\to N$ be a [[Definition:Function|function]]. Then there exists a [[Definition:Unique|unique]] [[Definition:Multilinear Mapping|multilinear map]] $\phi:M_1\times\cdots\times M_n\to N$ such that $\phi(b)=f(b)$ for all $b\in B_1\times\cdots\times B_n$.	1
By [[Elementary Matrix corresponding to Elementary Row Operation/Scale Row|Elementary Matrix corresponding to Elementary Row Operation: Scale Row]], the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e_1$ is of the form: :$E_{a b} = \begin {cases} \delta_{a b} & : a \ne k \\ \lambda \cdot \delta_{a b} & : a = k \end{cases}$ where: :$E_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf E_1$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$ :$\delta_{a b}$ is the [[Definition:Kronecker Delta|Kronecker delta]]: ::$\delta_{a b} = \begin {cases} 1 & : \text {if $a = b$} \\ 0 & : \text {if $a \ne b$} \end {cases}$ Thus when $a \ne b$, $E_{a b} = 0$. This means that $\mathbf E_1$ is a [[Definition:Diagonal Matrix|diagonal matrix]]. {{begin-eqn}} {{eqn | l = \displaystyle \map \det {\mathbf E_1} | r = \prod_i E_{i i} | c = [[Determinant of Diagonal Matrix]] | cc= where the [[Definition:Index Variable of Indexed Product|index variable]] $i$ ranges over the [[Definition:Order of Square Matrix|order]] of $\mathbf E_1$ }} {{eqn | r = \prod_i \paren {\begin {cases} 1 & : i \ne k \\ \lambda & : a = k \end{cases} } | c = }} {{eqn | r = \prod_{i \mathop \ne k} 1 \times \prod_{i \mathop = k} \lambda | c = }} {{eqn | r = 1 \times \lambda | c = }} {{eqn | r = \lambda | c = }} {{end-eqn}} {{qed}}	1
:$\sequence {x_n + y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] and $\displaystyle \lim_{n \mathop \to \infty} \paren {x_n + y_n} = l + m$	1
Because the [[Definition:Determinant of Matrix|determinants]] of the [[Definition:Element|elements]] of $\SL {n, K}$ are not $0_K$, they are [[Definition:Invertible Matrix|invertible]]. So $\SL {n, K}$ is a [[Definition:Subset|subset]] of $\GL {n, K}$. Now we need to show that $\SL {n, K}$ is a [[Definition:Subgroup|subgroup]] of $\GL {n, K}$. Let $\mathbf A$ and $\mathbf B$ be elements of $\SL {n, K}$. As $\mathbf A$ is [[Definition:Invertible Matrix|invertible]] we have that it has an [[Definition:Inverse Matrix|inverse]] $\mathbf A^{-1} \in \GL {n, K}$. From [[Determinant of Inverse Matrix]]: :$\map \det {\mathbf A^{-1} } = \dfrac 1 {\map \det {\mathbf A} }$ and so: :$\map \det {\mathbf A^{-1} } = 1$ So $\mathbf A^{-1} \in \SL {n, K}$. Also, from [[Determinant of Matrix Product]]: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \map \det {\mathbf B} = 1$ Hence the result from the [[Two-Step Subgroup Test]]. {{qed}}	1
{{ProofWanted}} [[Category:Free Modules]] ngdn9dpydyzxfulu90d31j8vry2dutc	1
Let $\mathbf A$ be an [[Definition:Matrix|$m \times n$ matrix]]. Let $\mathbf B$ be an [[Definition:Matrix|$n \times m$ matrix]]. Let $m > n$. Then: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = 0$	1
Let $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ satisfy: :for all sequences $\sequence {x_n}$ in $R:\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm{\, \cdot \,}_1 \iff \sequence {x_n}$ is a [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l$ in $\norm {\, \cdot \,}_2$ Then for all sequences $\sequence {x_n}$ in $R$: :$\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_1 \iff \sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_2$	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative Operation|commutative]] [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathbf A = \sqbrk a_{m n}$ be an [[Definition:Matrix|$m \times n$ matrix]] over $R$. Let $\mathbf B = \sqbrk b_{n p}$ be an [[Definition:Matrix|$n \times p$ matrix]] over $R$. Let $\hat o_1, \ldots, \hat o_{\hat n}$ be a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] that can be performed on a matrix over $R$ with $m$ [[Definition:Row of Matrix|rows]]. Let $\mathbf A'$ denote the $m \times n$-matrix that results from using $\hat o_1, \ldots, \hat o_{\hat n}$ on $\mathbf A$. Let $\mathbf C = \mathbf A \mathbf B$ be the [[Definition:Matrix Product (Conventional)|matrix product]] of $\mathbf A$ and $\mathbf B$. Let $\mathbf C'$ denote the $m \times p$-matrix that results from using $\hat o_1, \ldots, \hat o_{\hat n}$ on $\mathbf C$. Then: :$\mathbf C' = \mathbf A' \mathbf B$	1
Let $z_1$ and $z_2$ denote [[Definition:Complex Number as Vector|complex numbers in vector form]]. Let $\map {\pr_1} {z_1, z_2}$ denote the [[Definition:Projection (Analytic Geometry)|projection]] of $z_1$ on $z_2$. {{explain|We really need another page to explain the concept of [[Definition:Projection (Analytic Geometry)]] in the context of the [[Definition:Complex Plane]]}} Then: :$\cmod {\map {\pr_1} {z_1, z_2} } = \dfrac {\cmod {z_1 \circ z_2} } {\cmod {z_2} }$ where: :$z_1 \circ z_2$ denotes [[Definition:Complex Dot Product|complex dot product]] :$\cmod {z_2}$ denotes [[Definition:Complex Modulus|complex modulus]].	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$. Let $\displaystyle \lim_{n \mathop \to \infty} \norm {x_{n + 1} - x_n} = 0$. Then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]].	1
Assume values $\set { x_1,\ldots,x_n,y_1,\ldots,y_n }$ are distinct in matrix {{begin-eqn}} {{eqn | l = C | r = \paren {\begin{smallmatrix} \dfrac {1} {x_1 - y_1} & \dfrac {1} {x_1 - y_2} & \cdots & \dfrac {1} {x_1 - y_n} \\ \dfrac {1} {x_2 - y_1} & \dfrac 1 {x_2 - y_2} & \cdots & \dfrac {1} {x_2 - y_n} \\ \vdots & \vdots & \cdots & \vdots \\ \dfrac {1} {x_n - y_1} & \dfrac {1} {x_n - y_2} & \cdots & \dfrac {1} {x_n - y_n} \\ \end{smallmatrix} } | c = [[Definition:Cauchy Matrix|Cauchy matrix]] of order $n$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = C | r = -P V_x^{-1} V_y Q^{-1} | c = [[Definition:Vandermonde Matrix|Vandermonde matrix]] identity for a [[Definition:Cauchy Matrix|Cauchy matrix]] }} {{end-eqn}}	1
Let $x \in R_2$. By definition of the [[Definition:Completion (Normed Division Ring)|completion]] $\struct {R_2, \norm {\, \cdot \,}_2 }$: :$\map {\phi^\to} {R_1}$ is a [[Definition:Everywhere Dense |dense subset]] of $\struct {R_2, \norm {\, \cdot \,}_2 }$. By the definition of a [[Definition:Everywhere Dense|dense subset]]: :$\map \cl {\map {\phi^\to} {R_1}} = R_2$ By [[Closure of Subset of Metric Space by Convergent Sequence]]: :there exists a [[Definition:Sequence|sequence]] $\sequence {y_n} \subseteq \map {\phi^\to} {R_1}$ that [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $x$ That is: :$\displaystyle \lim_{n \mathop \to \infty} y_n = x$ From [[Injection to Image is Bijection]], we can define: :$\forall n \in \N : x_n = \map {\phi^{-1}} {y_n}$ Then $\sequence{x_n}$ is a [[Definition:Sequence|sequence]] in $R_1$: :$\displaystyle \lim_{n \mathop \to \infty} \map \phi {x_n} = \lim_{n \mathop \to \infty} y_n = x$ {{qed}} [[Category:Completion of Normed Division Ring]] 618jsapz7vqhlo8rv16ux84u37xkck8	1
For ease of presentation, let $\breve {\mathbf X}$ be the inverse of $\mathbf X$. We have that $\mathbf A$ can be transformed into $\mathbf I$ by a sequence of elementary row operations. By repeated application of [[Elementary Row Operations as Matrix Multiplications]], we can write this assertion as: {{begin-eqn}} {{eqn | l = \mathbf E_t \mathbf E_{t - 1} \cdots \mathbf E_2 \mathbf E_1 \mathbf A | r = \mathbf I }} {{end-eqn}} From [[Elementary Row Matrix is Invertible]]: :$\mathbf E_1, \dotsc, \mathbf E_t \in \GL {n, \R}$ {{MissingLinks|$\GL {n, \R}$, and explain the significance of this. It's General Linear Group, clearly.}} We can multiply on the left both sides of this equation by: {{begin-eqn}} {{eqn | l = \breve {\mathbf E}_1 \breve {\mathbf E}_2 \cdots \breve {\mathbf E}_{t - 1} \breve {\mathbf E}_t \mathbf E_t \mathbf E_{t - 1} \cdots \mathbf E_2 \mathbf E_1 \mathbf A | r = \breve {\mathbf E}_1 \breve {\mathbf E}_2 \cdots \breve {\mathbf E}_{t - 1} \breve {\mathbf E}_t \mathbf I }} {{eqn | ll= \leadsto | l = \mathbf {II} \cdots \mathbf {IIA} | r = \breve {\mathbf E}_1 \breve {\mathbf E}_2 \cdots \breve {\mathbf E}_{t - 1} \breve {\mathbf E}_t \mathbf I | c = definition of inverse }} {{eqn | ll= \leadsto | l = \mathbf A | r = \breve {\mathbf E}_1 \breve {\mathbf E}_2 \cdots \breve {\mathbf E}_{t - 1} \breve {\mathbf E}_t | c = definition of identity }} {{eqn | ll= \leadsto | l = \breve {\mathbf A} | r = \breve {\breve {\mathbf E} }_t \breve {\breve {\mathbf E} }_{t - 1} \cdots \breve {\breve {\mathbf E} }_2 \breve {\breve {\mathbf E} }_1 | c = [[Inverse of Matrix Product]], [[Axiom:Leibniz's Law|Leibniz's Law]] }} {{eqn | r = \mathbf E_t \mathbf E_{t - 1} \cdots \mathbf E_2 \mathbf E_1 | c = [[Inverse of Group Inverse]] }} {{eqn | r = \mathbf E_t \mathbf E_{t - 1} \cdots \mathbf E_2 \mathbf E_1 \mathbf I | c = {{Defof|Unit Matrix}} }} {{end-eqn}} By repeated application of [[Elementary Row Operations as Matrix Multiplications]], each $\mathbf E_n$ on the right hand side corresponds to an elementary row operation. Hence the result. {{qed}} {{proofread}} [[Category:Unit Matrices]] [[Category:Inverse Matrices]] [[Category:Elementary Row Operations]] fqmfgxd8fey29torcvzebsnm83fzq1f	1
{{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = \sum_{i \mathop = 1}^n u_i^2 | r = 0 | c = {{Defof|Dot Product|index = 1}} }} {{eqn | ll= \leadstoandfrom | lo= \forall i: | l = u_i | r = 0 | c = }} {{eqn | ll= \leadstoandfrom | l = \mathbf u | r = \bszero | c = {{Defof|Zero Vector}} }} {{end-eqn}} {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring |normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean norm]] $\norm {\,\cdot\,}$. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $R$. Then: :$\sequence {x_n}$ is a [[Definition:Cauchy Sequence (Normed Division Ring)|Cauchy sequence]] {{iff}} $\displaystyle \lim_{n \mathop \to \infty} \norm {x_{n + 1} - x_n} = 0$.	1
Utilizing the [[Definition:Vector Space Axioms|vector space axioms]]: {{begin-eqn}} {{eqn | l = \mathbf a + \mathbf c | r = \mathbf b + \mathbf c }} {{eqn | ll= \leadsto | l = \paren {\mathbf a + \mathbf c} - \mathbf c | r = \paren {\mathbf b + \mathbf c} - \mathbf c }} {{eqn | ll= \leadsto | l = \mathbf a + \paren {\mathbf c - \mathbf c} | r = \mathbf b + \paren {\mathbf c - \mathbf c} }} {{eqn | ll= \leadsto | l = \mathbf a + \mathbf 0 | r = \mathbf b + \mathbf 0 }} {{eqn | ll= \leadsto | l = \mathbf a | r = \mathbf b }} {{end-eqn}} {{qed}}	1
An element of $F \sqbrk X$ whose [[Definition:Degree (Polynomial)|degree]] is $0$ is merely an element of $F$. But note that $0_F$, considered as an element of $F \sqbrk X$, has a [[Definition:Degree (Polynomial)|degree]] which is not defined, so the [[Definition:Null Polynomial over Ring|null polynomial]] is seen to be excluded. Any element $a$ of $F$ has an [[Definition:Inverse Element|inverse]] $1_F / a$. So all the elements of $F \sqbrk X$ whose [[Definition:Degree (Polynomial)|degree]] is $0$ are [[Definition:Unit of Ring|units]] of $F$ and hence of $F \sqbrk X$. Now suppose $\map a X \in F \sqbrk X$ is a [[Definition:Unit of Ring|unit]] of $F \sqbrk X$. Then $\map a X \, \map q X = 1_F$ for some $\map q X \in F \sqbrk X$. Neither $\map a X$ nor $\map q X$ can be [[Definition:Null Polynomial over Ring|null]]. So by [[Properties of Degree]] $\map \deg {\map a X} + \map \deg {\map q X} = 0$. So $\map \deg {\map a X} = 0$. {{qed}}	1
Let $z_1$ and $z_2$ be represented by the [[Definition:Point|points]] $A = \tuple {x_1, y_1}$ and $B = \tuple {x_2, y_2}$ respectively in the [[Definition:Complex Plane|complex plane]]. Let $z$ be an arbitrary [[Definition:Point|point]] on $L$ represented by the [[Definition:Point|point]] $P$. :[[File:Line-in-Complex-Plane-through-Two-Points.png|500px]] As $AP$ and $AB$ are collinear: :$m AP = n PB$ and so: :$m \paren {z - z_1} = n \paren {z_2 - z_1}$ The result follows. {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$, $\sequence {y_n} $ be [[Definition:Sequence|sequences in $R$]]. Let $\sequence {x_n}$ and $\sequence {y_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\, \cdot \,}$ to the following [[Definition:Limit of Sequence (Normed Division Ring)|limits]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = l$ :$\displaystyle \lim_{n \mathop \to \infty} y_n = m$ Let $\lambda, \mu \in R$. Then the following results hold: === [[Combination Theorem for Sequences/Normed Division Ring/Sum Rule|Sum Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Sum Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Difference Rule|Difference Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Difference Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Multiple Rule|Multiple Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Multiple Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Combined Sum Rule|Combined Sum Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Combined Sum Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|Product Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Product Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Inverse Rule|Inverse Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Inverse Rule}} === [[Combination Theorem for Sequences/Normed Division Ring/Quotient Rule|Quotient Rule]] === {{:Combination Theorem for Sequences/Normed Division Ring/Quotient Rule}}	1
Let $I = \closedint a b$ be a [[Definition:Closed Interval|closed interval]]. Let $\struct {\map \CC I, +, \, \cdot \,}_\R$ be the [[Space of Continuous on Closed Interval Real-Valued Functions with Pointwise Addition and Pointwise Scalar Multiplication form Vector Space|vector space of real-valued functions, continuous on]] $I$. Let $\map x t \in \map \CC I$ be a [[Definition:Continuous Real Function on Subset|continuous real function]]. Let $\size {\, \cdot \,}$ be the [[Definition:Absolute Value|absolute value]]. Let $\norm {\, \cdot \,}_\infty$ be the [[Definition:Supremum Norm/Continuous on Closed Interval Real-Valued Function|supremum norm on real-valued functions, continuous on]] $I$. Then $\norm {\, \cdot \,}_\infty$ is a [[Definition:Norm on Vector Space|norm]] over $\struct {\map \CC I, +, \, \cdot \,}_\R$.	1
Let $T = \struct {S, \tau_p}$ be a [[Definition:Particular Point Topology|particular point space]]. Then $T$ is [[Definition:Separable Space|separable]].	1
=== Construction of Complex Vector Space === From the definition, a [[Definition:Vector Space|vector space]] is a [[Definition:Unitary Module|unitary module]] whose [[Definition:Scalar Ring of Unitary Module|scalar ring]] is a [[Definition:Field (Abstract Algebra)|field]]. In order to call attention to the precise scope of the operators, let [[Definition:Complex Addition|complex addition]] and [[Definition:Complex Multiplication|complex multiplication]] be expressed as $+_\C$ and $\times_\C$ respectively. Then we can express the [[Definition:Field of Complex Numbers|field of complex numbers]] as $\struct {\C, +_\C, \times_\C}$. From [[Complex Numbers under Addition form Abelian Group]], $\struct {\C, +}$ is a [[Definition:Group|group]]. Again, in order to call attention to the precise scope of the operator, let [[Definition:Complex Addition|complex addition]] be expressed on $\struct {\C, +}$ as $+_G$. That is, the [[Definition:Group|group]] under consideration is $\struct {\C, +_G}$. Consider the [[Definition:Cartesian Product|cartesian product]]: :$\displaystyle \C^n = \prod_{i \mathop = 1}^n \struct {\C, +_G} = \underbrace {\struct {\C, +_G} \times \cdots \times \struct {\C, +_G} }_{n \text{ copies} }$ Let: :$\mathbf a = \tuple {a_1, a_2, \ldots, a_n}$ :$\mathbf b = \tuple {b_1, b_2, \ldots, b_n}$ be arbitrary elements of $\C^n$. Let $\lambda$ be an arbitrary element of $\C$. Let $+$ be the [[Definition:Binary Operation|binary operation]] defined on $\C^n$ as: :$\mathbf a + \mathbf b = \tuple {a_1 +_G b_1, a_2 +_G b_2, \ldots, a_n +_G b_n}$ Also let $\cdot$ be the [[Definition:Binary Operation|binary operation]] defined on $\C \times \C^n$ as: :$\lambda \cdot \mathbf a = \tuple {\lambda \times_\C a_1, \lambda \times_\C a_2, \ldots, \lambda \times_\C a_n}$ In this context, $\lambda \times_\C a_j$ is defined as [[Definition:Complex Multiplication|complex multiplication]], as is appropriate (both $\lambda$ and $a_j$ are [[Definition:Complex Number|complex numbers]]). With this set of definitions, the structure $\struct {\C^n, +, \cdot}$ is a [[Definition:Vector Space|vector space]], as is shown in [[Complex Vector Space is Vector Space#Proof of Complex Vector Space|Proof of Complex Vector Space]] below. === Proof of Complex Vector Space === In order to show that $\struct {\C^n, +, \cdot}$ is a [[Definition:Vector Space|vector space]], we need to show that: $\forall \mathbf x, \mathbf y \in \C^n, \forall \lambda, \mu \in \C$: :$(1): \quad \lambda \cdot \paren {\mathbf x + \mathbf y} = \paren {\lambda \cdot \mathbf x} + \paren {\lambda \cdot \mathbf y}$ :$(2): \quad \paren {\lambda +_\C \mu} \cdot x = \paren {\lambda \cdot \mathbf x} + \paren {\mu \cdot \mathbf x}$ :$(3): \quad \paren {\lambda \times_\C \mu} \cdot x = \lambda \cdot \paren {\mu \cdot \mathbf x}$ :$(4): \quad \forall \mathbf x \in \C^n: 1 \cdot \mathbf x = \mathbf x$. where $1$ in this context means $1 + 0 i$, as derived in [[Complex Multiplication Identity is One]]. From [[External Direct Product of Groups is Group]], we have that $\struct {\C^n, +}$ is a [[Definition:Group|group]] in its own right. Let: :$\mathbf x = \tuple {x_1, x_2, \ldots, x_n}$ :$\mathbf y = \tuple {y_1, y_2, \ldots, y_n}$ Checking the criteria in turn: $(1): \quad \lambda \cdot \paren {\mathbf x + \mathbf y} = \paren {\lambda \cdot \mathbf x} + \paren {\lambda \cdot \mathbf y}$: {{begin-eqn}} {{eqn | l = \lambda \cdot \paren {\mathbf x + \mathbf y} | r = \tuple {\lambda \times_\C \paren {x_1 +_G y_1}, \lambda \times_\C \paren {x_2 +_G y_2}, \ldots, \lambda \times_\C \paren {x_n +_G y_n} } | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \tuple {\paren {\lambda \times_\C x_1 +_G \lambda \times_\C y_1}, \paren {\lambda \times_\C x_2 +_G \lambda \times_\C y_2}, \ldots, \paren {\lambda \times_\C x_n +_G \lambda \times_\C y_n} } | c = [[Complex Multiplication Distributes over Addition|$\times_\C$ distributes over $+_G$]] }} {{eqn | r = \tuple {\lambda \times_\C x_1, \lambda \times_\C x_2, \ldots, \lambda \times_\C x_n} + \tuple {\lambda \times_\C y_1, \lambda \times_\C y_2, \ldots, \lambda \times_\C y_n} | c = Definition of $+$ over $\C^n$ }} {{eqn | r = \lambda \cdot \tuple {x_1, x_2, \ldots, x_n} + \lambda \cdot \tuple {y_1, y_2, \ldots, y_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \lambda \cdot \mathbf x + \lambda \cdot \mathbf y | c = Definition of $\mathbf x$ and $\mathbf y$ }} {{end-eqn}} So $(1)$ has been shown to hold. $(2): \quad \paren {\lambda +_\C \mu} \cdot x = \paren {\lambda \cdot \mathbf x} + \paren {\mu \cdot \mathbf x}$: {{begin-eqn}} {{eqn | l = \paren {\lambda +_\C \mu} \cdot x | r = \tuple {\paren {\lambda +_\C \mu} \times_\C x_1, \paren {\lambda +_\C \mu} \times_\C x_2, \ldots, \paren {\lambda +_\C \mu} \times_\C x_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \tuple {\paren {\lambda \times_\C x_1 +_G \mu \times_\C x_1}, \paren {\lambda \times_\C x_2 +_G \mu \times_\C x_2}, \ldots, \paren {\lambda \times_\C x_n +_G \mu \times_\C x_n} } | c = [[Complex Multiplication Distributes over Addition|$\times_\C$ distributes over $+_\C$]], and $+_\C$ is the same operation as $+_G$ }} {{eqn | r = \tuple {\lambda \times_\C x_1, \lambda \times_\C x_2, \ldots, \lambda \times_\C x_n} + \tuple {\mu \times_\C x_1, \mu \times_\C x_2, \ldots, \mu \times_\C x_n} | c = Definition of $+$ over $\C^n$ }} {{eqn | r = \lambda \cdot \tuple {x_1, x_2, \ldots, x_n} + \mu \cdot \tuple {x_1, x_2, \ldots, x_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \lambda \cdot \mathbf x + \mu \cdot \mathbf x | c = Definition of $\mathbf x$ and $\mathbf y$ }} {{end-eqn}} So $(2)$ has been shown to hold. $(3): \quad \paren {\lambda \times_\C \mu} \cdot x = \lambda \cdot \paren {\mu \cdot \mathbf x}$: {{begin-eqn}} {{eqn | l = \paren {\lambda \times_\C \mu} \cdot x | r = \tuple {\paren {\lambda \times_\C \mu} \times_\C x_1, \paren {\lambda \times_\C \mu} \times_\C x_2, \ldots, \paren {\lambda \times_\C \mu} \times_\C x_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \tuple {\lambda \times_\C \paren {\mu \times_\C x_1}, \lambda \times_\C \paren {\mu \times_\C x_2}, \ldots, \lambda \times_\C \paren {\mu \times_\C x_n} } | c = [[Complex Multiplication is Associative]] }} {{eqn | r = \lambda \cdot \tuple {\mu \times_\C x_1, \mu \times_\C x_2, \ldots, \mu \times_\C x_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \lambda \cdot \paren {\mu \cdot \tuple {x_1, x_2, \ldots, x_n} } | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \lambda \cdot \paren {\mu \cdot \mathbf x} | c = Definition of $\mathbf x$ }} {{end-eqn}} So $(3)$ has been shown to hold. $(4): \quad \forall \mathbf x \in \C^n: 1 \cdot \mathbf x = \mathbf x$: {{begin-eqn}} {{eqn | l = 1 \cdot \mathbf x = \mathbf x | r = \tuple {1 \times_\C x_1, 1 \times_\C x_2, \ldots, 1 \times_\C x_n} | c = Definition of $\cdot$ over $\C \times \C^n$ }} {{eqn | r = \tuple {x_1, x_2, \ldots, x_n} | c = [[Complex Multiplication Identity is One]] }} {{eqn | r = \mathbf x | c = Definition of $\mathbf x$ }} {{end-eqn}} So $(4)$ has been shown to hold. So the [[Definition:Module on Cartesian Product|$\C$-module $\C^n$]] is a [[Definition:Vector Space|vector space]], as we were to prove. {{qed}}	1
Let $\left({R,+,*}\right)$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let the [[Definition:Characteristic of Ring|characteristic]] of $R$ be $m$. Let $\left({\Z_m, +_m, \times_m}\right)$ be the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. Let $\circ$ be the [[Definition:Mapping|mapping]] from $\Z_m \times R$ to $R$ defined as: :$\forall \left[\!\left[a\right]\!\right]_m \in \Z_m: \forall x \in R: \left[\!\left[a\right]\!\right]_m \circ x = a \cdot x$ where $\left[\!\left[a\right]\!\right]_m$ is the [[Definition:Integers Modulo m|residue class]] of $a$ modulo $m$ and $a \cdot x$ is the [[Powers of Ring Elements|$a$th power of $x$]]. Then $\left({R, +, \circ}\right)_{\Z_m}$ is a [[Definition:Unitary Module|unitary $\Z_m$-module]].	1
From: :[[Integers form Ring]] :[[Rational Numbers form Ring]] :[[Real Numbers form Ring]] :[[Complex Numbers form Ring]] the [[Definition:Standard Number System|standard number systems]] $\Z$, $\Q$, $\R$ and $\C$ are [[Definition:Ring (Abstract Algebra)|rings]]. Hence we can apply [[Matrix Entrywise Addition over Ring is Commutative]]. {{qed|lemma}} The above cannot be applied to the [[Definition:Natural Numbers|natural numbers]] $\N$, as they do not form a [[Definition:Ring (Abstract Algebra)|ring]]. However, from [[Natural Numbers under Addition form Commutative Monoid]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\N, +}$ is a [[Definition:Commutative Monoid|commutative monoid]]. By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Addition|addition of numbers]]. The result follows from [[Commutativity of Hadamard Product]]. {{qed}}	1
Let $\sequence {x_n} $ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence in $R$]]. Then by definition: :$\forall \epsilon \in \R_{\gt 0}: \exists N \in \N : \forall n, m \ge N: \norm {x_n - x_m} < \epsilon$ Let $n_1$ satisfy: :$\forall n, m \ge n_1: \norm {x_n - x_m} < 1$ Then $\forall n \ge n_1$: {{begin-eqn}} {{eqn | l = \norm {x_n} | r = \norm {x_n - x_{n_1} + x_{n_1} } }} {{eqn | o = \le | r = \norm {x_n - x_{n_1} } + \norm {x_{n_1} } | c = [[Definition:Norm Axioms|Norm axiom (N3)]] (Triangle Inequality). }} {{eqn | o = \le | r = 1 + \norm {x_{n_1} } | c = as $n, n_1 \ge n_1$ }} {{end-eqn}} Let $K = \max \set {\norm {x_1}, \norm {x_2}, \dots, \norm {x_{n_1 - 1}}, 1 + \norm {x_{n_1} } }$. Then: :$\forall n < n_1: \norm {x_n} \le K$ :$\forall n \ge n_1: \norm {x_n} \le 1 + \norm {x_{n_1} } \le K$ It follows by definition that $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
Let $\struct {R, \norm{\,\cdot\,} }$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]] with [[Definition:Ring Zero|zero]] $0_R$ Let $\sequence {x_n}$ be a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] such that $\sequence {x_n}$ does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to $0_R$. Then: :$\exists N \in \N: \forall n, m \ge N: \norm {x_n} = \norm {x_m}$	1
Let $\left({H_i}\right)_{i \in I}$ be a [[Definition:Indexing Set|$I$-indexed family]] of [[Definition:Hilbert Space|Hilbert spaces]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $H = \displaystyle \bigoplus_{i \mathop \in I} H_i$ be their [[Definition:Hilbert Space Direct Sum|Hilbert space direct sum]]. Then $H$ is a [[Definition:Hilbert Space|Hilbert space]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $M$ be a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$. Denote by $M^\perp$ its [[Definition:Orthocomplement|orthocomplement]]. Then the [[Definition:Hilbert Space Direct Sum|direct sum]] $M \oplus M^\perp$ is [[Definition:Isomorphism (Hilbert Spaces)|isomorphic]] to $H$.	1
Let $\struct {X, \norm {\,\cdot\,}}$ be a [[Definition:Finite Dimensional Vector Space|finite-dimensional]] [[Definition:Normed Vector Space|normed vector space]]. A [[Definition:Subset|subset]] $K \subset X$ is [[Definition:Compact Subset of Normed Vector Space|compact]] {{iff}} $K$ is [[Definition:Closed Set in Normed Vector Space|closed]] and [[Definition:Bounded Normed Vector Space|bounded]].	1
From [[G-Submodule Test]] it suffices to prove that $\phi \sqbrk {\struct {G, \map \ker f} } \subseteq \map \ker f$. That is, it is to be shown that, if $g \in G$ and $v \in \map \ker f$, then $\map \phi {g, v} \in \map \ker f$. Assume that $g \in G$ and $v \in \map \ker f$. {{begin-eqn}} {{eqn | l = \map f {\map \phi {g, v} } | r = \map \mu {g, \map f v} | c = $f$ is a [[Definition:G-Module Homomorphism|$G$-module homomorphism]] }} {{eqn | r = \map \mu {g, 0} | c = $v \in \map \ker f$ }} {{eqn | r = 0 | c = $\mu$ is a [[Definition:Linear Group Action|linear action]] }} {{end-eqn}} Thus $\map \phi {g, v} \in \map \ker f$. Hence $\map \ker f$ is a [[Definition:G-Submodule|$G$-submodule]] of $V$. {{qed}} [[Category:Representation Theory]] 2woti1bqyrmd6d01v2dsexhnha80exm	1
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be [[Definition:R-Algebraic Structure|$R$-algebraic structures]]. Let $\phi: S \to T$ be an [[Definition:R-Algebraic Structure Homomorphism|$R$-algebraic structure homomorphism]]. Then $\phi$ is an [[Definition:R-Algebraic Structure Isomorphism|$R$-algebraic structure isomorphism]] {{iff}} $\phi$ is a [[Definition:Bijection|bijection]].	1
The [[Definition:Dimension of Vector Space|one-dimensional]] [[Definition:Vector Subspace|subspaces]] of $\R^2$ are precisely the [[Definition:Homogeneous (Analytic Geometry)|homogeneous lines]] of [[Definition:Plane Analytic Geometry|plane analytic geometry]].	1
Let $x \in R$. Let $\sequence {x_n}$ be the [[Definition:Sequence|sequence]] defined by: $\forall n: x_n = x^n$. {{begin-eqn}} {{eqn | l = \norm x_1 < 1 \quad | o = \leadstoandfrom | c = $\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_1$ | cc= [[Sequence of Powers of Number less than One/Normed Division Ring|Sequence of Powers of Number less than One in Normed Division Ring]] }} {{eqn | o = \leadstoandfrom | c = $\sequence {x_n}$ is a [[Definition:Null Sequence in Normed Division Ring|null sequence]] in $\norm {\, \cdot \,}_2$ | cc= Assumption }} {{eqn | o = \leadstoandfrom | c = $\norm x_2 < 1$ | cc= [[Sequence of Powers of Number less than One/Normed Division Ring|Sequence of Powers of Number less than One in Normed Division Ring]] }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A, B$ be [[Definition:Subset|subsets]] of $H$, and let $A \subseteq B$. Then $B^\perp \subseteq A^\perp$, where $\perp$ signifies [[Definition:Orthocomplement|orthocomplementation]].	1
{{proof wanted|This will be a long one: First vector space, then normed vector space, then completeness.}}	1
Let $\mathbf A = \sqbrk a_{m n}$ be [[Definition:Matrix|matrices]]. Let $\mathbf 0 \mathbf A$ be defined. Then $\mathbf 0$ is of [[Definition:Order of Matrix|order]] $r \times m$ for $r \in \Z_{>0}$. Thus we have: {{begin-eqn}} {{eqn | l = \mathbf 0 \mathbf A | r = \mathbf C | c = }} {{eqn | l = \sqbrk 0_{r m} \sqbrk a_{m n} | r = \sqbrk c_{r n} | c = Definition of $\mathbf 0$ and $\mathbf A$ }} {{eqn | ll= \leadsto | lo= \forall i \in \closedint 1 r, j \in \closedint 1 n: | l = c_{i j} | r = \sum_{k \mathop = 1}^m 0_{i k} \times a_{k j} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | r = \sum_{k \mathop = 1}^m 0 | c = {{Defof|Zero Matrix}} }} {{eqn | r = 0 | c = }} {{eqn | ll= \leadsto | l = \mathbf 0 \mathbf A | r = \sqbrk 0_{r n} | c = }} {{end-eqn}} Hence $\mathbf 0 \mathbf A$ is the [[Definition:Zero Matrix|Zero Matrix]] of [[Definition:Order of Matrix|order]] $r \times n$. Let $\mathbf A \mathbf 0$ be defined. Then $\mathbf 0$ is of [[Definition:Order of Matrix|order]] $n \times s$ for $s \in \Z_{>0}$. Thus we have: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf 0 | r = \mathbf C | c = }} {{eqn | l = \sqbrk a_{m n} \sqbrk 0_{n s} | r = \sqbrk c_{m s} | c = Definition of $\mathbf A$ and $\mathbf 0$ }} {{eqn | ll= \leadsto | lo= \forall i \in \closedint 1 m, j \in \closedint 1 s: | l = c_{i j} | r = \sum_{k \mathop = 1}^n a_{i k} \times 0_{k j} | c = {{Defof|Matrix Product (Conventional)}} }} {{eqn | r = \sum_{k \mathop = 1}^n 0 | c = {{Defof|Zero Matrix}} }} {{eqn | r = 0 | c = }} {{eqn | ll= \leadsto | l = \mathbf 0 \mathbf A | r = \sqbrk 0_{m s} | c = }} {{end-eqn}} Hence $\mathbf A \mathbf 0$ is the [[Definition:Zero Matrix|Zero Matrix]] of [[Definition:Order of Matrix|order]] $m \times s$. {{qed|lemma}} If $\mathbf 0$ is of [[Definition:Order of Matrix|order]] $n \times m$,then both $\mathbf A \mathbf 0$ and $\mathbf 0 \mathbf A$ are defined, and: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf 0 | r = \sqbrk 0_{m m} }} {{eqn | l = \mathbf 0 \mathbf A | r = \sqbrk 0_{n n} }} {{end-eqn}} {{qed}}	1
Let $V_n$ be the [[Definition:Vandermonde Matrix|Vandermonde matrix]] of [[Definition:Order of Square Matrix|order $n$]] given by: :$V_n = \begin{bmatrix} x_1 & x_2 & \cdots & x_n \\ x_1^2 & x_2^2 & \cdots & x_n^2 \\ \vdots & \vdots & \ddots & \vdots \\ x_1^n & x_2^n & \cdots & x_n^n \end{bmatrix}$ Let $V_n^{-1}$ be its [[Definition:Inverse Matrix|inverse]], from [[Inverse of Vandermonde Matrix]]: :$b_{i j} = \begin{cases} \paren {-1}^{j - 1} \paren {\dfrac{\displaystyle \sum_{\substack {1 \mathop \le m_1 \mathop < \ldots \mathop < m_{n - j} \mathop \le n \\ m_1, \ldots, m_{n - j} \mathop \ne i} } x_{m_1} \cdots x_{m_{n - j} } } {x_i \displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne i} } \paren {x_m - x_i} } } & : 1 \le j < n \\ \qquad \qquad \qquad \dfrac 1 {x_i \displaystyle \prod_{\substack {1 \mathop \le m \mathop \le n \\ m \mathop \ne i} } \paren {x_i - x_m} } & : j = n \end{cases}$ The sum of all the [[Definition:Element of Matrix|elements]] of $V_n^{-1}$ is: :$\displaystyle \sum_{1 \mathop \le i, \ j \mathop \le n} b_{i j} = 1 - \prod_{k \mathop = 1}^n \paren {1 - \dfrac 1 {x_k} }$	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Module|module]] over $R$. Let $G^*$ be the [[Definition:Algebraic Dual|algebraic dual]] of $G$. Let $M$ be a [[Definition:Submodule|submodule]] of $G$. The '''annihilator of $M$''', denoted $M^\circ$, is defined as: :$\ M^\circ := \set {t' \in G^*: \forall x \in M: \map {t'} x = 0}$	1

Let $\left({S, \circ}\right)$ be a [[Definition:Commutative Monoid|commutative monoid]]. Let $e \in S$ be the [[Definition:Identity Element|identity element]] of $\left({S, \circ}\right)$. Let $I$ be the [[Definition:Set|set]] of all [[Definition:Element|elements]] of $S$ that are [[Definition:Idempotent Element|idempotent]] under $\circ$. That is: :$I = \left\{{x \in S: x \circ x = x}\right\}$ Then $\left({I, \circ}\right)$ is a [[Definition:Submonoid|submonoid]] of $\left({S, \circ}\right)$ with identity $e$.	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $*: G \times G \to G$ be the [[Definition:Group Action|group action]]: :$\forall g, h \in G: g * h = \map {\rho_{g^{-1} } } h$ where $\rho_g$ is the [[Definition:Right Regular Representation|right regular representation]] of $G$ with respect to $g$. Then $*$ is a [[Definition:Transitive Group Action|transitive group action]].	0
=== Necessary Condition === Suppose $\struct {L, +, \circ}$ is a [[Definition:Division Subring|division subring]] of $\struct {K, +, \circ}$. The conditions $(1)$ to $(3)$ hold by virtue of the [[Subring Test]]. Then $(4)$ also holds by the definition of a [[Definition:Division Ring|division ring]]: :$\forall x \in L^*: \exists ! x^{-1} \in L^*: x^{-1} \circ x = x \circ x^{-1} = 1_L$ {{qed|lemma}} === Sufficient Condition === Suppose the conditions $(1)$ to $(4)$ hold. By $(1)$ to $(3)$, it follows from [[Subring Test]] that $\struct {L, +, \circ}$ is a [[Definition:Subring|subring]] of $\struct {K, +, \circ}$. By $(4)$, every element of $L^*$ has a [[Definition:Ring Product Inverse|product inverse]]. Thus, from the [[Two-Step Subgroup Test]], $\struct {L^*, \circ}$ is a [[Definition:Group|group]]. Therefore, $\struct {L, +, \circ}$ is a [[Definition:Ring (Abstract Algebra)|ring]] in which every [[Definition:Element|element]] has a [[Definition:Ring Product Inverse|product inverse]], which makes $\struct {L, +, \circ}$ a [[Definition:Division Ring|division ring]]. {{qed}} [[Category:Subrings]] 1o60mugxbp4aeldykaapiwlbxt3ytex	0
{{begin-eqn}} {{eqn | l = \csc 210^\circ | r = \csc \left({360^\circ - 150^\circ}\right) | c = }} {{eqn | r = -\csc 150^\circ | c = [[Cosecant of Conjugate Angle]] }} {{eqn | r = -2 | c = [[Cosecant of 150 Degrees|Cosecant of $150^\circ$]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \cos x \rd x | r = \frac 1 2 \int \paren {e^{i x} + e^{-i x} } \rd x | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 1 {2 i} \paren {e^{i x} - e^{-i x} } + C | c = [[Primitive of Exponential of a x]] }} {{eqn | r = \sin x + C | c = [[Sine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
The largest [[Definition:Integer Power|$n$th power]] which has $n$ [[Definition:Digit|digits]] is $9^{21}$: :$9^{21} = 109 \, 418 \, 989 \, 131 \, 512 \, 359 \, 209$	0
:$\map \Gamma 0$ is not defined.	0
:$\displaystyle \sum_{j \mathop = 0}^n \dfrac 1 {7^j} = \frac 7 6 \left({1 - \frac 1 {7^{n + 1} } }\right)$	0
:$\displaystyle \int \frac {\mathrm d x} {x \left({x^2 - a^2}\right)^n} = \frac {-1} {2 \left({n - 1}\right) a^2 \left({x^2 - a^2}\right)^{n - 1} } - \frac 1 {a^2} \int \frac {\mathrm d x} {x \left({x^2 - a^2}\right)^{n - 1} }$ for $x^2 > a^2$.	0
Let $\family {x_i}_{i \mathop \in I} \in \displaystyle \paren{ \prod_{i \mathop \in I} S_i } \cap \paren{ \prod_{i \mathop \in I} T_i }$. By definition of [[Definition:Set Intersection|intersection]], this is equivalent to the [[Definition:Conjunction|conjunction]] of: :$\family {x_i}_{i \mathop \in I} \in \displaystyle \prod_{i \mathop \in I} S_i$ :$\family {x_i}_{i \mathop \in I} \in \displaystyle \prod_{i \mathop \in I} T_i$ By definition of [[Definition:Finite Cartesian Product|Cartesian product]], this means, for all $i \in I$: :$x_i \in S_i$ and $x_i \in T_i$ Again by definition of [[Definition:Set Intersection|intersection]], this is equivalent to, for all $i \in I$: :$x_i \in S_i \cap T_i$ Finally, by definition of [[Definition:Finite Cartesian Product|Cartesian product]], this is equivalent to: :$\family {x_i}_{i \mathop \in I} \in \displaystyle \prod_{i \mathop \in I} \paren{S_i \cap T_i}$ The result follows by definition of [[Definition:Set Equality|set equality]]. {{qed}} [[Category:Cartesian Product of Intersections]] 7933c0rqsliz6350edef2esv35htdqb	0
Let $P = \tuple {x, y}$ be a [[Definition:Point|point]] on the [[Definition:Circumference of Circle|circumference]] of a [[Definition:Unit Circle|unit circle]] whose [[Definition:Center of Circle|center]] is at the [[Definition:Origin|origin]] of a [[Definition:Cartesian Plane|cartesian plane]]. From [[Sine of Angle in Cartesian Plane]] and [[Cosine of Angle in Cartesian Plane]]: :$P = \tuple {\cos \theta, \sin \theta}$ The [[Definition:Graph of Relation|graph]] of the [[Definition:Unit Circle|unit circle]] is the [[Definition:Locus|locus]] of: :$x^2 + y^2 = 1$ as given by [[Equation of Circle]]. Substituting $x = \cos \theta$ and $y = \sin \theta$ yields: :$\cos^2 \theta + \sin^2 \theta = 1$ {{qed}}	0
Let $S \subseteq \N_{>0}$ denote the [[Definition:set|set]] of [[Definition:Strictly Positive Integer|(strictly positive)]] [[Definition:Natural Number|natural numbers]] for which $(1)$ holds. === Basis for the Induction === We have: {{begin-eqn}} {{eqn | l = 2^1 - 1 | r = 2 - 1 | c = }} {{eqn | r = 1 | c = }} {{eqn | r = 2^0 | c = }} {{eqn | r = \sum_{j \mathop = 0}^{1 - 1} 2^j | c = }} {{end-eqn}} So $1 \in S$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === We now show that, if $k \in S$ is true, where $k \ge 1$, then it logically follows that $k + 1 \in S$. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 0}^{k - 1} 2^j = 2^k - 1$ Then we need to show: :$\displaystyle \sum_{j \mathop = 0}^k 2^j = 2^{k + 1} - 1$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^k 2^j | r = \sum_{j \mathop = 0}^{k - 1} 2^j + 2^k | c = }} {{eqn | r = 2^k - 1 + 2^k | c = [[Sum of Powers of 2/Proof 2#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = 2 \times 2^k - 1 | c = }} {{eqn | r = 2^{k + 1} - 1 | c = }} {{end-eqn}} So $k \in S \implies k + 1 \in S$. It follows by the [[Principle of Finite Induction]] that $S = \N_{>0}$. That is: :$\displaystyle \forall n \in \N_{> 0}: \sum_{j \mathop = 0}^{n - 1} 2^j = 2^n - 1$ {{qed}}	0
Let $\left({S, \circ}\right)$ be an [[Definition:Algebraic Structure|algebraic structure]] in which $\circ$ has an [[Definition:Identity Element|identity]] $e_S$. From [[Epimorphism Preserves Identity]], it follows that $\left({T, *}\right)$ also has an identity, which is $\phi \left({e_S}\right)$. Let $y$ be an [[Definition:Inverse Element|inverse]] of $x$ in $\left({S, \circ}\right)$. Then: {{begin-eqn}} {{eqn | l = \phi \left({x}\right) * \phi \left({y}\right) | r = \phi \left({x \circ y}\right) | c = }} {{eqn | r = \phi \left({e_S}\right) | c = }} {{eqn | r = \phi \left({y \circ x}\right) | c = }} {{eqn | r = \phi \left({y}\right) * \phi \left({x}\right) | c = }} {{end-eqn}} So $\phi \left({y}\right)$ is an [[Definition:Inverse Element|inverse]] of $\phi \left({x}\right)$ in $\left({T, *}\right)$. As $\phi$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]], it follows from [[Inverse of Algebraic Structure Isomorphism is Isomorphism]] that $\phi^{-1}$ is also a [[Definition:Isomorphism (Abstract Algebra)|isomorphism]]. Thus the result for $\phi \left({x}\right)$ can be applied to $\phi^{-1} \left({\phi \left({x}\right)}\right)$. {{qed}}	0
Let $\left({S, \vee, \preceq}\right)$ be a [[Definition:Join Semilattice|join semilattice]]. Then $\left({S, \vee, \preceq}\right)$ is an [[Definition:Ordered Structure|ordered structure]]. That is, $\preceq$ is [[Definition:Relation Compatible with Operation|compatible]] with $\vee$.	0
=== Existence === By induction on the [[Definition:Length of Ordered Tuple|length]] of $w$. ==== Basis for the induction ==== Let $w$ have length $0$. Then $w$ is the [[Definition:Empty Associative Word on Set|empty word]]. By [[Empty Group Word is Reduced]], $w$ is [[Definition:Reduced Group Word on Set|reduced]]. ==== Induction step ==== Let $w$ have length $n \geq 1$. If $w$ is [[Definition:Reduced Group Word on Set|reduced]], we are done. Otherwise, let $k \in \{1, \ldots, n-1\}$ be such that $w_{k+1} = w_k^{-1}$. Then $w$ has an [[Definition:Elementary Reduction of Group Word on Set|elementary reduction]] of length $n-2$. By the induction hypothesis, $w$ has a [[Definition:Reduction of Group Word on Set|reduction]]. The result follows by [[Principle of Mathematical Induction|induction]]. {{qed|lemma}} === Uniqueness === By induction on the [[Definition:Length of Ordered Tuple|length]] of $w$. ==== Basis for the induction ==== Let $w$ have length $0$. By [[Length of Reduced Form of Group Word is at most Length of Word]], a [[Definition:Reduced Form of Group Word|reduced form]] of $w$ must have length $0$. Thus a [[Definition:Reduced Form of Group Word|reduced form]] of $w$ can only be $w$ itself. {{qed|lemma}} ==== Induction step ==== Let $w$ have length $n \geq 1$. Let $u$ and $v$ be [[Definition:Reduced Form of Group Word|reduced forms]] of $w$. Let $u_1$ and $v_1$ be [[Definition:Elementary Reduction of Group Word on Set|elementary reductions]] of $w$ with [[Definition:Reduced Form of Group Word|reduced forms]] $u$ and $v$. By [[Distinct Elementary Reductions of Group Word have Equal Elementary Reduction]], there exists a [[Definition:Group Word on Set|word]] $t$ which is an [[Definition:Elementary Reduction of Group Word on Set|elementary reduction]] of $u_1$ and $v_1$. Let $s$ be a [[Definition:Reduced Form of Group Word|reduced form]] of $t$. Then $s$ is a [[Definition:Reduced Form of Group Word|reduced form]] of $u_1$ and $v_1$. By the induction hypothesis, $s=u=v$. The result follows by [[Principle of Mathematical Induction|induction]]. {{qed}} [[Category:Group Words]] [[Category:Proofs by Induction]] g30odvifjgviqb49p320x3p2rdd1lqo	0
$\le$ is a [[Definition:Total Ordering|total ordering]] on $\R$. The trichotomy follows directly from [[Trichotomy Law (Ordering)|Trichotomy Law]]. {{Qed}}	0
:$\map \Gamma {-1}$ is not defined.	0
Let $\left({S, \circ}\right)$ be an [[Definition:Algebraic Structure|algebraic structure]]. Let $\mathcal R$ be a [[Definition:Congruence Relation|congruence relation]] on $\left({S, \circ}\right)$. Let $S / \mathcal R$ be the [[Definition:Quotient Set|quotient set of $S$ by $\mathcal R$]]. Let $\circ_\mathcal R$ be the [[Definition:Operation Induced on Quotient Set|operation induced on $S / \mathcal R$ by $\circ$]]. The '''quotient structure defined by $\mathcal R$''' is the [[Definition:Algebraic Structure|algebraic structure]]: : $\left({S / \mathcal R, \circ_\mathcal R}\right)$	0
From [[Canonical Injection is Injection]] we have that the [[Definition:Canonical Injection (Abstract Algebra)|canonical injections]] are in fact [[Definition:Injection|injective]]. It remains to prove the [[Definition:Morphism Property|morphism property]]. Let $x, y \in \struct {S_1, \circ_1}$. Then: {{begin-eqn}} {{eqn | l = \map {\inj_1} {x \circ_1 y} | r = \tuple {x \circ_1 y, e_2} }} {{eqn | r = \tuple {x \circ_1 y, e_2 \circ_2 e_2} }} {{eqn | r = \tuple {x, e_2} \circ \tuple {y, e_2} }} {{eqn | r = \map {\inj_1} x \circ \map {\inj_1} y }} {{end-eqn}} and the [[Definition:Morphism Property|morphism property]] has been demonstrated to hold for $\inj_1$. Thus $\inj_1: \struct {S_1, \circ_1} \to \struct {S_1, \circ_1} \times \struct {S_2, \circ_2}$ has been shown to be an [[Definition:Injection|injective]] [[Definition:Homomorphism (Abstract Algebra)|homomorphism]] and therefore a [[Definition:Monomorphism (Abstract Algebra)|monomorphism]]. The same argument applies to $\inj_2$. {{Qed}}	0
Let: :$\struct {R_1, +_1, \circ_1}$ :$\struct {R_2, +_2, \circ_2}$ :$\struct {R_3, +_3, \circ_3}$ be [[Definition:Ring (Abstract Algebra)|rings]]. Let: :$\phi: \struct {R_1, +_1, \circ_1} \to \struct {R_2, +_2, \circ_2}$ :$\psi: \struct {R_2, +_2, \circ_2} \to \struct {R_3, +_3, \circ_3}$ be [[Definition:Ring Epimorphism|(ring) epimorphisms]]. Then the [[Definition:Composition of Mappings|composite]] of $\phi$ and $\psi$ is also a [[Definition:Ring Epimorphism|(ring) epimorphism]].	0
=== Proof of Preservation of Subsets === From [[Subset Maps to Subset]], we have: :$(a) \quad \forall X, X' \in \mathbb K: X \subseteq X' \implies \map {\phi^\to} X \subseteq \map {\phi^\to} {X'}$ :$(b) \quad \forall Y, Y' \in \mathbb S: Y \subseteq Y' \implies \map {\paren {\phi^\to}^{-1} } Y \subseteq \map {\paren {\phi^\to}^{-1} } {Y'}$ So $\phi^\to$ and its [[Definition:Inverse Mapping|inverse]] both preserve [[Definition:Subset|subsets]], and $(1)$ has been demonstrated to hold. {{qed|lemma}} === Proof that Inverse Image is a Subring === Let $U \in \mathbb S$, that is, let $U$ be a [[Definition:Subring|subring]] of $\Img \phi$. From [[Preimage of Subring under Ring Homomorphism is Subring]], we have that $\map {\paren {\phi^\to}^{-1} } U$ is a [[Definition:Subring|subring]] of $R$ such that $K \subseteq R$ and so: :$\map {\paren {\phi^\to}^{-1} } U \in \mathbb K$ {{qed|lemma}} === Proof that Image is a Subring === Let $V \in \mathbb K$, that is, a [[Definition:Subring|subring]] of $R$ containing $K$. From [[Ring Homomorphism Preserves Subrings]], we have that $\map {\phi^\to} V$ is a [[Definition:Subring|subring]] of $S$ and so: :$\map {\phi^\to} V \in \mathbb S$ {{qed|lemma}} === Proof that $f$ is a Bijection === By [[Subset of Codomain is Superset of Image of Preimage]], we have that: :$U = \map {\phi^\to} {\map {\paren {\phi^\to}^{-1} } U}$ {{questionable|It doesn't, actually, it shows it's the superset. IIRC it needs to be a surjection for that. Check this to see whether $\phi^\to$ is indeed a surjection.}} We also have from [[Subset of Domain is Subset of Preimage of Image]] that: :$V \subseteq \map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V}$ Now let $r \in \map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V}$. Thus $\map \phi r \in \map {\phi^\to} V$ and so: :$\exists v \in V: \map \phi r = \map \phi v$ So $\map \phi {r + \paren {-v} } = 0_S$ and so $r - v \in K$ by definition of [[Definition:Kernel of Ring Homomorphism|kernel]]. So: :$\exists k \in K: r + k = v$ But by assumption, $K \subseteq V$ as $V \in \mathbb K$. So $k \in V$ and so it follows that $r \in V$ as well, by the fact that $V$ is [[Definition:Subring|subring]] and so [[Definition:Closure (Abstract Algebra)|closed]] for $+$. So: :$\map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V} \subseteq V$ Putting that together with $V \subseteq \map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V}$ and we see: :$V = \map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V}$ So we have: :$U = \map {\phi^\to} {\map {\paren {\phi^\to}^{-1} } U}$ :$V = \map {\paren {\phi^\to}^{-1} } {\map {\phi^\to} V}$ That is: :$\phi^\to \circ \paren {\phi^\to}^{-1} = I_\mathbb S$ :$\paren {\phi^\to}^{-1} \circ \phi^\to = I_\mathbb K$ where $I_\mathbb S$ and $I_\mathbb K$ are the [[Definition:Identity Mapping|identity mappings]] of $\mathbb S$ and $\mathbb K$ respectively. By [[Composite of Bijection with Inverse is Identity Mapping]], it follows that $\phi^\to: \mathbb K \to \mathbb S$ is a [[Definition:Bijection|bijection]]. {{qed}} === Proof of Preservation of Ideals === Let $V \in \mathbb K$ be an [[Definition:Ideal of Ring|ideal]] of $R$. Let $U = \map {\phi^\to} V$. Then from [[Ring Epimorphism Preserves Ideals]], $U$ is an [[Definition:Ideal of Ring|ideal]] of $S$. Similarly, let $U = \map {\phi^\to} V$ be an [[Definition:Ideal of Ring|ideal]] of $S$. Then by [[Preimage of Ideal under Ring Homomorphism is Ideal]], $V = \map {\paren {\phi^\to}^{-1} } U$ is an [[Definition:Ideal of Ring|ideal]] of $R$ such that $K \subseteq V$. Hence [[Definition:Ideal of Ring|ideals]] are preserved in both directions. {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \cosh {-x} | r = \frac {e^{-x} + e^{-\paren {-x} } } 2 | c = {{Defof|Hyperbolic Cosine}} }} {{eqn | r = \frac {e^{-x} + e^x} 2 }} {{eqn | r = \frac {e^x + e^{-x} } 2 }} {{eqn | r = \cosh x }} {{end-eqn}} {{qed}}	0
Let $S = \set {r_1, r_2, \ldots, r_n}$ be a [[Definition:Set|set]] of $n$ [[Definition:Even Integer|even numbers]]. By definition of [[Definition:Even Integer|even number]], this can be expressed as: :$S = \set {2 s_1, 2 s_2, \ldots, 2 s_n}$ where: :$\forall k \in \closedint 1 n: r_k = 2 s_k$ Then: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n r_k | r = \sum_{k \mathop = 1}^n 2 s_k | c = }} {{eqn | r = 2 \sum_{k \mathop = 1}^n s_k | c = }} {{end-eqn}} Thus, by definition, $\displaystyle \sum_{k \mathop = 1}^n r_k$ is [[Definition:Even Integer|even]]. {{qed}}	0
On all the number systems: :[[Definition:Natural Numbers|natural numbers]] $\N$ :[[Definition:Integer|integers]] $\Z$ :[[Definition:Rational Number|rational numbers]] $\Q$ :[[Definition:Real Number|real numbers]] $\R$ :[[Definition:Complex Number|complex numbers]] $\C$ the operation of [[Definition:Multiplication|multiplication]] is [[Definition:Distributive Operation|distributive]] over [[Definition:Addition|addition]]: :$m \paren {n + p} = m n + m p$ :$\paren {m + n} p = m p + n p$	0
By definition, [[Definition:Open Set (Topology)|open sets]] of $S$ are precisely the [[Definition:Open Set (Topology)|open sets]] of $S \setminus \set p$ under the [[Definition:Discrete Topology|discrete topology]]. Let $x, y \in S \setminus \set p: x \ne y$. Then $\set x$ and $\set y$ are both [[Definition:Open Set (Topology)|open sets]] of $T$ such that $\set x \cap \set y = \O$. Hence the result, by definition of [[Definition:Irreducible Space|irreducible]]. {{qed}}	0
Let $T$ be a [[Definition:Countable Finite Complement Topology|countable finite complement space]]. From [[Finite Complement Space is Irreducible]], $T$ is an [[Definition:Irreducible Space|irreducible space]]. From [[Countable Finite Complement Space is not Path-Connected]], $T$ is not [[Definition:Path-Connected Space|path-connected]]. Hence the result. {{qed}}	0
The [[Definition:Trivial Group|trivial group]] is a [[Definition:Cyclic Group|cyclic group]] and therefore [[Definition:Abelian Group|abelian]].	0
Let $\map f x$ be the [[Definition:Real Function|real function]] defined on $\openint {-\pi} \pi$ as: :[[File:Sneddon-1-Exercise-6.png|400px|thumb|right|$\map f x$ and its $4$th approximation]] :$\map f x = \pi^2 - x^2$ $f$ can be expressed as a [[Definition:Half-Range Fourier Cosine Series|half-range Fourier cosine series]] thus: {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac {2 \pi^2} 3 + 4 \sum_{n \mathop = 1}^\infty \paren {-1}^{n - 1} \frac {\cos n x} {n^2} | c = }} {{eqn | r = \frac {2 \pi^2} 3 + 4 \paren {\cos x - \frac 1 4 \cos 2 x + \frac 1 9 \cos 3 x - \cdots} | c = }} {{end-eqn}}	0
We have that this [[Construction of Inverse Completion/Quotient Mapping is Injective|quotient mapping $\psi: S \to T'$ is an injection]]. Let $x, y \in S$. Then: {{begin-eqn}} {{eqn | l = \psi \left({x}\right) \oplus' \psi \left({y}\right) | r = \left[\!\left[{\left({x \circ a, a}\right)}\right]\!\right]_\boxtimes \oplus' \left[\!\left[{\left({y \circ a, a}\right)}\right]\!\right]_\boxtimes | c = Definition of $\psi$ }} {{eqn | r = \left[\!\left[{\left({x \circ a, a}\right) \oplus' \left({y \circ a, a}\right)}\right]\!\right]_\boxtimes | c = Definition of $\oplus'$ }} {{eqn | r = \left[\!\left[{\left({x \circ a \circ y \circ a, a \circ a}\right)}\right]\!\right]_\boxtimes | c = Definition of $\boxtimes$ }} {{eqn | r = \left[\!\left[{\left({\left({x \circ y}\right) \circ \left({a \circ a}\right), a \circ a}\right)}\right]\!\right]_\boxtimes | c = [[Definition:Commutative Operation|Commutativity]] of $\circ$ }} {{eqn | r = \psi \left({x \circ y}\right) | c = as $a \circ a \in C$ }} {{end-eqn}} So $\psi \left({x \circ y}\right) = \psi \left({x}\right) \oplus' \psi \left({y}\right)$, and the [[Definition:Morphism Property|morphism property]] is proven. Thus $\psi$ is an [[Definition:Injection|injective]] [[Definition:Homomorphism (Abstract Algebra)|homomorphism]], and so by definition a [[Definition:Monomorphism (Abstract Algebra)|monomorphism]]. {{Qed}}	0
We have the [[Weierstrass Factorization Theorem|Weierstrass products]]: :$\displaystyle \map \sin {\pi z} = \pi z \prod_{n \mathop \ne 0} \paren {1 - \frac z n} \map \exp {\frac z n}$ From the [[Definition:Weierstrass Form of Gamma Function|Weierstrass form of the Gamma function]]: :$\displaystyle \frac 1 {\map \Gamma z} = z e^{\gamma z} \prod_{n \mathop = 1}^\infty \paren {1 + \frac z n} \map \exp {-\frac z n}$ from which: {{begin-eqn}} {{eqn | l = \dfrac 1 {-z \, \map \Gamma z \, \map \Gamma {-z} } | r = \frac {-z^2 \, \map \exp {\gamma z} \, \map \exp {-\gamma z} } {-z} \prod_{n \mathop = 1}^\infty \paren {1 + \frac z n} \paren {1 - \frac z n} \, \map \exp {\frac z n} \, \map \exp {-\frac z n} }} {{eqn | r = z \prod_{n \mathop = 1}^\infty \paren {1 - \frac {z^2} {n^2} } }} {{eqn | r = \dfrac {\map \sin {\pi z} } \pi | c = [[Euler Formula for Sine Function]] }} {{end-eqn}} whence: {{begin-eqn}} {{eqn | l = \map \Gamma z \, \map \Gamma {1 - z} | r = -z \, \map \Gamma z \, \map \Gamma {-z} | c = [[Gamma Difference Equation]] }} {{eqn | r = \frac \pi {\map \sin {\pi z} } }} {{end-eqn}} {{qed}} {{Namedfor|Leonhard Paul Euler|cat = Euler}}	0
Let $\left({S, \circ_1}\right)$ and $\left({T, \circ_2}\right)$ be [[Definition:Algebraic Structure|algebraic structures]]. The '''(external) direct product''' $\left({S \times T, \circ}\right)$ of two algebraic structures $\left({S, \circ_1}\right)$ and $\left({T, \circ_2}\right)$ is the [[Definition:Set|set]] of [[Definition:Ordered Pair|ordered pairs]]: :$\left({S \times T, \circ}\right) = \left\{{\left({s, t}\right): s \in S, t \in T}\right\}$ where the operation $\circ$ is defined as: :$\left({s_1, t_1}\right) \circ \left({s_2, t_2}\right) = \left({s_1 \circ_1 s_2, t_1 \circ_2 t_2}\right)$	0
:$\map \sec {x + \pi} = -\sec x$	0
Let $a_n : \N \to \C$ be an [[Definition:Arithmetic Function|arithmetic function]]. Let $\displaystyle f \left({s}\right) = \sum_{n \mathop \in \N} a_n n^{-s}$ be its [[Definition:Dirichlet Series|Dirichlet series]]. Let $\sigma_a$ be its [[Definition:Abscissa of Absolute Convergence|abscissa of absolute convergence]]. Then for $\Re \left({s}\right) > \sigma_a$: :$\displaystyle \sum_{n \mathop = 1}^\infty a_n n^{-s} = \prod_p \frac 1 {1 - a_p p^{-s} }$ where $p$ ranges over the [[Definition:Prime Number|primes]]. This representation for $f$ is called an '''Euler product''' for the [[Definition:Dirichlet Series|Dirichlet series]]. {{stub|Completely multiplicative hypothesis not mentioned. Needs also the statement: $\displaystyle f \left({s}\right) {{=}} \prod_p \left\{{\sum_{k \mathop \ge 1} a_{p^k} p^{-k s}}\right\}$ or however it goes for multiplicative functions which are not completely multiplicative}}	0
Every [[Definition:Equivalence Relation|equivalence relation]] is a [[Definition:Congruence Relation|congruence relation]] for the [[Definition:Constant Operation|constant operation]].	0
{{begin-eqn}} {{eqn | l = \int \sqrt {x^2 + a^2} \rd x | r = \frac {x \sqrt {x^2 + a^2} } 2 + \frac {a^2} 2 \sinh^{-1} \frac x a + C | c = [[Primitive of Root of x squared plus a squared/Inverse Hyperbolic Sine Form|Primitive of $\sqrt {x^2 + a^2}$ in $\sinh^{-1}$ form]] }} {{eqn | r = \frac {x \sqrt {x^2 + a^2} } 2 + \frac {a^2} 2 \paren {\map \ln {x + \sqrt {x^2 + a^2} } - \ln a} + C | c = [[Inverse Hyperbolic Sine of x over a in Logarithm Form|$\sinh^{-1} \dfrac x a$ in Logarithm Form]] }} {{eqn | r = \frac {x \sqrt {x^2 + a^2} } 2 + \frac {a^2} 2 \map \ln {x + \sqrt {x^2 + a^2} } - \frac {a^2 \ln a} 2 + C | c = simplifying }} {{eqn | r = \frac {x \sqrt {x^2 + a^2} } 2 + \frac {a^2} 2 \map \ln {x + \sqrt {x^2 + a^2} } + C | c = subsuming $\dfrac {-a^2 \ln a} 2$ into the [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
A [[Definition:Ring Epimorphism|ring epimorphism]] is a [[Definition:Ring Homomorphism|ring homomorphism]] which is also a [[Definition:Surjection|surection]]. From [[Composition of Ring Homomorphisms is Ring Homomorphism]], $\psi \circ \phi$ is a [[Definition:Ring Homomorphism|ring homomorphism]]. From [[Composite of Surjections is Surjection]], $\psi \circ \phi$ is a [[Definition:Surjection|surection]]. {{qed}}	0
:$\displaystyle \forall n \in \N: \sum_{i \mathop = 0}^n \left({2 i}\right)^2 = \frac {2 n \left({n + 1}\right) \left({2 n + 1}\right)} 3$	0
{{begin-eqn}} {{eqn | l = \map \tan {x + \frac {3 \pi} 2} | r = \frac {\map \sin {x + \frac {3 \pi} 2} } {\map \cos {x + \frac {3 \pi} 2} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac {-\cos x} {\sin x} | c = [[Sine of Angle plus Three Right Angles]] and [[Cosine of Angle plus Three Right Angles]] }} {{eqn | r = -\cot x | c = [[Cotangent is Cosine divided by Sine]] }} {{end-eqn}} {{qed}}	0
Suppose $f$ is a [[Definition:Mapping|mapping]] which is not [[Definition:Surjection|surjective]]. Then: : $\exists y_1 \in Y: \neg \exists x \in X: f \paren x = y_1$ Let $Z = \set {a, b}$. Let $h_1$ and $h_2$ be defined as follows. : $h_1 \paren y = a: y \in Y$ : $h_2 \paren y = \begin{cases} a & : y \ne y_1 \\ b & : y = y_1 \end{cases}$ Thus we have $h_1 \ne h_2$ such that $h_1 \circ f = h_2 \circ f$. Therefore $f$ is not [[Definition:Right Cancellable Mapping|right cancellable]]. It follows from the [[Rule of Transposition]] that if $f$ is [[Definition:Right Cancellable Mapping|right cancellable]], then $f$ must be [[Definition:Surjection|surjective]].	0
{{begin-eqn}} {{eqn | l = \size {x + y}^2 | r = \paren {x + y}^2 | c = }} {{eqn | r = x^2 + 2 x y + y^2 | c = }} {{eqn | r = \size x^2 + 2 x y + \size y^2 | c = }} {{eqn | o = \le | r = \size x^2 + 2 \size {x y} + \size y^2 | c = [[Negative of Absolute Value]] }} {{eqn | r = \size x^2 + 2 \size x \cdot \size y + \size y^2 | c = [[Absolute Value Function is Completely Multiplicative]] }} {{eqn | r = \paren {\size x + \size y}^2 | c = }} {{end-eqn}} Then by [[Order is Preserved on Positive Reals by Squaring]]: :$\size {x + y} \le \size x + y$ {{qed}}	0
Let $\left({R, +, \circ}\right)$ be a [[Definition:Division Ring|division ring]] whose [[Definition:Ring Zero|zero]] is $\left\{{0_R}\right\}$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. By definition of [[Definition:Division Ring|division ring]], every element $x$ of $R^* = R \setminus \left\{{0_R}\right\}$ has an element $y$ such that: :$y \circ x = x \circ y = 1_R$ That is, by definition, every element of $R^*$ is a [[Definition:Unit of Ring|unit]] of $R$. The result follows from [[Unit Not Zero Divisor]]. {{qed}}	0
:$\displaystyle \map {\operatorname S} x \sim \frac 1 2 - \frac 1 {\sqrt {2 \pi} } \paren {\map \cos {x^2} \paren {\frac 1 x - \frac {1 \times 3} {2^2 x^5} + \frac {1 \times 3 \times 5 \times 7} {2^4 x^9} - \ldots} + \map \sin {x^2} \paren {\frac 1 {2 x^3} - \frac {1 \times 3 \times 5} {2^3 x^7} + \ldots} }$	0
{{begin-eqn}} {{eqn | l = 10! | r = 7! \times 8 \times 9 \times 10 | c = {{Defof|Factorial}} }} {{eqn | r = 7! \times 2 \times 4 \times 3 \times 3 \times 2 \times 5 | c = }} {{eqn | r = 7! \times 2 \times 3 \times 4 \times 5 \times 6 | c = }} {{eqn | r = 6! \, 7! | c = {{Defof|Factorial}} }} {{end-eqn}} {{qed}}	0
Let $L = \left({S, \preceq, \tau}\right)$ be a [[Definition:Complete Lattice|complete]] [[Definition:Continuous Ordered Set|continuous]] [[Definition:Topological Lattice|topological lattice]] with [[Definition:Scott Topology|Scott topology]]. Then $\left\{ {x^\gg: x \in S}\right\}$ is [[Definition:Analytic Basis|basis]] of $L$.	0
{{begin-eqn}} {{eqn | l = \int \csc x \rd x | r = \ln \size {\tan \frac x 2} | c = [[Primitive of Cosecant Function/Tangent Form|Primitive of $\csc x$: Tangent Form]] }} {{eqn | ll= \leadsto | l = \int \csc a x \rd x | r = \frac 1 a \ln \size {\tan \frac {a x} 2} + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
A [[Definition:Semiprime Number|semiprime]] with [[Definition:Distinct|distinct]] [[Definition:Prime Factor|prime factors]] is a [[Definition:Square-Free|square-free]] [[Definition:Integer|integer]]. :$\map \phi n = \displaystyle \prod_{\substack {p \mathop \divides n \\ p \mathop > 2} } \paren {p - 1}$ where $p \divides n$ denotes the [[Definition:Prime Number|primes]] which [[Definition:Divisor of Integer|divide]] $n$. As there are $2$ [[Definition:Prime Factor|prime factors]]: $p$ and $q$, this devolves to: :$\map \phi n = \paren {p - 1} \paren {q - 1}$ except when $p = 2$, in which case: :$\map \phi n = q - 1$ But when $p = 2$, we have that $p - 1 = 1$ and so: :$\paren {p - 1} \paren {q - 1} = q - 1$ Hence the result. {{qed}}	0
:$\cot 120 \degrees = \cot \dfrac {2 \pi} 3 = -\dfrac {\sqrt 3} 3$	0
Let $M$ be the set of all [[Definition:Monomial|monomials]] on the indexed set $\family {X_j: j \in J}$. We are required to show that the following properties hold: {{begin-axiom}} {{axiom | q = \forall m_1, m_2 \in M | m = m_1 \circ m_2 \in M | t = [[Definition:Closed Algebraic Structure|Closure]] }} {{axiom | q = \forall m_1, m_2, m)3 \in M | m = \paren {m_1 \circ m_2} \circ m_3 = m_1 \circ \paren {m_2 \circ m_3} | t = [[Definition:Associative|Associativity]] }} {{axiom | q = \forall m_1, m_2 \in M | m = m_1 \circ m_2 = m_2 \circ m_1 | t = [[Definition:Commutative Operation|Commutativity]] }} {{axiom | q = \exists e_m \in M: \forall m_1 \in M | m = m_1 \circ e_M = m_1 | t = [[Definition:Identity Element|Identity]] }} {{end-axiom}} First note that using the [[Definition:Multiindex|multiindex]] notation described in the [[Definition:Monomial|definition of monomials]], for $r \in \N$, $m_i = \mathbf X^{k^i} \in M$, $i = 1, \ldots, r$, the product of the $m_i$ is given by: :$m_1 \circ \cdots \circ m_r = \mathbf X^{k^1 + \cdots + k^r}$ Here the superscripts enumerate the [[Definition:Multiindex|multiindices]], and do not indicate raising to a power. So to show the closure, associativity and commutativity of monomials under $\circ$, it is sufficient to show the corresponding properties for multiindices under addition defined by: :$\family {k^1 + k^2}_j := k^1_j + k^2_j$ In the following $k^1, k^2, k^3$ are multiindices, that is, families of non-negative integers indexed by $J$ such that only finitely many entries are non-zero. === Proof of Closure === Let $\family {k^1 + k^2}_j = k^1_j + k^2_j \ne 0$. By definition of [[Definition:Multiindex|multiindex]], at least one of $k^1_j$ and $k^2_j$ must be non-zero, and this can only be true for a finite number of entries. Furthermore, since $k^1_j,\ k^2_j \ge 0$, we have $k^1_j + k^2_j \ge 0$. Therefore $k^1 + k^2$ has finitely many non-zero entries, and these are all positive, and multiindices are [[Definition:Closed Algebraic Structure|closed]] under addition. === Proof of Associativity === Using associativity of integer addition, we have: :$\family {\paren {k^1 + k^2} + k^3}_j = \paren {k^1_j + k^2_j} + k^3_j = k^1_j + \paren {k^2_j + k^3_j} = \family {k^1 + \paren {k^2 + k^3} }_j$ So addition of multiindices is [[Definition:Associative|associative]]. === Proof of Commutativity === Using commutativity of integer addition, we have :$\family {k^1 + k^2}_j = k^1_j + k^2_j = k^2_j + k^1_j = \family {k^2 + k^1}_j$ So addition of multiindices is [[Definition:Commutative Operation|commutative]]. === Proof of Existence of Identity === Let $e_M$ be the multiindex such that $\family {e_M}_j = 0$ for all $j \in J$. Then: :$\family {e_M + k^1}_j = \family {e_M}_j + k^1_j = \family {k^1}_j$ so $e_M$ is an [[Definition:Identity Element|identity]] for the set of monomials. {{qed}} [[Category:Examples of Monoids]] [[Category:Polynomial Theory]] igkuc9y7o4iylu6pr4yx1afrbjflqrh	0
Let $\epsilon > 0$. We need to show that: :$\exists N \in \N: n > N \implies \size {\dfrac 1 {n^r} } < \epsilon$ That is, that $n^r > 1 / \epsilon$. Let us choose $N = \ceiling {\paren {1 / \epsilon}^{1/r} }$. By [[Reciprocal of Strictly Positive Real Number is Strictly Positive]] and [[Power of Positive Real Number is Positive/Rational Number|power of positive real number is positive]], it follows that: :$\paren {\dfrac 1 \epsilon}^{1/r} \gt 0$ Then by [[Positive Power Function on Non-negative Reals is Strictly Increasing]]: :$\forall n > N: n^r > N^r \ge 1 / \epsilon$ {{qed}}	0
Let $\sequence {a_n}$ be a [[Definition:Real Cauchy Sequence|Cauchy sequence in $\R$]]. By [[Real Cauchy Sequence is Bounded]], $\sequence {a_n}$ is [[Definition:Bounded Real Sequence|bounded]]. By the [[Bolzano-Weierstrass Theorem]], $\sequence {a_n}$ has a [[Definition:Convergent Real Sequence|convergent]] [[Definition:Subsequence|subsequence]] $\sequence {a_{n_r} }$. Let $a_{n_r} \to l$ as $r \to \infty$. It is to be shown that $a_n \to l$ as $n \to \infty$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|(strictly) positive real number]]. Then $\dfrac \epsilon 2 > 0$. Hence: :$(1): \quad \exists R \in \R: \forall r > R: \size {a_{n_r} - l} < \dfrac \epsilon 2$ We have that $\sequence {a_n}$ is a [[Definition:Real Cauchy Sequence|Cauchy sequence]]. Hence: :$(2): \quad \exists N \in \R: \forall m > N, n > N: \size {x_m - x_n} \le \dfrac \epsilon 2$ Let $n > N$. Let $r \in \N$ be [[Definition:Sufficiently Large|sufficiently large]] that: :$n_r > N$ and: :$r > R$ Then $(1)$ is satisfied, and $(2)$ is satisfied with $m = n_r$. So: {{begin-eqn}} {{eqn | lo= \forall n > N: | l = \size {a_n - l} | r = \size {a_n - a_{n_r} + a_{n_r} - l} | c = }} {{eqn | o = \le | r = \size {a_n - a_{n_r} } + \size{a_{n_r} - l} | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | o = < | r = \dfrac \epsilon 2 + \dfrac \epsilon 2 | c = }} {{eqn | r = \epsilon | c = }} {{end-eqn}} So, given $\epsilon > 0$, we have found $n \in \R$ such that: :$\forall n > N: \size {a_n - l} < \epsilon$ Thus: :$x_n \to l$ as $n \to \infty$. {{qed}}	0
Let $\displaystyle f \left({z}\right) = \sum_{n \mathop = 0}^\infty a_n \left({z - \xi}\right)^n$ be a [[Definition:Complex Power Series|complex power series]] about $\xi \in \C$. Let $R$ be the [[Definition:Radius of Convergence of Complex Power Series|radius of convergence]] of $f$. Then, $f$ is of [[Definition:Differentiability Class|differentiability class $C^\infty$]]. For all $n \in \N$: :$a_n = \dfrac{f^{\left({n}\right) } \left({\xi}\right) }{ n! }$ Hence, $f$ is equal to its [[Definition:Taylor Series|Taylor series expansion]] about $\xi$: :$\displaystyle \forall z \in \C, \left\vert{z - \xi}\right\vert < R: \quad f \left({z}\right) = \sum_{n \mathop = 0}^\infty \dfrac{\left({z - \xi}\right)^n}{n!} f^{\left({n}\right) } \left({\xi}\right)$	0
Let $x$ be a [[Definition:Real Number|real number]]. Let $x \in \hointl {-\infty} {-1} \cup \hointr 1 {\infty}$. Then: :$\displaystyle \arcsec x = -i \map \Ln {i \sqrt {1 - \frac 1 {x^2} } + \frac 1 x}$ where: :$\arcsec$ is the [[Definition:Inverse Secant/Real/Arcsecant|arcsecant function]] :$\Ln$ is the [[Definition:Natural Logarithm/Complex/Principal Branch|principal branch]] of the [[Definition:Complex Natural Logarithm|complex logarithm]] whose imaginary part lies in $\hointl {-\pi} \pi$.	0
We list out all the [[Definition:Left Coset|left cosets]] of $H \cap K$ in $H$: :$H / \paren {H \cap K} = \set {h_n \paren {H \cap K}: h_n \in H, n \in I}$ where $I$ is some [[Definition:Finite Set|finite]] indexing set. For each pair $h_i, h_j \in H \subseteq G$, where $i \ne j$: :$h_i^{-1} h_j \notin H \cap K \quad$ [[Cosets are Equal iff Product with Inverse in Subgroup]] Since $H$ is a [[Definition:Subgroup|subgroup]], $h_i^{-1} h_j \in H$. Thus: :$h_i^{-1} h_j \notin K$ By [[Cosets are Equal iff Product with Inverse in Subgroup]], $h_i K$ and $h_j K$ are different [[Definition:Left Coset|left cosets]]. Hence there must be at least $\index H {H \cap K}$ [[Definition:Left Coset|left cosets]] of $K$ in $G$, namely $h_n K$ for each $n \in I$. It follows that $\index H {H \cap K} \le \index G K$. {{qed|lemma}} === Equality === Suppose equality holds. Then the construction above gives all [[Definition:Left Coset|left cosets]] of $K$ in $G$. For any $x \in G$, $x \in h_i K$ for some $h_i \in H$. Hence $G \subseteq H K$. The inclusion $H K \subseteq G$ is seen from {{GroupAxiom|0}}. Hence $G = H K$ by [[Definition:Set Equality|definition of set equality]]. {{qed|lemma}} Now suppose $G = H K$. To show equality we must show that the construction above gives all [[Definition:Left Coset|left cosets]] of $K$ in $G$. Let $g K$ be a [[Definition:Left Coset|left coset]] of $K$ in $G$, where $x \in G$. Since $G = H K$: :$\exists h \in H, k \in K: g = h k$ Then by [[Left Coset Equals Subgroup iff Element in Subgroup]]: :$g K = h \paren {k K} = h K$ Since $h \in H$: :$\exists i \in I: h \in h_i \paren {H \cap K}$ By [[Element in Left Coset iff Product with Inverse in Subgroup]]: :$h^{-1} h_i \in H \cap K \subseteq K$ Thus by [[Left Cosets are Equal iff Product with Inverse in Subgroup]]: :$h_i K = h K = g K$ Hence all [[Definition:Left Coset|left cosets]] of $K$ in $G$ are obtained in the construction. This gives equality. {{qed}}	0
From the formal definition of [[Definition:Complex Number/Definition 2|complex numbers]], we have: :$z = \tuple {x_1, y_1}$ :$w = \tuple {x_2, y_2}$ where $x_1, x_2, y_1, y_2 \in \R$. Then from the definition of [[Definition:Complex Number/Definition 2/Addition|complex addition]]: :$z + w = \tuple {x_1 + x_2, y_1 + y_2}$ From [[Real Numbers under Addition form Abelian Group]], [[Definition:Real Addition|real addition]] is [[Definition:Closed Operation|closed]]. So: :$\paren {x_1 + x_2} \in \R$ and $\paren {y_1 + y_2} \in \R$ and hence the result. {{qed}}	0
Let $x \in \R$ such that $-1 < x \le 1$. Then: :$\dfrac 1 {\sqrt [3] {1 + x} } = 1 - \dfrac 1 3 x + \dfrac {1 \times 4} {3 \times 6} x^2 - \dfrac {1 \times 4 \times 7} {3 \times 6 \times 9} x^3 + \cdots$	0
Let $e$ be the [[Definition:Identity Element|group identity]] of $G$. By [[User:Dfeuer/CRG2]] $(1)$: :$(1): \quad x \mathrel{\mathcal R} y \iff e \mathrel{\mathcal R} y \circ x^{-1}$ By [[User:Dfeuer/CRG2]] $(2)$, also: :$(2): \quad y^{-1} \mathrel{\mathcal R} x^{-1} \iff e \mathrel{\mathcal R} \left({y^{-1}}\right)^{-1} \circ x^{-1}$ By [[Inverse of Group Inverse]] $\left({y^{-1}}\right)^{-1} = y$. Thus, we can rewrite $(2)$ as: :$(3): \quad y^{-1} \mathrel{\mathcal R} x^{-1} \iff e \mathrel {\mathcal R} y \circ x^{-1}$ Now note that the {{RHS}} of $(3)$ is the same as the {{RHS}} in $(1)$. We conclude that: :$x \mathrel{\mathcal R} y \iff y^{-1} \mathrel{\mathcal R} x$ {{qed}} [[Category:Compatible Relations]] [[Category:Group Theory]] bxgnpz4l16tutays15dlp8v9poi7o7m	0
Let $\left({S, \circ}\right)$ be a [[Definition:Band|band]]. Let $\left({\mathcal P \left({S}\right), \circ_\mathcal P}\right)$ be the [[Definition:Algebraic Structure|algebraic structure]] consisting of: : the [[Definition:Power Set|power set]] $\mathcal P \left({S}\right)$ of $S$ and : the [[Definition:Operation Induced on Power Set|operation $\circ_\mathcal P$ induced on $\mathcal P \left({S}\right)$ by $\circ$]]. Let $T \subseteq \mathcal P \left({S}\right)$. Let $\left({T, \circ_\mathcal P}\right)$ be a [[Definition:Subband|subband]] of $\left({\mathcal P \left({S}\right), \circ_\mathcal P}\right)$. Then every [[Definition:Element|element]] of $T$ is a subband of $\left({S, \circ}\right)$.	0
We have that: :$\displaystyle \sum_{i \mathop = 1}^n i = 1 + 2 + \cdots + n$ Consider $\displaystyle 2 \sum_{i \mathop = 1}^n i$. Then: {{begin-eqn}} {{eqn | l = 2 \sum_{i \mathop = 1}^n i | r = 2 \paren {1 + 2 + \dotsb + \paren {n - 1} + n} | c = }} {{eqn | r = \paren {1 + 2 + \dotsb + \paren {n - 1} + n} + \paren {n + \paren {n - 1} + \dotsb + 2 + 1} | c = }} {{eqn | r = \paren {1 + n} + \paren {2 + \paren {n - 1} } + \dotsb + \paren {\paren {n - 1} + 2} + \paren {n + 1} | c = [[Integer Addition is Commutative]], [[Integer Addition is Associative]] }} {{eqn | r = \paren {n + 1}_1 + \paren {n + 1}_2 + \dotsb + \paren {n + 1}_n | c = }} {{eqn | r = n \paren {n + 1} | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = 2 \sum_{i \mathop = 1}^n i | r = n \paren {n + 1} | c = }} {{eqn | ll= \leadsto | l = \sum_{i \mathop = 1}^n i | r = \frac {n \paren {n + 1} } 2 | c = }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int x^2 \paren {\sqrt {a^2 - x^2} }^3 \rd x = \frac {-x \paren {\sqrt {a^2 - x^2} }^5} 6 + \frac {a^2 x \paren {\sqrt {a^2 - x^2} }^3} {24} + \frac {a^4 x \sqrt {a^2 - x^2} } {16} + \frac {a^6} {16} \arcsin \frac x a + C$	0
Let $S$ be a [[Definition:Set|set]] whose [[Definition:Cardinality|cardinality]] is $n$. The number $N$ of possible different [[Definition:Binary Operation|binary operations]] which have an [[Definition:Identity Element|identity element]] that can be applied to $S$ is given by: :$N = n^{\paren {n - 1}^2 + 1}$	0
From the formal definition of [[Definition:Complex Number/Definition 2|complex numbers]], we have: :$z = \tuple {x_1, y_1}$ :$w = \tuple {x_2, y_2}$ where $x_1, x_2, y_1, y_2 \in \R$. Then from the definition of [[Definition:Complex Number/Definition 2/Addition|complex addition]]: :$z + w = \tuple {x_1 + x_2, y_1 + y_2}$ From [[Real Numbers under Addition form Abelian Group]], [[Definition:Real Addition|real addition]] is [[Definition:Closed Operation|closed]]. So: :$\paren {x_1 + x_2} \in \R$ and $\paren {y_1 + y_2} \in \R$ and hence the result. {{qed}}	0
:$\displaystyle \map {\operatorname C} x = \sqrt {\frac 2 \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{4 n + 1} } {\paren {4 n + 1} \paren {2 n}!}$	0
Let $a_j \divides a_{j + 1}$ for some $j \in \set {0, 1, \ldots, n - 1}$. Then by definition of [[Definition:Divisor of Integer|integer divisibility]]: :$\exists r \in \Z: r a_j = a_{j + 1}$ Thus the [[Definition:Common Ratio|common ratio]] of $Q_n$ is $r$. So by definition of [[Definition:Geometric Sequence|geometric sequence]]: :$\forall j, k \in \set {0, 1, \ldots, n}, j < k: r^{k - j} a_j = a_k$ and so $a_j \divides a_k$. The [[Definition:Converse Statement|converse]] is trivial. {{qed}} [[Category:Geometric Sequences of Integers]] 9wvw4e48rd7y1uyoccxqq5hzi2ue3tc	0
:$\displaystyle \int x^m \operatorname{csch}^{-1} \frac x a \rd x = \begin{cases} \displaystyle \frac {x^{m + 1} } {m + 1} \operatorname{csch}^{-1} \frac x a + \frac 1 {m + 1} \int \frac {x^m} {\sqrt {x^2 + a^2} } \rd x + C & : x > 0 \\ \displaystyle \frac {x^{m + 1} } {m + 1} \operatorname{csch}^{-1} \frac x a - \frac 1 {m + 1} \int \frac {x^m} {\sqrt {x^2 + a^2} } \rd x + C & : x < 0 \\ \end{cases}$	0
The proof proceeds by [[Second Principle of Mathematical Induction|strong induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$a_n = \dfrac {3^n - \left({-2}\right)^n} 5$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \dfrac {3^0 - \left({-2}\right)^0} 5 | r = \dfrac {1 - 1} 5 | c = }} {{eqn | r = 0 | c = }} {{eqn | r = a_0 | c = }} {{end-eqn}} Thus $P \left({0}\right)$ is seen to hold. $P \left({1}\right)$ is the case: {{begin-eqn}} {{eqn | l = \dfrac {3^1 - \left({-2}\right)^1} 5 | r = \dfrac {3 - \left({-2}\right)} 5 | c = }} {{eqn | r = \dfrac {3 + 2} 5 | c = }} {{eqn | r = 1 | c = }} {{eqn | r = a_1 | c = }} {{end-eqn}} Thus $P \left({1}\right)$ is seen to hold. This is the [[Second Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({j}\right)$ is true, for all $j$ such that $0 \le j \le k$, then it logically follows that $P \left({k + 1}\right)$ is true. This is the [[Second Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$a_k = \dfrac {3^k - \left({-2}\right)^k} 5$ and: :$a_{k - 1} = \dfrac {3^{k - 1} - \left({-2}\right)^{k - 1} } 5$ from which it is to be shown that: :$a_{k + 1} = \dfrac {3^{k + 1} - \left({-2}\right)^{k + 1} } 5$ === Induction Step === This is the [[Second Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = a_{k + 1} | r = a_k + 6 a_{k - 1} | c = Definition of $a_n$ }} {{eqn | r = \dfrac {3^k - \left({-2}\right)^k} 5 + 6 \times \dfrac {3^{k - 1} - \left({-2}\right)^{k - 1} } 5 | c = }} {{eqn | r = \dfrac {3^k - \left({-2}\right)^k + 6 \times 3^{k - 1} - 6 \left({-2}\right)^{k - 1} } 5 | c = }} {{eqn | r = \dfrac {3^{k - 1} \left({3 + 6}\right) - \left({-2}\right)^{k - 1} \left({-2 + 6}\right)} 5 | c = }} {{eqn | r = \dfrac {3^{k - 1} \times 9 - \left({-2}\right)^{k - 1} \times 4} 5 | c = }} {{eqn | r = \dfrac {3^{k - 1} \times 3^2 - \left({-2}\right)^{k - 1} \times \left({-2}\right)^2} 5 | c = }} {{eqn | r = \dfrac {3^{k + 1} - \left({-2}\right)^{k + 1} } 5 | c = }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Second Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{\ge 0}: a_n = \dfrac {3^n - \left({-2}\right)^n} 5$ {{qed}} {{improve|Given that this result was given as a question in a chapter on the Fibonacci numbers, one presumes there is a way to solve this by using Fibonacci numbers, or a Fibonacci-like approach. Maybe use generating functions.}}	0
From the definition of [[Definition:Modulo Addition|addition modulo $z$]], we have: :$\eqclass x z +_z \eqclass y z = \eqclass {x + y} z$ As $x, y \in R$, we have that $x + y \in \R$ as [[Real Addition is Closed]]. Hence by definition of [[Definition:Congruence (Number Theory)|congruence]], $\eqclass {x + y} z \in \R_z$. {{qed}}	0
Proof by [[Principle of Mathematical Induction|induction]]: For all $k \in \N$, let $\map P k$ be the [[Definition:Proposition|proposition]]: :$m < n \iff m + k < n + k$ $\map P 0$ is true, as this just says $m + 0 = m < n = n + 0$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P j$ is true, where $j \ge 0$, then it logically follows that $\map P {j^+}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$m < n \iff m + j < n + j$ Then we need to show: :$m < n \iff m + j^+ < n + j^+$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: Let $m < n$. Then: {{begin-eqn}} {{eqn | l = m | o = < | r = n | c = }} {{eqn | ll= \leadstoandfrom | l = m | o = \subsetneq | r = n | c = [[Element of Finite Ordinal iff Subset]] }} {{eqn | ll= \leadstoandfrom | l = m^+ | o = \subset | r = n | c = {{Defof|Successor Set}} }} {{eqn | ll= \leadstoandfrom | l = m^+ | o = \subsetneq | r = n^+ | c = {{Defof|Successor Set}} }} {{eqn | n = 1 | ll= \leadstoandfrom | l = m^+ | o = < | r = n^+ | c = [[Element of Finite Ordinal iff Subset]] }} {{end-eqn}} This gives: {{begin-eqn}} {{eqn | l = m + j | o = < | r = n + j | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {m + j}^+ | o = < | r = \paren {n + j}^+ | c = from $(1)$ above }} {{eqn | ll= \leadstoandfrom | l = m + j^+ | o = < | r = n + j^+ | c = {{Defof|Addition in Minimal Infinite Successor Set}} }} {{end-eqn}} So $\map P j \implies \map P {j^+}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall m, n, k \in \N: m < n \iff m + k < n + k$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot 15^\circ | r = \frac {\cos 15^\circ} {\sin 15^\circ} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac{\frac {\sqrt 6 + \sqrt 2} 4} {\frac {\sqrt 6 - \sqrt 2} 4} | c = [[Cosine of 15 Degrees]] and [[Sine of 15 Degrees]] }} {{eqn | r = \frac {\sqrt 6 + \sqrt 2} {\sqrt 6 - \sqrt 2} | c = simplifying }} {{eqn | r = \frac {\left({\sqrt 6 + \sqrt 2}\right)^2} {\left({\sqrt 6 - \sqrt 2}\right) \left({\sqrt 6 + \sqrt 2}\right)} | c = multiplying top and bottom by $\sqrt 6 + \sqrt 2$ }} {{eqn | r = \frac {6 + 2 \sqrt 6 \sqrt 2 + 2 } {6 - 2} | c = multiplying out, and [[Difference of Two Squares]] }} {{eqn | r = \frac {8 + 4 \sqrt 3} 4 | c = simplifying }} {{eqn | r = 2 + \sqrt 3 | c = dividing top and bottom by $4$ }} {{end-eqn}} {{qed}}	0
First we note that by [[Integer Less One divides Power Less One]]: :$\paren {m - 1} \divides \paren {m^n - 1}$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Thus $m^n - 1$ is [[Definition:Composite Number|composite]] for all $m \in \Z: m > 2$. Let $m = 2$, and consider $2^n - 1$. Suppose $n$ is [[Definition:Composite Number|composite]]. Then $n = r s$ where $r, s \in \Z_{> 1}$. Then by [[Integer Less One divides Power Less One/Corollary|the corollary to Integer Less One divides Power Less One]]: :$\paren {2^r - 1} \divides \paren {2^{r s} - 1}$ Thus if $n$ is [[Definition:Composite Number|composite]], then so is $2^n - 1$. So $2^n - 1$ can be [[Definition:Prime Number|prime]] only when $n$ is [[Definition:Prime Number|prime]]. {{qed}}	0
By definition of [[Definition:Complex Logarithm|complex logarithm]]: :$-\map \ln {1 + z} = \displaystyle \sum_{n \mathop = 1}^\infty \frac {\paren {-z}^n} n$ Thus {{begin-eqn}} {{eqn | l = \cmod {1 - \frac {\map \ln {1 + z} } z} | r = \cmod {\sum_{n \mathop = 2}^\infty \frac {\paren {-1}^n z^{n - 1} }n} | c = [[Linear Combination of Convergent Series]] }} {{eqn | o = \le | r = \sum_{n \mathop = 2}^\infty \cmod {\frac{\paren {-z}^{n - 1} } n} | c = [[Triangle Inequality for Series]] }} {{eqn | o = \le | r = \sum_{n \mathop = 2}^\infty \frac {\cmod z^{n - 1} } 2 | c = $n \ge 2$ }} {{eqn | r = \frac {\cmod z / 2} {1 - \cmod z} | c = [[Sum of Geometric Sequence]] }} {{eqn | o = \le | r = \frac 1 2 | c = $\cmod z \le \dfrac 1 2$ }} {{end-eqn}} By the [[Triangle Inequality]]: :$\displaystyle \frac 1 2 \le \cmod {\frac {\map \ln {1 + z} } z} \le \dfrac 3 2$ {{qed}}	0
From [[Intersection Distributes over Union]]: :$R \cap \paren {S \cup T} = \paren {R \cap S} \cup \paren {R \cap T}$ From the [[Duality Principle for Sets]], exchanging $\cup$ for $\cap$ throughout, and vice versa, reveals the result: :$R \cup \paren {S \cap T} = \paren {R \cup S} \cap \paren {R \cup T}$ {{qed}}	0
:$\left({T', \oplus'}\right)$ is a [[Definition:Commutative Semigroup|commutative semigroup]].	0
=== [[Order of Subgroup Product/Lemma|Lemma]] === {{:Order of Subgroup Product/Lemma}}{{qed|lemma}} We have that $H K$ is the [[Definition:Set Union|union]] of all [[Definition:Left Coset|left cosets]] $h K$ with $h \in H$: :$\displaystyle H K = \bigcup_{h \mathop \in H} h K$ From [[Left Coset Space forms Partition]], unequal $h K$ are [[Definition:Disjoint Sets|disjoint]]. From [[Cosets are Equivalent]], each $h K$ contains $\order K$ [[Definition:Element|elements]]. From the [[Order of Subgroup Product/Lemma|Lemma]], the number of different such [[Definition:Left Coset|left cosets]] is: :$\index H {H \cap K}$ where $\index H {H \cap K}$ denotes the [[Definition:Index of Subgroup|index]] of $H \cap K$ in $H$. From [[Lagrange's Theorem (Group Theory)|Lagrange's Theorem]]: $\index H {H \cap K} = \dfrac {\order H} {\order {H \cap K} }$ Hence: :$\order {H K} = \dfrac {\order H \order K} {\order {H \cap K} }$ {{qed}}	0
By definition, we have that: :$\alpha \beta := \begin {cases} \size \alpha \, \size \beta & : \alpha \ge 0^*, \beta \ge 0^* \\ -\paren {\size \alpha \, \size \beta} & : \alpha < 0^*, \beta \ge 0^* \\ -\paren {\size \alpha \, \size \beta} & : \alpha \ge 0^*, \beta < 0^* \\ \size \alpha \, \size \beta & : \alpha < 0^*, \beta < 0^* \end {cases}$ where: :$\size \alpha$ denotes the [[Definition:Absolute Value of Cut|absolute value of $\alpha$]] :$\size \alpha \, \size \beta$ is defined as in [[Definition:Multiplication of Positive Cuts|Multiplication of Positive Cuts]] :$\ge$ denotes the [[Definition:Ordering of Cuts|ordering on cuts]]. First we note that from [[Absolute Value of Cut is Zero iff Cut is Zero]]: :$\size {0^*} = 0^*$ === Necessary Condition === Let $\alpha = 0^*$ or $\beta = 0^*$. Then from [[Product of Cut with Zero Cut equals Zero Cut]] it follows directly that: :$\alpha \beta = 0^*$ {{qed|lemma}} === Sufficient Condition === Let $\alpha \beta = 0^*$. Suppose $\alpha > 0^*$ and $\beta > 0^*$. By definition of [[Definition:Multiplication of Positive Cuts|multiplication of positive cuts]]: $\alpha \beta$ is the [[Definition:Set|set]] of all [[Definition:Rational Number|rational numbers]] $r$ such that either: :$r < 0$ or :$\exists p \in \alpha, q \in \beta: r = p q$ where $p \ge 0$ and $q \ge 0$. We have that $\alpha \beta = 0^*$. {{AimForCont}} $r = p q$ where $p \ge 0$ and $q \ge 0$. Then $r \ge 0$. But then $r \notin 0^*$ by definition of $0^*$. Thus there is no $p \in \alpha, q \in \beta$ such that $r = p q$ where $p \ge 0$ and $q \ge 0$. It follows that $\alpha > 0^*$ and $\beta > 0^*$ cannot both be satisfied. It follows further that for $\size \alpha \, \size \beta = 0^*$ it is not possible for $\size \alpha > 0^*$ and $\size \beta > 0^*$. The result follows. {{qed}}	0
Proof by [[Second Principle of Mathematical Induction|strong induction]]: Let us write $p_n = \map p n$. For all $n \in \N_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\map p n \le 2^{2^{n - 1} }$ === Basis for the Induction === $\map P 1$ is true, as this just says $2 \le 2^{2^0} = 2$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Suppose that, for some $k \in \N$, each of $\map P 1, \map P 2, \ldots, \map P k$ is true. It remains to show that it logically follows that $\map P {k + 1}$ is true. That is, that: :$\map p {k + 1} \le 2^{2^k}$ This is our [[Definition:Induction Hypothesis|induction hypothesis]]. === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \map p {k + 1} | o = \le | r = p_1 p_2 \cdots p_k + 1 | c = [[Euclid's Theorem]] }} {{eqn | o = \le | r = 2 \times 2^2 \times 2^{2^2} \times \cdots \times 2^{2^{k - 1} } + 1 | c = [[Upper Bounds for Prime Numbers/Result 1#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = 2^{1 + 2 + 2^2 + 2^3 + \cdots + 2^{k - 1} } + 1 | c = [[Exponent Combination Laws]] }} {{eqn | r = 2^{2^k - 1} + 1 | c = [[Sum of Geometric Sequence]] }} {{eqn | o = \le | r = 2^{2^k - 1} + 2^{2^k - 1} | c = since $1 \le 2^{2^k - 1}$ for all $k$ }} {{eqn | r = 2^{2^k} | c = }} {{end-eqn}} The result follows by the [[Second Principle of Mathematical Induction]]. {{qed}}	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {R_n, \times}$ be the [[Definition:Complex Roots of Unity|complex $n$th roots of unity]] under [[Definition:Complex Multiplication|complex multiplication]]. Let $\struct {\Z_n, +_n}$ be the [[Definition:Integers Modulo m|integers modulo $n$]] under [[Definition:Modulo Addition|modulo addition]]. Then $\struct {R_n, \times}$ and $\struct {\Z_n, +_n}$ are [[Definition:Isomorphic Algebraic Structures|isomorphic algebraic structures]].	0
[[Proof by Counterexample]]: Consider the [[Definition:Symmetric Group|symmetric group]] $S_4$. Then the [[Definition:Order of Structure|order]] of the [[Definition:Alternating Group|alternating group]] $A_4$ is $12$. We list the [[Alternating Group on 4 Letters/Subgroups|subgroups of $A_4$]]: {{:Alternating Group on 4 Letters/Subgroups}} Now $6$ [[Definition:Divisor of Integer|divides]] $12$. But there is no [[Definition:Subgroup|subgroup]] of $A_4$ of [[Definition:Order of Structure|order]] $6$. {{qed}}	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Then $p$ is the smallest [[Definition:Prime Number|prime number]] which [[Definition:Divisor of Integer|divides]] $\paren {p - 1}! + 1$.	0
From [[Center of Quaternion Group]], we have: :$\map Z {\Dic 2} = \set {e, a^2}$ Thus from [[Conjugacy Class of Element of Center is Singleton]], $\set e$ and $\set {a^2}$ are two of those [[Definition:Conjugacy Class|conjugacy classes]]. By inspection of the [[Quaternion Group/Cayley Table|Cayley table]]: {{:Quaternion Group/Cayley Table}} we investigate the remaining $6$ elements in turn, starting with $a$: {{begin-eqn}} {{eqn | l = a a a^{-1} | o = | rr= = a | c = }} {{eqn | l = a^2 a \paren {a^2}^{-1} | r = a^2 a a^2 = a^5 | rr= = a | c = }} {{eqn | l = a^3 a \paren {a^3}^{-1} | r = a^3 a a = a^5 | rr= = a | c = }} {{eqn | l = b a b^{-1} | r = b a \paren {a^2 b} = b a^3 b | rr= = a^3 | c = }} {{eqn | l = \paren {a b} a \paren {a b}^{-1} | r = \paren {a b} a \paren {a^3 b} = b a^3 b | rr= = a^3 | c = }} {{eqn | l = \paren {a^2 b} a \paren {a^2 b}^{-1} | r = \paren {a^2 b} a b | rr= = a^3 | c = }} {{eqn | l = \paren {a^3 b} a \paren {a^3 b}^{-1} | r = \paren {a^3 b} a \paren {a b} = \paren {a^2 b} \paren {a b} | rr= = a^3 | c = }} {{end-eqn}} So we have a [[Definition:Conjugacy Class|conjugacy class]]: :$\set {a, a^3}$ Investigating the remaining $4$ elements in turn, starting with $b$: {{begin-eqn}} {{eqn | l = a b a^{-1} | r = a b a^3 | rr= = a^2 b | c = }} {{eqn | l = a^2 b \paren {a^2}^{-1} | r = a^2 b a^2 | rr= = b | c = }} {{eqn | l = a^3 b \paren {a^3}^{-1} | r = a^3 b a | rr= = a^2 b | c = }} {{eqn | l = b b b^{-1} | o = | rr= = b | c = }} {{eqn | l = \paren {a b} b \paren {a b}^{-1} | r = \paren {a b} b \paren {a^3 b} = \paren {a b} a^3 | rr= = a^2 b | c = }} {{eqn | l = \paren {a^2 b} b \paren {a^2 b}^{-1} | r = \paren {a^2 b} b b = \paren {a^2 b} a^2 | rr= = b | c = }} {{eqn | l = \paren {a^3 b} b \paren {a^3 b}^{-1} | r = \paren {a^3 b} b \paren {a b} = \paren {a^3 b} a | rr= = a^2 b | c = }} {{end-eqn}} So we have a [[Definition:Conjugacy Class|conjugacy class]]: :$\set {b, a^2 b}$ Investigating the remaining $2$ elements, starting with $a b$: {{begin-eqn}} {{eqn | l = a \paren {a b} a^{-1} | r = a \paren {a b} a^3 = a^2 b a^3 | rr= = a^3 b | c = }} {{end-eqn}} We need go no further: the remaining elements $a b$ and $a^3 b$ are in the same [[Definition:Conjugacy Class|conjugacy class]]: :$\set {a b, a^3 b}$ Hence the result. {{qed}}	0
Let $z_1 = x_1 + i y_1, z_2 = x_2 + i y_2$. Let us define the [[Definition:Mapping|mapping]] $\phi: \C \to \C$ defined as: :$\forall z \in \C: \map \phi z = \overline z$ We check that $\phi$ has the [[Definition:Morphism Property|morphism property]]: By [[Sum of Complex Conjugates]]: :$\map \phi {z_1 + z_2} = \map \phi {z_1} + \map \phi {z_2}$ By [[Product of Complex Conjugates]]: :$\map \phi {z_1 z_2} = \map \phi {z_1} \map \phi {z_2}$ So the [[Definition:Morphism Property|morphism property]] holds for both [[Definition:Complex Addition|complex addition]] and [[Definition:Complex Multiplication|complex multiplication]]. Hence we can say that [[Definition:Complex Conjugate|complex conjugation]] is a [[Definition:Field Homomorphism|field homomorphism]]. We note that $\overline z_1 = \overline z_2 \implies z_1 = z_2$ and so [[Definition:Complex Conjugate|complex conjugation]] is [[Definition:Injection|injective]]. Also, [[Definition:Complex Conjugate|complex conjugation]] is trivially [[Definition:Surjection|surjective]], and hence [[Definition:Bijection|bijective]]. The result then follows by definition of [[Definition:Field Automorphism|field automorphism]]. {{qed}}	0
Let $\left({S, \preceq}\right)$ be an [[Definition:Up-Complete|up-complete]] [[Definition:Bounded Below Set|lower bounded]] [[Definition:Join Semilattice|join semillattice]]. Then $\left({S, \preceq}\right)$ is [[Definition:Complete Lattice|complete]].	0
Let $n \in \N$ such that $n > 2$. Then there exists an [[Definition:Algebraic Structure|algebraic structure]] $\left({S, \circ}\right)$ of [[Definition:Order of Structure|order]] $n$ such that $\circ$ is [[Definition:Antiassociative Operation|antiassociative]] on $S$.	0
:$\displaystyle \int \frac {\d x} {x^2 \paren {x^3 + a^3} } = \frac {-1} {a^3 x} - \frac 1 {6 a^4} \map \ln {\frac {x^2 - a x + a^2} {\paren {x + a}^2} } - \frac 1 {a^4 \sqrt 3} \arctan \frac {2 x - a} {a \sqrt 3}$	0
Let: {{begin-eqn}} {{eqn | l = x | r = a \tan \theta | c = }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d \theta} | r = a \sec^2 \theta | c = [[Derivative of Tangent Function]] }} {{eqn | ll= \leadsto | l = \int \frac {x^2 \rd x} {\paren {x^2 + a^2}^2} | r = \int \frac {a^2 \tan^2 \theta a \sec^2 \theta} {\paren {a^2 \tan^2 \theta + a^2}^2} \rd \theta | c = [[Integration by Substitution]] }} {{eqn | r = \int \frac {a^3 \tan^2 \theta \sec^2 \theta} {a^4 \sec^4 \theta} \rd \theta | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = \frac 1 a \int \frac {\tan^2 \theta} {\sec^2 \theta} \rd \theta | c = simplification }} {{eqn | r = \frac 1 a \int \tan^2 \theta \cos^2 \theta \rd \theta | c = {{Defof|Real Secant Function}} }} {{eqn | r = \frac 1 a \int \frac {\sin^2 \theta} {\cos^2 \theta} \cos^2 \theta \rd \theta | c = {{Defof|Real Tangent Function}} }} {{eqn | r = \frac 1 a \int \sin^2 \theta \rd \theta | c = simplification }} {{eqn | r = \frac \theta {2 a} - \frac {\sin 2 \theta} {4 a} + C | c = [[Primitive of Square of Sine Function]] }} {{eqn | r = \frac \theta {2 a} - \frac 1 {2 a} \frac {\tan \theta} {1 + \tan^2 \theta} + C | c = [[Tangent Half-Angle Substitution for Sine]] }} {{eqn | r = -\frac x {2 \paren {x^2 + a^2} } + \frac 1 {2 a} \arctan \frac x a + C | c = substituting for $\theta$ }} {{end-eqn}} {{qed}}	0
Let $\struct {F, +, \circ}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Unity of Ring|unity]] is $1_F$. Let $\struct {D, +, \circ}$ be a [[Definition:Subdomain|subdomain]] of $\struct {F, +, \circ}$ whose [[Definition:Unity of Ring|unity]] is $1_D$. Let: :$K = \set {\dfrac x y: x \in D, y \in D^*}$ where $\dfrac x y$ is the [[Definition:Division Product|division product]] of $x$ by $y$. Then $\struct {K, +, \circ}$ is a [[Definition:Field of Quotients|field of quotients]] of $\struct {D, +, \circ}$.	0
Let $\displaystyle \sum_{n \mathop = 1}^\infty b_n$ be a [[Definition:Divergent Series|divergent series]] of [[Definition:Positive Real Number|positive real numbers]]. Let $\sequence {a_n}$ be a [[Definition:Sequence|sequence in $\R$]]. Let: :$\forall n \in \N_{>0}: b_n \le a_n$ Then the [[Definition:Series|series]] $\displaystyle \sum_{n \mathop = 1}^\infty a_n$ [[Definition:Divergent Series|diverges]].	0
{{begin-eqn}} {{eqn | lo= \forall t \ge 1: | l = \size {\cos t} | o = \le | r = 1 | c = [[Real Cosine Function is Bounded]] }} {{eqn | ll= \leadsto | l = \size {\cos t} | o = < | r = 2 }} {{eqn | r = 2 e^{0 t} | c = [[Exponential of Zero]] }} {{end-eqn}} The result follows from the definition of [[Definition:Exponential Order to Real Index|exponential order]] with $M = 1$, $K = 2$, and $a = 0$. {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {x \left({x^n + a^n}\right)} = \frac 1 {n a^n} \ln \left\vert{\frac {x^n} {x^n + a^n} }\right\vert + C$	0
Let $\family{X_i}_{i \mathop \in I}$ be an [[Definition:Indexed Family|indexed family]] of [[Definition:Non-Empty Set|non-empty]] [[Definition:Topological Space|topological spaces]] where $I$ is an arbitrary [[Definition:Indexing Set|index set]]. Let $\displaystyle X := \prod_{i \mathop \in I} X_i$ be the corresponding [[Definition:Product Space of Topological Spaces|product space]]. Let $\pr_i: X \to X_i$ denote the [[Definition:Projection (Mapping Theory)|projection]] from $X$ onto $X_i$. Let $\FF \subset \powerset X$ be a [[Definition:Filter on Set|filter]] on $X$. Then $\FF$ [[Definition:Convergent Filter|converges]] {{iff}} for all $i \in I$ the [[Definition:Image Filter|image filter]] $\map {\pr_i} \FF$ converges.	0
Let $x \in a H \cap b K$. Then: {{begin-eqn}} {{eqn | l = x | o = \in | r = a H }} {{eqn | ll= \leadsto | l = x H | r = a H | c = [[Left Cosets are Equal iff Element in Other Left Coset]] }} {{end-eqn}} and similarly: {{begin-eqn}} {{eqn | l = x | o = \in | r = b K }} {{eqn | ll= \leadsto | l = x K | r = b K | c = [[Left Cosets are Equal iff Element in Other Left Coset]] }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = a H \cap b K | r = x H \cap x K }} {{eqn | r = x \paren {H \cap K} | c = [[Product of Subset with Intersection/Corollary|Corollary to Product of Subset with Intersection]] }} {{end-eqn}} Hence the result by definition of [[Definition:Left Coset|left coset]]. {{qed}}	0
Let $\struct {G, \circ}$ be the [[Definition:Internal Group Direct Product/General Definition|internal group direct product]] of $H_1, H_2, \ldots, H_n$. Let $h_i$ and $h_j$ be [[Definition:Element|elements]] of $H_i$ and $H_j$ respectively, $i \ne j$. Then $h_i \circ h_j = h_j \circ h_i$.	0
Consider the [[Sequence of Best Rational Approximations to Square Root of 2]]: :$\sequence S := \dfrac 1 1, \dfrac 3 2, \dfrac 7 5, \dfrac {17} {12}, \dfrac {41} {29}, \dfrac {99} {70}, \dfrac {239} {169}, \dfrac {577} {408}, \ldots$ where $S_1 := \dfrac 1 1$. The [[Definition:Numerator|numerators]] of the [[Definition:Term of Sequence|terms]] of $\sequence S$ are all [[Definition:Odd Integer|odd]]. For all $n$, the [[Definition:Parity of Integer|parity]] of the [[Definition:Denominator|denominator]] of [[Definition:Term of Sequence|term]] $S_n$ is the same as the [[Definition:Parity of Integer|parity]] of $n$.	0
{{begin-eqn}} {{eqn | l = \map \arcsin {-x} | r = y | c = }} {{eqn | ll= \leadstoandfrom | l = -x | r = \sin y: | rr = -\frac \pi 2 \le y \le \frac \pi 2 | c = {{Defof|Arcsine}} }} {{eqn | ll= \leadstoandfrom | l = x | r = -\sin y: | rr = -\frac \pi 2 \le y \le \frac \pi 2 | c = }} {{eqn | ll= \leadstoandfrom | l = x | r = \map \sin {-y}: | rr = -\frac \pi 2 \le y \le \frac \pi 2 | c = [[Sine Function is Odd]] }} {{eqn | ll= \leadstoandfrom | l = \arcsin x | r = -y | c = {{Defof|Arcsine}} }} {{end-eqn}} {{qed}}	0
Let $m, n \in \Z$ be [[Definition:Integer|integers]]. Let $\alpha \in \R$ be a [[Definition:Real Number|real number]]. Then: :$\ds \int_\alpha^{\alpha + 2 \pi} \cos m x \cos n x \rd x = \begin {cases} 0 & : m \ne n \\ \pi & : m = n \end {cases}$ That is: :$\ds \int_\alpha^{\alpha + 2 \pi} \cos m x \cos n x \rd x = \pi \delta_{m n}$ where $\delta_{m n}$ is the [[Definition:Kronecker Delta|Kronecker delta]].	0
Gauss's lemma on [[Definition:Polynomial|polynomials]] may refer to any of the following statements.	0
Let $\family {F_i}_{i \mathop \in I}$ be a non-empty [[Definition:Indexed Family of Sets|indexed family of sets]]. Suppose that all the [[Definition:Set|sets]] in the $\family {F_i}_{i \mathop \in I}$ are the same. That is, suppose that for some [[Definition:Set|set]] $S$: :$\forall i \in I: F_i = S$ Then: :$\displaystyle \bigcap_{i \mathop \in I} F_i = S$ where $\displaystyle \bigcap_{i \mathop \in I} F_i$ is the [[Definition:Intersection of Family|intersection of $\family {F_i}_{i \mathop \in I}$]].	0
:$(1): \quad x \mathrel {\mathcal R} y \iff e \mathrel {\mathcal R} y \circ x^{-1}$ :$(2): \quad x \mathrel {\mathcal R} y \iff e \mathrel {\mathcal R} x^{-1} \circ y$ :$(3): \quad x \mathrel {\mathcal R} y \iff x \circ y^{-1} \mathrel {\mathcal R} e$ :$(4): \quad x \mathrel {\mathcal R} y \iff y^{-1} \circ x \mathrel {\mathcal R} e$	0
Let $x \in G: x \ne e$. {{begin-eqn}} {{eqn | l = \order x | r = 2 | c = }} {{eqn | ll= \leadstoandfrom | l = x \circ x | r = e | c = {{Defof|Order of Group Element}} }} {{eqn | ll= \leadstoandfrom | l = x | r = x^{-1} | c = [[Equivalence of Definitions of Self-Inverse]] }} {{end-eqn}} {{qed}} {{expand|Free generalisation to monoids}}	0
:$\displaystyle \int \frac 1 {\sqrt {a^2 - x^2} } \rd x = \arcsin \frac x a + C$	0
{{begin-eqn}} {{eqn | l = \sum_{n \mathop = 0}^\infty z^n | r = \frac 1 {1 - z} | c = [[Sum of Infinite Geometric Sequence]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d z} } {\sum_{n \mathop = 1}^\infty z^n} | r = \dfrac \d {\d z} \frac 1 {1 - z} | c = }} {{eqn | r = \dfrac 1 {\paren {1 - z}^2} | c = [[Power Rule for Derivatives]] and the [[Chain Rule for Derivatives]] }} {{end-eqn}} Now we have: {{begin-eqn}} {{eqn | o = | r = \map {\frac \d {\d z} } {\sum_{n \mathop = 0}^\infty z^n} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\frac \d {\d z} z^n} | c = [[Derivative of Absolutely Convergent Series]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty n z^{n - 1} | c = [[Power Rule for Derivatives]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty n z^{n - 1} | c = as the [[Definition:Zeroth|zeroth]] term vanishes when $n = 0$ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {n + 1} z^n | c = [[Translation of Index Variable of Summation]] }} {{end-eqn}} Hence the result. {{qed}}	0
Let $\sequence {a_n}$ be the [[Definition:Sequence|sequence]] defined as: :$\forall n \in \N_{> 0}: a_n = H_n$ where $H_n$ denotes the $n$th [[Definition:Harmonic Number|harmonic number]]. Let $\map G z$ be the [[Definition:Generating Function|generating function]] for $\sequence {a_n}$: :$\map G z = \dfrac 1 {1 - z} \map \ln {\dfrac 1 {1 - z} }$ from [[Generating Function for Sequence of Harmonic Numbers]]. Then the [[Definition:Derivative|derivative]] of $\map G z$ {{WRT|Differentiation}} $z$ is given by: :$\map {G'} z = \dfrac 1 {\paren {1 - z}^2} \map \ln {\dfrac 1 {1 - z} } + \dfrac 1 {\paren {1 - z}^2}$	0
Let $X = \left\{{x}\right\}$ be a [[Definition:Singleton|singleton]]. Let $M$ be the [[Definition:Free Commutative Monoid|free commutative monoid]] on $X$. Then $M$ is [[Definition:Monoid Isomorphism|isomorphic]] to the [[Definition:Additive Monoid of Natural Numbers|additive monoid of natural numbers]].	0
Let $S$ be a [[Definition:Set|set]]. Let $f: \N \to S$ be a [[Definition:Mapping|mapping]], where $\N$ denotes the [[Definition:Set|set]] of [[Definition:Natural Numbers|natural numbers]]. Then $f$ is a [[Definition:Surjection|surjection]] {{iff}} $f$ [[Definition:Existential Quantifier|admits]] a [[Definition:Right Inverse Mapping|right inverse]].	0
Let $p$ and $q$ be [[Definition:Prime Number|prime numbers]] such that $p \ne q$. Let $G$ be a [[Definition:Group|group]] of [[Definition:Order of Group|order]] $p^2 q$. Then $G$ has a [[Definition:Normal Subgroup|normal]] [[Definition:Sylow p-Subgroup|Sylow $p$-subgroup]].	0
Let $\lambda \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map f x: \openint {-\lambda} \lambda \to \R$ be the [[Definition:Absolute Value|absolute value function]] on the [[Definition:Open Real Interval|open real interval]] $\openint {-\lambda} \lambda$: :$\forall x \in \openint {-\lambda} \lambda: \map f x = \size x$ The [[Definition:Fourier Series|Fourier series]] of $f$ over $\openint {-\lambda} \lambda$ can be given as: {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \sum_{n \mathop = 0}^\infty \frac 1 {\paren {2 n + 1}^2} \cos \dfrac {\paren {2 n + 1} \pi x} \lambda | c = }} {{eqn | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \paren {\cos \dfrac {\pi x} \lambda + \frac 1 {3^2} \cos \dfrac {3 \pi x} \lambda + \frac 1 {5^2} \dfrac {5 \pi x} \lambda + \dotsb} | c = }} {{end-eqn}}	0
:$\map {\dfrac \d {\d x} } {\arccot u} = -\dfrac 1 {1 + u^2} \dfrac {\d u} {\d x}$	0
From [[Metric Induces Topology]], the [[Definition:Metric Space|metric spaces]] described are [[Definition:Topological Space|topological spaces]]. The result follows from [[Composite of Continuous Mappings is Continuous]]. {{qed}}	0
:$\displaystyle \int \frac {\sech^{-1} \dfrac x a \rd x} x = \begin {cases} \displaystyle -\frac {\map \ln {\dfrac a x} \map \ln {\dfrac {4 a} x} } 2 - \sum_{k \mathop \ge 0} \frac {\paren {2 k + 1}!} {2^{2 k} \paren {k!}^2 \paren {2 k + 1}^3 \paren {2 k}^2} \paren {\frac x a}^{2 k} + C & : \sech^{-1} \dfrac x a > 0 \\ \displaystyle \frac {\map \ln {\dfrac a x} \map \ln {\dfrac {4 a} x} } 2 + \sum_{k \mathop \ge 0} \frac {\paren {2 k + 1}!} {2^{2 k} \paren {k!}^2 \paren {2 k + 1}^3 \paren {2 k}^2} \paren {\frac x a}^{2 k} + C & : \sech^{-1} \dfrac x a < 0 \\ \end {cases}$	0
{{begin-eqn}} {{eqn | l = \frac 1 {1 - \sin x} + \frac 1 {1 + \sin x} | r = \frac {1 + \sin x + 1 - \sin x} {1 - \sin^2 x} | c = [[Difference of Two Squares]] }} {{eqn | r = \frac 2 {\cos^2 x} | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 2 \sec^2 x | c = {{Defof|Secant Function}} }} {{end-eqn}} {{qed}} [[Category:Trigonometric Identities]] [[Category:Sine Function]] eoc9yum0imskau1zik8jiechj2yjo89	0
From [[Form of Geometric Sequence of Integers]]: :$P = \tuple {k p^2, k p q, k q^2}$ for some $k, p, q \in \Z$. If $a = k p^2$ is a [[Definition:Square Number|square number]] it follows that $k$ is a [[Definition:Square Number|square number]]: $k = r^2$, say. So: :$P = \tuple {r^2 p^2, r^2 p q, r^2 q^2}$ and so $c = r^2 q^2 = \paren {r q}^2$. {{qed}} {{Euclid Note|22|VIII}}	0
:$\map \sin {\theta + n \pi} = \paren {-1}^n \sin \theta$	0
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\left({P, \subseteq}\right)$ be the [[Definition:Ordered Set|ordered set]] consisting of all [[Definition:Ideal of Ring|ideals]] of $R$, ordered by [[Definition:Subset|inclusion]]. Let $\left\{{I_\alpha}\right\}_{\alpha \in A}$ be a [[Definition:Non-Empty Set|non-empty]] [[Definition:Chain (Set Theory)|chain]] of ideals in $P$. Let $\displaystyle I = \bigcup_{\alpha \in A} I_\alpha$ be their [[Definition:Set Union|union]]. Then $I$ is an [[Definition:Ideal of Ring|ideal]] of $R$.	0
:$\cos 90^\circ = \cos \dfrac \pi 2 = 0$	0
We need to check that all of [[Axiom:Peano's Axioms|Peano's axioms]] hold for $\left({\omega, \cdot^+, \varnothing}\right)$. Suppose first that for $m, n \in \omega$, we have $m^+ = n^+$. Since $n \in n^+$ it follows that $n \in m^+$. Hence, either $n \in m$ or $n = m$. Similarly, either $m \in n$ or $m = n$. Now if $n \ne m$, both $m \in n$ and $n \in m$. By [[Element of Minimal Infinite Successor Set is Transitive Set]], it follows that $n \subseteq m$. As $m \in n$, this contradicts [[Finite Ordinal is not Subset of one of its Elements]]. Hence it must be that $n = m$, and [[Axiom:Peano's Axioms|Axiom $(P3)$]] holds. Next, since $n \in n^+$ for all $n \in \omega$, it follows that $n^+ \ne \varnothing$. Hence, [[Axiom:Peano's Axioms|Axiom $(P4)$]] holds as well. Finally, let $S \subseteq \omega$ satisfy: :$\varnothing \in S$ :$\forall n \in S: n^+ \in S$ Then by definition, $S$ is an [[Definition:Infinite Successor Set|infinite successor set]]. Therefore, by definition of $\omega$ as the [[Definition:Minimal Infinite Successor Set|minimal infinite successor set]]: :$\omega \subseteq S$ Consequently $S = \omega$ by the definition of [[Definition:Set Equality/Definition 2|set equality]]. Thus [[Axiom:Peano's Axioms|Axiom $(P5)$]] is seen to hold. That is, $\left({\omega, \cdot^+, \varnothing}\right)$ is a [[Definition:Peano Structure|Peano structure]]. {{qed}}	0
Let the [[Definition:Field Zero|zero]] of $F$ be $0_F$. === Base Result === First we need to show that: :$\paren {m \cdot a} \times b = m \cdot \paren {a \times b}$ This will be done by [[Principle of Mathematical Induction|induction]]: For all $m \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\paren {m \cdot a} \times b = m \cdot \paren {a \times b}$ First we verify $\map P 0$. When $m = 0$, we have: {{begin-eqn}} {{eqn | l = \paren {0 \cdot a} \times b | r = 0_F \times b | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 0 \cdot x = 0_F$ }} {{eqn | r = 0_F | c = {{Defof|Field Zero}} }} {{eqn | r = 0 \cdot \paren {a \times b} | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 0 \cdot x = 0_F$ }} {{end-eqn}} So $\map P 0$ holds. ==== Basis for the Induction ==== Now we verify $\map P 1$: {{begin-eqn}} {{eqn | l = \paren {1 \cdot a} \times b | r = a \times b | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 1 \cdot x = x$ }} {{eqn | r = 1 \cdot \paren {a \times b} | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 1 \cdot x = x$ }} {{end-eqn}} So $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. ==== Induction Hypothesis ==== Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\paren {k \cdot a} \times b = k \cdot \paren {a \times b}$ Then we need to show: :$\paren {\paren {k + 1} \cdot a} \times b = \paren {k + 1} \cdot \paren {a \times b}$ ==== Induction Step ==== This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \paren {\paren {k + 1} \cdot a} \times b | r = \paren {a + \paren {k \cdot a} } \times b | c = {{Defof|Integral Multiple|subdef = Rings and Fields}} }} {{eqn | r = a \times b + \paren {k \cdot a} \times b | c = {{Field-axiom|D}} }} {{eqn | r = a \times b + k \cdot \paren {a \times b} | c = [[Product of Integral Multiples#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {k + 1} \cdot \paren {a \times b} | c = {{Defof|Integral Multiple|subdef = Rings and Fields}} }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall m \in \N: \paren {m \cdot a} \times b = m \cdot \paren {a \times b}$ {{qed|lemma}} The result for $m < 0$ follows directly from [[Powers of Group Elements]]. {{qed}} === Full Result === Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\forall m \in \Z: \paren {m \cdot a} \times \paren {n \cdot b} = \paren {m n} \cdot \paren {a \times b}$ First we verify $\map P 0$. When $n = 0$, we have: {{begin-eqn}} {{eqn | l = \paren {m \cdot a} \times \paren {0 \cdot b} | r = \paren {m \cdot a} \times 0_F | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 0 \cdot x = 0_F$ }} {{eqn | r = 0_F | c = {{Defof|Field Zero}} }} {{eqn | r = 0 \cdot \paren {a \times b} | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 0 \cdot x = 0_F$ }} {{eqn | r = \paren {m 0} \cdot \paren {a \times b} | c = }} {{end-eqn}} So $\map P 0$ holds. ==== Full Result - Basis for the Induction ==== Next we verify $\map P 1$. When $n = 1$, we have: {{begin-eqn}} {{eqn | l = \paren {m \cdot a} \times \paren {1 \cdot b} | r = \paren {m \cdot a} \times b | c = {{Defof|Integral Multiple|subdef = Rings and Fields}}: $\forall x \in F: 1 \cdot x = x$ }} {{eqn | r = m \cdot \paren {a \times b} | c = [[Product of Integral Multiples#Base Result|Base Result]] }} {{eqn | r = \paren {m 1} \cdot \paren {a \times b} | c = }} {{end-eqn}} So $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. ==== Full Result - Induction Hypothesis ==== Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\paren {m \cdot a} \times \paren {k \cdot b} = \paren {m k} \cdot \paren {a \times b}$ Then we need to show: :$\paren {m \cdot a} \times \paren {\paren {k + 1} \cdot b} = \paren {m \paren {k + 1} } \cdot \paren {a \times b}$ ==== Full Result - Induction Step ==== This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \paren {m \cdot a} \times \paren {\paren {k + 1} \cdot b} | r = \paren {m \cdot a} \times \paren {k \cdot b + b} | c = {{Defof|Integral Multiple|subdef = Rings and Fields}} }} {{eqn | r = \paren {m \cdot a} \times \paren {k \cdot b} + \paren {m \cdot a} \times b | c = {{Field-axiom|D}} }} {{eqn | r = \paren {m k} \cdot \paren {a \times b} + m \cdot \paren {a \times b} | c = [[Product of Integral Multiples#Full Result - Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {m k + k} \cdot \paren {a \times b} | c = [[Integral Multiple Distributes over Ring Addition]] }} {{eqn | r = \paren {m \paren {k + 1} } \cdot \paren {a \times b} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall m \in \Z: \forall n \in \N: \paren {m \cdot a} \times \paren {n \cdot b} = \paren {m n} \cdot \paren {a \times b}$ {{qed|lemma}} The result for $n < 0$ follows directly from [[Powers of Group Elements]]. {{qed}}	0
From [[Finite Subsets form Ideal]], the set of [[Definition:Finite Subset|finite subsets]] of $I$ form an [[Definition:Ideal (Order Theory)|ideal]] of $I$. From [[Elements with Support in Ideal form Submagma of Direct Product]], $T$ is a [[Definition:Submagma|submagma]] of $S$. [[Category:Direct Products]] a8j7iq8367fpt48etb26zeo5nb77b67	0
Let $\mathbf{Grp}$ be the [[Definition:Category of Groups|category of groups]]. Let $1 = \left\{{e}\right\}$ be the [[Definition:Trivial Group|trivial group]]. Then $1$ is an [[Definition:Initial Object|initial object]] of $\mathbf{Grp}$.	0
{{begin-eqn}} {{eqn | l = \cot 45 \degrees | r = \frac {\cos 45 \degrees} {\sin 45 \degrees} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac {\frac {\sqrt 2} 2} {\frac {\sqrt 2} 2} | c = [[Cosine of 45 Degrees|Cosine of $45 \degrees$]] and [[Sine of 45 Degrees|Sine of $45 \degrees$]] }} {{eqn | r = 1 | c = multiplying top and bottom by $\dfrac {\sqrt 2} 2$ }} {{end-eqn}} {{qed}}	0
Let $x \in \R$ be a [[Definition:Real Number|real number]]. Let $n \in \Z$ be an [[Definition:Even Integer|even integer]]. Then $x^n \ge 0$. That is, all [[Definition:Even Power|even powers]] are [[Definition:Positive Real Number|positive]].	0
Let $\map f x$ be a [[Definition:Minimal Polynomial|minimal polynomial]] of $\alpha$ over $K$ of [[Definition:Degree of Polynomial|degree]] $n$. From [[Minimal Polynomial is Unique]] we have that $\map f x$ is [[Definition:Unique|unique]]. Since $K$ is a [[Definition:Field (Abstract Algebra)|field]], we may assume that the [[Definition:Polynomial Coefficient|coefficient]] of $x^n$ is $1$. {{AimForCont}} that $f$ is not [[Definition:Irreducible Polynomial|irreducible]]. Then there exist non-[[Definition:Constant Polynomial|constant polynomials]] $g, h \in K \sqbrk x$ such that $f = g h$. By definition of $f$ as the [[Definition:Minimal Polynomial|minimal polynomial]] in $\alpha$: :$0 = \map f \alpha = \map g \alpha \, \map h \alpha$ Since $L$ is a [[Definition:Field (Abstract Algebra)|field]], it is an [[Definition:Integral Domain|integral domain]]. Therefore, as $\map g \alpha \, \map h \alpha \in L$, either $\map g \alpha = 0$ or $\map h \alpha = 0$. This contradicts the [[Definition:Minimal Polynomial|minimality]] of the [[Definition:Degree of Polynomial|degree]] of $f$. Hence the result, by [[Proof by Contradiction]]. {{qed}}	0
Let $I_1$ and $I_2$ be [[Definition:Real Interval|real intervals]]. Then $I_1 \cup I_2$ is not necessarily a [[Definition:Real Interval|real interval]].	0
Let $Q = \Dic 2 = \gen {a, b: a^4 = e, b^2 = a^2, a b a = b}$ be the [[Definition:Quaternion Group|quaternion group]]. The [[Definition:Conjugacy Class|conjugacy classes]] of $\Dic 2$ are: :$\set e, \set {a^2}, \set {a, a^3}, \set {b, a^2 b}, \set {a b, a^3 b}$	0
=== $(1)$ implies $(2)$ === If $F$ is a [[Definition:Complex Primitive|primitive]] of $f$, we have: {{begin-eqn}} {{eqn | l = \int_{C_1} \map f z \rd z | r = \map F {z_2} - \map F {z_1} | c = [[Fundamental Theorem of Calculus for Contour Integrals]] }} {{eqn | r = \int_{C_2} \map f z \rd z }} {{end-eqn}} {{qed|lemma}} === $(2)$ implies $(3)$ === Let $C$ be a [[Definition:Closed Contour (Complex Plane)|closed contour]] in $D$ with [[Definition:Endpoints of Contour (Complex Plane)|endpoints]] $z_0$. Let the [[Definition:Constant Mapping|constant function]] $\gamma: \closedint 0 1 \to D$ with $\map \gamma t = z_0$ be the [[Definition:Parameterization of Contour (Complex Plane)|parameterization of a contour]] $C_0$. Then: {{begin-eqn}} {{eqn | l = \oint_C \map f z \rd z | r = \oint_{C_0} \map f z \rd z | c = by assumption }} {{eqn | r = \int_0^1 \map f {\map \gamma t} \map {\gamma'} t \rd t | c = {{Defof|Complex Contour Integral}} }} {{eqn | r = 0 | c = by [[Derivative of Complex Polynomial]], as $\gamma$ is constant }} {{end-eqn}} {{qed|lemma}} === $(3)$ implies $(1)$ === This follows from [[Zero Staircase Integral Condition for Primitive]]. {{qed|lemma}} === Construction of a Primitive === If the conditions hold, we choose $z_0 \in D$ and define a [[Definition:Complex Function|function]] $F: D \to \C$ of $f$ by: :$\displaystyle \map F w = \int_{C_w} \map f z \rd z$ where $C_w$ is any [[Definition:Contour (Complex Plane)|contour]] in $D$ with [[Definition:Start Point of Contour (Complex Plane)|start point]] $z_0$ and [[Definition:End Point of Contour (Complex Plane)|end point]] $w$. From [[Connected Domain is Connected by Staircase Contours]], it follows that we can choose $C_w$ to be a [[Definition:Staircase Contour|staircase contour]]. If $C_w'$ is another [[Definition:Contour (Complex Plane)|contour]] in $D$ with the same [[Definition:Endpoint of Contour (Complex Plane)|endpoints]] as $C_w$, then: :$\displaystyle \int_{C_w'} \map f z \rd z = \int_{C_w} \map f z \rd z$ by condition $(2)$, so $F$ is well-defined. From [[Zero Staircase Integral Condition for Primitive]], it follows that $F$ is a [[Definition:Complex Primitive|primitive]] of $f$. {{qed}}	0
By continuity of $x \mapsto 1 / x$: :$\ds \lim_{N \mathop \to \infty} \prod_{n \mathop = 1}^N \frac 1 {1 + a_n} = \frac 1 a$ It remains to prove the absolute convergence. Because $\ds \prod_{n \mathop = 1}^\infty \paren {1 + a_n}$ [[Definition:Absolute Convergence of Product|converges absolutely]], $\ds \sum_{n \mathop = 1}^\infty a_n$ [[Definition:Absolutely Convergent Series|converges absolutely]]. By [[Factors in Absolutely Convergent Product Converge to One]], $\norm {a_n} \le \dfrac 1 2 $ for $n$ [[Definition:Sufficiently Large|sufficiently large]]. Thus $\norm {\dfrac 1 {a_n + 1} - 1} = \norm {\dfrac {a_n} {a_n + 1} } \le 2 \norm {a_n}$ for $n$ [[Definition:Sufficiently Large|sufficiently large]]. By the [[Comparison Test]], $\ds \sum_{n \mathop = 1}^\infty \paren {\frac 1 {a_n + 1} - 1}$ [[Definition:Absolutely Convergent Series|converges absolutely]]. Thus $\ds \prod_{n \mathop = 1}^\infty \frac 1 {1 + a_n}$ [[Definition:Absolute Convergence of Product|converges absolutely]]. {{qed}} [[Category:Infinite Products]] bvj860sg8ra24s02t8zbbtpfd9gxcwc	0
Let $\struct {S, \circ}$ be a [[Definition:Monoid|monoid]] whose [[Definition:Identity Element|identity element]] is $e$. For $a \in S$, let $\circ^n a = a^n$ be the [[Definition:Power of Element of Magma with Identity|$n$th power of $a$]]. Then: :$\forall m, n \in \N: a^{n m} = \paren {a^n}^m = \paren {a^m}^n$	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Then: :$z^{2 n} - 1 = \paren {z - 1} \paren {z + 1} \displaystyle \prod_{k \mathop = 1}^n \paren {z^2 - 2 \cos \dfrac {k \pi} n + 1}$	0
{{begin-eqn}} {{eqn | r = \paren {1 + \sin \dfrac \pi 5 + i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | o = | c = }} {{eqn | r = \frac {\paren {1 + \sin \dfrac \pi 5 + i \cos \dfrac \pi 5}^5} {\paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5} \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = }} {{eqn | r = \paren {\sin \dfrac \pi 5 + i \cos \dfrac \pi 5}^5 \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = [[Complex Division/Examples/(1 + sin theta + i cos theta) (1 + sin theta - i cos theta)^-1|Complex Division Examples: $\dfrac {1 + \sin \theta + i \cos \theta} {1 + \sin \theta - i \cos \theta}$]] }} {{eqn | r = i^5 \paren {-i \sin \dfrac \pi 5 + \cos \dfrac \pi 5}^5 \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = multiplying left hand term by $i^5 \times -i = 1$ }} {{eqn | r = i \paren {\cos \pi - i \sin \pi} \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = [[De Moivre's Formula]] }} {{eqn | r = i \paren {-1} \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = [[Sine of Straight Angle]] and [[Cosine of Straight Angle]] }} {{eqn | r = -i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 + i \paren {1 + \sin \dfrac \pi 5 - i \cos \dfrac \pi 5}^5 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}}	0
The [[Definition:Integer Sequence|sequence]] of [[Definition:Square Number|square numbers]] which can be expressed as the [[Definition:Integer Addition|sum]] of a [[Definition:Integer Sequence|sequence]] of [[Definition:Odd Number|odd]] [[Definition:Cube Number|cubes]] from $1$ begins: :$1, 1225, 1 \, 413 \, 721, 1 \, 631 \, 432 \, 881, \dotsc$ {{OEIS|A046177}} The [[Definition:Integer Sequence|sequence]] of [[Definition:Square Root|square roots]] of this [[Definition:Integer Sequence|sequence]] is: :$1, 35, 1189, 40 \, 391, \dotsc$ {{OEIS|A046176}}	0
Let $S$ and $T$ be [[Definition:Set|sets]]. Let $\RR$ be an [[Definition:Equivalence Relation|equivalence relation]] on $S$. Let $f: S \to T$ be a [[Definition:Mapping|mapping]] from $S$ to $T$. Let $S / \RR$ be the [[Definition:Quotient Set|quotient set of $S$ induced by $\RR$]]. Let $q_\RR: S \to S / \RR$ be the [[Definition:Quotient Mapping|quotient mapping induced by $\RR$]]. Then: :there exists a [[Definition:Mapping|mapping]] $\phi: S / \RR \to T$ such that $\phi \circ q_\RR = f$ {{iff}}: :$\forall x, y \in S: \tuple {x, y} \in \RR \implies \map f x = \map f y$ ::$\begin {xy} \xymatrix@L + 2mu@ + 1em { S \ar[r]^*{f} \ar[d]_*{q_\RR} & T \\ S / \RR \ar@{-->}[ur]_*{\phi} } \end {xy}$	0
From [[Reciprocal of Strictly Positive Real Number is Strictly Positive]]: :$(1): \quad x > 0 \implies \dfrac 1 x > 0$ :$(2): \quad y > 0 \implies \dfrac 1 y > 0$ Then: {{begin-eqn}} {{eqn | l = x | o = > | r = y | c = }} {{eqn | ll= \leadsto | l = x \times \frac 1 x | o = > | r = y \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R O2$]]: [[Definition:Relation Compatible with Operation|compatibility]] with [[Definition:Real Multiplication|multiplication]] and from $(1)$ }} {{eqn | ll= \leadsto | l = x \times \frac 1 x \times \frac 1 y | o = > | r = y \times \frac 1 x \times \frac 1 y | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R O2$]]: [[Definition:Relation Compatible with Operation|compatibility]] with [[Definition:Real Multiplication|multiplication]] and from $(2)$ }} {{eqn | ll= \leadsto | l = \paren {x \times \frac 1 x} \times \frac 1 y | o = > | r = \paren {y \times \frac 1 y} \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R M1$ (Associativity) and $\R M2$ (Commutativity)]] }} {{eqn | ll= \leadsto | l = 1 \times \frac 1 y | o = > | r = 1 \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R M4$ (Inverses)]] }} {{eqn | ll= \leadsto | l = \frac 1 y | o = > | r = \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R M3$ (Identity)]] }} {{eqn | ll= \leadsto | l = \frac 1 x | o = < | r = \frac 1 y | c = {{Defof|Dual Ordering}} }} {{end-eqn}} {{qed}}	0
The [[Definition:Integer Sequence|sequence]] of [[Definition:Positive Integer|positive integers]] $n$ which are the smallest such that they are the [[Definition:Integer Addition|sum]] of $2$ [[Definition:Odd Prime|odd primes]] in $k$ different ways begins as follows: :{| border="1" |- ! align="right" style = "padding: 2px 10px" | $k$ ! align="right" style = "padding: 2px 10px" | $n$ |- | align="right" style = "padding: 2px 10px" | $1$ | align="right" style = "padding: 2px 10px" | $6$ |- | align="right" style = "padding: 2px 10px" | $2$ | align="right" style = "padding: 2px 10px" | $10$ |- | align="right" style = "padding: 2px 10px" | $3$ | align="right" style = "padding: 2px 10px" | $22$ |- | align="right" style = "padding: 2px 10px" | $4$ | align="right" style = "padding: 2px 10px" | $34$ |- | align="right" style = "padding: 2px 10px" | $5$ | align="right" style = "padding: 2px 10px" | $48$ |- | align="right" style = "padding: 2px 10px" | $6$ | align="right" style = "padding: 2px 10px" | $60$ |} {{OEIS|A001172}}	0
Let $z = x + i y$. Then: {{begin-eqn}} {{eqn | l = \dfrac {\partial f} {\partial x} \left({z}\right) | r = \dfrac {\partial u} {\partial x} \left({x, y}\right) + i \dfrac {\partial v} {\partial x} \left({x, y}\right) }} {{eqn | r = \operatorname{Re} \left({f' \left({z}\right) }\right) + i \operatorname{Im} \left({f' \left({z}\right) }\right) | c = from the last part of the [[Cauchy-Riemann Equations/Sufficient Condition|proof for sufficient condition]] }} {{eqn | r = f' \left({z}\right) }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = -i \dfrac {\partial f} {\partial y} \left({z}\right) | r = -i \left({\dfrac {\partial u} {\partial y} \left({x, y}\right) + i \dfrac {\partial v} {\partial y} \left({x, y}\right) }\right) }} {{eqn | r = -i \left({ -\operatorname{Im} \left({f' \left({z}\right) }\right) + i \operatorname{Re} \left({f' \left({z}\right) }\right) }\right) | c = from the last part of the [[Cauchy-Riemann Equations/Sufficient Condition|proof for sufficient condition]] }} {{eqn | r = f' \left({z}\right) }} {{end-eqn}} {{qed}}	0
Let $E$ denote the [[Definition:Set|set]] of [[Definition:Even Integer|even]] [[Definition:Natural Numbers|natural numbers]]. {{AimForCont}} $E$ is [[Definition:Finite Set|finite]]. Then there exists $n \in \N$ such that $E$ has $n$ [[Definition:Element|elements]]. Let $m$ be the [[Definition:Greatest Element|greatest element]] of $E$. But then $m + 2$ is an [[Definition:Even Integer|even]] [[Definition:Natural Numbers|natural number]]. But $m + 2 > m$, and $m$ is the [[Definition:Greatest Element|greatest element]] of $E$. Therefore $m + 2$ is an [[Definition:Even Integer|even]] [[Definition:Natural Numbers|natural number]] that is not an [[Definition:Element|element]] of $E$. So $E$ does not contain all the [[Definition:Even Integer|even]] [[Definition:Natural Numbers|natural numbers]]. From that [[Definition:Contradiction|contradiction]] it follows by [[Proof by Contradiction]] that $E$ is not [[Definition:Finite Set|finite]]. {{Qed}}	0
From [[Null Ring is Ring]], the [[Definition:Null Ring|null ring]] $\struct {\set {0_R}, +, \circ}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. We also have that $\set {0_R}$ is a [[Definition:Subset|subset]] of $R$ Hence the result by definition of [[Definition:Subring|subring]]. {{qed}}	0
From [[Quotient Group is Group]], the operation: :$\forall a, b \in G: \paren {a \circ N} \circ \paren {b \circ N} = \paren {a \circ b} \circ N$ is the [[Definition:Group Operation|group operation]] in the [[Definition:Quotient Group|quotient group]] $\struct {G / N, \circ}$. The result follows directly by definition of [[Definition:Power of Group Element|power of group element]]. {{qed}}	0
The only [[Definition:Divisor of Integer|divisors]] of $1$ are $1$ and $-1$. That is: :$a \divides 1 \iff a = \pm 1$	0
First it is shown that $\map N z = 0 \iff z = 0$. {{begin-eqn}} {{eqn | l = z | r = 0 | c = }} {{eqn | r = 0 + 0 i | c = }} {{eqn | ll= \leadsto | l = \map N z | r = 0^2 + 0^2 | c = Definition of $N$ }} {{eqn | r = 0 | c = }} {{end-eqn}} Let $z = x + i y$. {{begin-eqn}} {{eqn | l = \map N z | r = 0 | c = }} {{eqn | ll= \leadsto | l = \map N {x + i y} | r = 0 | c = Definition of $z$ }} {{eqn | ll= \leadsto | l = x^2 + y^2 | r = 0 | c = Definition of $N$ }} {{eqn | ll= \leadsto | l = a | r = 0 | c = [[Square of Real Number is Non-Negative]] }} {{eqn | l = b | r = 0 | c = }} {{eqn | ll= \leadsto | l = z | r = 0 | c = Definition of $z$ }} {{end-eqn}} Then we have: {{begin-eqn}} {{eqn | l = \map N z | r = \map N {x + i y} | c = Definition of $z$ }} {{eqn | r = x^2 + y^2 | c = Definition of $N$ }} {{eqn | 0 = \ge | r = 0 | c = [[Square of Real Number is Non-Negative]] }} {{end-eqn}} Hence the result by definition of [[Definition:Positive Definite (Ring)|positive definite]]. {{qed}} [[Category:Field Norm of Complex Number]] fgrcmu847s49n7be08imferkgspbgaq	0
First, from [[Closed Form for Triangular Numbers]]: :$\displaystyle \sum_{i \mathop = 1}^n i = \frac {n \paren {n + 1} } 2$ So: :$\displaystyle \paren {\sum_{i \mathop = 1}^n i}^2 = \dfrac {n^2 \paren {n + 1}^2} 4$ Next we use [[Principle of Mathematical Induction|induction]] on $n$ to show that: :$\displaystyle \sum_{i \mathop = 1}^n i^3 = \dfrac {n^2 \paren {n + 1}^2} 4$ The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{i \mathop = 1}^n i^3 = \dfrac {n^2 \paren {n + 1}^2} 4$ === Basis for the Induction === $\map P 1$ is the case: :$1^3 = \dfrac {1 \paren {1 + 1}^2} 4$ {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^1 i^3 | r = 1^3 | c = }} {{eqn | r = \dfrac {1^2 \paren {1 + 1}^2} 4 | c = }} {{end-eqn}} Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{i \mathop = 1}^k i^3 = \dfrac {k^2 \paren {k + 1}^2} 4$ from which it is to be shown that: :$\displaystyle \sum_{i \mathop = 1}^{k + 1} i^3 = \dfrac {\paren {k + 1}^2 \paren {k + 2}^2} 4$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^{k + 1} i^3 | r = \sum_{i \mathop = 1}^k i^3 + \paren {k + 1}^3 | c = }} {{eqn | r = \frac {k^2 \paren {k + 1}^2} 4 + \paren {k + 1}^3 | c = [[Sum of Sequence of Cubes/Proof by Induction#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \frac {k^4 + 2 k^3 + k^2} 4 + \frac {4 k^3 + 12 k^2 + 12 k + 4} 4 | c = }} {{eqn | r = \frac {k^4 + 6 k^3 + 13 k^2 + 12 k + 4} 4 | c = }} {{eqn | r = \frac {\paren {k + 1}^2 \paren {k + 2}^2} 4 | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{>0}: \displaystyle \sum_{i \mathop = 1}^n i^3 = \dfrac {n^2 \paren {n + 1}^2} 4$	0
An [[Definition:Group Isomorphism|group isomorphism]] is by definition a [[Definition:Group Epimorphism|group epimorphism]]. The result follows from [[Epimorphism Preserves Identity]]. {{Qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {x^3 \ \mathrm d x} {\left({x^2 + a^2}\right)^2} | r = \int \frac {x \left({x^2 + a^2 - a^2}\right)} {\left({x^2 + a^2}\right)^2} \ \mathrm d x | c = }} {{eqn | r = \int \frac {x \left({x^2 + a^2}\right)} {\left({x^2 + a^2}\right)^2} \ \mathrm d x - a^2 \int \frac {x \ \mathrm d x} {\left({x^2 + a^2}\right)^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \int \frac {x \ \mathrm d x} {x^2 + a^2} - a^2 \int \frac {x \ \mathrm d x} {\left({x^2 + a^2}\right)^2} | c = simplification }} {{eqn | r = \frac 1 2 \ln \left({x^2 + a^2}\right) - a^2 \int \frac {x \ \mathrm d x} {\left({x^2 + a^2}\right)^2} + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{eqn | r = \frac 1 2 \ln \left({x^2 + a^2}\right) - a^2 \left({\frac {-1} {2 \left({x^2 - a^2}\right)} }\right) + C | c = [[Primitive of x over x squared minus a squared squared|Primitive of $\dfrac x {\left({x^2 + a^2}\right)^2}$]] }} {{eqn | r = \frac {a^2} {2 \left({x^2 + a^2}\right)} + \frac 1 2 \ln \left({x^2 + a^2}\right) + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\dfrac 1 {\map \exp z} = \map \exp {-z}$	0
Let $S_3$ denote the [[Symmetric Group on 3 Letters|symmetric group on $3$ letters]]. Let $G$ be the [[Definition:Group Generated by Reciprocal of z and 1 minus z|group generated by $1 / z$ and $1 - z$]]. Then $S_3$ and $G$ are [[Definition:Isomorphic Algebraic Structures|isomorphic algebraic structures]].	0
{{begin-eqn}} {{eqn | l = \map {\coth^{-1} } {-x} | r = \frac 1 2 \map \ln {\frac {-z + 1} {-z - 1} } | c = {{Defof|Inverse Hyperbolic Cotangent|subdef = Real|index = 2}} }} {{eqn | r = \frac 1 2 \map \ln {\frac {z - 1} {z + 1} } | c = multiplying the argument by $\dfrac {-1} {-1}$ }} {{eqn | r = \frac 1 2 \paren {\map \ln {z - 1} - \map \ln {z + 1} } | c = [[Difference of Logarithms]] }} {{eqn | r = -\frac 1 2 \paren {\map \ln {z + 1} - \map \ln {z - 1} } | c = }} {{eqn | r = -\frac 1 2 \map \ln {\frac {z + 1} {z - 1} } | c = [[Difference of Logarithms]] }} {{eqn | r = -\coth^{-1} x | c = {{Defof|Inverse Hyperbolic Cotangent|subdef = Real|index = 2}} }} {{end-eqn}} {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \paren {x h}^2 h^{-1} \paren {x^{-1} }^2 | r = x h x h h^{-1} x^{-1} x^{-1} | c = {{GroupAxiom|1}} }} {{eqn | r = x h x x^{-1} x^{-1} | c = {{GroupAxiom|3}} }} {{eqn | r = x h x^{-1} | c = {{GroupAxiom|3}} }} {{end-eqn}} Because $\paren {x h}^2$ and $\paren {x^{-1} }^2$ are in the form $x^2$ for $x \in G$, they are both [[Definition:Element|elements]] of $H$. Thus: :$x h x^{-1} \in H$ and so $H$ is [[Definition:Normal Subgroup|normal]] in $G$ by definition. {{qed}}	0
Because $a$ has [[Definition:Multiplicity (Polynomial)|multiplicity]] at least $2$, we can write: : $f \left({X}\right) = \left({X - a}\right)^2 g \left({X}\right)$ with $g \left({X}\right) \in R \left[{X}\right]$. From [[Formal Derivative of Polynomials Satisfies Leibniz's Rule]]: :$f' \left({X}\right) = 2 \left({X - a}\right) g \left({X}\right) + \left({X - a}\right)^2 g' \left({X}\right)$ and thus: :$f' \left({a}\right) = 0$ {{qed}} [[Category:Polynomial Theory]] 1sf2w1ms5ssraoogsc824daz8mq01ov	0
:$\sin 18 \degrees = \sin \dfrac \pi {10} = \dfrac {\sqrt 5 - 1} 4$ where $\sin$ denotes the [[Definition:Sine Function|sine function]].	0
In the [[Definition:Prime Decomposition|prime decompositions]] $(1)$ and $(2)$, we have that: :$q_1 < q_2 < \dotsb < q_r$ and: :$s_1 < s_2 < \dotsb < s_u$ Hence we can define: {{begin-eqn}} {{eqn | l = E | r = \set {q_1, q_2, \ldots, q_r} }} {{eqn | l = F | r = \set {s_1, s_2, \ldots, s_u} }} {{end-eqn}} as all the $q_1, q_2, \dotsc, q_r$ are [[Definition:Distinct Elements|distinct]], and all the $s_1, s_2, \dotsc, s_u$ are [[Definition:Distinct Elements|distinct]]. Then let: :$T = E \cup F$ and let the [[Definition:Element|elements]] of $T$ be renamed as: :$T = \set {t_1, t_2, \ldots, t_v}$ where all the $t_1, t_2, \dotsc, t_v$ are [[Definition:Distinct Elements|distinct]], and: :$t_1 < t_2 < \dotsb < t_v$ Let $\iota: E \to T$ be the [[Definition:Inclusion Mapping|inclusion mapping]]: :$\forall q_i \in E: \map \iota {q_i} = q_i$ Let $\iota: F \to T$ be the [[Definition:Inclusion Mapping|inclusion mapping]]: :$\forall s_i \in F: \map \iota {s_i} = s_i$ Then we have that: {{begin-eqn}} {{eqn | l = a | r = \prod_{i \mathop = 1}^r {q_i}^{e_i} | c = }} {{eqn | r = \prod_{q_i \mathop \in E} {q_i}^{e_i} \times \prod_{t_i \mathop \in T \mathop \setminus E} {t_i}^0 | c = }} {{eqn | r = \prod_{t_j \mathop \in T} {t_j}^{g_j} | c = where $g_j = \begin {cases} e_i & : t_j = q_i \\ 0 & : t_j \notin E \end{cases}$ }} {{eqn | r = {t_1}^{g_1} {t_2}^{g_2} \dotsm {t_v}^{g_v} | c = for some $g_1, g_2, \dotsc, g_v \in \Z_{\ge 0}$ }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = b | r = \prod_{i \mathop = 1}^r {s_i}^{f_i} | c = }} {{eqn | r = \prod_{s_i \mathop \in F} {s_i}^{f_i} \times \prod_{t_i \mathop \in T \mathop \setminus F} {t_i}^0 | c = }} {{eqn | r = \prod_{t_j \mathop \in T} {t_j}^{h_j} | c = where $h_j = \begin {cases} f_i & : t_j = s_i \\ 0 & : t_j \notin F \end{cases}$ }} {{eqn | r = {t_1}^{h_1} {t_2}^{h_2} \dotsm {t_v}^{h_v} | c = for some $h_1, h_2, \dotsc, h_v \in \Z_{\ge 0}$ }} {{end-eqn}} Thus $a$ and $b$ can be expressed as the [[Definition:Integer Multiplication|product]] of [[Definition:Integer Power|powers]] of the same [[Definition:Prime Number|primes]], on the understanding that one or more of the [[Definition:Integer Power|powers]] in either [[Definition:Integer Multiplication|product]] may be [[Definition:Zero (Number)|zero]]. {{qed}}	0
:$\map \Gamma {-\dfrac 5 2} = -\dfrac {8 \sqrt \pi} {15}$	0
We have, by [[Euler's Formula/Corollary|Euler's Formula: Corollary]]: :$\map \exp {-i a x^2} = -i \map \sin {a x^2} + \map \cos {a x^2}$ As $\map \sin {a x^2}$ and $\map \cos {a x^2}$ are both [[Definition:Real Number|real]] for real $a, x$, we therefore have: {{begin-eqn}} {{eqn | l = \int_0^\infty \map \sin {a x^2} \rd x | r = -\int_0^\infty \map \Im {\map \exp {-i a x^2} } \rd x }} {{eqn | r = -\map \Im {\int_0^\infty \map \exp {-i a x^2} \rd x} }} {{eqn | r = -\frac 1 {\sqrt a} \map \Im {\int_0^\infty \map \exp {-i t^2} \rd t} | c = [[Integration by Substitution|substituting]] $\sqrt a x = t$ }} {{eqn | r = -\frac 1 {\sqrt a} \map \Im {\frac 1 2 \sqrt {\frac \pi 2} \paren {1 - i} } | c = [[Definite Integral to Infinity of Exponential of -i x^2|Definite Integral to Infinity of $\map \exp {-i x^2}$]] }} {{eqn | r = \frac 1 2 \sqrt {\frac \pi {2 a} } }} {{end-eqn}} {{qed}}	0
Let $\left({S, \circ}\right)$ be the [[Definition:External Direct Product|external direct product]] of the [[Definition:Algebraic Structure|algebraic structures]] $\left({S_1, \circ_1}\right), \left({S_2, \circ_2}\right), \ldots, \left({S_k, \circ_k}\right), \ldots, \left({S_n, \circ_n}\right)$. Then: :for each $j \in \left[{1 \,.\,.\, n}\right]$, $\operatorname{pr}_j$ is an [[Definition:Epimorphism (Abstract Algebra)|epimorphism]] from $\left({S, \circ}\right)$ to $\left({S_j, \circ_j}\right)$ where $\operatorname{pr}_j: \left({S, \circ}\right) \to \left({S_j, \circ_j}\right)$ is the [[Definition:Projection (Mapping Theory)|$j$th projection]] from $\left({S, \circ}\right)$ to $\left({S_j, \circ_j}\right)$.	0
Let $f: \N \to \C$ be a [[Definition:Multiplicative Arithmetic Function|multiplicative function]]. If $f$ is not identically zero, then $\map f 1 = 1$.	0
:$\displaystyle \int \frac {\mathrm d x} {x^2 \sqrt {x^2 + a^2} } = \frac {-\sqrt {x^2 + a^2} } {a^2 x} + C$	0
Let $\phi: \struct {S, \circ, \preceq} \to \struct {T, *, \preccurlyeq}$ be an [[Definition:Ordered Semigroup Monomorphism|ordered semigroup monomorphism]]. Then $\phi$ is an [[Definition:Injection|injection]] into $\struct {T, *, \preccurlyeq}$ by definition. From [[Restriction of Mapping to Image is Surjection]], a [[Definition:Mapping|mapping]] from a [[Definition:Set|set]] to the [[Definition:Image of Mapping|image]] of that [[Definition:Mapping|mapping]] is a [[Definition:Surjection|surjection]]. Thus the [[Definition:Surjective Restriction|surjective restriction]] of $\phi$ onto $S'$ is an [[Definition:Ordered Semigroup Monomorphism|ordered semigroup monomorphism]] which is also a [[Definition:Surjection|surjection]]. Hence the result from [[Ordered Semigroup Isomorphism is Surjective Monomorphism]]. {{qed}}	0
Let $S$ be [[Definition:Bounded Above Set|bounded above]] by $x \in \R$. By the [[Archimedean Principle]], there exists an [[Definition:Integer|integer]] $n \ge x$. Then $S$ is [[Definition:Bounded Above Set|bounded above]] by $n$. By [[Set of Integers Bounded Above by Integer has Greatest Element]], $S$ has a [[Definition:Greatest Element|greatest element]]. {{qed}}	0
$n$ can be represented as: {{begin-eqn}} {{eqn | l = n | r = \sum_{j \mathop = 0}^m a_j p^j | c = where $0 \le a_j < p$ }} {{eqn | r = a_0 + a_1 p + a_2 p^2 + \cdots + a_m p^m | c = for some $m > 0$ }} {{end-eqn}} Using [[De Polignac's Formula]], we may extract all the [[Definition:Integer Power|powers of $p$]] from $n!$. :$\mu = \displaystyle \sum_{k \mathop > 0} \floor {\dfrac n {p^k} }$ where $\mu$ is the [[Definition:Multiplicity of Prime Factor|multiplicity]] of $p$ in $n!$: :$p^\mu \divides n!$ :$p^{\mu + 1} \nmid n!$ We have that: {{begin-eqn}} {{eqn | l = \floor {\dfrac {n!} p} | r = \floor {\dfrac {a_0 + a_1 p + a_2 p^2 + a_3 p^3 + \cdots + a_m p^m} p} | c = }} {{eqn | r = a_1 + a_2 p + a_3 p^2 + \cdots + a_m p^{m - 1} | c = }} {{eqn | l = \floor {\dfrac {n!} {p^2} } | r = \floor {\dfrac {a_0 + a_1 p + a_2 p^2 + + a_2 p^2 + \cdots + a_m p^m} {p^2} } | c = }} {{eqn | r = a_2 + a_3 p + \cdots + a_m p^{m - 2} | c = }} {{eqn | o = \vdots }} {{eqn | l = \floor {\dfrac {n!} {p^m} } | r = \floor {\dfrac {a_0 + a_1 p + a_2 p^2 + + a_2 p^2 + \cdots + a_m p^m} {p^m} } | c = }} {{eqn | r = a_m | c = }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = \mu | r = a_m \paren {p^{m - 1} + p^{m - 2} + \cdots + p + 1} | c = }} {{eqn | o = | ro= + | r = a_{m - 1} \paren {p^{m - 2} + p^{m - 3} + \cdots + p + 1} | c = }} {{eqn | o = \vdots }} {{eqn | o = | ro= + | r = a_2 \paren {p + 1} | c = }} {{eqn | o = | ro= + | r = a_1 | c = }} {{eqn | r = a_m \paren {\dfrac {p^m - 1} {p - 1} } + a_{m - 1} \paren {\dfrac {p^{m - 1} - 1} {p - 1} } | c = [[Sum of Geometric Sequence]] }} {{eqn | o = \vdots }} {{eqn | o = | ro= + | r = a_2 \paren {\dfrac {p^2 - 1} {p - 1} } + a_1 \paren {\dfrac {p^1 - 1} {p - 1} } + a_0 \paren {\dfrac {p^0 - 1} {p - 1} } | c = where the last term evaluates to $0$ }} {{eqn | r = \dfrac {\paren {a_m p^m + a_{m - 1} p^{m - 1} + \cdots + a_2 p^2 + a_1 p + a_0} - \paren {a_m + a_{m - 1} + \cdots + a_2 + a_1 + a_0} } {p - 1} | c = }} {{eqn | r = \dfrac {n - r} {p - 1} | c = }} {{end-eqn}} Hence the result. {{qed}} {{Namedfor|Adrien-Marie Legendre|cat = Legendre}}	0
By [[Riemann Removable Singularities Theorem]], as $f$ has a [[Definition:Removable Singularity|removable singularity]] at $\infty$, $f$ must be [[Definition:Bounded Mapping/Complex-Valued|bounded]] in a [[Definition:Neighborhood of Infinity (Complex Analysis)|neighborhood of $\infty$]]. That is, there exists a [[Definition:Real Number|real number]] $M > 0$ such that $\cmod {\map f z} \le M$ for all $z \in \set{z : \cmod z > r}$ for some real $r \ge 0$. However, by [[Continuous Function on Compact Space is Bounded]], $f$ is also bounded on $\set{z : \cmod z \le r}$. As $\set{z : \cmod z > r} \cup \set{z : \cmod z \le r} = \C$, $f$ is therefore bounded on $\C$. Therefore by [[Liouville's Theorem (Complex Analysis)|Liouville's Theorem]], $f$ is constant. {{qed}} [[Category:Complex Analysis]] ft58yz7272i4zngzeagf3r0b1n94if3	0
If $\psi = \chi_0$, then it is straightforward that: {{begin-eqn}} {{eqn | l = \sum_{x \mathop \in G} \psi \left({x}\right) | r = \sum_{x \mathop \in G} \chi_0 \left({x}\right) | c = Assumption }} {{eqn | r = \sum_{x \mathop \in G} 1 | c = {{Defof|Trivial Character}} }} {{eqn | r = \vert G \vert | c = }} {{end-eqn}} If $\psi \neq \chi_0$, then $\exists y \in G$ such that $\psi \left({y}\right) \neq 1$. As $x$ runs through $G$ in the summation, $yx$ also runs through $G$. So: {{begin-eqn}} {{eqn | l = \sum_{x \mathop \in G} \psi \left({x}\right) | r = \sum_{x \mathop \in G} \psi \left({y x}\right) | c = by the above claim }} {{eqn | r = \sum_{x \mathop \in G} \psi \left({y}\right) \psi \left({x}\right) | c = $\psi$ as a [[Definition:Character (Number Theory)|character]] is by definition a [[Definition:Group Homomorphism|homomorphism]] }} {{eqn | r = \psi \left({y}\right) \sum_{x \mathop \in G} \psi \left({x}\right) | c = }} {{end-eqn}} Since by assumption $\psi \left({y}\right) \ne 1$, it must be true that: :$\displaystyle \sum_{x \mathop \in G} \psi \left({x}\right) = 0$ {{qed}} {{refactor|Suggestion: Move the dual space version into a corollary, as it is a direct application of the main theorem on the dual space, with identifying $x \in G$ as a character: $x: \chi \mapsto \chi \left({x}\right)$}} [[Category:Analytic Number Theory]] abb5loovdpeici0lkr1ik5j3bcfxspw	0
From [[Transpose of Upper Triangular Matrix is Lower Triangular]], the [[Definition:Transpose of Matrix|transpose]] $\mathbf T_n^\intercal$ of $\mathbf T_n$ is an [[Definition:Upper Triangular Matrix|upper triangular matrix]]. From [[Determinant of Upper Triangular Matrix]], the [[Definition:Determinant of Matrix|determinant]] of $\mathbf T_n^\intercal$ is equal to the product of all the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf T_n^\intercal$. From [[Determinant of Transpose]], the [[Definition:Determinant of Matrix|determinant]] of $\mathbf T_n^\intercal$ equals the [[Definition:Determinant of Matrix|determinant]] of $\mathbf T_n$. {{qed}}	0
Let us verify the axioms $(\text C 1)$ up to $(\text C 3)$ for a [[Definition:Metacategory|metacategory]]. For any two mappings their [[Definition:Composition of Mappings|composition]] (in the usual [[Definition:Set Theory|set theoretic]] sense) is again a mapping by [[Composite Mapping is Mapping]]. For any set $X$, we have the [[Definition:Identity Mapping|identity mapping]] $\operatorname{id}_X$. By [[Identity Mapping is Left Identity]] and [[Identity Mapping is Right Identity]], this is the [[Definition:Identity Morphism|identity morphism]] for $X$. Finally by [[Composition of Mappings is Associative]], the associative property is satisfied. Hence $\mathbf{Set}$ is a [[Definition:Metacategory|metacategory]]. {{qed}} [[Category:Examples of Categories]] [[Category:Category of Sets]] rpd0zesyfpvfk4le8e9ow3t0xrsnjyl	0
:$\displaystyle \int \frac {\d x} {x \sqrt {x^2 + a^2} } = \frac 1 a \map \ln {\frac x {a + \sqrt {x^2 + a^2} } } + C$	0
By definition, $\kappa_x: G \to G$ is a [[Definition:Mapping|mapping]] defined as: :$\forall g \in G: \map {\kappa_x} g = x g x^{-1}$ We need to show that $\kappa_x$ is an [[Definition:Group Automorphism|automorphism]]. First we show $\kappa_x$ is a [[Definition:Group Homomorphism|homomorphism]]. {{begin-eqn}} {{eqn | ll= \forall g, h \in G: | l = \map {\kappa_x} g \map {\kappa_x} h | r = \paren {x g x^{-1} } \paren {x h x^{-1} } | c = Definition of $\kappa_x$ }} {{eqn | r = x g \paren {x^{-1} x} h x^{-1} | c = {{GroupAxiom|1}} }} {{eqn | r = x \paren {g e h} x^{-1} | c = {{GroupAxiom|3}} }} {{eqn | r = x \paren {g h} x^{-1} | c = {{GroupAxiom|2}} }} {{eqn | r = \map {\kappa_x} {g h} | c = Definition of $\kappa_x$ }} {{end-eqn}} Thus the [[Definition:Morphism Property|morphism property]] is demonstrated. Next we show that $\kappa_x$ is [[Definition:Injection|injective]]. {{begin-eqn}} {{eqn | l = \map {\kappa_x} g | r = \map {\kappa_x} h }} {{eqn | ll= \leadsto | l = x g x^{-1} | r = x h x^{-1} | c = Definition of $\kappa_x$ }} {{eqn | ll= \leadsto | l = g | r = h | c = [[Cancellation Laws]] }} {{end-eqn}} So $\kappa_x$ is [[Definition:Injection|injective]]. Finally we show that $\kappa_x$ is [[Definition:Surjection|surjective]]. Note that $\forall h \in G: x^{-1} h x \in G$ from fact that $G$ is a [[Definition:Group|group]] and therefore [[Definition:Closure (Abstract Algebra)|closed]]. So: {{begin-eqn}} {{eqn | ll= \forall h \in G: | l = \map {\kappa_x} {x^{-1} h x} | r = x \paren {x^{-1} h x} x^{-1} | c = Definition of $\kappa_x$ }} {{eqn | r = \paren {x x^{-1} } h \paren {x x^{-1} } | c = {{GroupAxiom|1}} }} {{eqn | r = e h e | c = {{GroupAxiom|3}} }} {{eqn | r = h | c = {{GroupAxiom|2}} }} {{end-eqn}} Thus every element of $G$ is the image of some element of $G$ under $\kappa_x$ (that is, of $x^{-1} h x$), and [[Definition:Surjection|surjectivity]] is proved. {{Qed}}	0
Observe that: : $3 i \left({i + 1}\right) = i \left({i + 1}\right) \left({i + 2}\right) - i \left({i + 1}\right) \left({i - 1}\right)$ That is: : $(1): \quad 6 T_i = \left({i + 1}\right) \left({\left({i + 1}\right) + 1}\right) \left({\left({i + 1}\right) - 1}\right) - i \left({i + 1}\right) \left({i - 1}\right)$ Then: {{begin-eqn}} {{eqn | l = n^2 | r = \frac {n^2 + n + n^2 - n} 2 }} {{eqn | r = \frac {n \left({n + 1}\right) + n \left({n - 1}\right)} 2 }} {{eqn | r = \frac {n \left({n + 1}\right)} 2 + \frac {n \left({n - 1}\right)} 2 }} {{eqn | r = T_n + T_{n-1} }} {{end-eqn}} where $T_n$ is the $n$th Triangular number. Then: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n i^2 | r = 1 + \left({T_1 + T_2}\right) + \left({T_2 + T_3}\right) + \left({T_3 + T_4}\right) + \cdots + \left({T_{n - 1} + T_n}\right) }} {{eqn | r = 1 + 2 T_2 + 2 T_3 + 2 T_4 + \cdots + 2 T_{n - 1} + T_n }} {{eqn | r = 1 - T_1 - T_n + 2 \left({T_1 + T_2 + T_3 + T_4 + \cdots + T_n}\right) }} {{eqn | r = 2 \left({\sum_{i \mathop = 1}^n T_i} \right) - \frac {n \left({n + 1}\right)} 2 }} {{eqn | r = 2 \left({\frac {n \left({n + 1}\right) \left({n + 2}\right)} 6}\right) - \frac {n \left({n + 1}\right)} 2 | c = [[Definition:Telescoping Series|Telescoping Series]] from $(1)$ above }} {{eqn | r = \frac {2 n \left({n^2 + 3 n + 2}\right) - \left({3 n^2 + 3 n}\right)} 6 }} {{eqn | r = \frac {2 n^3 + 3 n^2 + n} 6 }} {{eqn | r = \frac {n \left({n + 1}\right) \left({2 n + 1}\right)} 6 }} {{end-eqn}} {{qed}}	0
A direct application of the [[Distributive Property]]: :$\dfrac 1 n b + \dfrac 1 n d = \dfrac 1 n \paren {b + d}$ {{qed}}	0
Let $n$ be the [[Definition:Cardinality of Finite Set|cardinality]] of $S$ and $T$. Let $\N_{<n}$ be an [[Definition:Initial Segment of Natural Numbers|initial segment of the natural numbers]]. Let $h : \N_{<n} \to T$ be a [[Definition:Bijection|bijection]]. By definition of [[Definition:Summation|summation]]: : $\displaystyle \sum_{t \mathop \in T} f \left({g \left({t}\right)}\right) = \displaystyle \sum_{i \mathop = 0}^{n - 1} f \left({g \left({h \left({i}\right)}\right)}\right)$ By [[Composite of Bijections is Bijection]], the [[Definition:Composition of Mappings|composition]] $g \circ h : \N_{<n} \to S$ is a [[Definition:Bijection|bijection]]. By definition of [[Definition:Summation|summation]]: : $\displaystyle \sum_{s \mathop\in S} f \left({s}\right) = \displaystyle \sum_{i \mathop = 0}^{n - 1} f \left({g \left({h \left({i}\right)}\right)}\right)$ {{qed}}	0
From [[Topological Closure is Closed]], $S^-$ is [[Definition:Closed Set (Complex Analysis)|closed]]. From [[Sequence of Imaginary Reciprocals/Boundedness|Sequence of Imaginary Reciprocals: Boundedness]], $S$ is [[Definition:Bounded Subset of Complex Plane|bounded in $\C$]]. It follows trivially that $S^-$ is also [[Definition:Bounded Subset of Complex Plane|bounded in $\C$]]. Hence the result by definition of [[Definition:Compact Subset of Complex Plane|compact]]. {{qed}}	0
Let us take the axioms of a [[Definition:Boolean Algebra|Boolean algebra]] $\struct {S, \wedge, \vee}$: {{begin-axiom}} {{axiom | n = \text {BA} 0 | lc= | t = $S$ is [[Definition:Closed Algebraic Structure|closed]] under both $\vee$ and $\wedge$ }} {{axiom | n = \text {BA} 1 | lc= | t = Both $\vee$ and $\wedge$ are [[Definition:Commutative Operation|commutative]] }} {{axiom | n = \text {BA} 2 | lc= | t = Both $\vee$ and $\wedge$ [[Definition:Distributive Operation|distribute]] over the other }} {{axiom | n = \text {BA} 3 | lc= | t = Both $\vee$ and $\wedge$ have [[Definition:Identity Element|identities]] $\bot$ and $\top$ respectively }} {{axiom | n = \text {BA} 4 | lc= | t = $\forall a \in S: \exists \neg a \in S: a \vee \neg a = \top, a \wedge \neg a = \bot$ }} {{end-axiom}} It can be seen by inspection, that exchanging $\wedge$ and $\vee$, and $\bot$ and $\top$ throughout does not change the axioms. Thus, what you get is a [[Definition:Boolean Algebra|Boolean algebra]] again. Hence the result. {{qed}}	0
Let $G$ be a [[Definition:Group|group]]. Let $N$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$. Let $q: G \to \dfrac G N$ be the [[Definition:Quotient Group Epimorphism|quotient epimorphism]] from $G$ to the [[Definition:Quotient Group|quotient group]] $\dfrac G N$. Let $K$ be the [[Definition:Kernel of Group Homomorphism|kernel]] of $q$. Then: :$\dfrac G N \cong \dfrac {G / K} {N / K}$	0
Let $f: C \to D$ be an [[Definition:Object|object]] of $C \mathop / \mathbf C$. Then there is a [[Definition:Morphism (Category Theory)|morphism]] $a: \operatorname{id}_C \to f$ [[Definition:Iff|iff]]: :$f = a \circ \operatorname{id}_C = a$ Thus, $f$ itself defines the unique [[Definition:Morphism (Category Theory)|morphism]] $\operatorname{id}_C \to f$ in $C \mathop / \mathbf C$. We therefore have the following [[Definition:Commutative Diagram|commutative diagram]] in $\mathbf C$: ::$\begin{xy} <-3em,0em>*+{C} = "X", <3em,0em>*+{D} = "X2", <0em,4em>*+{C} = "C", "X";"X2" **@{--} ?>*@{>} ?*!/^1em/{f}, "C";"X" **@{-} ?>*@{>} ?*!/^.6em/{\operatorname{id}_C}, "C";"X2" **@{-} ?>*@{>} ?<>(.6)*!/_1em/{f'}, \end{xy}$ Hence the result, by definition of [[Definition:Initial Object|initial object]]. {{qed}}	0
Let $\struct {\Q, \tau_d}$ be the [[Definition:Rational Number Space|rational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Then $\struct {\Q, \tau_d}$ is not a [[Definition:Locally Compact Hausdorff Space|locally compact Hausdorff Space]].	0
{{begin-eqn}} {{eqn | l = \int \sec x \rd x | r = \ln \size {\map \tan {\frac \pi 4 + \frac x 2} } | c = [[Primitive of Secant Function/Tangent plus Angle Form|Primitive of $\sec x$: Tangent plus Angle Form]] }} {{eqn | ll= \leadsto | l = \int \sec a x \rd x | r = \frac 1 a \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
Let $r \in \R_{\ge 0}$ be a [[Definition:Positive Real Number|positive real number]]. Then: :$\exists y_1 \in \R_{\ge 0}: {y_1}^2 = r$ :$\exists y_2 \in \R_{\le 0}: {y_2}^2 = r$	0
Let $m \ne n$. {{begin-eqn}} {{eqn | l = \int \sin m x \sin n x \rd x | r = \frac {\sin \paren {m - n} x} {2 \paren {m - n} } - \frac {\sin \paren {m + n} x} {2 \paren {m + n} } + C | c = [[Primitive of Sine of p x by Sine of q x|Primitive of $\sin m x \sin n x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \sin n x \rd x | r = \intlimits {\frac {\sin \paren {m - n} x} {2 \paren {m - n} } - \frac {\sin \paren {m + n} x} {2 \paren {m + n} } } 0 \pi | c = }} {{eqn | r = \paren {\frac {\sin \paren {m - n} \pi} {2 \paren {m - n} } - \frac {\sin \paren {m + n} \pi} {2 \paren {m + n} } } | c = }} {{eqn | o = | ro= - | r = \paren {\frac {\sin 0} {2 \paren {m - n} } - \frac {\sin 0} {2 \paren {m + n} } } | c = }} {{eqn | r = \frac {\sin \paren {m - n} \pi} {2 \paren {m - n} } - \frac {\sin \paren {m + n} \pi} {2 \paren {m + n} } | c = [[Sine of Zero is Zero]] }} {{eqn | r = 0 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} {{qed|lemma}} When $m = n$ we have: {{begin-eqn}} {{eqn | l = \int \sin m x \sin m x \rd x | r = \int \sin^2 m x \rd x | c = }} {{eqn | r = \frac x 2 - \frac {\sin 2 m x} {4 m} + C | c = [[Primitive of Square of Sine of a x|Primitive of $\sin^2 m x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \sin m x \rd x | r = \intlimits {\frac x 2 - \frac {\sin 2 m x} {4 m} } 0 \pi | c = }} {{eqn | r = \paren {\frac \pi 2 - \frac {\sin \paren {2 m \pi} } {4 m} } - \paren {\frac 0 2 - \frac {\sin 0} {4 m} } | c = }} {{eqn | r = \frac \pi 2 - \frac {\sin \paren {2 m \pi} } {4 m} | c = [[Sine of Zero is Zero]] }} {{eqn | r = \frac \pi 2 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} {{qed}}	0
By definition of [[Definition:Order Isomorphism|order isomorphism]]: :$f$ is an [[Definition:Order Embedding|order embedding]]. === Sufficient Condition === Assume that :$x$ is [[Definition:Lower Bound of Set|lower bound]] for $X$. By [[Order Embedding is Increasing Mapping]]: :$f$ is an [[Definition:Increasing Mapping|increasing mapping]]. Thus by [[Increasing Mapping Preserves Lower Bounds]]: :$f \left({x}\right)$ is [[Definition:Lower Bound of Set|lower bound]] for $f \left[{X}\right]$. {{qed|lemma}} === Necessary Condition === Assume that :$f \left({x}\right)$ is [[Definition:Lower Bound of Set|lower bound]] for $f \left[{X}\right]$. Let $y \in X$. By definition of [[Definition:Image of Subset under Mapping|image of set]]: :$f \left({y}\right) \in f \left[{X}\right]$ By definition of [[Definition:Lower Bound of Set|lower bound]]: :$f \left({y}\right) \preceq' f \left({x}\right)$ Thus by definition of [[Definition:Order Embedding|order embedding]]: :$y \preceq x$ {{qed}}	0
As the [[Definition:Codomain of Mapping|codomain]] of $f$ is $\closedint a b$, it follows that the [[Image is Subset of Codomain|image of $f$ is a subset of $\closedint a b$]]. Thus $\map f a \ge a$ and $\map f b \le b$. Let us define the [[Definition:Real Function|real function]] $g: \closedint a b \to \R$ by $\map g x = \map f x - x$. Then by the [[Combined Sum Rule for Continuous Functions]], $\map g x$ is [[Definition:Continuous on Interval|continuous]] on $\closedint a b$. But $\map g a \ge 0$ and $\map g b \le 0$. By the [[Intermediate Value Theorem]], $\exists \xi: \map g \xi = 0$. Thus $\map f \xi = \xi$. {{qed}}	0
:$\sin 60 \degrees = \sin \dfrac \pi 3 = \dfrac {\sqrt 3} 2$	0
First we note that: :$3\,430\,751\,869 - 87\,297\,210 = 3\,343\,454\,659 = 17\,203 \times 194\,353$ and so this [[Definition:Arithmetic Sequence|arithmetic sequence]] of [[Definition:Prime Number|primes]] does not extend to $n < 0$. {{begin-eqn}} {{eqn | l = 3\,430\,751\,869 + 0 \times 87\,297\,210 | r = 3\,430\,751\,869 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 1 \times 87\,297\,210 | r = 3\,518\,049\,079 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 2 \times 87\,297\,210 | r = 3\,605\,346\,289 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 3 \times 87\,297\,210 | r = 3\,692\,643\,499 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 4 \times 87\,297\,210 | r = 3\,779\,940\,709 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 5 \times 87\,297\,210 | r = 3\,867\,237\,919 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 6 \times 87\,297\,210 | r = 3\,954\,535\,129 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 7 \times 87\,297\,210 | r = 4\,041\,832\,339 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 8 \times 87\,297\,210 | r = 4\,129\,129\,549 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 9 \times 87\,297\,210 | r = 4\,216\,426\,759 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 10 \times 87\,297\,210 | r = 4\,303\,723\,969 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 11 \times 87\,297\,210 | r = 4\,391\,021\,179 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 12 \times 87\,297\,210 | r = 4\,478\,318\,389 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 13 \times 87\,297\,210 | r = 4\,565\,615\,599 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 14 \times 87\,297\,210 | r = 4\,652\,912\,809 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 15 \times 87\,297\,210 | r = 4\,740\,210\,019 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 3\,430\,751\,869 + 16 \times 87\,297\,210 | r = 4\,827\,507\,229 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} But note that $3\,430\,751\,869 + 17 \times 87\,297\,210 = 4\,914\,804\,439 = 41 \times 97 \times 1 235807$ and so is not [[Definition:Prime Number|prime]]. {{ProofWanted|It remains to be shown that there are no smaller such APs}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x \paren {x^2 - a^2} } | r = \int \paren {\frac x {a^2 \paren {x^2 - a^2} } - \frac 1 {a^2 x} } \rd x | c = [[Primitive of Reciprocal of x by x squared minus a squared/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac 1 {a^2} \int \frac {x \rd x} {x^2 - a^2} - \frac 1 {a^2} \int \frac {\d x} x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \int \frac {x \rd x} {x^2 - a^2} - \frac 1 {a^2} \ln \size x + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^2} \paren {\frac 1 2 \map \ln {x^2 - a^2} } - \frac 1 {a^2} \ln \size x + C | c = [[Primitive of x over x squared minus a squared|Primitive of $\dfrac x {x^2 - a^2}$]] }} {{eqn | r = \frac 1 {2 a^2} \map \ln {x^2 - a^2} - \frac 1 {2 a^2} \ln \size {x^2} + C | c = [[Logarithm of Power]] }} {{eqn | r = \frac 1 {2 a^2} \map \ln {x^2 - a^2} - \frac 1 {2 a^2} \map \ln {x^2} + C | c = as $x^2 > 0$ }} {{eqn | r = \frac 1 {2 a^2} \map \ln {\frac {x^2 - a^2} {x^2} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
:$\csc 120 \degrees = \csc \dfrac {2 \pi} 3 = \dfrac {2 \sqrt 3} 3$	0
Recall the definition of the [[Definition:Complex Sine Function|sine function]]: {{begin-eqn}} {{eqn | l = \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = }} {{eqn | r = z - \frac {z^3} {3!} + \frac {z^5} {5!} - \frac {z^7} {7!} + \cdots + \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} + \cdots | c = }} {{end-eqn}} Recall the definition of the [[Definition:Exponential Function/Complex/Sum of Series|exponential as a power series]]: {{begin-eqn}} {{eqn | l = \exp z | r = \sum_{n \mathop = 0}^\infty \frac {z^n} {n!} | c = }} {{eqn | r = 1 + \frac z {1!} + \frac {z^2} {2!} + \frac {z^3} {3!} + \cdots + \frac {z^n} {n!} + \cdots | c = }} {{end-eqn}} Then, starting from the {{RHS}}: {{begin-eqn}} {{eqn | l = \frac {\exp \paren {i z} - \exp \paren {-i x} } {2 i} | r = \frac 1 {2 i} \paren {\sum_{n \mathop = 0}^\infty \frac {\paren {i z}^n} {n!} - \sum_{n \mathop = 0}^\infty \frac {\paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^n - \paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^{2 n} - \paren {-i z}^{2 n} } {\paren {2 n}!} + \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = split into [[Definition:Even Integer|even]] and [[Definition:Odd Integer|odd]] $n$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-i z}^{2 n} = \paren {i z}^{2 n}$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {2 \paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-1}^{2 n + 1} = -1$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = cancel $2$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {i \paren {-1}^n z^{2 n + 1} } {\paren {2 n + 1}!} | c = as $i^{2 n + 1} = i \paren {-1})^n $ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1!} } | c = cancel $i$ }} {{eqn | r = \sin z }} {{end-eqn}} {{qed}}	0
A [[Definition:Group Monomorphism|monomorphism]] is a [[Definition:Group Homomorphism|homomorphism]] which is also an [[Definition:Injection|injection]]. From [[Composite of Group Homomorphisms is Homomorphism]], $\psi \circ \phi$ is a [[Definition:Group Homomorphism|homomorphism]]. From [[Composite of Injections is Injection]], $\psi \circ \phi$ is an [[Definition:Injection|injection]]. {{qed}}	0
=== Necessary Condition === Let $\struct {T, \circ}$ be [[Definition:Closed Algebraic Structure|closed]]. Let $x \in S$ be arbitrary. Then: {{begin-eqn}} {{eqn | lo= \forall f, g \in T^S: | l = \map {f \oplus g} x | r = \map f x \circ \map g x | c = {{Defof|Pointwise Operation}} }} {{eqn | o = \in | r = T | c = as $\struct {T, \circ}$ is [[Definition:Closed Algebraic Structure|closed]] }} {{end-eqn}} So $\oplus$ is [[Definition:Closed Operation|closed]] on $T^S$. {{qed|lemma}} === Sufficient Condition === Let $\oplus$ is [[Definition:Closed Operation|closed]] on $T^S$. {{AimForCont}} $\struct {T, \circ}$ is not [[Definition:Closed Algebraic Structure|closed]]. Then: :$(1): \quad \exists s, t \in T: s \circ t \notin T$ By definition, $T^S$ is the [[Definition:Set of All Mappings|set of all mappings]] from $S$ \to $T$. As $S \ne \O$ it follows that $\exists x \in S$. Thus, let $x \in S$ be arbitrary. Let $f, g \in T^S$ such that: :$(2): \quad \map f x = s, \map g x = t$ Then: {{begin-eqn}} {{eqn | l = \map {f \oplus g} x | r = \map f x \circ \map g x | c = {{Defof|Pointwise Operation}} }} {{eqn | r = s \circ t | c = from $(2)$ }} {{eqn | o = \notin | r = T | c = from $(1)$ }} {{end-eqn}} That is, $\oplus$ is not [[Definition:Closed Operation|closed]] on $T^S$. From that [[Proof by Contradiction|contradiction]] it follows that $\struct {T, \circ}$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed}} [[Category:Pointwise Operations]] aawr0dnrannkhib34n0pcxs1d4xq9t9	0
Let $X$ be a [[Definition:Set|set]], and let $\Phi := \left\{{\phi_i: i \in I}\right\}$ be a collection of [[Definition:Partial Mapping|partial mappings]] with [[Definition:Codomain of Relation|codomain]] $\mathcal P \left({X}\right)$, the [[Definition:Power Set|power set]] of $X$. Let $\mathcal G \subseteq \mathcal P \left({X}\right)$ be a collection of [[Definition:Subset|subsets]] of $X$. Then the [[Definition:Magma of Sets Generated by Collection of Subsets|magma of sets generated by $\mathcal G$]] exists and is unique.	0
:$\displaystyle \int_0^{2 \pi} \frac {\d x} {\paren {a + b \sin x}^2} = \frac {2 \pi a} {\paren {a^2 - b^2}^{3/2} }$	0
[[Proof by Counterexample]]: We have: {{begin-eqn}} {{eqn | l = \gcd \set {2, 3} | r = 1 | c = }} {{eqn | l = \gcd \set {3, 4} | r = 1 | c = }} {{eqn | l = \gcd \set {2, 4} | r = 2 | c = }} {{end-eqn}} Hence we have: :$2 \perp 3$ and $3 \perp 4$ However, it is not the case that $2 \perp 4$. Thus $\perp$ is not [[Definition:Transitive Relation|transitive]]. Then we have: {{begin-eqn}} {{eqn | l = \gcd \set {2, 3} | r = 1 | c = }} {{eqn | l = \gcd \set {3, 5} | r = 1 | c = }} {{eqn | l = \gcd \set {2, 5} | r = 1 | c = }} {{end-eqn}} :$2 \perp 3$ and $3 \perp 5$ and also: :$2 \perp 5$ Thus $\perp$ is not [[Definition:Antitransitive Relation|antitransitive]] either. The result follows by definition of [[Definition:Non-Transitive Relation|non-transitive relation]]. {{qed}}	0
From [[Sum of Indices of Real Number/Positive Integers|Sum of Indices of Real Number: Positive Integers]], we have that: :$m \in \Z_{\ge 0}: \forall n \in \Z: r^{n + m} = r^n \times r^m$ It remains to be shown that: :$\forall m \in \Z_{<0}: \forall n \in \Z: r^{n + m} = r^n \times r^m$ The proof will proceed by [[Principle of Mathematical Induction|induction]] on $m$. As $m < 0$ we have that $m = -p$ for some $p \in \Z_{> 0}$. For all $p \in \Z_{>0}$, let $\map P p$ be the [[Definition:Proposition|proposition]]: :$\forall n \in \Z: r^{n + \paren {-p} } = r^n \times r^{-p}$ that is: :$\forall n \in \Z: r^{n - p} = r^n \times r^{-p}$ === Basis for the Induction === $\map P 1$ is true, as follows: When $n > 0$: {{begin-eqn}} {{eqn | l = r^{n - 1} \times r | r = r^n | c = {{Defof|Integer Power}} }} {{eqn | ll= \leadsto | l = r^{n - 1} | r = r^n \dfrac 1 r | c = multiplying both sides by $\dfrac 1 r$ }} {{eqn | r = r^n \times r^{-1} | c = [[Real Number to Negative Power/Integer|Real Number to Negative Power: Integer]] }} {{end-eqn}} When $n \le 0$: {{begin-eqn}} {{eqn | l = r^{n - 1} | r = \dfrac {r^n} r | c = {{Defof|Integer Power}} }} {{eqn | r = r^n \times r^{-1} | c = [[Real Number to Negative Power/Integer|Real Number to Negative Power: Integer]] }} {{end-eqn}} This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\forall n \in \Z: r^{n - k} = r^n \times r^{- k}$ Then we need to show: :$\forall n \in \Z: r^{n - \paren {k + 1} } = r^n \times r^{-\paren {k + 1} }$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = r^n \times r^{-\paren {k + 1} } | r = r^n \times \dfrac {r^{-k} } r | c = {{Defof|Integer Power}} }} {{eqn | r = \paren {r^n \times r^{-k} } \times \dfrac 1 r | c = [[Real Multiplication is Associative]] }} {{eqn | r = r^{n - k} \times \dfrac 1 r | c = [[Sum of Indices of Real Number/Integers#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = r^{n - k} \times r^{-1} | c = [[Real Number to Negative Power/Integer|Real Number to Negative Power: Integer]] }} {{eqn | r = r^{n - k - 1} | c = [[Sum of Indices of Real Number/Integers#Basis for the Induction|Basis for the Induction]] }} {{eqn | r = r^{n - \paren {k + 1} } | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n, m \in \Z: r^{n + m} = r^n \times r^m$ {{qed}}	0
:$\csc 15^\circ = \csc \dfrac \pi {12} = \sqrt 6 + \sqrt 2$	0
Consider the [[Definition:Determinant|determinant]]: :$\Delta = \begin {vmatrix} x & z & y \\ y & x & z \\ z & y & x \end {vmatrix}$ We have: {{begin-eqn}} {{eqn | l = \Delta | r = x \paren {x^2 - y z} - z \paren {y x - z^2} + y \paren {y^2 - x z} | c = {{Defof|Determinant of Order 3}} }} {{eqn | r = x^3 + y^3 + z^3 - 3 x y z | c = }} {{end-eqn}} Then we note that adding [[Definition:Row of Matrix|rows]] $2$ and $3$ to [[Definition:Row of Matrix|rows]] $1$ gives: {{begin-eqn}} {{eqn | l = \Delta | r = \begin {vmatrix} x + y + z & x + y + z & x + y + z \\ y & x & z \\ z & y & x \end {vmatrix} | c = [[Multiple of Row Added to Row of Determinant]] }} {{eqn | r = \paren {x + y + z} \paren {x^2 - y z} - \paren {x + y + z} \paren {y x - z^2} + \paren {x + y + z} \paren {y^2 - x z} | c = {{Defof|Determinant of Order 3}} }} {{eqn | r = \paren {x + y + z} \paren {\paren {x^2 - y z} - \paren {y x - z^2} + \paren {y^2 - x z} } | c = }} {{eqn | ll= \leadsto | l = \paren {x + y + z} | o = \divides | r = \Delta | c = }} {{end-eqn}} Let $\omega$ denote the [[Cube Roots of Unity|complex cube root of unity]]: :$\omega = -\dfrac 1 2 + \dfrac {\sqrt 3} 2$ Hence adding $\omega$ times [[Definition:Row of Matrix|row]] $2$ and $\omega^2$ times [[Definition:Row of Matrix|row]] $3$ to [[Definition:Row of Matrix|rows]] $1$: {{begin-eqn}} {{eqn | l = \Delta | r = \begin {vmatrix} x + \omega y + \omega^2 z & \omega x + \omega^2 y + z & \omega^2 x + y + \omega z \\ y & x & z \\ z & y & x \end {vmatrix} | c = [[Multiple of Row Added to Row of Determinant]]: }} {{eqn | r = \begin {vmatrix} x + \omega y + \omega^2 z & \omega \paren {x + \omega y + \omega^2 z} & \omega^2 \paren {x + \omega y + \omega^2 z} \\ y & x & z \\ z & y & x \end {vmatrix} | c = }} {{eqn | ll= \leadsto | l = \paren {x + \omega y + \omega^2} | o = \divides | r = \Delta | c = expanding as above }} {{end-eqn}} and adding $\omega^2$ times [[Definition:Row of Matrix|row]] $2$ and $\omega$ times [[Definition:Row of Matrix|row]] $3$ to [[Definition:Row of Matrix|rows]] $1$: {{begin-eqn}} {{eqn | l = \Delta | r = \begin {vmatrix} x + \omega^2 y + \omega z & \omega^2 x + \omega y + z & \omega x + y + \omega^2 z \\ y & x & z \\ z & y & x \end {vmatrix} | c = [[Multiple of Row Added to Row of Determinant]] }} {{eqn | r = \begin {vmatrix} x + \omega^2 y + \omega z & \omega^2 \paren {x + \omega^2 y + \omega z} & \omega^2 \paren {x + \omega^2 y + \omega z} \\ y & x & z \\ z & y & x \end {vmatrix} | c = }} {{eqn | ll= \leadsto | l = \paren {x + \omega^2 y + \omega} | o = \divides | r = \Delta | c = }} {{end-eqn}} Thus we have $3$ [[Definition:Divisor of Polynomial|divisors]] of $x^3 + y^3 + z^3 - 3 x y z$, which is a [[Definition:Polynomial|polynomial]] of [[Definition:Degree of Polynomial|degree $3$]]. There can be no other [[Definition:Divisor of Polynomial|divisors]] except for a [[Definition:Constant|constant]]. By examining the [[Definition:Polynomial Coefficient|coefficient]] of $x^3$, for example, the [[Definition:Constant|constant]] is seen to be $1$. Hence the result. {{qed}}	0
:$\log_b x - \log_b y = \map {\log_b} {\dfrac x y}$	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $S = \set {a_1, a_2, \ldots, a_p}$ be a [[Definition:Complete Residue System|complete residue system modulo $p$]]. Then for all [[Definition:Integer|integers]] $n \in \Z$ and [[Definition:Non-Negative Integer|non-negative integer]] $s \in \Z_{\ge 0}$, there exists a [[Definition:Congruence Modulo Integer|congruence]] of the form: :$n \equiv \displaystyle \sum_{j \mathop = 0}^s b_j p^j \pmod {p^{s + 1} }$ where $b_j \in S$.	0
Let $z_n = x_n + i y_n$. Let $w_n = u_n + i v_n$. Let $c = a + i b$ Let $d = e + i f$. By definition of [[Definition:Convergent Complex Sequence|convergent complex sequence]]: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} z_n | r = c | c = }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} x_n + i \lim_{n \mathop \to \infty} y_n | r = a + i b | c = {{Defof|Convergent Complex Sequence}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} w_n | r = d | c = }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} u_n + i \lim_{n \mathop \to \infty} v_n | r = e + i f | c = {{Defof|Convergent Complex Sequence}} }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} z_n w_n | r = \lim_{n \mathop \to \infty} \paren {\paren {x_n u_n - y_n v_n} + i \paren {y_n u_n + x_n v_n} } | c = {{Defof|Complex Multiplication}} }} {{eqn | r = \lim_{n \mathop \to \infty} \paren {x_n u_n - y_n v_n} + i \lim_{n \mathop \to \infty} \paren {y_n u_n + x_n v_n} | c = {{Defof|Convergent Complex Sequence}} }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {x_n u_n} - \lim_{n \mathop \to \infty} \paren {y_n v_n} } + i \paren {\lim_{n \mathop \to \infty} \paren {y_n u_n} + \lim_{n \mathop \to \infty} \paren {x_n v_n} } | c = [[Sum Rule for Real Sequences]] }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {x_n} \lim_{n \mathop \to \infty} \paren {u_n} - \lim_{n \mathop \to \infty} \paren {y_n} \lim_{n \mathop \to \infty} \paren {v_n} } + i \paren {\lim_{n \mathop \to \infty} \paren {y_n} \lim_{n \mathop \to \infty} \paren {u_n} + \lim_{n \mathop \to \infty} \paren {x_n} \lim_{n \mathop \to \infty} \paren {v_n} } | c = [[Product Rule for Real Sequences]] }} {{eqn | r = \paren {a e - b f} + i \paren {b e + a f} | c = }} {{eqn | r = \paren {a + i b} \paren {e + i f} | c = {{Defof|Complex Multiplication}} }} {{eqn | r = c d | c = }} {{end-eqn}} {{qed}}	0
{{ProofWanted|Use the same construction as in [[Definition:Zermelo-Fraenkel Set Theory|ZF]]}}	0
A coastal town in Spain has an infinite row of hotels along a road leading down to the beach. The tourist bureau which rates these hotels has a special designation for any hotel having a view of the sea. Any hotel which is at least as tall as the rest of the hotels on the road to the sea receives this view-rating. Starting with the hotel farthest from the sea, let $x_1, x_2, \ldots$ denote the heights of the hotels as we travel along the road to the beach. One possibility is that an infinite number of hotels are view-rated. In this case the heights of these hotels form a [[Definition:Decreasing Sequence|decreasing (i.e. nonincreasing)]] [[Definition:Subsequence|subsequence]] of the original sequence of heights. The other possibility is that only a finite number of hotels are view-rated. In this case we can obtain an [[Definition:Increasing Sequence|increasing]] [[Definition:Subsequence|subsequence]] as follows. Walk along the road to the beach past all the view-rated hotels. Let $n_1$ be the index of the next hotel. Since it does not have a view of the sea, there is a taller hotel nearer the shore which blocks the view. Let $n_2$ be the index of that taller hotel. Since it is not view-rated, it does not have a view of the sea. Thus there must be an even taller hotel nearer the shore which blocks the view. In this way we generate a list of indices $n_1 < n_2 < n_3 \cdots$ of taller and taller hotels. This gives an [[Definition:Increasing Sequence|increasing]] [[Definition:Subsequence|subsequence]] $x_{n_1}, x_{n_2}, x_{n_3}, \ldots$ of the original sequence. {{qed}}	0
:$\displaystyle \sum_{n \mathop \ge 1} n \paren {n + 1} x^{n - 1} = \frac 2 {\paren {1 - x}^3}$	0
Let $x, y \in S$ be arbitrary. Then: {{begin-eqn}} {{eqn | l = x \circ a | r = y \circ a | c = }} {{eqn | ll= \implies | l = \left({x \circ a}\right) \circ b | r = \left({y \circ a}\right) \circ b | c = }} {{eqn | ll= \implies | l = \left({x \circ a}\right) \circ b | r = \left({y \circ a}\right) \circ b | c = as $\left({S, \circ}\right)$ is a [[Definition:Semigroup|semigroup]], $\circ$ is [[Definition:Associative|associative]] }} {{eqn | ll= \implies | l = x \circ e_R | r = y \circ e_R | c = [[Definition:By Hypothesis|By hypothesis]] }} {{eqn | ll= \implies | l = x | r = y | c = Definition of [[Definition:Right Identity|Right Identity]] }} {{end-eqn}} The result follows by definition of [[Definition:Right Cancellable Element|right cancellable]]. {{qed}}	0
Let $n, k \in \N$. Then $\alpha_k = \map \exp {\dfrac {2 \pi i k} n}$ is a [[Definition:Primitive Complex Root of Unity|primitive $n$th root of unity]] {{iff}} $\gcd \set {n, k} = 1$.	0
We have: {{begin-eqn}} {{eqn | l = \left({\left({f * g}\right) * h}\right) \left({n}\right) | r = \sum_{a b \mathop = n} \left({f * g}\right) \left({a}\right) h \left({b}\right) | c = }} {{eqn | r = \sum_{a b \mathop = n} \ \sum_{c d \mathop = a} f \left({c}\right) g \left({d}\right) h \left({b}\right) | c = }} {{eqn | r = \sum_{b c d \mathop = n} f \left({c}\right) g \left({d}\right) h \left({b}\right) | c = }} {{end-eqn}} and {{begin-eqn}} {{eqn | l = \left({f * \left({g * h}\right)}\right) \left({n}\right) | r = \sum_{a b \mathop = n} f \left({a}\right) \left({g * h}\right) \left({b}\right) | c = }} {{eqn | r = \sum_{a b \mathop = n} f \left({a}\right) \sum_{c d \mathop = b} g \left({c}\right) h \left({d}\right) | c = }} {{eqn | r = \sum_{a c d \mathop = n} f \left({a}\right) g \left({c}\right) h \left({d}\right) | c = }} {{end-eqn}} and associativity follows. {{qed}}	0
We have that $\struct {F^*, \times}$ is the [[Definition:Multiplicative Group|multiplicative group]] of $\struct {F, +, \times}$. Let $a \in F^*$, that is, $a \in F: a \ne 0_F$, where $0_F$ is the [[Definition:Field Zero|zero]] of $F$. This is an instance of [[Powers of Group Elements]] when expressed in [[Definition:Additive Notation|additive notation]]: :$\forall m, n \in \Z: \paren {m n} a = m \paren {n a}$ {{qed|lemma}} Now suppose $a = 0_F$. Then by definition of the [[Definition:Field Zero|zero element]] of $F$, we have that: :$\paren {m n} \cdot a = 0_F = m \cdot \paren {n \cdot a}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \phi {a -_1 b} | r = \map \phi {a +_1 \paren {-b} } | c = {{Defof|Ring Subtraction}} }} {{eqn | r = \map \phi a +_2 \map \phi {-b} | c = {{Defof|Ring Homomorphism}} }} {{eqn | r = \map \phi a +_2 \paren {-\map \phi b} | c = [[Ring Homomorphism Preserves Negatives]] }} {{eqn | r = \map \phi a -_2 \map \phi b | c = {{Defof|Ring Subtraction}} }} {{end-eqn}} {{qed}}	0
Let $S = \left\langle{a_n}\right\rangle$ be a [[Definition:Rational Sequence|rational sequence]]. Let $S$ be [[Definition:Convergent Rational Sequence|convergent]] to a [[Definition:Limit of Rational Sequence|limit]] $L$. Then it is not necessarily the case that $L$ is itself a [[Definition:Rational Number|rational number]].	0
Let $Z$ be the set of all [[Definition:Multiindex|multiindices]] indexed by $J$. Let $k_j$ be the $j$th component of a [[Definition:Multiindex|multiindex]] $k$. Let $\displaystyle f = \sum_{k \mathop \in Z} a_k \prod_{j \mathop \in J} X_j^{k_j}$ be a [[Definition:Polynomial (Abstract Algebra)|polynomial]] over $R$. Define: :$\displaystyle \phi \left({f}\right) = \sum_{k \mathop \in Z} \psi \left({a_k}\right) \prod_{j \mathop \in J}s_j^{k_j}$ It is clear that $\phi$ extends $\psi$. If $\displaystyle g = \sum_{k \mathop \in Z} b_k \prod_{j \mathop \in J} X_j^{k_j}$, then: {{begin-eqn}} {{eqn | l = \phi \left({f + g}\right) | r = \phi \left({\sum_{k \mathop \in Z} \left({a_k + b_k}\right) \prod_{j \mathop \in J} X_j^{k_j} }\right) | c = {{Defof|Addition of Polynomial Forms}} }} {{eqn | r = \sum_{k \mathop \in Z} \left({\psi \left({a_k + b_k}\right)}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = Definition of $\phi$ }} {{eqn | r = \sum_{k \mathop \in Z} \left({\psi \left({a_k}\right) + \psi \left({b_k}\right)}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = {{Defof|Ring Homomorphism}} }} {{eqn | r =\sum_{k \mathop \in Z} \psi \left({a_k}\right) \prod_{j \mathop \in J} s_j^{k_j} + \sum_{k \mathop \in Z} \psi \left({b_k}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = [[Definition:Ring Axioms|Ring Axioms of $S$]] }} {{eqn | r =\phi \left({f}\right) + \phi \left({g}\right) | c = Definition of $\phi$ }} {{end-eqn}} Therefore $\phi$ preserves addition. Also: {{begin-eqn}} {{eqn | l = \phi \left({f g}\right) | r = \phi \left({\sum_{k \mathop \in Z} \left({\sum_{p \mathop + q \mathop = k} a_p b_q}\right) \prod_{j \mathop \in J} X_j^{k_j} }\right) | c = {{Defof|Multiplication of Polynomial Forms}} }} {{eqn | r = \sum_{k \mathop \in Z} \psi \left({\sum_{p \mathop + q \mathop = k} a_p b_q}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = Definition of $\phi$ }} {{eqn | r = \sum_{k \mathop \in Z} \left({\sum_{p \mathop + q \mathop = k} \psi \left({a_p}\right) \psi \left({b_q}\right)}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = {{Defof|Ring Homomorphism}} }} {{eqn | r = \left({\sum_{k \mathop \in Z} \left({\psi \left({a_k}\right)}\right) \prod_{j \mathop \in J} s_j^{k_j} }\right) \left({\sum_{k \mathop \in Z} \left({\psi \left({b_k}\right)}\right) \prod_{j \mathop \in J} s_j^{k_j} }\right) | c = [[Definition:Ring Axioms|Ring Axioms of $S$]] }} {{eqn | r =\phi \left({f}\right) \phi \left({g}\right) | c = Definition of $\phi$ }} {{end-eqn}} This shows that $\phi$ is a [[Definition:Ring Homomorphism|homomorphism]]. Now suppose that $\phi'$ is another such [[Definition:Ring Homomorphism|homomorphism]]. For each $j \in J$, $\phi'$ must satisfy $\phi'(X_j)=s_j$ and $\phi' \left({r}\right) = \psi \left({r}\right)$ for all $r \in R$. In addition $\phi'$ must be a [[Definition:Ring Homomorphism|homomorphism]], so we compute: {{begin-eqn}} {{eqn | l = \phi' \left({\sum_{k \mathop \in Z} a_k \prod_{j \mathop \in J} X_j^{k_j} }\right) | r = \sum_{k \mathop \in Z} \phi' \left({a_k \prod_{j \mathop \in J} X_j^{k_j} }\right) | c = $\phi'$ preserves [[Definition:Ring Addition|Ring Addition]] }} {{eqn | r = \sum_{k \mathop \in Z} \phi' \left({a_k}\right) \prod_{j \mathop \in J} \phi' \left({X_j}\right)^{k_j} | c = $\phi'$ preserves [[Definition:Ring Product|Ring Product]] }} {{eqn | r = \sum_{k \mathop \in Z} \psi \left({a_k}\right) \prod_{j \mathop \in J} s_j^{k_j} | c = as $\phi' \left({X_j}\right) = s_j$ and $\phi' \left({r}\right) = \psi \left({r}\right)$ for all $r \in R$ }} {{end-eqn}} and therefore $\phi' = \phi$. This concludes the proof. {{qed}}	0
From the definition of the [[Definition:Real Cosine Function|cosine function]], we have: :$\displaystyle \cos x = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!}$ Then: {{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\cos x} | r = \sum_{n \mathop = 1}^\infty \paren {-1}^n 2 n \frac {x^{2 n - 1} } {\paren {2 n}!} | c = [[Power Series is Differentiable on Interval of Convergence]] }} {{eqn | r = \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n - 1} } {\paren {2 n - 1}!} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^{n + 1} \frac {x^{2 n + 1} } {\paren {2 n + 1}!} | c = changing summation index }} {{eqn | r = -\sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} }} {{end-eqn}} The result follows from the definition of the [[Definition:Real Sine Function|sine function]]. {{qed}}	0
Firstly, we will prove that $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$, where $n \in \N$. Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: : $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ === Basis for the Induction === $\map P 1$ is true, as this says $\displaystyle \frac {\sin z} z = \frac {\sin z} z$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 0$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i}$ Then we need to show: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Sine of X over X as Infinite Product#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {\frac {2^k} z} \paren {2 \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Double Angle Formulas/Sine|Double Angle Formula for Sine]] }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ And then: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } \prod_{i \mathop = 1}^n \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } } \prod_{i \mathop = 1}^{\infty} \cos \frac z {2^i} | c = }} {{eqn | r = \paren 1 \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = [[Limit of Sine of X over X]] }} {{eqn | r = \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = }} {{end-eqn}} {{qed}}	0
:$\dbinom n m_q = \dbinom {n - 1} m_q q^m + \dbinom {n - 1} {m - 1}_q$	0
:$\forall x \in X: n, m \in \N_{>0} \implies x^m \circ x^n = 0 \circ x^{n-m}$	0
From [[Center of Group is Normal Subgroup]], $\map Z G$ is a [[Definition:Normal Subgroup|normal subgroup]] of $G$. By [[Definition:Lagrange's Theorem (Group Theory)|Lagrange's Theorem]], the [[Definition:Order of Group|order]] of $\map Z G$ is either $1$, $p$, $q$ or $p q$. Because $G$ is not [[Definition:Abelian Group|abelian]], $G \ne \map Z G$. Hence $\order {\map Z G} \ne p q$. From [[Quotient of Group by Center Cyclic implies Abelian]]: :$G / \map Z G$ cannot be a [[Definition:Cyclic Group|cyclic group]] which is non-[[Definition:Trivial Subgroup|trivial]]. Then we have: :$\map C x \subset G$ where $\map C x$ is the [[Definition:Centralizer of Group Element|centralizer]] of $x$. Hence by [[Prime Group is Cyclic]], $G / \map Z G$ cannot be [[Definition:Cyclic Group|cyclic]] of [[Definition:Order of Group|order]] $p$ or $q$. {{qed}}	0
Proof by [[Principle of Mathematical Induction|induction]]: === Base case === For $n = 0$, it follows from [[Sine and Cosine are Periodic on Reals/Corollary]]. === Induction Hypothesis === This is our [[Definition:Induction Hypothesis|induction hypothesis]]: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{end-eqn}} Now we need to show true for $n=k+1$: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n + \dfrac 3 2} \pi < x < \paren {2 n + \dfrac 5 2} \pi$ }} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 5 2} \pi < x < \paren {2 n + \dfrac 7 2} \pi$ }} {{end-eqn}} === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n + \dfrac 3 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 5 2} \pi$ }} {{eqn | l = \map \cos {x + 2 \pi} | o = > | r = 0 | c = for $\paren {2 n + \dfrac 3 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 5 2} \pi$ | cc = [[Sine and Cosine are Periodic on Reals]] }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n + \dfrac 3 2} \pi < x < \paren {2 n + \dfrac 5 2} \pi$ | cc = Replacing $x + 2 \pi$ by $x$ }} {{end-eqn}} Also: {{begin-eqn}} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 5 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 7 2} \pi$ }} {{eqn | l = \map \cos {x + 2 \pi} | o = < | r = 0 | c = for $\paren {2 n + \dfrac 5 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 7 2} \pi$ | cc = [[Sine and Cosine are Periodic on Reals]] }} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 5 2} \pi < x < \paren {2 n + \dfrac 7 2} \pi$ | cc = Replacing $x + 2 \pi$ by $x$ }} {{end-eqn}} The result follows by [[Principle of Mathematical Induction|induction]]. For negative $n$: === Induction Hypothesis === This is our [[Definition:Induction Hypothesis|induction hypothesis]]: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \cos x | o = < | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{end-eqn}} Now we need to show true for $n=k-1$: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 5 2} \pi < x < \paren {2 n - \dfrac 3 2} \pi$ }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 3 2} \pi < x < \paren {2 n - \dfrac 1 2} \pi$ }} {{end-eqn}} === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \map \cos {x + 2 \pi} | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 1 2} \pi$ | cc= replacing $x$ by $x + 2 \pi$ }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 1 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 1 2} \pi$ | cc= [[Sine and Cosine are Periodic on Reals]] }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 5 2} \pi < x < \paren {2 n - \dfrac 3 2} \pi$ }} {{end-eqn}} Also: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{eqn | l = \map \cos {x + 2 \pi} | o = > | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 3 2} \pi$ | cc= replacing $x$ by $x + 2 \pi$ }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n + \dfrac 1 2} \pi < x + 2 \pi < \paren {2 n + \dfrac 3 2} \pi$ | cc= [[Sine and Cosine are Periodic on Reals]] }} {{eqn | l = \cos x | o = > | r = 0 | c = for $\paren {2 n - \dfrac 3 2} \pi < x < \paren {2 n - \dfrac 1 2} \pi$ }} {{end-eqn}} The result follows by [[Principle of Mathematical Induction|induction]]. {{qed}}	0
Let $A \subseteq \R$ be the [[Definition:Set|set]] of all points on $\R$ defined as: :$A := \set 0 \cup \set {\dfrac 1 n : n \in \Z_{>0} }$ Let $\struct {A, \tau_d}$ be the [[Definition:Integer Reciprocal Space|integer reciprocal space]] with [[Definition:Zero (Number)|zero]] under the [[Definition:Euclidean Topology on Real Number Line|usual (Euclidean) topology]]. Then $A$ is [[Definition:Totally Separated Space|totally separated]].	0
[[Proof by Counterexample]]: Let $S = \set {a, b}$. Let $\mathcal R = \set {\tuple {a, b}, \tuple {b, b} }$. $\mathcal R$ is a [[Definition:Serial Relation|serial relation]], as can be seen by definition. Now let $T = \set a$. Then $\mathcal R {\restriction_T} = \O$. So $\not \exists y \in T: \tuple {a, y} \in \mathcal R {\restriction_T}$. That is, $\mathcal R {\restriction_T}$ is not a [[Definition:Serial Relation|serial relation]] on $T$. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\cos^3 a x} | r = \int \sec^3 a x \rd x | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac {\sec a x \tan a x} {2 a} + \frac 1 2 \int \sec a x \rd x | c = [[Primitive of Power of Secant of a x|Primitive of $\sec^n a x$]] }} {{eqn | r = \frac {\sin a x} {2 a \cos^2 a x} + \frac 1 2 \int \sec a x \rd x | c = [[Secant is Reciprocal of Cosine|Secant is $\dfrac 1 \cos$]] and [[Tangent is Sine divided by Cosine|Tangent is $\dfrac \sin \cos$]] }} {{eqn | r = \frac {\sin a x} {2 a \cos^2 a x} + \frac 1 {2 a} \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Primitive of Secant of a x/Tangent Form|Primitive of $\sec a x$]] }} {{end-eqn}} {{qed}}	0
=== Sufficient Condition === Let $z_1, z_2, z_3$ be [[Definition:Collinear|collinear]]. Then by [[Condition for Collinearity of Points in Complex Plane/Formulation 1|Condition for Collinearity of Points in Complex Plane: Formulation 1]] there exists a [[Definition:Real Number|real number]] $b$ such that: :$z_2 - z_1 = b \paren {z_3 - z_1}$ Then: {{begin-eqn}} {{eqn | l = z_2 - z_1 | r = b \paren {z_3 - z_1} | c = }} {{eqn | ll= \leadsto | l = z_2 - z_1 - b z_3 + b z_1 | r = 0 | c = }} {{eqn | ll= \leadsto | l = \paren {b - 1} z_1 + z_2 - b z_3 | r = 0 | c = }} {{end-eqn}} Setting $\alpha = b - 1, \beta = 1, \gamma = -b$ fits the bill, as $\paren {b - 1} + 1 + \paren {-b} = 0$. {{qed|lemma}} === Necessary Condition === Let $\alpha + \beta + \gamma = 0$ such that: :$\alpha z_1 + \beta z_2 + \gamma z_3 = 0$ :at least one of $\alpha, \beta, \gamma$ is not zero. {{WLOG}} let $\alpha \ne 0$. Then it follows that as $\alpha + \beta + \gamma = 0$, at least one of $\beta$ and $\gamma$ is also non-zero. {{WLOG}} let $\beta \ne 0$. In the following it is immaterial whether $\gamma = 0$ or not. We have: {{begin-eqn}} {{eqn | l = \alpha + \beta + \gamma | r = 0 | c = }} {{eqn | ll= \leadsto | l = \gamma | r = -\paren {\alpha + \beta} | c = }} {{eqn | ll= \leadsto | l = \alpha z_1 + \beta z_2 | r = \paren {\alpha + \beta} z_3 | c = from $\alpha z_1 + \beta z_2 + \gamma z_3 = 0$ }} {{eqn | ll= \leadsto | l = z_1 + \frac \beta \alpha z_2 | r = \frac {\alpha + \beta} \alpha z_3 | c = which can be done because $\alpha \ne 0$ }} {{eqn | ll= \leadsto | l = z_1 + \frac \beta \alpha z_2 - \paren {1 + \frac \beta \alpha} z_1 | r = \frac {\alpha + \beta} \alpha z_3 - \paren {1 + \frac \beta \alpha} z_1 | c = }} {{eqn | ll= \leadsto | l = \frac \beta \alpha \paren {z_2 - z_1} | r = \frac {\alpha + \beta} \alpha \paren {z_3 - z_1} | c = simplifying }} {{eqn | ll= \leadsto | l = z_2 - z_1 | r = \frac \alpha \beta \frac {\alpha + \beta} \alpha \paren {z_3 - z_1} | c = multiplying both sides by $\dfrac \alpha \beta$, which can be done because $\beta \ne 0$ }} {{eqn | ll= \leadsto | l = z_2 - z_1 | r = \frac {\alpha + \beta} \beta \paren {z_3 - z_1} | c = }} {{end-eqn}} Thus it is seen that: :$z_2 - z_1 = b \paren {z_3 - z_1}$ for some $b \in \R$. Hence by [[Condition for Collinearity of Points in Complex Plane/Formulation 1|Condition for Collinearity of Points in Complex Plane: Formulation 1]], $z_1$, $z_2$ and $z_3$ are [[Definition:Collinear|collinear]]. {{qed}}	0
Let $x_1, x_2, y_1, y_2 \in \R_{>0}$ be [[Definition:Strictly Positive Real Number|strictly positive real numbers]]. Let $\epsilon \in \openint 0 {\min \set {y_1, y_2, 1} }$. Then: :$\size {x_1 - y_1} < \epsilon \land \size {x_2 - y_2} < \epsilon \implies \size {x_1 x_2 - y_1 y_2} < \epsilon \paren {y_1 + y_2 + 1}$	0
Let $a, b, z \in \R$. Let $a \equiv b \pmod z$ denote that [[Definition:Congruence (Number Theory)|$a$ is congruent to $b$ modulo $z$]]. Then $\forall y \in \R, y \ne 0$: :$a \equiv b \pmod z \iff y a \equiv y b \pmod {y z}$	0
{{begin-eqn}} {{eqn | l = \cos^2 x + \sin^2 x | r = \paren {\frac {e^{i x} + e^{-i x} } 2}^2 + \sin^2 x | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \paren {\frac {e^{i x} + e^{-i x} } 2}^2 + \paren {\frac {e^{i x} - e^{-i x} } {2 i} }^2 | c = [[Sine Exponential Formulation]] }} {{eqn | r = \frac {\paren {e^{i x} }^2 + 2 e^{-i x} e^{i x} + \paren {e^{-i x} }^2} 4 + \paren {\frac {e^{i x} - e^{-i x} } {2 i} }^2 | c = [[Square of Sum]] }} {{eqn | r = \frac {\paren {e^{i x} }^2 + 2 e^{-i x} e^{i x} + \paren {e^{-i x} }^2} 4 + \frac {\paren {e^{i x} }^2 - e^{-i x} e^{i x} + \paren {e^{-i x} }^2} {-4} | c = [[Square of Difference]] and $i^2 = -1$ }} {{eqn | r = \frac {e^{2 i x} + 2 + e^{-2 i x} } 4 + \frac {e^{2 i x} - 2 + e^{-2 i x} } {-4} | c = [[Exponential of Sum]] }} {{eqn | r = \frac {e^{2 i x} + 2 + e^{-2 i x} - e^{2 i x} + 2 - e^{-2 i x} } 4 | c = simplifying }} {{eqn | r = \frac 4 4 | c = simplifying }} {{eqn | r = 1 | c = }} {{end-eqn}} {{qed}}	0
Let $\struct {S, \circ}$ be a [[Definition:Semilattice|semilattices]]. Let $\preceq$ be the [[Definition:Ordering|ordering]] on $S$ defined by: :$a \preceq b \iff \paren {a \circ b} = b$ Let $T$ be a [[Definition:Subset|subset]] of $S$. Let the [[Definition:Ordered Subset|ordered subset]] $\struct{T, \preceq \restriction_T}$ be a [[Definition:Join Semilattice|join semilattice]]. Let $\vee$ be the [[Definition:Binary Operation|binary operation]] on $S$ defined by: :for all $a, b \in S$, $a \vee b$ is the [[Definition:Join (Order Theory)|join]] of $a$ and $b$ with respect to $\preceq$. Then: :$\struct{T, \vee}$ may not be a [[Definition:Subsemilattice|subsemilattice]] of $\struct {S, \circ}$.	0
First a [[Equality of Ordered Pairs/Lemma|lemma]]: {{:Equality of Ordered Pairs/Lemma}}{{qed|lemma}} Let $\tuple {a, b} = \tuple {c, d}$. From the [[Definition:Kuratowski Formalization of Ordered Pair|Kuratowski formalization]]: :$\set {\set a, \set {a, b} } = \set {\set c, \set {c, d} }$ There are two cases: either $a = b$, or $a \ne b$. ==== Case 1 ==== Suppose $a = b$. Then: :$\set {\set a, \set {a, b} } = \set {\set a, \set a} = \set {\set a}$ Thus $\set {\set c, \set {c, d} }$ has only one [[Definition:Element|element]]. Thus $\set c = \set {c, d}$ and so $c = d$. So: : $\set {\set c, \set {c, d} } = \set {\set a}$ and so $a = c$ and $b = d$. Thus the result holds. {{qed|lemma}} ==== Case 2 ==== Now suppose $a \ne b$. By the same argument it follows that $c \ne d$. So that means that either $\set a = \set c$ or $\set a = \set {c, d}$. Since $\set {c, d}$ has $2$ [[Definition:Distinct Elements|distinct elements]], $\set a \ne \set {c, d}$. Thus: :$\set a = \set c$ and so $a = c$. Then: :$\set {a, b} = \set {c, d}$ But as $a = c$ that means we have: :$\set {a, b} = \set {a, d}$ It follows from [[Equality of Ordered Pairs/Lemma|the lemma]] that: :$b = d$ {{qed}}	0
Let $x \in m \Z \cap n \Z$. Then by definition of [[Definition:Set Intersection|set intersection]]: :$m \divides x$ and $n \divides x$ So from [[LCM Divides Common Multiple]]: :$\lcm \set {m, n} \divides x$ and so $x \in \lcm \set {m, n} \Z$ That is: :$m \Z \cap n \Z \subseteq \lcm \set {m, n} \Z$ {{qed|lemma}} Now suppose $x \in \lcm \set {m, n} \Z$. Then $\lcm \set {m, n} \divides x$. Thus by definition of [[Definition:Lowest Common Multiple of Integers|lowest common multiple]]: :$m \divides x$ and: :$n \divides x$ and so: :$x \in m \Z \land x \in n \Z$ That is: :$x \in \Z \cap n \Z$ and so: :$\lcm \set {m, n} \Z \subseteq m \Z \cap n \Z$ {{qed|lemma}} The result follows by definition of [[Definition:Set Equality|set equality]]. {{qed}}	0
Let $I$ and $J$ be [[Definition:Indexing Set|indexing sets]]. Let $\family {A_\alpha}_{\alpha \mathop \in I}$ and $\family {B_\beta}_{\beta \mathop \in J}$ be [[Definition:Indexed Family of Subsets|indexed families of subsets]] of a [[Definition:Set|set]] $S$. Then: :$\displaystyle \bigcap_{\tuple{\alpha, \beta} \mathop \in I \times J} \paren {A_\alpha \cup B_\beta} = \paren {\bigcap_{\alpha \mathop \in I} A_\alpha} \cup \paren {\bigcap_{\beta \mathop \in J} B_\beta}$ where $\displaystyle \bigcap_{\alpha \mathop \in I} A_\alpha$ denotes the [[Definition:Intersection of Family|intersection]] of $\family {A_\alpha}_{\alpha \mathop \in I}$.	0
:$720 = 6 \times 5 \times 4 \times 3 \times 2 = 10 \times 9 \times 8$	0
=== [[Second Isomorphism Theorem/Groups|Groups]] === {{:Second Isomorphism Theorem/Groups}} === [[Second Isomorphism Theorem/Rings|Rings]] === {{:Second Isomorphism Theorem/Rings}} This result is also referred to by some sources as the '''first isomorphism theorem'''.	0
Let $\mathbf C$ be a [[Definition:Metacategory|metacategory]], and let $C \in \mathbf C_0$ be an [[Definition:Object|object]] of $\mathbf C$. Let $\operatorname{id}_C: C \to C$ be the [[Definition:Identity Morphism|identity morphism]] for $C$. Then $\operatorname{id}_C$ is an [[Definition:Initial Object|initial object]] in the [[Definition:Coslice Category|coslice category]] $C \mathop / \mathbf C$.	0
:$\paren {R \cup S} \setminus T = \paren {R \setminus T} \cup \paren {S \setminus T}$	0
Let $X$ and $Y$ be two [[Definition:Plane Projective Curve|plane projective curves]] defined over a [[Definition:Field (Abstract Algebra)|field]] $F$ that do not have a common component. (This condition is true if both $X$ and $Y$ are defined by different [[Definition:Irreducible Polynomial|irreducible polynomials]]. In particular, it holds for a pair of "generic" curves.) {{refactor|Link to a proof of the above, and (preferably) move this statement to the proof itself.}} Then the total number of [[Definition:Intersection (Geometry)|intersection points]] of $X$ and $Y$ with [[Definition:Cartesian Coordinate System|coordinates]] in an [[Definition:Algebraically Closed Field|algebraically closed field]] $E$ which contains $F$, counted with their [[Definition:Multiplicity (Real Analysis)|multiplicities]], is equal to the product of the [[Definition:Degree (Polynomial)|degrees]] of $X$ and $Y$.	0
{{begin-eqn}} {{eqn | l = \cos^{2 n + 1} \theta | r = \paren {\frac {e^{i \theta} + e^{-i \theta} } 2}^{2 n + 1} | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {e^{i \theta} + e^{-i \theta} }^{2 n + 1} | c = [[Exponent Combination Laws/Power of Product|Exponent Combination Laws: Power of Product]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \sum^{2 n + 1}_{k \mathop = 0} \binom {2 n + 1} k e^{k i \theta} e^{-\paren {2 n - k + 1} i \theta} | c = [[Binomial Theorem]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \sum^{2 n + 1}_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} | c = [[Exponential of Sum]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^{2 n + 1}_{k \mathop = n + 1} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} } | c = Partitioning the sum }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^n_{k \mathop = 0} \binom {2 n + 1} {2 n + 1 - k} e^{\paren {2 \paren {2 n - k + 1} - 2 n - 1} i \theta} } | c = $k \mapsto 2 n + 1 - k$ }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{\paren {2 n - 2 k + 1} i \theta} } | c = [[Symmetry Rule for Binomial Coefficients]] }} {{eqn | r = \frac 1 {2^{2 n} } \sum^n_{k \mathop = 0} \binom {2 n + 1} k \, \map \cos {2 n - 2 k + 1} \theta | c = [[Cosine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
Fix $p$ and set: :$\displaystyle \map I \alpha = \int_0^\infty \frac {e^{-\alpha x} } {x \csc p x} \rd x$ for all $\alpha \ge 0$. Then: :$\displaystyle \int_0^\infty \frac {e^{-a x} - e^{-b x} } {x \csc p x} \rd x = \map I a - \map I b$ We have: {{begin-eqn}} {{eqn | l = \map {I'} \alpha | r = \frac \d {\d \alpha} \int_0^\infty \frac {e^{-\alpha x} } {x \csc p x} \rd x }} {{eqn | r = \int_0^\infty \frac \partial {\partial \alpha} \paren {\frac {e^{-\alpha x} } {x \csc p x} } \rd x | c = [[Definite Integral of Partial Derivative]] }} {{eqn | r = -\int_0^\infty e^{-\alpha x} \sin p x \rd x | c = [[Derivative of Exponential of a x|Derivative of $e^{a x}$]], {{Defof|Cosecant/Real Function|Cosecant}} }} {{eqn | r = -\frac p {\alpha^2 + p^2} | c = [[Definite Integral to Infinity of Exponential of -a x by Sine of b x|Definite Integral to Infinity of $e^{-a x} \sin b x$]] }} {{end-eqn}} so: {{begin-eqn}} {{eqn | l = \map I \alpha | r = -p \int \frac 1 {\alpha^2 + p^2} \rd \alpha }} {{eqn | r = -\arctan \frac \alpha p + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{end-eqn}} for all $\alpha \ge 0$ and constant $C \in \R$. We therefore have: {{begin-eqn}} {{eqn | l = \int_0^\infty \frac {e^{-a x} - e^{-b x} } {x \csc p x} \rd x | r = \map I a - \map I b }} {{eqn | r = \paren {-\arctan \frac a p + C} - \paren {-\arctan \frac b p + C} }} {{eqn | r = \arctan \frac b p - \arctan \frac a p }} {{end-eqn}} {{qed}}	0
Let $\left({D, +, \circ}\right)$ be a [[Definition:Division Ring|Division Ring]] whose [[Definition:Ring Zero|zero]] is $0$. :Let $\left({J, +, \circ}\right)$ be a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $D$. Then $J = \left\{{0}\right\}$.	0
Let $\left({X, \circ}\right)$ be a [[Definition:B-Algebra|$B$-algebra]]. Then: :$\forall x,y \in X: x \circ y = x \circ\left({0 \circ\left({ 0 \circ y}\right)}\right)$	0
It is confirmed that: :$113, 131, 311$ are all [[Definition:Prime Number|prime]] :$199, 919, 991$ are all [[Definition:Prime Number|prime]] :$337, 373, 733$ are all [[Definition:Prime Number|prime]]. From [[Digits of Permutable Prime]], all [[Definition:Permutable Prime|permutable primes]] contain [[Definition:Digit|digits]] in the [[Definition:Set|set]]: :$\left\{ {1, 3, 7, 9}\right\}$ The [[Definition:Integer Addition|sum]] of a [[Definition:Digit|$3$-digit]] [[Definition:Repdigit Number|repdigit number]] is [[Definition:Divisor of Integer|divisible]] by $3$. By [[Divisibility by 3]] it follows that all [[Definition:Digit|$3$-digit]] [[Definition:Repdigit Number|repdigit numbers]] are [[Definition:Divisor of Integer|divisible]] by $3$ and therefore [[Definition:Composite Number|composite]]. Hence the possibly [[Definition:Permutable Prime|permutable primes]] are: :$113, 117, 119, 133, 137, 139, 177, 179, 199, 337, 339, 377, 379, 399, 779, 799$ and their [[Definition:Anagram|anagrams]]. We eliminate $113, 199, 337$ from this list, as it has been established that they are [[Definition:Permutable Prime|permutable primes]]. Of those remaining, the following are [[Definition:Composite Number|composite]]: {{begin-eqn}} {{eqn | l = 117 | r = 3^2 \times 13 }} {{eqn | l = 119 | r = 7 \times 17 }} {{eqn | l = 133 | r = 7 \times 19 }} {{eqn | l = 177 | r = 3 \times 59 }} {{eqn | l = 339 | r = 3 \times 113 }} {{eqn | l = 377 | r = 13 \times 29 }} {{eqn | l = 779 | r = 19 \times 41 }} {{eqn | l = 799 | r = 17 \times 47 }} {{end-eqn}} It remains to demonstrate that at least one [[Definition:Anagram|anagram]] of the remaining numbers: :$137, 139, 179, 379$ is [[Definition:Composite Number|composite]]. We find that: {{begin-eqn}} {{eqn | l = 371 | r = 7 \times 53 }} {{eqn | l = 319 | r = 11 \times 29 }} {{eqn | l = 791 | r = 7 \times 113 }} {{eqn | l = 793 | r = 13 \times 61 }} {{end-eqn}} All contenders are eliminated except for the established [[Definition:Permutable Prime|permutable primes]] $113, 119, 337$ and their [[Definition:Anagram|anagrams]]. {{qed}}	0
Let $\map e {\mathbf A}$ be the [[Definition:Elementary Column Operation|elementary column operation]]: :$e := \kappa_k \to \lambda \kappa_k$ where $\lambda \ne 0$. Then $\kappa'_k$ is such that: :$\forall a'_{k i} \in \kappa'_k: a'_{k i} = \lambda a_{k i}$ Now let $\map {e'} {\mathbf A'}$ be the [[Definition:Elementary Column Operation|elementary column operation]] which transforms $\mathbf A'$ to $\mathbf A''$: :$e' := \kappa_k \to \dfrac 1 \lambda \kappa_k$ Because it is stipulated in the definition of an [[Definition:Elementary Column Operation|elementary column operation]] that $\lambda \ne 0$, it follows by definition of a [[Definition:Field (Abstract Algebra)|field]] that $\dfrac 1 \lambda$ exists. Hence $e'$ is defined. So applying $e'$ to $\mathbf A'$ we get: {{begin-eqn}} {{eqn | lo= \forall a''_{i k} \in \kappa''_k: | l = a''_{i k} | r = \dfrac 1 \lambda a'_{i k} | c = }} {{eqn | r = \dfrac 1 \lambda \paren {\lambda a_{i k} } | c = }} {{eqn | r = a_{i k} | c = }} {{eqn | ll= \leadsto | lo= \forall a''_{i k} \in \kappa''_k: | l = a''_{i k} | r = a_{i k} | c = }} {{eqn | ll= \leadsto | l = \kappa''_k | r = \kappa_k | c = }} {{eqn | ll= \leadsto | l = \mathbf A'' | r = \mathbf A | c = }} {{end-eqn}} It is noted that for $e'$ to be an [[Definition:Elementary Column Operation|elementary column operation]], the only possibility is for it to be as defined.	0
As $\Box BC$ is a [[Definition:Square (Geometry)|square]], then $\Box AD$ is also a [[Definition:Square (Geometry)|square]]. Since $\Box CD = \Box BC$, we subtract $\Box CE$ from each. Therefore $\Box BF = \Box AD$. From [[Sides of Equal and Equiangular Parallelograms are Reciprocally Proportional]], in $\Box BF$ and $\Box AD$ the sides about the equal angles are [[Definition:Reciprocal Proportion|reciprocally proportional]]. Therefore $FE : ED = AE : EB$. But $FE = AB$ and $ED = AE$. So $BA : AE = AE : EB$. Also $AB > AE$ and so $AE > EB$. Hence the result. {{qed}} {{Euclid Note|30|VI}}	0
We will prove that :$\forall x, y \in S: \left({ y \npreceq x \implies \exists p \in X: x \preceq p \land y \npreceq p }\right)$ Let $x, y \in S$ such that :$y \npreceq x$ By [[Not Preceding implies There Exists Meet Irreducible Element Not Preceding]] :$\exists p \in S: p$ is [[Definition:Meet Irreducible|meet irreducible]] and $x \preceq p$ and $y \npreceq p$ By definition of [[Definition:Greatest Element|greatest element]]: :$p \ne \top$ and $p \in \mathit{IRR}\left({L}\right)$ By definitions of [[Definition:Set Difference|difference]] and [[Definition:Singleton|singleton]]: :$p \in X$ Thus :$\exists p \in X: x \preceq p \land y \npreceq p$ {{qed|lemma}} Hence by [[Order Generating iff Not Preceding implies There Exists Element Preceding and Not Preceding]]: :$X$ is [[Definition:Order Generating|order generating]]. {{qed}}	0
:$159 = 14 \times 1^4 + 4 \times 2^4 + 3^4$	0
From [[Factorisation of z^n-a|Factorisation of $z^n - a$]]: :$x^{2 n + 1} - y^{2 n + 1} = \displaystyle \prod_{k \mathop = 0}^{2 n} \paren {x - \alpha^k y}$ where $\alpha$ is a [[Definition:Primitive Complex Root of Unity|primitive complex $2 n + 1$th roots of unity]], for example: {{begin-eqn}} {{eqn | l = \alpha | r = e^{2 i \pi / \paren {2 n + 1} } | c = }} {{eqn | r = \cos \dfrac {2 \pi} {2 n + 1} + i \sin \dfrac {2 \pi} {2 n + 1} | c = }} {{end-eqn}} From [[Complex Roots of Unity occur in Conjugate Pairs]]: :$U_{2 n + 1} = \set {1, \tuple {\alpha, \alpha^{2 n} }, \tuple {\alpha^2, \alpha^{2 n - 1} }, \ldots, \tuple {\alpha^k, \alpha^{2 n - k + 1} }, \ldots, \tuple {\alpha^n, \alpha^{n + 1} } }$ where $U_{2 n + 1}$ denotes the [[Definition:Complex Roots of Unity|complex $2 n + 1$th roots of unity]]: :$U_{2 n + 1} = \set {z \in \C: z^{2 n + 1} = 1}$ The case $k = 0$ is taken care of by setting $\alpha^0 = 1$, from whence we have the factor $x - y$. Taking the [[Definition:Complex Multiplication|product]] of each of the remaining factors of $x^{2 n + 1} - y^{2 n + 1}$ in pairs: {{begin-eqn}} {{eqn | l = \paren {x - \alpha^k y} \paren {x - \alpha^{2 n - k + 1} y} | r = \paren {x - \alpha^k y} \paren {x - \overline {\alpha^k} y} | c = [[Complex Roots of Unity occur in Conjugate Pairs]] }} {{eqn | r = x^2 - x \paren {\alpha^k + \overline {\alpha^k} } y + \alpha^k y \, \overline {\alpha^k} y | c = }} {{eqn | r = x^2 - x y \paren {\alpha^k + \overline {\alpha^k} } + \cmod {\alpha^k}^2 y^2 | c = [[Modulus in Terms of Conjugate]] }} {{eqn | r = x^2 - x y \paren {\alpha^k + \overline {\alpha^k} } + y^2 | c = [[Modulus of Complex Root of Unity equals 1]] }} {{eqn | r = x^2 - x y \paren {\cos \dfrac {2 k \pi} {2 n + 1} + i \sin \dfrac {2 k \pi} {2 n + 1} + \cos \dfrac {2 k \pi} {2 n + 1} - i \sin \dfrac {2 k \pi} {2 n + 1} } + y^2 | c = Definition of $\alpha$ }} {{eqn | r = x^2 - 2 x y \cos \dfrac {2 k \pi} {2 n + 1} + y^2 | c = simplification }} {{end-eqn}} Hence the result. {{qed}}	0
From the definition, an [[Definition:Permutation (Ordered Selection)|$r$-permutation of $S$]] is an ordered selection of $r$ [[Definition:Element|elements]] of $S$. It can be seen that an $r$-permutation is an [[Definition:Injection|injection]] from a [[Definition:Subset|subset]] of $S$ into $S$. From [[Cardinality of Set of Injections]], we see that the number of [[Definition:Permutation (Ordered Selection)|$r$-permutations]] ${}^r P_n$ on a [[Definition:Set|set]] of $n$ [[Definition:Element|elements]] is given by: :${}^r P_n = \dfrac {n!} {\left({n-r}\right)!}$ From this definition, it can be seen that a [[Definition:Bijection|bijection]] $f: S \to S$ is an [[Definition:Permutation (Ordered Selection)|$n$-permutation]]. Hence the number of $n$-permutations on a [[Definition:Set|set]] of $n$ [[Definition:Element|elements]] is: :${}^n P_n = \dfrac {n!} {\left({n - n}\right)!} = n!$ {{Qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \cos^{n - 1} a x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = -a \paren {n - 1} \cos^{n - 2} a x \sin a x | c = [[Derivative of Cosine of a x|Derivative of $\cos a x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \sin^m a x \cos a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {\sin^{m + 1} a x} {\paren {m + 1} a} | c = [[Primitive of Power of Sine of a x by Cosine of a x|Primitive of $\sin^n a x \cos a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \sin^m a x \cos^n a x \rd x | r = \int \paren {\cos^{n - 1} a x} \paren {\sin^m a x \cos a x} \rd v | c = }} {{eqn | r = \paren {\cos^{n - 1} a x} \paren {\frac {\sin^{m + 1} } {\paren {m + 1} a} } | c = [[Integration by Parts]] }} {{eqn | o = | ro= - | r = \int \paren {\frac {\sin^{m + 1} } {\paren {m + 1} a} } \paren {-a \paren {n - 1} \cos^{n - 2} a x \sin a x } \rd x + C | c = }} {{eqn | r = \frac {\sin^{m + 1} a x \cos^{n - 1} a x} {a \paren {m + 1} } + \frac {n - 1} {m + 1} \int \sin^{m + 2} a x \cos^{n - 2} a x \rd x + C | c = simplifying }} {{eqn | r = \frac {\sin^{m + 1} a x \cos^{n - 1} a x} {a \paren {m + 1} } | c = }} {{eqn | o = | ro= + | r = \frac {n - 1} {m + 1} \int \sin^m a x \paren {1 - \cos^2 a x} \cos^{n - 2} a x \rd x + C | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \frac {\sin^{m + 1} a x \cos^{n - 1} a x} {a \paren {m + 1} } + \frac {n - 1} {m + 1} \int \sin^m a x \cos^{n - 2} a x \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | o = | ro= - | r = \frac {n - 1} {m + 1} \int \sin^m a x \cos^n a x \rd x + C | c = }} {{end-eqn}} Hence after rearranging: {{begin-eqn}} {{eqn | r = \frac {\sin^{m + 1} a x \cos^{n - 1} a x} {a \paren {m + 1} } + \frac {n - 1} {m + 1} \int \sin^m a x \cos^{n - 2} a x \rd x | o = | c = }} {{eqn | r = \int \sin^m a x \cos^n a x \rd x + \frac {n - 1} {m + 1} \int \sin^m a x \cos^n a x \rd x + C | c = }} {{eqn | r = \frac {m + 1} {m + 1} \int \sin^m a x \cos^n a x \rd x + \frac {n - 1} {m + 1} \int \sin^m a x \cos^n a x \rd x + C | c = common [[Definition:Denominator|denominator]] }} {{eqn | r = \frac {m + n} {m + 1} \int \sin^m a x \cos^n a x \rd x + C | c = simplifying }} {{eqn | ll= \leadsto | l = \int \sin^m a x \cos^n a x \rd x | r = \frac {\sin^{m + 1} a x \cos^{n - 1} a x} {a \paren {m + n} } + \frac {n - 1} {m + n} \int \sin^m a x \cos^{n - 2} a x \rd x + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $\xi \in \R$ be a [[Definition:Real Number|real number]]. Let $\left \langle {a_n} \right \rangle$ be a [[Definition:Sequence|sequence in $\R$]]. Let $\displaystyle \sum_{m \mathop \ge 0} a_m \left({x - \xi}\right)^m$ be the [[Definition:Power Series|power series]] in $x$ about the point $\xi$. Then within the [[Definition:Interval of Convergence|interval of convergence]]: :$\displaystyle \frac {\mathrm d^n} {\mathrm d x^n} \sum_{m \mathop \ge 0} a_m \left({x - \xi}\right)^m = \sum_{m \mathop \ge n} a_m m^{\underline n} \left({x - \xi}\right)^{m - n}$ where $m^{\underline n}$ denotes the [[Definition:Falling Factorial|falling factorial]].	0
The smallest [[Definition:Fourth Power|$4$th power]] that can be expressed as the [[Definition:Integer Addition|sum]] of $2$ [[Definition:Fourth Power|$4$th powers]] [[Definition:Integer Subtraction|minus]] a $3$rd is: :$2401 = 7^4 = 227^4 + 157^4 - 239^4$ with all numbers less than $10^4$.	0
Let $\struct {\R, +, \times}$ denote the [[Definition:Field of Real Numbers|field of real numbers]]. Let $X$ be [[Definition:Transcendental over Field|transcendental over $\R$]]. Let $\R \sqbrk X$ be the [[Definition:Ring of Polynomials|ring of polynomials]] in $X$ over $F$. Consider the [[Definition:Field of Quotients|field of quotients]]: :$\R \sqbrk X / \ideal p$ where: :$p = X^2 + 1$ :$\ideal p$ denotes the [[Definition:Ideal of Ring|ideal]] [[Definition:Generator of Ideal|generated]] by $p$. Then $\R \sqbrk X / \ideal p$ is the [[Definition:Field of Complex Numbers|field of complex numbers]].	0
Let $M_1 = \left({A_1, d_1}\right)$ and $M_2 = \left({A_2, d_2}\right)$ be [[Definition:Metric Space|metric spaces]]. Let $f: A_1 \to A_2$ be a [[Definition:Mapping|mapping]]. Then $f$ is [[Definition:Continuous at Point of Metric Space|continuous]] at $a \in X$ [[Definition:Iff|iff]]: : whenever $\displaystyle \lim_{n \mathop \to \infty} x_n = a$ for a [[Definition:Sequence|sequence]] $\left \langle {x_n} \right \rangle$ of points of $A_1$ it is true that: : $\displaystyle \lim_{n \mathop \to \infty} f \left({x_n}\right) = f \left({a}\right)$	0
Consider the [[Sequence of Best Rational Approximations to Square Root of 2]]: :$\sequence S := \dfrac 1 1, \dfrac 3 2, \dfrac 7 5, \dfrac {17} {12}, \dfrac {41} {29}, \dfrac {99} {70}, \dfrac {239} {169}, \dfrac {577} {408}, \ldots$ Let $\dfrac {p_n} {q_n}$ and $\dfrac {p_{n + 1} } {q_{n + 1} }$ be adjacent [[Definition:Term of Sequence|terms]] of $\sequence S$. Then: :$\dfrac {p_{n + 1} } {q_{n + 1} } = \dfrac {p_n + 2 q_n} {p_n + q_n}$	0
The smallest [[Definition:Cunningham Chain of the First Kind|Cunningham chain of the first kind]] of [[Definition:Length of Sequence|length]] $7$ is: :$\left({1 \, 122 \, 659, 2 \, 245 \, 319, 4 \, 490 \, 639, 8 \, 981 \, 279, 17 \, 962 \, 559, 35 \, 925 \, 119, 71 \, 850 \, 239}\right)$	0
By the definition of the [[Definition:Ordering on Extended Real Numbers|usual ordering]] on the [[Definition:Extended Real Number Line|extended real numbers]]: :${\le} = {\le_\R} \cup \set {\tuple {x, +\infty}: x \in \overline \R} \cup \set {\tuple {-\infty, x}: x \in \overline \R}$ Suppose $x \in \overline \R$ and $+\infty \le x$. That is: : $\tuple {+\infty, x} \in {\le}$ By the definition of [[Definition:Set Union|union]], $\tuple {+\infty, x}$ must lie in one of the three sets whose union forms $\le$. Since ${\le_\R} \subseteq \R \times \R$ and $+\infty \notin \R$: :$\tuple {+\infty, x} \notin {\le_\R}$ Since $+\infty \ne -\infty$ by the definition of the [[Definition:Extended Real Number Line|extended real numbers]]: :$\tuple {+\infty, x} \notin \set {\tuple {-\infty, x}: x \in \overline \R}$ Therefore: :$\tuple {+\infty, x} \in \set {\tuple {x, +\infty}: x \in \overline \R}$ and we conclude that $x = +\infty$. That is, $+\infty$ is a [[Definition:Maximal Element|maximal element]] of $\overline \R$. {{qed}}	0
:$\displaystyle \int \frac {\arccos \frac x a \ \mathrm d x} {x^2} = \frac {-\arccos \frac x a} x + \frac 1 a \ln \left({\frac {a + \sqrt {a^2 - x^2} } x}\right) + C$	0
Let $G$ be a non-[[Definition:Abelian Group|abelian]] [[Definition:Finite Group|finite]] [[Definition:Simple Group|simple group]]. Let $t \in G$ be a [[Definition:Self-Inverse Element|self-inverse element]] of $G$. Then: :$\map {C_G} t \ne G$ where $\map {C_G} t$ denotes the [[Definition:Centralizer|centralizer]] of $t$ in $G$.	0
Let the [[Definition:Root of Polynomial|roots]] of $P$ be $z_1, z_2, \ldots, z_n$. Then $P$ can be written in factored form as: :$\displaystyle a_n \prod_{k \mathop = 1}^n \paren {z - z_k} = a_n \paren {z - z_1} \paren {z - z_2} \cdots \paren {z - z_n}$ Multiplying this out, $P$ can be expressed as: :$a_n \paren {z^n - \paren {z_1 + z_2 + \cdots + z_n} z^{n - 1} + \cdots + \paren {-1}^n z_1 z_2 \cdots z_n} = 0$ where the coefficients of $z^{n - 2}, z^{n - 3}, \ldots$ are more complicated and irrelevant. Equating powers of $z$, it follows that: :$-a_n \paren {z_1 + z_2 + \cdots + z_n} = a_{n - 1}$ from which: :$z_1 + z_2 + \cdots + z_n = - \dfrac {a_{n - 1} } {a_n}$ {{qed}}	0
By definition of the [[Definition:Beta Function/Definition 2|Beta function]]: :$\displaystyle \Beta \left({x, y}\right) := 2 \int_0^{\pi / 2} \left({\sin \theta}\right)^{2x - 1} \left({\cos \theta}\right)^{2y - 1} \rd \theta$ Thus: {{begin-eqn}} {{eqn | l = \Beta \left({\dfrac 1 2, \dfrac 1 2}\right) | r = 2 \int_0^{\pi / 2} \left({\sin \theta}\right)^{2 \times \frac 1 2 - 1} \left({\cos \theta}\right)^{2 \times \frac 1 2 - 1} \rd \theta | c = }} {{eqn | r = 2 \int_0^{\pi / 2} \left({\sin \theta}\right)^0 \left({\cos \theta}\right)^0 \rd \theta | c = }} {{eqn | r = 2 \int_0^{\pi / 2} \rd \theta | c = }} {{eqn | r = 2 \left[{\theta}\right]_0^{\pi / 2} | c = }} {{eqn | r = 2 \left({\pi / 2 - 0}\right) | c = }} {{eqn | r = \pi | c = }} {{end-eqn}} {{qed}}	0
Let $f: \R \to \R$ be a [[Definition:Real Antiperiodic Function|real antiperiodic function]] with an [[Definition:Antiperiod|anti-period]] of $A$. Then $f$ is also [[Definition:Real Periodic Function|periodic]] with a [[Definition:Period of Function|period]] of $2A$.	0
Let $D_n$ be the [[Definition:Dihedral Group|dihedral group]] of [[Definition:Order of Structure|order]] $2 n$. Let $D_n$ be defined by its [[Group Presentation of Dihedral Group|group presentation]]: :$D_n = \gen {\alpha, \beta: \alpha^n = \beta^2 = e, \beta \alpha \beta = \alpha^{−1} }$ Then for all $k \in \Z_{\ge 0}$: :$\beta \alpha^k = \alpha^{n - k} \beta$	0
The '''cube''' of a quantity is the [[Definition:Power (Algebra)|third power]] of that quantity.	0
Let $a \in \R_{\ne 0}$. Let $b^2 - 4 a c = 0$. Then: :$\displaystyle \int \frac {\mathrm d x} {\sqrt {a x^2 + b x + c} } = \frac 1 {\sqrt a} \ln \left\vert{2 a x + b}\right\vert + C$	0
Assume the truth of [[Schanuel's Conjecture]]. From [[Schanuel's Conjecture Implies Algebraic Independence of Pi and Log of Pi over the Rationals]], $\ln \pi$ and $\pi$ are [[Definition:Algebraically Independent|algebraically independent]] over the [[Definition:Rational Number|rational numbers $\Q$]]. Therefore, if [[Schanuel's Conjecture]] holds, $\ln \pi$ must be [[Definition:Transcendental Number|transcendental]]. {{qed}} [[Category:Transcendental Numbers]] [[Category:Pi]] [[Category:Logarithms]] [[Category:Schanuel's Conjecture]] mrrbpogcvdpyxo7t1dygcbazwaircyy	0
From [[Injection from Finite Set to Itself is Surjection]], $f$ is a [[Definition:Surjection|surjection]]. As $f$ is thus both an [[Definition:Injection|injection]] and a [[Definition:Surjection|surjection]], $f$ is a [[Definition:Bijection|bijection]] by definition. Thus as $f$ is a [[Definition:Bijection|bijection]] to itself, it is by definition a [[Definition:Permutation|permutation]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = S \times T | o = \ne | r = \O }} {{eqn | ll= \leadstoandfrom | l = \exists \tuple {s, t} | o = \in | r = S \times T | c = {{Defof|Empty Set}} }} {{eqn | ll= \leadstoandfrom | l = \exists s \in S | o = \land | r = \exists t \in T | c = {{Defof|Cartesian Product}} }} {{eqn | ll= \leadstoandfrom | l = S \ne \O | o = \land | r = T \ne \O | c = {{Defof|Empty Set}} }} {{eqn | ll= \leadstoandfrom | l = \neg \leftparen {S = \O} | o = \lor | r = \rightparen {T = \O} | c = [[De Morgan's Laws (Logic)/Conjunction of Negations|De Morgan's Laws: Conjunction of Negations]] }} {{end-eqn}} So by the [[Rule of Transposition]]: :$S = \O \lor T = \O \iff S \times T = \O$ {{Qed}}	0
For every $n$ greater than $23$, there exists a [[Definition:Binomial Coefficient|binomial coefficient]] $\dbinom n k$ that is not [[Definition:Square-Free Integer|square-free]]. More specifically, the list of numbers $n$ such that $\dbinom n k$ are squarefree for all $k = 0, \dots, n$ is given by: :$1, 2, 3, 5, 7, 11, 23$ {{OEIS|A048278}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {\sin^m a x \cos^n a x} | r = \int \frac {\sin^{-m} a x \ \mathrm d x} {\cos^n a x} | c = }} {{eqn | r = \frac {\sin^{-m + 1} a x} {a \left({n - 1}\right) \cos^{n - 1} a x} - \frac {-m - n + 2} {n - 1} \int \frac {\sin^{-m} a x} {\cos^{n - 2} a x} \ \mathrm d x + C | c = [[Primitive of Power of Sine of a x over Power of Cosine of a x/Reduction of Power of Cosine|Primitive of $\dfrac {\sin^m a x} {\cos^n a x}$]] }} {{eqn | r = \frac 1 {a \left({n - 1}\right) \sin^{m - 1} a x \cos^{n - 1} a x} + \frac {m + n - 2} {n - 1} \int \frac {\mathrm d x} {\sin^m a x \cos^{n - 2} a x} + C | c = }} {{end-eqn}} {{qed}}	0
Because $a_n = O(b_n)$, there exists $M\geq0$ and $n_0 \in\N$ such that $|a_n| \leq M \cdot |b_n|$ for $n\geq n_0$. Because $n_k$ [[Definition:Divergent Sequence|diverges]], there exists $k_0\in\N$ such that $n_k\geq n_0$ for $k\geq k_0$. Then $|a_{n_k}| \leq M\cdot |b_{n_k}|$ for $k\geq k_0$. Thus $a_{n_k} = O(b_{n_k})$. {{qed}} [[Category:Asymptotic Notation]] q917p4cyyh16q4kjx1g18nd1hkq82oy	0
Let $\struct {\Z \sqbrk i, +, \times}$ be the [[Definition:Ring of Gaussian Integers|ring of Gaussian integers]]. The [[Definition:Set|set]] of [[Definition:Unit of Ring|units]] of $\struct {\Z \sqbrk i, +, \times}$ is $\set {1, i, -1, -i}$.	0
The [[Definition:Real Inverse Hyperbolic Tangent|(real) inverse hyperbolic tangent]] function has a [[Definition:Taylor Series|Taylor series expansion]]: {{begin-eqn}} {{eqn | l = \tanh^{-1} x | r = \sum_{n \mathop = 0}^\infty \frac {x^{2 n + 1} } {2 n + 1} | c = }} {{eqn | r = x + \frac {x^3} 3 + \frac {x^5} 5 + \frac {x^7} 7 + \cdots | c = }} {{end-eqn}} for $\size x < 1$.	0
Let $X$ be a [[Definition:Compact Space|compact topological space]]. Let $\struct {\mathbb K, \norm{\,\cdot\,}}$ be a [[Definition:Valued Field|valued field]]. Let $\sequence {f_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Continuous Mapping|continuous mappings]] $f_n: X \to \mathbb K$. Let the [[Definition:Infinite Product|infinite product]] $\displaystyle \prod_{n \mathop = 1}^\infty f_n$ [[Definition:Uniform Convergence of Product|converge uniformly]] on $X$. Then for all $N \in \N$, $\displaystyle \prod_{n \mathop = N}^\infty f_n$ [[Definition:Uniform Convergence of Product|converges uniformly]] and the [[Definition:Sequence|sequence]] $\displaystyle \prod_{n \mathop = N}^\infty f_n$ [[Definition:Uniform Convergence|converges uniformly]] to $1$.	0
Let $a > b$. Then by definition of [[Definition:Indexed Summation|indexed summation]], both sides are $0$. Let $a \le b$. === Reduction to $a=1$ === By [[Indexed Summation over Translated Interval]]: :$\displaystyle \sum_{i \mathop = a}^b \map f i = \sum_{j \mathop = 1}^{b - a + 1} \map f {j - a + 1}$ :$\displaystyle \sum_{i \mathop = a}^b \map f {\map \sigma i} = \sum_{j \mathop = 1}^{b - a + 1} \map f {\map \sigma {j - a + 1} }$ Let $n = b - a + 1$. Because $a \le b$, we have $n \ge 1$. By [[Translation of Integer Interval is Bijection]], the [[Definition:Mapping|mapping]] $T : \closedint 1 n \to \closedint a b$ defined by: :$\map T i = i + a - 1$ is a [[Definition:Bijection|bijection]]. We have: :$\displaystyle \sum_{i \mathop = a}^b \map f i = \sum_{j \mathop = 1}^n \map f {\map T j}$ :$\displaystyle \sum_{i \mathop = a}^b \map f {\map \sigma i} = \sum_{j \mathop = 1}^n \map f {\map \sigma {\map T j} }$ By [[Inverse of Bijection is Bijection]] and [[Composite of Bijections is Bijection]], the [[Definition:Composition of Mappings|composition]] $T^{-1} \circ \sigma \circ T$ is a [[Definition:Permutation|permutation]] of $\closedint 1 n$. Suppose we have settled the case $a = 1$. Then: :$\displaystyle \sum_{j \mathop = 1}^n \map f {\map T j} = \sum_{j \mathop = 1}^n \map {\paren {f \circ T} \circ \paren {T^{-1} \circ \sigma \circ T} } j$ By [[Composition of Mappings is Associative]], this equals: :$\displaystyle \sum_{j \mathop = 1}^n \paren {f \circ \sigma \circ T} j$ Thus: :$\displaystyle \sum_{i \mathop = a}^b \map f i = \sum_{i \mathop = a}^b \map f {\map \sigma i}$ {{qed|lemma}} It remains to investigate the case $a = 1$. === The case $a = 1$ === It remains to show that :$\displaystyle \sum_{i \mathop = 1}^n \map f i = \sum_{i \mathop = 1}^n \map f {\map \sigma i}$ for any [[Definition:Permutation|permutation]] $\sigma$ of $\closedint 1 n$. Let $S_n$ be the [[Definition:Symmetric Group on n Letters|symmetric group on $n$ letters]], that is, the [[Definition:Symmetric Group|group of permutations]] of $\closedint 1 n$. Let $U \subset S_n$ be the [[Definition:Subset|subset]] of all [[Definition:Permutation|permutations]] $\sigma$ satisfying :$\displaystyle \sum_{i \mathop = 1}^n \map f i = \sum_{i \mathop = 1}^n \map f {\map \sigma i}$ for all [[Definition:mapping|mappings]] $f: \closedint 1 n \to \mathbb A$. {{qed|lemma}} ==== Summation-preserving permutations form subgroup ==== We prove that $U$ is a [[Definition:Subgroup|subgroup]] of $S_n$, using [[Two-Step Subgroup Test]]. We have to show that $U$ is [[Definition:Non-Empty Set|non-empty]]. By [[Identity Mapping is Permutation]] the [[Definition:Identity Mapping|identity mapping]] $I$ on $\closedint 1 n$ is a [[Definition:Permutation|permutation]]. By [[Identity Mapping is Right Identity]], $f \circ I = f$. Thus: :$\displaystyle \sum_{i \mathop = 1}^n \map f i = \sum_{i \mathop = 1}^n \map f {\map I i}$ Thus $I \in U$. Let $\sigma, \tau \in U$. We have to show that $\sigma \circ \tau$ is in $U$. We have: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \map {f \circ \paren {\sigma \circ \tau} } i | r = \sum_{i \mathop = 1}^n \map {\paren {\paren {f \circ \sigma} \circ \tau} } i | c = [[Composition of Mappings is Associative]] }} {{eqn | r = \sum_{i \mathop = 1}^n \map {\paren {f \circ \sigma} } i | c = $\tau \in U$ }} {{eqn | r = \sum_{i \mathop = 1}^n \map f i | c = $\sigma \in U$ }} {{end-eqn}} Thus $\sigma \circ \tau \in U$. We show that $\sigma^{-1} \in U$. We have: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \map {\paren {f \circ \sigma^{-1} } } i | r = \sum_{i \mathop = 1}^n \map {\paren {\paren {f \circ \sigma^{-1} } \circ \sigma} } i | c = $\sigma \in U$ }} {{eqn | r = \sum_{i \mathop = 1}^n \map {\paren {f \circ \paren {\sigma^{-1} \circ \sigma} } } i | c = [[Composition of Mappings is Associative]] }} {{eqn | r = \sum_{i \mathop = 1}^n \map {\paren {f \circ I} } i | c = [[Identity Mapping is Identity Element in Group of Permutations]] }} {{eqn | r = \sum_{i \mathop = 1}^n \map f i | c = $I \in U$ }} {{end-eqn}} Thus $\sigma^{-1} \in U$. Thus $U$ is a [[Definition:Subgroup|subgroup]] of $S_n$. {{qed|lemma}} ==== Last adjacent transposition preserves summations ==== If $n = 1$, then by [[First Symmetric Group is Trivial Group]] and [[Subgroups of Trivial Group]], $U = S_n$. Let $n \ge 2$. By [[Symmetric Group on n Letters is Generated by Standard Cycle and Adjacent Transposition]], it suffices to show that $U$ contains an [[Definition:Adjacent Transposition|adjacent transposition]] and the [[Definition:Standard Cycle|standard cycle]]. Let $\sigma \in S_n$ be the [[Definition:Adjacent Transposition|adjacent transposition]] $\begin {bmatrix} n - 1 & n \end {bmatrix}$. We have: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \map f {\map \sigma i} | r = \sum_{i \mathop = 1}^{n - 1} \map f {\map \sigma i} + \map f {\map \sigma n} | c = {{Defof|Indexed Summation}} }} {{eqn | r = \sum_{i \mathop = 1}^{n - 2} \map f {\map \sigma i} + \map f {\map \sigma {n - 1} } + \map f {\map \sigma n} | c = {{Defof|Indexed Summation}} }} {{eqn | r = \sum_{i \mathop = 1}^{n - 2} \map f i + \map f n + \map f {n - 1} | c = {{Defof|Adjacent Transposition}} }} {{eqn | r = \sum_{i \mathop = 1}^{n - 2} \map f i + \map f {n - 1} + \map f n | c = [[Commutative Law of Addition]] }} {{eqn | r = \sum_{i \mathop = 1}^{n - 1} \map f i + \map f n | c = {{Defof|Indexed Summation}} }} {{eqn | r = \sum_{i \mathop = 1}^n \map f i | c = {{Defof|Indexed Summation}} }} {{end-eqn}} Thus $\begin {bmatrix} n - 1 & n \end {bmatrix} \in U$. {{qed|lemma}} ==== Standard cycle preserves summations ==== Let $\sigma$ be the [[Definition:Standard Cycle|standard cycle]] $\begin {bmatrix} 1 & \ldots & n \end {bmatrix}$. We have: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \map f {\map \sigma i} | r = \sum_{i \mathop = 1}^{n - 1} \map f {\map \sigma i} + \map f {\map \sigma n} | c = {{Defof|Indexed Summation}}, $n \ge 1$ }} {{eqn | r = \sum_{i \mathop = 1}^{n - 1} \map f {i + 1} + \map f 1 | c = {{Defof|Standard Cycle}} }} {{eqn | r = \sum_{i \mathop = 2}^n \map f i + \map f 1 | c = [[Indexed Summation over Translated Interval]] }} {{eqn | r = \map f 1 + \sum_{i \mathop = 2}^n \map f i | c = [[Commutative Law of Addition]] }} {{eqn | r = \sum_{i \mathop = 1}^n \map f i | c = [[Indexed Summation without First Term]] }} {{end-eqn}} Thus $\begin {bmatrix} 1 & \ldots & n \end {bmatrix} \in U$. {{qed}}	0
By definition, if $a \divides b$ then $\exists d \in D: a \times d = b$. Then $\paren {a \times d} \times c = b \times c$, that is: :$\paren {a \times c} \times d = b \times c$ which follows because $\times$ is [[Definition:Commutative Operation|commutative]] and [[Definition:Associative|associative]] in an [[Definition:Integral Domain|integral domain]]. Hence the result. {{qed}}	0
:$\map {f'} {x_1} \not \equiv 0 \pmod p$	0
Let $G$ be a [[Definition:Topological Group|topological group]]. Let $H \le G$ be a [[Definition:Subgroup|subgroup]]. {{TFAE}} :$(1):\quad$ $G$ is [[Definition:Connected Topological Space|connected]] :$(2):\quad$ $H$ is [[Definition:Connected Topological Space|connected]] and the [[Definition:Left Coset Space|left quotient space]] $G / H$ is [[Definition:Connected Topological Space|connected]] :$(3):\quad$ $H$ is [[Definition:Connected Topological Space|connected]] and the [[Definition:Right Coset Space|right quotient space]] $G / H$ is [[Definition:Connected Topological Space|connected]].	0
We have that $35 = 5 \times 7$. Then we have that $5$ and $7$ are [[Definition:Prime Number|primes]] such that $5 < 7$ and $5$ does not [[Definition:Divisor of Integer|divide]] $7 - 1$. Thus [[Cyclic Groups of Order p q|Cyclic Groups of Order $p q$]] can be applied. {{Qed}}	0
Follows from [[Reciprocal Function is Strictly Decreasing]] and from [[Restriction of Monotone Function is Monotone]]. {{qed}}	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\left({a x + b}\right)^3} | r = \int \frac 1 a \frac {u - b} {a u^3} \ \mathrm d u | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\mathrm d u} {u^2} - \frac b {a^2} \int \frac {\mathrm d u} {u^3} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \frac {-1} {u} - \frac b {a^2} \frac {-1} {2 u^2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {a^2 \left({a x + b}\right)} + \frac b {2 a^2 \left({a x + b}\right)^2} + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
Let $x \in \R: \size x > 1$. Let $x > 1$. Then: :$-\map \ln {x + \sqrt {x^2 - a^2} } = \map \ln {x - \sqrt {x^2 - a^2} } - \map \ln {a^2}$	0
Let $\struct {S, \circ}$ be a [[Definition:Semigroup|semigroup]]. For $a \in S$, let $\circ^n a = a^n$ be defined as the [[Definition:Power of Element of Semigroup|$n$th power of $a$]]: :$a^n = \begin{cases} a & : n = 1 \\ a^x \circ a & : n = x + 1 \end{cases}$ That is: : $a^n = \underbrace {a \circ a \circ \cdots \circ a}_{n \text{ copies of } a} = \circ^n \paren a$ Then: :$\forall m, n \in \N_{>0}: a^{n + m} = a^n \circ a^m$	0
Let $f$ be a function [[Definition:Meromorphic Function|meromorphic]] in the [[Definition:Interior of Region|interior]] of some [[Definition:Simply Connected|simply connected]] [[Definition:Region (Complex Analysis)|region]] $D$. Let $f$ be [[Definition:Holomorphic Function|holomorphic]] with no [[Definition:Root of Mapping|zeroes]] on the [[Definition:Boundary of Region|boundary]] of $D$. Let $N$ denote the number of zeroes of $f$ in the interior of $D$, counted up to [[Definition:Multiplicity (Complex Analysis)|multiplicity]]. Let $P$ denote the number of [[Definition:Isolated Singularity/Pole|poles]] of $f$ in the interior of $D$, counted up to order. Then: :$\displaystyle N - P = \frac 1 {2\pi i} \oint_D \frac { f'\left({z}\right) } { f\left({z}\right) } \, \mathrm d z$	0
:[[File:Euclid-X-93.png|400px]] Let the [[Definition:Area|area]] $AB$ be [[Definition:Containment of Rectangle|contained]] by the [[Definition:Rational Line Segment|rational straight line]] $AC$ and the [[Definition:Sixth Apotome|sixth apotome]] $AD$. It is to be proved that the [[Definition:Square Root|"side"]] of $AB$ is a [[Definition:That which produces Medial Whole with Medial Area|straight line which produces with a medial area a medial whole]]. Let $DG$ be the [[Definition:Annex of Apotome|annex]] of the [[Definition:Sixth Apotome|sixth apotome]] $AD$. Then, by definition: :$AG$ and $GD$ are [[Definition:Rational Line Segment|rational straight lines]] which are [[Definition:Commensurable in Square Only|commensurable in square only]] :neither the [[Definition:Whole of Apotome|whole]] $AG$ nor the [[Definition:Annex of Apotome|annex]] $GD$ is [[Definition:Commensurable in Length|commensurable]] with the [[Definition:Rational Line Segment|rational straight line]] $AC$ :the [[Definition:Square (Geometry)|square]] on the [[Definition:Whole of Apotome|whole]] $AG$ is greater than the [[Definition:Square (Geometry)|square]] on the [[Definition:Annex of Apotome|annex]] $GD$ by the [[Definition:Square (Geometry)|square]] on a [[Definition:Line Segment|straight line]] which is [[Definition:Incommensurable in Length|incommensurable in length]] with $AG$. Let there be applied to $AG$ a [[Definition:Parallelogram|parallelogram]] equal to the fourth part of the [[Definition:Square (Geometry)|square]] on $GD$ and deficient by a [[Definition:Square (Geometry)|square]] figure. From {{EuclidPropLink|book = X|prop = 18|title = Condition for Incommensurability of Roots of Quadratic Equation}}: :that [[Definition:Parallelogram|parallelogram]] divides $AG$ into [[Definition:Incommensurable|incommensurable]] parts. Let $DG$ be [[Definition:Bisection|bisected]] at $E$. Let the [[Definition:Containment of Rectangle|rectangle contained]] by $AF$ and $FG$ be applied to $AG$ which is equal to the [[Definition:Square (Geometry)|square]] on $EG$ and deficient by a [[Definition:Square (Geometry)|square]] figure. Therefore $AF$ is [[Definition:Incommensurable in Length|incommensurable]] with $FG$. We have that $AF$ is [[Definition:Incommensurable in Length|incommensurable in length]] with $FG$. But from {{EuclidPropLink|book = VI|prop = 1|title = Areas of Triangles and Parallelograms Proportional to Base}}: :$AF : FG = AI : FK$ Therefore from {{EuclidPropLink|book = X|prop = 11|title = Commensurability of Elements of Proportional Magnitudes}}: :$AI$ is [[Definition:Incommensurable|incommensurable]] with $FK$. We have that $AG$ [[Definition:Commensurable in Length|commensurable in length]] with $AC$, and both are [[Definition:Rational Line Segment|rational]]. Therefore from {{EuclidPropLink|book = X|prop = 21|title = Medial is Irrational}}: :the [[Definition:Rectangle|rectangle]] $AK$ is [[Definition:Medial Area|medial]]. We have that $DG$ is [[Definition:Incommensurable in Length|incommensurable in length]] with $AC$, while both are [[Definition:Rational Line Segment|rational]]. Therefore from {{EuclidPropLink|book = X|prop = 21|title = Medial is Irrational}}: :the [[Definition:Rectangle|rectangles]] $DK$ is [[Definition:Medial Area|medial]]. But $AG$ and $GD$ are [[Definition:Rational Line Segment|rational straight lines]] which are [[Definition:Commensurable in Square Only|commensurable in square only]]. Therefore $AG$ is [[Definition:Incommensurable in Length|incommensurable in length]] with $GD$. But: :$AG : GD = AK : KD$ Therefore from: :{{EuclidPropLink|book = VI|prop = 1|title = Areas of Triangles and Parallelograms Proportional to Base}} and: :{{EuclidPropLink|book = X|prop = 11|title = Commensurability of Elements of Proportional Magnitudes}}: :$AK$ is [[Definition:Incommensurable|incommensurable]] with $KD$. Let the [[Definition:Square (Geometry)|square]] $LM$ be constructed equal to $AI$. Let the [[Definition:Square (Geometry)|square]] $NO$ be subtracted from $LM$ having the common angle $\angle LPM$ equal to $FK$. Therefore from {{EuclidPropLink|book = VI|prop = 26|title = Parallelogram Similar and in Same Angle has Same Diameter}}: :the [[Definition:Square (Geometry)|squares]] $LM$ and $NO$ are about the same [[Definition:Diameter of Parallelogram|diameter]]. Let $PR$ be the [[Definition:Diameter of Parallelogram|diameter]] of $LM$ and $NO$. It is to be shown that $LN$ is the [[Definition:Square Root|"side"]] of the [[Definition:Area|area]] $AB$. We have that $AK$ is [[Definition:Medial Area|medial]] and equals $LP^2 + PN^2$. Therefore $LP^2 + PN^2$ is [[Definition:Medial Area|medial]]. We have that the [[Definition:Rectangle|rectangle]] $DK$ is [[Definition:Medial Area|medial]]. But $DK$ equals twice the [[Definition:Containment of Rectangle|rectangle contained]] by $LP$ and $PN$. Therefore twice the [[Definition:Containment of Rectangle|rectangle contained]] by $LP$ and $PN$ is [[Definition:Medial Area|medial]]. We have that $AK$ is [[Definition:Incommensurable|incommensurable]] with $DK$. Therefore $LP^2 + PN^2$ is [[Definition:Incommensurable|incommensurable]] with $2 \cdot LP \cdot PN$. We have that $AI$ is [[Definition:Incommensurable|incommensurable]] with $FK$. Therefore $LP^2$ is [[Definition:Incommensurable|incommensurable]] with $PN^2$. Therefore $LP$ and $PN$ are [[Definition:Line Segment|straight lines]] which are [[Definition:Incommensurable in Square|incommensurable in square]] such that $LP^2 + PN^2$ is [[Definition:Medial Area|medial]] and such that $2 \cdot LP \cdot PN$ is also [[Definition:Medial Area|medial]]. Further, $LP^2 + PN^2$ is [[Definition:Incommensurable|incommensurable]] with $2 \cdot LP \cdot PN$. Therefore by definition $LN$ is a [[Definition:That which produces Medial Whole with Medial Area|straight line which produces with a medial area a medial whole]]. But $LN$ is the [[Definition:Square Root|"side"]] of the [[Definition:Area|area]] $AB$. Hence the result. {{qed}} {{Euclid Note|96|X|{{EuclidNoteConverse|prop=102|title=Square on Straight Line which produces Medial Whole with Medial Area applied to Rational Straight Line}}}}	0
Let $P = \tuple {a, b, c}$ be a [[Definition:Geometric Sequence of Integers|geometric sequence]]. Then by definition their [[Definition:Common Ratio|common ratio]] is: :$\dfrac b a = \dfrac c b$ From [[Two Coprime Integers have no Third Integer Proportional]] it cannot be the case that $a$ and $b$ are [[Definition:Coprime Integers|coprime]]. Thus condition $(1)$ is satisfied. From [[Form of Geometric Sequence of Integers]], $P$ is in the form: :$\tuple {k p^2, k p q, k q^2}$ from which it can be seen that: :$k p^2 \divides k^2 p^2 q^2$ demonstrating that condition $(2)$ is satisfied. {{qed}} {{Euclid Note|18|IX}}	0
:$\sin x - \cos x = \sqrt 2 \, \map \cos {x - \dfrac {3 \pi} 4}$	0
The [[Definition:Real Secant Function|(real) secant function]] has a [[Definition:Taylor Series|Taylor series expansion]]: {{begin-eqn}} {{eqn | l = \sec x | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {E_{2 n} x^{2 n} } {\paren {2 n}!} }} {{eqn | r = 1 + \frac {x^2} 2 + \frac {5 x^4} {24} + \frac {61 x^6} {720} + \cdots }} {{end-eqn}} where $E_{2 n}$ denotes the [[Definition:Euler Numbers|Euler numbers]]. This [[Definition:Convergent Series|converges]] for $\size x < \dfrac \pi 2$.	0
{{begin-eqn}} {{eqn | l = \paren {\sqrt {\dfrac {a + \sqrt {a^2 - b} } 2} + \sqrt {\dfrac {a - \sqrt {a^2 - b} } 2} }^2 | r = \dfrac {a + \sqrt {a^2 - b} } 2 + \dfrac {a - \sqrt {a^2 - b} } 2 + 2 \sqrt {\dfrac {a + \sqrt {a^2 - b} } 2} \sqrt {\dfrac {a - \sqrt {a^2 - b} } 2} | c = multiplying out }} {{eqn | r = a + \sqrt {a + \sqrt {a^2 - b} } \sqrt {a - \sqrt {a^2 - b} } | c = simplifying }} {{eqn | r = a + \sqrt {a^2 - \paren {a^2 - b} } | c = [[Difference of Two Squares]] }} {{eqn | r = a + \sqrt b | c = simplifying }} {{eqn | ll= \leadsto | l = \sqrt {\dfrac {a + \sqrt {a^2 - b} } 2} + \sqrt {\dfrac {a - \sqrt {a^2 - b} } 2} | r = \sqrt {a + \sqrt b} | c = taking [[Definition:Square Root|square root]] of both sides }} {{end-eqn}} {{finish|Report on the matter of the signs and magnitudes of $a$ and $b$ according to the constraints given}} {{qed}}	0
Let $\mathbf{OrdSet}$ be the [[Definition:Category of Ordered Sets|category of ordered sets]]. Then $\mathbf{OrdSet}$ is [[Definition:Cartesian Closed Category|Cartesian closed]].	0
{{begin-eqn}} {{eqn | lo= \forall x \ne 0: | l = \frac x x | r = x \times \frac 1 x | c = {{Defof|Real Division}} }} {{eqn | r = 1 | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R M 4$: Inverses]] }} {{end-eqn}} {{qed}}	0
From the [[Definition:Numerators and Denominators of Continued Fraction|recursive definition of continued fractions]], we have: {{begin-eqn}} {{eqn | l = p_i | r = a_i p_{i - 1} + p_{i - 2} | c = }} {{eqn | l = q_i | r = a_i q_{i - 1} + q_{i - 2} | c = }} {{end-eqn}} Let: {{begin-eqn}} {{eqn | l = \sqbrk {a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8, a_9, \ldots} | r = \sqbrk {1, 0, 1, 1, 2, 1, 1, 4, 1, 1, \ldots} }} {{end-eqn}} In other words: :$a_{3 n + 1} = 2 n$ and: :$a_{3 n + 0} = a_{3 n + 2} = 1$ Then $p_i$ and $q_i$ are as follows: :$\begin{array}{r|cccccccccc} \displaystyle i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline p_i & 1 & 1 & 2 & 3 & 8 & 11 & 19 & 87 & 106 & 193 \\ q_i & 1 & 0 & 1 & 1 & 3 & 4 & 7 & 32 & 39 & 71 \\ \hline \end{array}$ Furthermore, $p_i$ and $q_i$ satisfy the following $6$ [[Definition:Recursive Sequence|recurrence relations]]: {{begin-eqn}} {{eqn | n = 1 | l = p_{3 n + 0} | r = \paren {a_{3 n + 0} } p_{3 n - 1} + p_{3 n - 2} | rr= = p_{3 n - 1} + p_{3 n - 2} }} {{eqn | n = 2 | l = p_{3 n + 1} | r = \paren {a_{3 n + 1} } p_{3 n + 0} + p_{3 n - 1} | rr= = 2 n p_{3 n + 0} + p_{3 n - 1} }} {{eqn | n = 3 | l = p_{3 n + 2} | r = \paren {a_{3 n + 2} } p_{3 n + 1} + p_{3 n + 0} | rr= = p_{3 n + 1} + p_{3 n + 0} }} {{eqn | n = 4 | l = q_{3 n + 0} | r = \paren {a_{3 n + 0} } q_{3 n - 1} + q_{3 n - 2} | rr= = q_{3 n - 1} + q_{3 n - 2}, }} {{eqn | n = 5 | l = q_{3 n + 1} | r = \paren {a_{3 n + 1} } q_{3 n + 0} + q_{3 n - 1} | rr= = 2 n q_{3 n + 0} + q_{3 n - 1}, }} {{eqn | n = 6 | l = q_{3 n + 2} | r = \paren {a_{3 n + 2} } q_{3 n + 1} + q_{3 n + 0} | rr= = q_{3 n + 1} + q_{3 n + 0}, }} {{end-eqn}} Our ultimate aim is to prove that: :$\displaystyle \lim_{n \mathop \to \infty} \frac {p_n} {q_n} = e$ In the pursuit of that aim, let us define the [[Definition:Definite Integral|integrals]]: {{begin-eqn}} {{eqn | l = A_n | r = \int_0^1 \frac {x^n \paren {x - 1}^n} {n!} e^x \rd x }} {{eqn | l = B_n | r = \int_0^1 \frac {x^{n + 1} \paren {x - 1}^n} {n!} e^x \rd x }} {{eqn | l = C_n | r = \int_0^1 \frac {x^n \paren {x - 1}^{n + 1} } {n!} e^x \rd x }} {{end-eqn}} === [[Continued Fraction Expansion of Euler's Number/Proof 1/Lemma|Lemma]] === {{:Continued Fraction Expansion of Euler's Number/Proof 1/Lemma}}{{qed|lemma}} We assert that $A_n$, $B_n$ and $C_n$ all converge to $0$ as $n \to \infty$: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} A_n | r = \frac {\frac {x^{n + 1} \paren {x - 1}^{n + 1} } {\paren {n + 1}!} e^x} {\frac {x^n \paren {x - 1}^n } {n!} e^x} | c = [[Radius of Convergence from Limit of Sequence/Real Case|Radius of Convergence from Limit of Sequence: Real Case]] }} {{eqn | r = \frac {x \paren {x - 1} } {\paren {n + 1} } }} {{eqn | r = 0 }} {{eqn | l = \lim_{n \mathop \to \infty} B_n | r = \frac {\frac {x^{n + 2} \paren {x - 1}^{n + 1} } {\paren {n + 1}!} e^x} {\frac {x^{n + 1} \paren {x - 1}^n} {n!} e^x} | c = [[Radius of Convergence from Limit of Sequence/Real Case]] }} {{eqn | r = \frac {x \paren {x - 1} } {\paren {n + 1} } }} {{eqn | r = 0 }} {{eqn | l = \lim_{n \mathop \to \infty} C_n | r = \lim_{n \mathop \to \infty} B_n - \lim_{n \mathop \to \infty} A_n }} {{eqn | r = 0 }} {{end-eqn}} We now have: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} A_n | r = \lim_{n \mathop \to \infty} \paren {q_{3 n} e - p_{3 n} } | rr = = 0 }} {{eqn | l = \lim_{n \mathop \to \infty} B_n | r = \lim_{n \mathop \to \infty} \paren {p_{3 n + 1} - q_{3 n + 1} e} | rr = = 0 }} {{eqn | l = \lim_{n \mathop \to \infty} C_n | r = \lim_{n \mathop \to \infty} \paren {p_{3 n + 2} - q_{3 n + 2} e} | rr = = 0 }} {{end-eqn}} from which we conclude: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} \paren {p_n - q_n e} | r = 0 }} {{eqn | l = \lim_{n \mathop \to \infty} p_n | r = q_n e }} {{eqn | l = \lim_{n \mathop \to \infty} \frac {p_n} {q_n} | r = e }} {{end-eqn}} {{qed}}	0
Let $n \in \N_{> 0}$ be a [[Definition:Natural Number|natural number]] greater than $0$. Let $S$ be a [[Definition:Set|set]] of [[Definition:Cardinality|cardinality]] $n$. Let $S_n$ denote the [[Definition:Symmetric Group|symmetric group]] on $S$ of [[Definition:Order of Structure|order $n$]]. Let $R_e$ and $R_o$ denote the [[Definition:Subset|subsets]] of $S_n$ consisting of [[Definition:Even Permutation|even permutations]] and [[Definition:Odd Permutation|odd permutations]] respectively. Then the [[Definition:Cardinality|cardinality]] of both $R_e$ and $R_o$ is $\dfrac {n!} 2$.	0
Let $\N$ denote the [[Definition:Natural Numbers in Real Numbers|natural numbers]] as [[Definition:Subset|subset]] of the [[Definition:Real Number|real numbers]] $\R$. Then $\N$ is an [[Definition:Inductive Set as Subset of Real Numbers|inductive set]].	0
Using [[Integration by Parts]]: {{begin-eqn}} {{eqn | l = \map \Li x | r = \int_2^x \dfrac {\d t} {\ln t} | c = }} {{eqn | n = 1 | r = \dfrac x {\ln x} - \dfrac 2 {\ln 2} + \int_2^x \dfrac {\d t} {\paren {\ln t}^2} | c = }} {{end-eqn}} We have that $\dfrac 1 {\paren {\ln t}^2}$ is [[Definition:Positive Real Function|positive]] and [[Definition:Decreasing Real Function|decreasing]] for $t > 1$. Let $x \ge 4$. Then: {{begin-eqn}} {{eqn | l = 0 | o = < | r = \int_2^x \dfrac {\d t} {\paren {\ln t}^2} | c = }} {{eqn | r = \int_2^{\sqrt x} \dfrac {\d t} {\paren {\ln t}^2} + \int_{\sqrt x}^x \dfrac {\d t} {\paren {\ln t}^2} | c = }} {{eqn | o = < | r = \dfrac {\sqrt x - 2} {\paren {\ln 2}^2} + \dfrac {x - \sqrt x} {\paren {\ln \sqrt x}^2} | c = }} {{eqn | o = < | r = \dfrac {\sqrt x} {\paren {\ln 2}^2} + \dfrac {4 x} {\paren {\ln \sqrt x}^2} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \dfrac {\displaystyle \int_2^x \frac {\d t} {\paren {\ln t}^2} } {x / \ln x} | c = }} {{eqn | o = < | r = \dfrac {\ln x} {\sqrt x \paren {\ln 2}^2} + \frac 4 {\ln x} | c = }} {{eqn | n = 2 | ll= \leadsto | l = \lim_{x \mathop \to \infty} \dfrac {\displaystyle \int_2^x \frac {\d t} {\paren {\ln t}^2} } {x / \ln x} | r = 0 | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \frac {\map \Li x} {x / \ln x} | r = \dfrac {\dfrac x {\ln x} - \dfrac 2 {\ln 2} + \displaystyle \int_2^x \dfrac {\d t} {\paren {\ln t}^2} } {x / \ln x} | c = from $(1)$ }} {{eqn | r = 1 - \dfrac {2 \ln x} {x \ln 2} + \dfrac {\displaystyle \int_2^x \frac {\d t} {\paren {\ln t}^2} } {x / \ln x} | c = }} {{eqn | ll= \leadsto | l = \lim_{x \mathop \to \infty} \frac {\map \Li x} {x / \ln x} | r = 1 - \lim_{x \mathop \to \infty} \dfrac 2 {\ln 2} \dfrac x {\ln x} - 0 | c = from $(2)$ }} {{eqn | n = 3 | ll= \leadsto | l = \lim_{x \mathop \to \infty} \frac {\map \Li x} {x / \ln x} | r = 1 | c = as $\displaystyle \lim_{x \mathop \to \infty} \dfrac x {\ln x} = 0$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \lim_{x \mathop \to \infty} \frac {\map \pi x} {x / \ln x} | r = \lim_{x \mathop \to \infty} \frac {\map \pi x} {\map \Li x} \frac {\map \Li x} {x / \ln x} | c = }} {{eqn | r = \lim_{x \mathop \to \infty} \frac {\map \pi x} {\map \Li x} | c = from $(3)$ }} {{end-eqn}} {{qed}}	0
It is to be demonstrated that $\struct {S, \times}$ does not satisfy the [[Definition:Group Axioms|group axioms]]. First it is noted that [[Integer Multiplication is Closed]]. Then from [[Odd Number multiplied by Odd Number is Odd]], $S$ is [[Definition:Closed Algebraic Structure|closed]] under $\times$. Thus $\struct {S, \times}$ fulfils {{GroupAxiom|0}}. From [[Integer Multiplication is Associative]], we have that $\times$ is [[Definition:Associative Operation|associative]] on $S$. Thus $\struct {S, \times}$ fulfils {{GroupAxiom|1}}. Then we have that: :$\forall x \in S: 1 \times x = x = x \times 1$ and as $1 \in S$ it follows that $1$ is the [[Definition:Identity Element|identity element]] of $\struct {S, \times}$ Thus $\struct {S, \times}$ fulfils {{GroupAxiom|2}}. Now consider $3 \in S$. There exists no $x \in S$ such that $3 \times x = 1$. Thus $x$ has no [[Definition:Inverse Element|inverse element]] in $S$. Thus $\struct {S, \times}$ does not fulfil {{GroupAxiom|3}}. Thus it has been demonstrated that $\struct {S, \times}$ does not satisfy the [[Definition:Group Axioms|group axioms]]. Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \phi 1 \map \tau 1 | r = 1 \times 1 | c = {{EulerPhiLink|1}}, {{TauLink|1}} }} {{eqn | r = 1 | c = }} {{eqn | r = \map \sigma 1 | c = {{SigmaLink|1}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map \phi 3 \map \tau 3 | r = 2 \times 2 | c = {{EulerPhiLink|3}}, {{TauLink|3}} }} {{eqn | r = 4 | c = }} {{eqn | r = \map \sigma 3 | c = {{SigmaLink|3}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map \phi {14} \map \tau {14} | r = 6 \times 4 | c = {{EulerPhiLink|14}}, {{TauLink|14}} }} {{eqn | r = 24 | c = }} {{eqn | r = \map \sigma {14} | c = {{SigmaLink|14}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map \phi {42} \map \tau {42} | r = 12 \times 8 | c = {{EulerPhiLink|42}}, {{TauLink|42}} }} {{eqn | r = 96 | c = }} {{eqn | r = \map \sigma {42} | c = {{SigmaLink|42}} }} {{end-eqn}} To show that there are no more such [[Definition:Positive Integer|integers]], we use the formulas: {{begin-eqn}} {{eqn | l = n | m = \prod_{p_i \mathop \divides n} {p_i}^{k_i} | ro = = | r = \prod_{j \mathop = 1}^r {p_i}^{k_i} | c = [[Definition:Prime Decomposition|Prime Decomposition]] }} {{eqn | l = \map \phi n | m = n \prod_{p \mathop \divides n} \paren {1 - \frac 1 p} | ro = = | r = \prod_{i \mathop = 1}^r p_i^{k_i} \paren {1 - \frac 1 p_i} | c = [[Euler Phi Function of Integer]] }} {{eqn | l = \map \tau n | m = \prod_{i \mathop = 1}^r \paren {k_i + 1} | c = [[Tau Function from Prime Decomposition]] }} {{eqn | l = \map \sigma n | m = \prod_{i \mathop = 1}^r \frac {p_i^{k_i + 1} - 1} {p_i - 1} | c = [[Sigma Function of Integer]] }} {{end-eqn}} Which arises from [[Euler Phi Function is Multiplicative]], [[Tau Function is Multiplicative]] and [[Sigma Function is Multiplicative]]. We define the function $f = \phi \cdot \tau$ as follows: :$\map f {p^k} = \map \phi {p^k} \map \tau {p^k} = p^k \paren {1 - \dfrac 1 p} \paren {k + 1}$ where $p$ is a [[Definition:Prime Number|prime]], and $k$ is a [[Definition:Non-Negative Integer|non-negative integer]]. Note that $f$ and $\dfrac f \sigma$ are [[Definition:Multiplicative Arithmetic Function|multiplicative functions]], by [[Product of Multiplicative Functions is Multiplicative]]. Then $\map \phi n \map \tau n = \map \sigma n$ is equivalent to: :$\displaystyle \prod_{i \mathop = 1}^r \dfrac {\map f {p_i^{k_i} } } {\map \sigma {p_i^{k_i} } } = 1$ We consider the cases $k \ge 1$. Then: {{begin-eqn}} {{eqn | l = \map f {p^k} | o = \ge | r = \map \sigma {p^k} }} {{eqn | ll = \leadsto | l = p^k \paren {1 - \frac 1 p} \paren {k + 1} | o = \ge | r = \frac {p^{k + 1} - 1} {p - 1} }} {{eqn | ll = \leadsto | l = p^{k - 1} \paren {p - 1}^2 \paren {k + 1} | o = \ge | r = p^{k + 1} - 1 }} {{eqn | ll = \leadsto | l = k p^{k + 1} + \paren {k + 1} p^{k - 1} + 1 | o = \ge | r = 2 \paren {k + 1} p^k }} {{end-eqn}} Suppose [[Definition:Equality|equality]] holds. If $k \ge 2$, {{begin-eqn}} {{eqn | l = k p^{k + 1} + \paren {k + 1} p^{k - 1} + 1 | o = \ge | r = 2 \paren {k + 1} p^k }} {{eqn | ll = \leadsto | l = 1 | o = \equiv | r = 0 | rr = \pmod p | c = since $p^{k - 1}, p^k, p^{k + 1}$ are multiples of $p$ }} {{end-eqn}} Which is a [[Definition:Contradiction|contradiction]]. This forces $k = 1$. The equation becomes {{begin-eqn}} {{eqn | l = p^2 + 2 + 1 | r = 4 p }} {{eqn | ll = \leadsto | l = p^2 - 4 p + 3 | r = 0 }} {{eqn | ll = \leadsto | l = \paren {p - 1} \paren {p - 3} | r = 0 }} {{end-eqn}} Since $p \ne 1$, we must have $p = 3$. Indeed, $\map f 3 = \map \phi 3 \map \tau 3 = \map \sigma 3$. For the inequality case: $k p^{k + 1} + \paren {k + 1} p^{k - 1} + 1 > 2 \paren {k + 1} p^k$, we check each prime $p$ individually. For $p = 2$: {{begin-eqn}} {{eqn | l = k \cdot 2^{k + 1} + \paren {k + 1} 2^{k - 1} + 1 | o = > | r = 2 \paren {k + 1} 2^k }} {{eqn | ll = \leadsto | l = 4 k \paren {2^{k - 1} } + \paren {k + 1} \paren {2^{k - 1} } | o = \ge | r = 4 \paren {k + 1} \paren {2^{k - 1} } | c = Since both sides are [[Definition:Integer|integers]] }} {{eqn | ll = \leadsto | l = 4 k + \paren {k + 1} | o = \ge | r = 4 \paren {k + 1} }} {{eqn | ll = \leadsto | l = k | o = \ge | r = 3 }} {{end-eqn}} For $p \ge 3$, we consider the criterion $k p \ge 2 \paren {k + 1}$. This criterion implies $k p^{k + 1} \ge 2 \paren {k + 1} p^k$, which in turn implies our inequality $k p^{k + 1} + \paren {k + 1} + 1 > 2 \paren {k + 1} p^k$. For $p = 3$ and $k \ge 2$, $3 k = 2 k + k \ge 2 k + 2$. For $p \ge 5$, $k p \ge 5 k > 2 k + 2 k \ge 2 k + 2$. Therefore $\map f {p^k} > \map \sigma {p^k}$, except for $\map f 3 = \map \sigma 3$, $\map f 2 < \map \sigma 2$ and $\map f 4 < \map \sigma 4$. Now we try to find [[Definition:Integer|integers]] $n$ satisfying $\displaystyle \prod_{i \mathop = 1}^r \dfrac {\map f {p_i^{k_i} } } {\map \sigma {p_i^{k_i} } } = 1$. First observe that the [[Definition:Sequence|sequence]] $\sequence {\dfrac {\map f {p^k} } {\map \sigma {p^k} } }_{k \mathop = 1}^\infty$ is [[Definition:Strictly Increasing/Sequence|strictly increasing]]: {{begin-eqn}} {{eqn | l = \frac {\map f {p^{k + 1} } } {\map \sigma {p^{k + 1} } } - \frac {\map f {p^k} } {\map \sigma {p^k} } | r = \frac {p^k \paren {1 - 1 / p} \paren {k + 2} } {\paren {p^{k + 2} - 1} / \paren {p - 1} } - \frac {p^{k - 1} \paren {1 - 1 / p} \paren {k + 1} } {\paren {p^{k + 1} - 1} / \paren {p - 1} } }} {{eqn | r = \frac {p^{k - 1} \paren {p - 1}^2 \paren {k + 2} } {p^{k + 2} - 1} - \frac {p^{k - 2} \paren {p - 1}^2 \paren {k + 1} } {p^{k + 1} - 1} }} {{eqn | r = \frac {p^{k - 2} \paren {p - 1}^2} {\paren {p^{k + 2} - 1} \paren {p^{k + 1} - 1} } \paren {p \paren {p^{k + 1} - 1} \paren {k + 2} - \paren {p^{k + 2} - 1} \paren {k + 1} } }} {{eqn | r = \frac {p^{k - 2} \paren {p - 1}^2} {\paren {p^{k + 2} - 1} \paren {p^{k + 1} - 1} } \paren {p^{k + 2} - \paren {k + 2} p + k + 1} }} {{eqn | r = \frac {p^{k - 2} \paren {p - 1}^2} {\paren {p^{k + 2} - 1} \paren {p^{k + 1} - 1} } \paren {1 + \paren {k + 2} \paren {p - 1} + \sum_{m \mathop = 2}^{k + 2} \dbinom {k + 2} m \paren {p - 1}^m - \paren {k + 2} p + k + 1} | c = [[Binomial Theorem]] }} {{eqn | r = \frac {p^{k - 2} \paren {p - 1}^2} {\paren {p^{k + 2} - 1} \paren {p^{k + 1} - 1} } \sum_{m \mathop = 2}^{k + 2} \dbinom {k + 2} m \paren {p - 1}^m }} {{eqn | o = > | r = 0 | c = The sum is non-empty and each term is [[Definition:Strictly Positive/Real Number|strictly positive]] }} {{end-eqn}} Now we consider these particular values for $\dfrac f \sigma$: :$\dfrac {\map f 2 } {\map \sigma 2 } = \dfrac 2 3$ :$\dfrac {\map f 4 } {\map \sigma 4 } = \dfrac 6 7$ :$\dfrac {\map f 3 } {\map \sigma 3 } = 1$ :$\dfrac {\map f 5 } {\map \sigma 5 } = \dfrac 4 3$ :$\dfrac {\map f {25} } {\map \sigma {25} } = \dfrac {60} {31} > \dfrac 3 2$ :$\dfrac {\map f 7 } {\map \sigma 7 } = \dfrac 3 2$ And for $p \ge 11$: {{begin-eqn}} {{eqn | l = \frac {\map f p } {\map \sigma p } | r = \frac {2 \paren {p - 1} } {\paren {p^2 - 1} / \paren {p - 1} } }} {{eqn | r = \frac {2 \paren {p - 1} } {p + 1} }} {{eqn | r = 2 - \frac 4 {p + 1} }} {{eqn | o = \ge | r = 2 - \frac 4 {12} > \frac 3 2 }} {{end-eqn}} By the [[Definition:Strictly Increasing/Sequence|strictly increasing property]] above, every higher [[Definition:Prime Power|power of these primes]] has $\dfrac f \sigma > \dfrac 3 2$. Since $\dfrac f \sigma$ is [[Definition:Multiplicative Arithmetic Function|multiplicative]], only $n = 3, 14, 42$ can give $\dfrac {\map f n} {\map \sigma n} = 1$. Since $\map \phi 1 \map \tau 1 = \map \sigma 1$, the only [[Definition:Positive Integer|positive integers]] whose [[Definition:Euler Phi Function|Euler $\phi$ function]] [[Definition:Integer Multiplication|multiplied by]] its [[Definition:Tau Function|$\tau$ function]] equals its [[Definition:Sigma Function|$\sigma$ function]] are $1, 3, 14, 42$. {{qed}}	0
Let $G$ be a [[Definition:Group|group]]. Let $a, b, x \in G$. Then: : $(1): \quad a x = b \iff x = a^{-1} b$ : $(2): \quad x a = b \iff x = b a^{-1}$	0
:$\displaystyle \int \frac {\d x} {\paren {p x + q} \sqrt {a x + b} } = \begin {cases} \dfrac 1 {\sqrt {b p - a q} \sqrt p} \ln \size {\dfrac {\sqrt {p \paren {a x + b} } - \sqrt {b p - a q} } {\sqrt {p \paren {a x + b} } + \sqrt {b p - a q} } } & : b p - a q > 0 \\ \dfrac 2 {\sqrt {a q - b p} \sqrt p} \arctan \sqrt {\dfrac {p \paren {a x + b} } {a q - b p} } & : b p - a q < 0 \\ \end {cases}$	0
Since $a > b > 0$, we have $a^2 > b^2$. So: {{begin-eqn}} {{eqn | l = \int_0^{\pi/2} \frac 1 {a + b \cos x} \rd x | r = \intlimits {\frac 2 {\sqrt {a^2 - b^2} } \map \arctan {\sqrt {\frac {a - b} {a + b} } \tan \frac x 2} } 0 1 | c = [[Primitive of Reciprocal of p plus q by Cosine of a x|Primitive of $\dfrac 1 {p + q \cos x}$]] }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \paren {2 \map \arctan {\sqrt {\frac {a - b} {a + b} } } } }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \paren {2 \map \arctan {\sqrt {\frac {1 - \frac b a} {1 + \frac b a} } } } }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \map \arccos {\frac b a} | c = [[Arccosine in terms of Arctangent]] }} {{end-eqn}} {{qed}}	0
Let $n, m \in \Z$ be [[Definition:Integer|integers]]. Let $r^n$ be defined as [[Definition:Integer Power|$r$ to the power of $n$]]. Then: :$r^{n + m} = r^n \times r^m$	0
=== [[Definition:Absolute Value/Definition 1|Definition 1]] === {{:Definition:Absolute Value/Definition 1}} === [[Definition:Absolute Value/Definition 2|Definition 2]] === {{:Definition:Absolute Value/Definition 2}}	0
By hypothesis, there exist only [[Definition:Finite Set|finitely many]] [[Definition:Prime Power|prime powers]] $p^k$ such that $\size {\map f {p^k} } > 1$. Let $\displaystyle A = \prod_{\size {\map f {p^k} } \mathop > 1} \size {\map f {p^k} }$. Thus $A \ge 1$. Let $0 < \dfrac \epsilon A$. There exist only [[Definition:Finite Set|finitely many]] [[Definition:Prime Power|prime powers]] $p^k$ such that $\size {\map f {p^k} } > \dfrac \epsilon A$. Therefore there are only [[Definition:Finite Set|finitely many]] [[Definition:Integer|integers]] $n$ such that: :$\size {\map f {p^k} } > \dfrac \epsilon A$ for every [[Definition:Prime Power|prime power]] $p^k$ that [[Definition:Divisor of Integer|divides]] $n$. Therefore if $n$ is [[Definition:Sufficiently Large|sufficiently large]] there exists a [[Definition:Prime Power|prime power]] $p^k$ that [[Definition:Divisor of Integer|divides]] $n$ such that: :$\size {\map f {p^k} } < \dfrac \epsilon A$ Therefore $n$ can be written as: :$\displaystyle n = \prod_{i \mathop = 1}^ r p_i^{k_i} \prod_{i \mathop = r + 1}^{r + s} p_i^{k_i} \prod_{i \mathop = r + s + 1}^{r + s + t} p_i^{k_i}$ where $t \ge 1$ and: {{begin-eqn}} {{eqn | l = 1 | o = \le | m = \size {\map f {p_i^{k_i} } } | mo= | r = | c = for $i = 1, \ldots, r$ }} {{eqn | l = \frac \epsilon A | o = \le | m = \size {\map f {p_i^{k_i} } } | mo= < | r = 1 | c = for $i = r + 1, \ldots, r + s$ }} {{eqn | o = | m = \size {\map f {p_i^{k_i} } } | mo= < | r = \frac \epsilon A | c = for $i = r + s + 1, \ldots, r + s + t$ }} {{end-eqn}} Therefore: {{begin-eqn}} {{eqn | l = \size {\map f n} | r = \prod_{i \mathop = 1}^r \map f {p_i^{k_i} } \prod_{i \mathop = r + 1}^{r + s} \map f {p_i^{k_i} } \prod_{i \mathop = r + s + 1}^{r + s + t} \map f {p_i^{k_i} } | c = because $f$ is [[Definition:Multiplicative Arithmetic Function|multiplicative]] }} {{eqn | o = < | r = A \paren {\frac \epsilon A}^t | c = }} {{eqn | o = < | r = \epsilon | c = because $t \ge 1$ }} {{end-eqn}} This shows that $\map f n$ can be made arbitrarily small for [[Definition:Sufficiently Large|sufficiently large]] $n$. {{qed}} [[Category:Analytic Number Theory]] [[Category:Multiplicative Functions]] sl3n43xpxm69xs440y1uacxtzcykkja	0
Let: : $\left({R, +, \circ}\right)$ be a [[Definition:Commutative Ring|commutative ring]] : $\left({D, +, \circ}\right)$ be an [[Definition:Subdomain|integral subdomain]] of $R$ whose [[Definition:Ring Zero|zero]] is $0_D$ and whose [[Definition:Unity of Ring|unity]] is $1_D$ : $X \in R$ be [[Definition:Transcendental over Integral Domain|transcendental]] over $D$. It has been demonstrated that the [[Definition:Boubaker Polynomials|Boubaker Polynomials]] sub-sequence $B_{4 n} \left({x}\right)$, defined in $D \left[{X}\right]$ as: :$\displaystyle B_{4 n} \left({x}\right) = 4 \sum_{p \mathop = 0}^{2 n} \frac {n - p} {4 n - p} \binom {4 n - p} p \left({-1}\right)^p x^{2 \left({2n - p}\right)}$ satisfies the properties: {{begin-eqn}} {{eqn | n = 1 | l = \sum_{k \mathop = 1}^N {p_n \left({0}\right)} | r = -2N }} {{eqn | n = 2 | l = \sum_{k \mathop = 1}^N {p_n \left({\alpha_k}\right)} | r = 0 }} {{eqn | n = 3 | l = \left.{\sum_{k \mathop = 1}^N \frac {\mathrm d p_x \left({x}\right)} {\mathrm d x} }\right\vert_{x \mathop = 0} | r = 0 }} {{eqn | n = 4 | l = \left.{\sum_{k \mathop = 1}^N \frac {\mathrm d {p_n}^2 \left({x}\right)} {\mathrm d x^2} }\right\vert_{x \mathop = 0} | r = \frac 8 3 N \left({N^2 - 1}\right) }} {{end-eqn}} with $\left. {\alpha_k}\right\vert_{k \mathop = 1 \,.\,.\, N}$ roots of $B_{4 n}$. Suppose there exists another $4 n$-indexed polynomial $q_{4 n} \left({x}\right)$, with $N$ roots $\left.{\beta_k}\right\vert_{k \mathop = 1 \,.\,.\, N}$ in $F$ and which also satisfies simultaneously properties $(1)$ to $(4)$. Let: :$\displaystyle B_{4 n} \left({x}\right) = \sum_{p \mathop = 0}^{2 n} a_{4 n, p} x^{2 \left({2 n - p}\right)}$ and: :$\displaystyle q_{4 n} \left({x}\right) = \sum_{p \mathop = 0}^{2 n} b_{4 n, p} x^{2 \left({2 n - p}\right)}$ and: :$\displaystyle \mathrm d_{4 n, p} = a_{4 n, p} - b_{4 n, p}$ for $p = 0 \,.\,.\, 2 n$ then, simultaneous expressions of conditions $(1)$ and $(3)$ give: :$ \quad \displaystyle \sum_{k \mathop = 1}^N \mathrm d_{4 n, 2 n} = 0$ :$ \quad \displaystyle \sum_{k \mathop = 1}^N \mathrm d_{4 n, 2 n - 2} = 0$ It has also been demonstrated that $ B_{4 n}$ has exactly $4 n - 2$ real roots inside the domain $\left[{-2 \,.\,.\, 2}\right]$. So application of conditions $(3)$ and $(4)$ give $4n-2$ linear equation with variables $\left.{ d_{4n,p}}\right|_{p \mathop = 0 \,.\,.\, 2n-3}$. Finally, since $B_{4 n}$ contains $2 n$ monomial terms (see definition), we obtain a Cramer system in variables $\left.{ d_{4 n, p}}\right\vert_{p \mathop = 0 \,.\,.\, 2 n}$, with evident solution: :$\left.{\mathrm d_{4 n, p}}\right\vert_{p \mathop = 0 \,.\,.\, 2 n} = 0 $ and consequently: :$\left.{a_{4 n, p} }\right\vert_{p \mathop = 0 \,.\,.\, 2 n} = \left.{b_{4 n, p} }\right\vert_{p \mathop = 0 \,.\,.\, 2 n}$ which means: :$q_{4 n} \left({x}\right) = B_{4 n} \left({x}\right)$ {{qed}}	0
These results are an extension of the results in [[Powers of Commutative Elements in Semigroups]] in which the [[Definition:Domain of Mapping|domain]] of the indices is extended to include all [[Definition:Integer|integers]]. Let $\left ({S, \circ}\right)$ be a [[Definition:Monoid|monoid]] whose [[Definition:Identity Element|identity]] is $e_S$. Let $a, b \in S$ be [[Definition:Invertible Element|invertible elements]] for $\circ$ that also [[Definition:Commute|commute]]. Then the following results hold.	0
Let $G_1$ and $G_2$ be [[Definition:Prime Group|prime groups]], both of [[Definition:Finite|finite order]] $p$. From [[Prime Group is Cyclic]], both $G_1$ and $G_2$ are [[Definition:Cyclic Group|cyclic]]. The result follows directly from [[Cyclic Groups of Same Order are Isomorphic]]. {{Qed}}	0
Let $T_S = \struct {S, \tau_S}$ be the [[Definition:Arens-Fort Space|Arens-Fort space]]. Let $T_D = \struct {S, \tau_D}$ be the [[Definition:Discrete Space|discrete space]], also on $S$. Let $I_S: S \to S$ be the [[Definition:Identity Mapping|identity mapping]] on $S$. From [[Mapping from Discrete Space is Continuous]], we have that $I_S$ is a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]. Then we have: :[[Discrete Space is First-Countable]] :[[Arens-Fort Space is not First-Countable]] Thus we have demonstrated a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]] from a [[Definition:First-Countable Space|first-countable space]] to a space which is not [[Definition:First-Countable Space|first-countable space]]. {{qed}}	0
Let $\N_{> 0}$ be the [[Axiom:Axiomatization of 1-Based Natural Numbers|$1$-based natural numbers]]. Let $+$ be [[Definition:Addition on 1-Based Natural Numbers|addition]] on $\N_{>0}$. Then: :$\forall a, b, c \in \N_{>0}: a + c = b + c \implies a = b$ :$\forall a, b, c \in \N_{>0}: a + b = a + c \implies b = c$ That is, $+$ is [[Definition:Cancellable Operation|cancellable]] on $\N_{>0}$.	0
Follows directly from the definition of [[Definition:Binomial Coefficient|binomial coefficient]], as follows. If $k < 0$ then $n - k > n$. Similarly, if $k > n$, then $n - k < 0$. In both cases: :$\dbinom n k = \dbinom n {n - k} = 0$ Let $0 \le k \le n$. {{begin-eqn}} {{eqn | l = \binom n k | r = \frac {n!} {k! \paren {n - k}!} | c = }} {{eqn | r = \frac {n!} {\paren {n - k}! k!} | c = }} {{eqn | r = \frac {n!} {\paren {n - k}! \paren {n - \paren {n - k} } !} | c = }} {{eqn | r = \binom n {n - k} | c = }} {{end-eqn}} {{qed}}	0
Let $n \in \Z_{>0}$ be an [[Definition:Odd Integer|odd]] [[Definition:Positive Integer|positive integer]] such that all smaller [[Definition:Odd Integer|odd integers]] greater than $1$ which are [[Definition:Coprime Integers|coprime]] to it are [[Definition:Prime Number|prime]]. The complete list of such $n$ is as follows: :$1, 3, 5, 7, 9, 15, 21, 45, 105$ {{OEIS|A327823}}	0
Let $n \in \Z$ such that $n \ge 4$. Let $\dbinom n k$ denote a [[Definition:Binomial Coefficient|binomial coefficient]] for $k \in \Z$. Then: :$\dbinom n k = \dbinom {n - 2} {k - 2} + 2 \dbinom {n - 2} {k - 1} + \dbinom {n - 2} k$ for $2 \le k \le n - 2$.	0
From the informal definition of [[Definition:Complex Number/Definition 1|complex numbers]], we define the following: :$z = x_1 + i y_1$ :$w = x_2 + i y_2$ where $i = \sqrt {-1}$ and $x_1, x_2, y_1, y_2 \in \R$. Then from the definition of [[Definition:Complex Addition|complex addition]]: :$z + w = \paren {x_1 + x_2} + i \paren {y_1 + y_2}$ From [[Real Numbers under Addition form Abelian Group]], [[Definition:Real Addition|real addition]] is [[Definition:Closed Operation|closed]]. So: :$\paren {x_1 + x_2} \in \R$ and $\paren {y_1 + y_2} \in \R$ Hence the result. {{qed}}	0
Let $G$ be a [[Definition:Multigraph|multigraph]]. Let the [[Definition:Vertex Set|vertex set]] of $G$ be [[Definition:Finite Set|finite]]. Then it is not necessarily the case that $G$ is also [[Definition:Finite Graph|finite]].	0
For all $n \in \N$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: : $\displaystyle \sum_{i \mathop = 0}^n \binom n i = 2^n$ $P(0)$ is true, as this just says $\dbinom 0 0 = 1$. This holds by [[Definition:Binomial Coefficient|definition]]. === Basis for the Induction === $P(1)$ is true, as this just says $\dbinom 1 0 + \dbinom 1 1 = 2$. This holds by [[Binomial Coefficient with Zero]] and [[Binomial Coefficient with One]] (or [[Binomial Coefficient with Self]]). This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{i \mathop = 0}^k \binom k i = 2^k$ Then we need to show: :$\displaystyle \sum_{i \mathop = 0}^{k + 1} \binom {k + 1} i = 2^{k + 1}$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 0}^{k + 1} \binom {k + 1} i | r = \binom {k + 1} 0 + \sum_{i \mathop = 1}^k \binom {k + 1} i + \binom {k + 1} {k + 1} | c = }} {{eqn | r = \binom {k + 1} 0 + \sum_{i \mathop = 1}^k \left({\binom k {i - 1} + \binom k i}\right) + \binom {k + 1} {k + 1} | c = [[Pascal's Rule]] }} {{eqn | r = \left({\sum_{i \mathop = 0}^{k - 1} \binom k i + \binom {k + 1} {k + 1} }\right) + \left({\binom {k + 1} 0 + \sum_{i \mathop = 1}^k \binom k i}\right) | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \left({\sum_{i \mathop = 0}^{k - 1} \binom k i + \binom k k}\right) + \left({\binom k 0 + \sum_{i \mathop = 1}^k \binom k i}\right) | c = as $\displaystyle \binom {k + 1} {k + 1} = \binom k k = 1$ and $\displaystyle \binom {k + 1} 0 = \binom k 0 = 1$ }} {{eqn | r = \sum_{i \mathop = 0}^k \binom k i + \sum_{i \mathop = 0}^k \binom k i | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = 2^k + 2^k | c = }} {{eqn | r = 2^{k + 1} | c = }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \N: \sum_{i \mathop = 0}^n \binom n i = 2^n$ {{qed}}	0
By definition of [[Definition:Cut-Vertex|cut-vertex]], $G - v$ contains at least $2$ components. That it can contain more components than $2$ is best proved by illustration: :[[File:BigCutVertex.png|500px]] {{qed}}	0
From [[Generating Function for Fibonacci Numbers]], a [[Definition:Generating Function|generating function]] for the [[Definition:Fibonacci Number|Fibonacci numbers]] is: :$\map G z = \dfrac z {1 - z - z^2}$ Hence: {{begin-eqn}} {{eqn | l = \map G z | r = \dfrac z {1 - z - z^2} | c = }} {{eqn | r = \dfrac 1 {\sqrt 5} \paren {\dfrac 1 {1 - \phi z} - \dfrac 1 {1 - \hat \phi z} } | c = [[Definition:Partial Fractions|Partial Fraction Expansion]] }} {{end-eqn}} where: :$\phi = \dfrac {1 + \sqrt 5} 2$ :$\hat \phi = \dfrac {1 - \sqrt 5} 2$ By [[Sum of Infinite Geometric Sequence]]: :$\dfrac 1 {1 - \phi z} = 1 + \phi z + \phi^2 z^2 + \cdots$ and so: :$\map G z = \dfrac 1 {\sqrt 5} \paren {1 + \phi z + \phi^2 z^2 + \cdots - 1 - \hat \phi z - \hat \phi^2 z^2 - \cdots}$ By definition, the [[Definition:Coefficient|coefficient]] of $z^n$ in $\map G z$ is exactly the $n$th [[Definition:Fibonacci Number|Fibonacci number]]. That is: :$F_n = \dfrac {\phi^n - \hat \phi^n} {\sqrt 5}$ {{qed}}	0
The [[Definition:Second Order ODE|second order ODE]]: :$(1): \quad y'' - y = 3 e^{-x}$ has the [[Definition:General Solution to Differential Equation|general solution]]: :$y = C_1 e^x + C_2 e^{-x} - \dfrac {3 x e^{-x} } 2$	0
We have that: :$\omega^+ = \omega \cup \set \omega$ and so: :$\omega \subseteq \omega^+$ {{qed|lemma}} By definition: :$\bigcup \omega^+ = \set {x: \exists y \in \omega^+: x \in y}$ Thus: :$x \in \bigcup \omega^+ \implies x \in \omega$ {{qed|lemma}} So by definition of [[Definition:Set Equality|set equality]]: :$\bigcup \omega^+ = \omega$ {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \cos^{m - 1} a x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = -\paren {m - 1} a \cos^{m - 2} a x \sin a x | c = [[Derivative of Cosine of a x|Derivative of $\cos a x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac {\cos a x} {\sin^n a x} | c = }} {{eqn | r = \sin^{-n} a x \cos a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {\sin^{-n + 1} a x} {a \paren {-n + 1} } | c = [[Primitive of Power of Sine of a x by Cosine of a x|Primitive of $\sin^n a x \cos a x$]] }} {{eqn | r = \frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\cos^m a x} {\sin^n a x} \rd x | r = \int \cos^{m - 1} a x \frac {\cos a x} {\sin^n a x} \rd x | c = }} {{eqn | r = \paren {\cos^{m - 1} a x} \paren {\frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} } | c = [[Integration by Parts]] }} {{eqn | o = | ro= - | r = \int \paren {\frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} } \paren {-\paren {m - 1} a \cos^{m - 2} a x \sin a x } \rd x + C | c = }} {{eqn | r = \frac {-\cos^{m - 1} a x} {a \paren {n - 1} \sin^{n - 1} a x} - \frac {m - 1} {n - 1} \int \frac {\cos^{m - 2} a x} {\sin^{n - 2} a x} \rd x + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \overline {\paren {\overline z} } | r = \overline {\paren {\overline {x + i y} } } | c = Definition of $z$ }} {{eqn | r = \overline {x - i y} | c = {{Defof|Complex Conjugate}} }} {{eqn | r = x + i y | c = {{Defof|Complex Conjugate}} }} {{eqn | r = z | c = Definition of $z$ }} {{end-eqn}} {{qed}}	0
By definition, $\tilde C \in \mathcal C_x$. From [[Set is Subset of Union]], $\tilde C \subseteq C$. By [[Definition:Maximal Set|maximality]] of $\tilde C$ then $\tilde C = C$	0
For any [[Definition:Finite Set|finite set]] of [[Definition:Prime Number|prime numbers]], there exists a [[Definition:Prime Number|prime number]] not in that [[Definition:Set|set]]. {{:Euclid:Proposition/IX/20}}	0
{{begin-eqn}} {{eqn | l = \sin^2 x - \cos^2 x | r = \left({\sin^2 x - \cos^2 x}\right) \left({\sin^2 x + \cos^2 x}\right) | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \sin^4 x - \cos^4 x | c = [[Difference of Two Squares]] }} {{end-eqn}} {{qed}}	0
Let $G \left({z}\right)$ be the [[Definition:Exponential Generating Function|exponential generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{\dfrac {a_n} {n!} }\right\rangle$. Let $H \left({z}\right)$ be the [[Definition:Exponential Generating Function|exponential generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{\dfrac {b_n} {n!} }\right\rangle$. Then $G \left({z}\right) H \left({z}\right)$ is the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{\dfrac {c_n} {n!} }\right\rangle$, where: :$\forall n \in \Z_{\ge 0}: c_n = \displaystyle \sum_{k \mathop \in \Z} \dbinom n k a_k b_{n - k}$	0
Consider the [[Definition:First Order Ordinary Differential Equation|first order ordinary differential equation]]: :$\map M {x, y} + \map N {x, y} \dfrac {\d y} {\d x} = 0$ We can put our equation: :$(1): \quad \dfrac {\d y} {\d x} + \map P x y = \map Q x$ into this format by identifying: :$\map M {x, y} \equiv \map P x y - \map Q x, \map N {x, y} \equiv 1$ We see that: :$\dfrac {\partial M} {\partial y} - \dfrac {\partial N} {\partial x} = \map P x$ and hence: :$\map P x = \dfrac {\dfrac {\partial M} {\partial y} - \dfrac {\partial N} {\partial x} } N$ is a [[Definition:Real Function|function]] of $x$ only. It immediately follows from [[Integrating Factor for First Order ODE]] that: :$e^{\int \map P x \rd x}$ is an [[Definition:Integrating Factor|integrating factor]] for $(1)$. So, multiplying $(1)$ by this factor: :$e^{\int \map P x \rd x} \dfrac {\d y} {\d x} + e^{\int \map P x \rd x} \map P x y = e^{\int \map P x \rd x} \map Q x$ The result follows by an application of [[Solution to Exact Differential Equation]]. {{qed}}	0
By [[Permutation of Indices of Summation]]: :$\displaystyle \sum_{k \mathop = 1}^n \floor {\dfrac k 2} = \sum_{k \mathop = 1}^n \floor {\dfrac {n + 1 - k} 2}$ and so: :$\displaystyle \sum_{k \mathop = 1}^n \floor {\dfrac k 2} = \dfrac 1 2 \sum_{k \mathop = 1}^n \paren {\floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} }$ First take the case where $n$ is [[Definition:Even Integer|even]]. For $k$ [[Definition:Odd Integer|odd]]: :$\floor {\dfrac k 2} = \dfrac k 2 - \dfrac 1 2$ and: :$\floor {\dfrac {n + 1 - k} 2} = \dfrac {n + 1 - k} 2$ Hence: {{begin-eqn}} {{eqn | l = \floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} | r = \dfrac k 2 - \dfrac 1 2 + \dfrac {n + 1 - k} 2 | c = }} {{eqn | r = \dfrac {k - 1 + n + 1 - k} 2 | c = }} {{eqn | r = \dfrac n 2 | c = }} {{end-eqn}} For $k$ [[Definition:Even Integer|even]]: :$\floor {\dfrac k 2} = \dfrac k 2$ and: :$\floor {\dfrac {n + 1 - k} 2} = \dfrac {n + 1 - k} 2 - \dfrac 1 2 = \dfrac {n - k} 2$ Hence: {{begin-eqn}} {{eqn | l = \floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} | r = \dfrac k 2 + \dfrac {n - k} 2 | c = }} {{eqn | r = \dfrac {k + n - k} 2 | c = }} {{eqn | r = \dfrac n 2 | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \floor {\dfrac k 2} | r = \dfrac 1 2 \sum_{k \mathop = 1}^n \paren {\floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} } | c = }} {{eqn | r = \dfrac 1 2 \sum_{k \mathop = 1}^n \paren {\dfrac n 2} | c = }} {{eqn | r = \dfrac 1 2 n \dfrac n 2 | c = }} {{eqn | r = \dfrac {n^2} 4 | c = }} {{eqn | r = \floor {\dfrac {n^2} 4} | c = as $\dfrac {n^2} 4$ is an [[Definition:Integer|integer]] }} {{end-eqn}} {{qed|lemma}} Next take the case where $n$ is [[Definition:Odd Integer|odd]]. For $k$ [[Definition:Odd Integer|odd]]: :$\floor {\dfrac k 2} = \dfrac k 2 - \dfrac 1 2$ and: :$\floor {\dfrac {n + 1 - k} 2} = \dfrac {n + 1 - k} 2 - \dfrac 1 2$ Hence: {{begin-eqn}} {{eqn | l = \floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} | r = \dfrac k 2 - \dfrac 1 2 + \dfrac {n + 1 - k} 2 - \dfrac 1 2 | c = }} {{eqn | r = \dfrac {k - 1 + n + 1 - k - 1} 2 | c = }} {{eqn | r = \dfrac {n - 1} 2 | c = }} {{end-eqn}} For $k$ [[Definition:Even Integer|even]]: :$\floor {\dfrac k 2} = \dfrac k 2$ and: :$\floor {\dfrac {n + 1 - k} 2} = \dfrac {n + 1 - k} 2$ Hence: {{begin-eqn}} {{eqn | l = \floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} | r = \dfrac k 2 + \dfrac {n - k + 1} 2 | c = }} {{eqn | r = \dfrac {k + n - k + 1} 2 | c = }} {{eqn | r = \dfrac {n + 1} 2 | c = }} {{end-eqn}} Let $n = 2 t + 1$. Then: {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \floor {\dfrac k 2} | r = \dfrac 1 2 \sum_{k \mathop = 1}^n \paren {\floor {\dfrac k 2} + \floor {\dfrac {n + 1 - k} 2} } | c = }} {{eqn | r = \dfrac 1 2 \sum_{k \mathop = 1}^{2 t + 1} \paren {\floor {\dfrac k 2} + \floor {\dfrac {2 t + 2 - k} 2} } | c = }} {{eqn | r = \dfrac t 2 \dfrac {\paren {2 t + 1} + 1} 2 + \dfrac {t + 1} 2 \dfrac {\paren {2 t + 1} - 1} 2 | c = there are $t$ [[Definition:Even Integer|even]] terms and $t + 1$ [[Definition:Odd Integer|odd]] terms }} {{eqn | r = \dfrac {2 t^2 + 2 t} 4 + \dfrac {2 t^2 + 2 t} 4 | c = multiplying out }} {{eqn | r = \dfrac {4 t^2 + 4 t} 4 + \dfrac 1 4 - \dfrac 1 4 | c = }} {{eqn | r = \dfrac {\paren {2 t + 1}^2} 4 - \dfrac 1 4 | c = }} {{eqn | r = \dfrac {n^2} 4 - \dfrac 1 4 | c = }} {{eqn | r = \floor {\dfrac {n^2} 4} | c = }} {{end-eqn}} {{qed}}	0
Let $\map \pi n$ denote the [[Definition:Prime-Counting Function|prime-counting function]]. Let $a \uparrow b$ be interpreted as [[Definition:Knuth Uparrow Notation|Knuth notation]] for $a^b$.	0
By definition of [[Definition:Completely Multiplicative Function|complete multiplicativity]]: :$\forall m, n \in \Z: \map f {m n} = \map f m \map f n$ Hence by [[True Statement is implied by Every Statement]]: :$\forall m, n \in \Z: m \perp n \implies \map f {m n} = \map f m \map f n$ So $f$ is [[Definition:Multiplicative Arithmetic Function|multiplicative]]. {{qed}} [[Category:Number Theory]] [[Category:Completely Multiplicative Functions]] [[Category:Multiplicative Functions]] st2el7aotybal8kzslr6xuxckfq4fqy	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \map \ln {x^2 + a^2} | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {2 x} {x^2 + a^2} | c = [[Derivative of Natural Logarithm|Derivative of $\ln x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = x^m | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^{m + 1} } {m + 1} | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^m \map \ln {x^2 + a^2} \rd x | r = \frac {x^{m + 1} } {m + 1} \map \ln {x^2 + a^2} - \int \frac {x^{m + 1} } {m + 1} \frac {2 x \rd x} {x^2 + a^2} + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^{m + 1} \map \ln {x^2 + a^2} } {m + 1} - \frac 2 {m + 1} \int \frac {x^{m + 2} } {x^2 + a^2} \d x | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} {{qed}}	0
Let $A \subseteq \R$ be the [[Definition:Set|set]] of all points on $\R$ defined as: :$A := \set {\dfrac 1 n : n \in \Z_{>0} }$ Let $T = \struct {A, \tau_d}$ be the [[Definition:Integer Reciprocal Space|integer reciprocal space]] under the [[Definition:Euclidean Topology on Real Number Line|usual (Euclidean) topology]]. Let $B$ be the [[Definition:Uncountable Set|uncountable set]]: :$B := A \cup \closedint 2 3$ where $\closedint 2 3$ is a [[Definition:Closed Real Interval|closed interval]] of $\R$. $2$ and $3$ are to all intents arbitrary, but convenient. Then $0$ is a [[Definition:Limit Point of Set|limit point]] of $B$ in $\R$.	0
:$\displaystyle \int \frac {\mathrm d x} {\left({p x + q}\right)^n \sqrt{a x + b} } = \frac {\sqrt{a x + b} } {\left({n - 1}\right) \left({a q - b p}\right) \left({p x + q}\right)^{n-1} } + \frac {\left({2 n - 3}\right) a} {2 \left({n - 1}\right) \left({a q - b p}\right)} \int \frac {\mathrm d x} {\left({p x + q}\right)^{n-1} \sqrt{a x + b} }$	0
Let $\displaystyle \lim_{x \mathop \to +\infty} \map f x = +\infty$. By the definition of [[Definition:Infinite Limit at Infinity|infinite limit at infinity]], this means: :$\forall M_1 \in \R_{>0}: \exists N_1 \in \R_{>0}: x > N_1 \implies \map f x > M_1$ Now, the assertion that $\map g x \to +\infty$ is: :$\forall M_1 \in \R_{>0}: \exists N_2 \in \R_{>0}: x > N_2 \implies \map g x > M_1$ By the premise that $\map g x > \map f x$ for [[Definition:Sufficiently Large|sufficiently large]] $x$, there is an $N$ such that: :$x > N \implies \map g x > \map f x$ Now, given $M_1$, let $N_2$ be greater than both $N_1$ and $N$. Then since $N_2 > N$ and $N_2 > N_1$ respectively: :$\map g x > \map f x > M_1$ whence: :$\displaystyle \lim_{x \mathop \to +\infty} \map g x = +\infty$ {{qed}}	0
Let $M_1 = \left({A_1, d_1}\right)$ and $M_2 = \left({A_2, d_2}\right)$ be [[Definition:Metric Space|metric spaces]]. Let $f: M_1 \to M_2$ be an [[Definition:Isometry (Metric Spaces)|isometry]]. Then $f$ is a [[Definition:Homeomorphism (Metric Spaces)|homeomorphism]] from $M_1$ to $M_2$.	0
Let $z \in \C$. Let $\sequence {z_n}$ be the [[Definition:Complex Sequence|sequence in $\C$]] defined as $z_n = z^n$. Then: :$\size z < 1$ {{iff}} $\sequence {z_n}$ is a [[Definition:Complex Null Sequence|null sequence]].	0
Each element $z = x + i y$ of the [[Definition:Set|set]] of [[Definition:Complex Number|non-zero complex numbers]] $\C_{\ne 0}$ has an [[Definition:Inverse Element|inverse element]] $\dfrac 1 z$ under the operation of [[Definition:Complex Multiplication|complex multiplication]]: :$\forall z \in \C_{\ne 0}: \exists \dfrac 1 z \in \C_{\ne 0}: z \times \dfrac 1 z = 1 + 0 i = \dfrac 1 z \times z$ This inverse can be expressed as: :$\dfrac 1 z = \dfrac {x - i y} {x^2 + y^2} = \dfrac {\overline z} {z \overline z}$ where $\overline z$ is the [[Definition:Complex Conjugate|complex conjugate]] of $z$.	0
:$\displaystyle \int \frac {\mathrm d x} {\left({x^2 + a^2}\right)^n} = \frac x {2 \left({n - 1}\right) a^2 \left({x^2 + a^2}\right)^{n - 1} } + \frac {2 n - 3} {\left({2 n - 2}\right) a^2} \int \frac {\mathrm d x} {\left({x^2 + a^2}\right)^{n - 1} }$	0
Let $T = \struct {\CC, \tau_d}$ be the [[Definition:Cantor Space|Cantor space]]. Then $T$ is not [[Definition:Extremally Disconnected Space|extremally disconnected]].	0
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{j \mathop = 1}^n {F_j}^2 = F_n F_{n + 1}$ === Basis for the Induction === $\map P 1$ is the case ${F_1}^2 = 1 = F_3 - 1$, which holds from the definition of [[Definition:Fibonacci Numbers|Fibonacci numbers]]. {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 1}^1 {F_j}^2 | r = {F_1}^2 | c = }} {{eqn | r = 1 \times 1 | c = }} {{eqn | r = F_1 \times F_2 | c = }} {{end-eqn}} demonstrating that $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 1}^k {F_j}^2 = F_k F_{k + 1}$ Then we need to show: :$\displaystyle \sum_{j \mathop = 1}^{k + 1} {F_j}^2 = F_{k + 1} F_{k + 2}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 1}^{k + 1} {F_j}^2 | r = \sum_{j \mathop = 1}^k {F_j}^2 + {F_{k + 1} }^2 | c = }} {{eqn | r = F_k F_{k + 1} + {F_{k + 1} }^2 | c = [[Sum of Sequence of Squares of Fibonacci Numbers#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {F_k + F_{k + 1} } F_{k + 1} | c = }} {{eqn | r = F_{k + 2} F_{k + 1} | c = {{Defof|Fibonacci Number}} }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \ge 1: \displaystyle \sum_{j \mathop = 1}^n {F_j}^2 = F_n F_{n + 1}$ {{qed}}	0
If $G$ [[Definition:Continuous Group Action|acts continuously]], then by [[Continuous Group Action is by Homeomorphisms]], $G$ [[Definition:Group Action by Homeomorphisms|acts by homeomorphisms]] Let $G$ [[Definition:Group Action by Homeomorphisms|act by homeomorphisms]] Let $\phi: G \times X \to X$ denote the [[Definition:Group Action|group action]]. For $g \in G$, denote $\phi_g : X \to X : x \mapsto \map \phi {g, x}$ Let $U \subset X$ be [[Definition:Open Set (Topology)|open]]. By [[Inverse Image of Set Under Mapping from Product of Sets]]: :$\displaystyle \map {\phi^{-1} } U = \bigcup_{g \mathop \in G} \paren {\set g \times \map {\phi_g^{-1} } U}$ By definition of [[Definition:Product Topology|product topology]], $\map {\phi^{-1} } U$ is [[Definition:Open Set (Topology)|open]] in $G \times X$. Thus $\phi$ is [[Definition:Continuous Mapping|continuous]]. {{qed}}	0
Let $a, b, r, s \in \Z$. Let $r \perp s$ (that is, let $r$ and $s$ be [[Definition:Coprime Integers|coprime]]). Then: :$a \equiv b \pmod {r s}$ {{iff}} $a \equiv b \pmod r$ and $a \equiv b \pmod s$ where $a \equiv b \pmod r$ denotes that $a$ is [[Definition:Congruence (Number Theory)|congruent modulo $r$]] to $b$.	0
Let $z_1 := \polar {r_1, \theta_1}$ and $z_2 := \polar {r_2, \theta_2}$ be [[Definition:Polar Form of Complex Number|complex numbers expressed in polar form]]. Then: :$z_1 z_2 = r_1 r_2 \paren {\map \cos {\theta_1 + \theta_2} + i \map \sin {\theta_1 + \theta_2} }$	0
is an [[Definition:Exact Differential Equation|exact differential equation]] with [[Definition:General Solution|solution]]: :$x^2 y^3 + y \sin x = C$	0
Let $M = \struct {A, d}$ be [[Definition:Totally Bounded Metric Space|totally bounded]]. Then there exist $n \in \N$ and points $x_0, \dots, x_n \in A$ such that: :$\displaystyle \inf_{0 \mathop \le i \mathop \le n} \map d {x_i, x} \le 1$ for all $x \in A$. Let us set: :$a := x_0$ :$\displaystyle D := \max_{0 \mathop \le i \mathop \le n} \map d {x_0, x_i}$ :$K := D + 1$ Now let $x \in A$ be arbitrary. Then by assumption there exists $i$ such that $\map d {x_i, x} \le 1$. Hence: :$\map d {a, x} \le \map d {a, x_i} + \map d {x_i, x} \le 1 + D = K$ So $M$ is [[Definition:Bounded Metric Space|bounded]], as claimed. {{qed}}	0
Let $\closedint 0 \Omega$ denote the [[Definition:Uncountable Closed Ordinal Space|closed ordinal space]] on $\Omega$. From [[Uncountable Closed Ordinal Space is Countably Compact]], $\closedint 0 \Omega$ is a [[Definition:Countably Compact Space|countably compact space]]. So every [[Definition:Sequence|sequence]] in $\hointr 0 \Omega$ has an [[Definition:Accumulation Point of Sequence|accumulation point]] in $\closedint 0 \Omega$. {{LinkWanted|[[Definition:Sequence|sequence]] in $\hointr 0 \Omega$ has an [[Definition:Accumulation Point of Sequence|accumulation point]] in $\closedint 0 \Omega$}} But $\Omega$ cannot be an [[Definition:Accumulation Point of Sequence|accumulation point]] of any [[Definition:Sequence|sequence]] in $\closedint 0 \Omega$. {{LinkWanted|$\Omega$ cannot be an [[Definition:Accumulation Point of Sequence|accumulation point]] of any [[Definition:Sequence|sequence]] in $\closedint 0 \Omega$}} So every [[Definition:Sequence|sequence]] in $\hointr 0 \Omega$ has an [[Definition:Accumulation Point of Sequence|accumulation point]] in $\hointr 0 \Omega$. This means that $\hointr 0 \Omega$ is [[Definition:Countably Compact Space|countably compact]]. {{qed}}	0
::$11^4 = 14641$	0
:$\dfrac {\d^n} {\d s^n} \laptrans {\map f t} = \paren {-1}^n \laptrans {t^n \, \map f t}$	0
:$\displaystyle \sum_{j \mathop = 1}^n \paren {2 j - 1}^3 = 1^3 + 3^3 + 5^3 + \dotsb + \paren {2 n − 1}^3 = n^2 \paren {2 n^2 − 1}$	0
The set $S$ is not [[Definition:Compact Subset of Complex Plane|compact]].	0
By [[Riemann Zeta Function as a Multiple Integral]], :$\displaystyle \map \zeta 2 = \int_0^1 \int_0^1 \frac 1 {1 - x y} \rd A$ Let $\tuple {u, v} = \tuple {\dfrac {x + y} 2, \dfrac{y - x} 2}$ so that: :$\tuple {x, y} = \tuple {u - v, u + v}$ Let: :$\size J = \size {\dfrac {\partial \tuple {x, y} } {\partial \tuple {u, v} } } = 2$ Then, by [[Change of Variables Theorem (Multivariable Calculus)]]: :$\map \zeta 2 = \displaystyle 2 \iint \limits_S \frac {\d u \rd v} {1 - u^2 + v^2}$ where $S$ is the square defined by the coordinates: :$\tuple {0, 0}, \ \tuple {\dfrac 1 2, -\dfrac 1 2}, \ \tuple {1, 0}, \ \tuple {\dfrac 1 2, \dfrac 1 2}$ Exploiting the symmetry of the square and the function over the $u$-axis, we have: :$\map \zeta 2 = \displaystyle 4 \paren {\int_0^{\frac 1 2} \! \int_0^u \frac {\d v \rd u} {1 - u^2 + v^2} + \int_{\frac 1 2}^1 \! \int_0^{1 - u} \frac {\d v \rd u} {1 - u^2 + v^2} }$ Factoring $1 - u^2$ gives us: :$\map \zeta 2 = \displaystyle 4 \paren {\int_0^{\frac 1 2} \! \int_0^u \frac 1 {1 - u^2} \frac {\d v \rd u} {\frac {v^2} {1 - u^2} + 1} + \int_{\frac 1 2}^1 \! \int_0^{1 - u} \frac 1 {1 - u^2} \frac {\d v \rd u} {\frac {v^2} {1 - u^2} + 1} }$ and letting: :$s = \dfrac v {\sqrt {1 - u^2} }, \rd s = \dfrac 1 {\sqrt {1 - u^2} }$ allows us to make a [[Integration by Substitution|substitution into each integral]], giving: :$\map \zeta 2 = \displaystyle 4 \paren {\int_0^{\frac 1 2} \frac 1 {\sqrt {1 - u^2} } \map \arctan {\frac u {\sqrt {1 - u^2} } } \rd u + \int_{\frac 1 2}^1 \frac 1 {\sqrt {1 - u^2} } \map \arctan {\frac {1 - u} {\sqrt {1 - u^2} } } \rd u}$ Consider the [[Definition:Right Triangle|right triangle]] with [[Definition:Side of Polygon|sides]] $1$, $x$ and $\sqrt {1 - x^2}$. Applying [[Pythagoras's Theorem]]: :$\arcsin x = \arctan \dfrac x {\sqrt {1 - x^2} }$ Let: {{begin-eqn}} {{eqn | l = \theta | r = \map \arctan {\dfrac{1 - u} {\sqrt{1 - u^2} } } | c = }} {{eqn | ll= \leadsto | l = \tan^2 \theta | r = \frac {1 - u} {1 + u} | c = }} {{eqn | ll= \leadsto | l = \sec^2 \theta | r = \frac 2 {1 + u} | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | ll= \leadsto | l = \cos 2 \theta | r = u | c = [[Double Angle Formulas/Cosine/Corollary 1|Double Angle Formula for Cosine: Corollary 1]] }} {{eqn | ll= \leadsto | l = \theta | r = \dfrac 1 2 \arccos u | c = taking [[Definition:Arccosine|arccosines]] of both sides }} {{eqn | r = \frac \pi 4 - \frac {\arcsin u} 2 | c = [[Sine of Complement equals Cosine]] }} {{end-eqn}} This allows us to convert the [[Definition:Arctangent|arctangents]] from the integrals into [[Definition:Arcsine|arcsines]]: :$\map \zeta 2 = \displaystyle 4 \paren {\int_0^{\frac 1 2} {\frac {\arcsin u} {\sqrt {1 - u^2} } \rd u} + \int_{\frac 1 2}^1 {\frac 1 {\sqrt {1 - u^2} } \paren {\frac \pi 4 - \frac {\arcsin u} 2} \rd u} }$ Substituting: :$s = \arcsin u$, $\rd s = \dfrac 1 {\sqrt{1 - u^2} }$ into the [[Definition:Arcsine|arcsines]], and splitting the second integral: :$\map \zeta 2 = 4 \paren {\dfrac {\pi^2} {72} + \dfrac {\pi^2} {36} } = \dfrac {\pi^2} 6$ {{qed}}	0
As [[Sine Function is Absolutely Convergent]] and [[Cosine Function is Absolutely Convergent]], we have: {{begin-eqn}} {{eqn | l = \cos \theta + i \sin \theta | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {\theta^{2 n} } {\paren {2 n}!} + i \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {\theta^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Complex Cosine Function}} and {{Defof|Complex Sine Function}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\paren {-1}^n \dfrac {\theta^{2 n} } {\paren {2 n}!} + i \paren {-1}^n \dfrac {\theta^{2 n + 1} } {\paren {2 n + 1}!} } | c = [[Sum of Absolutely Convergent Series]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\dfrac {\paren {i \theta}^{2 n} } {\paren {2 n}!} + \dfrac {\paren {i \theta}^{2 n + 1} } {\paren {2 n + 1}!} } | c = {{Defof|Imaginary Unit}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \dfrac {\paren {i \theta}^n} {n!} }} {{eqn | r = e^{i \theta} | c = {{Defof|Exponential Function/Complex|subdef = Sum of Series|Complex Exponential Function}} }} {{end-eqn}} {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]]. Then $T$ is a [[Definition:Baire Space (Topology)|Baire space]] {{iff}} every [[Definition:Non-Empty Set|non-empty]] [[Definition:Open Set (Topology)|open]] set of $T$ is [[Definition:Non-Meager Space|non-meager]] in $T$.	0
Let $a, b \in \R$, such that: :$\forall \epsilon \in \R_{>0}: a < b + \epsilon$ where $\R_{>0}$ is the set of [[Definition:Strictly Positive|strictly positive]] [[Definition:Real Number|real numbers]]. That is: :$\epsilon > 0$ Then: :$a \le b$	0
From [[Form of Geometric Sequence of Integers]]: :$P = \tuple {k p^2, k p q, k q^2}$ for some $k, p, q \in \Z$. If $a = k p^2$ is a [[Definition:Square Number|square number]] it follows that $k$ is a [[Definition:Square Number|square number]]: $k = r^2$, say. So: :$P = \tuple {r^2 p^2, r^2 p q, r^2 q^2}$ and so $c = r^2 q^2 = \paren {r q}^2$. {{qed}} {{Euclid Note|22|VIII}}	0
{{begin-eqn}} {{eqn | l = \map \sinh {i z} | r = \frac {e^{i z} - e^{-i z} } 2 | c = {{Defof|Hyperbolic Sine}} }} {{eqn | r = i \frac {e^{i z} - e^{-i z} } {2 i} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $i$ }} {{eqn | r = i \sin z | c = [[Sine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
:$\csc 150 \degrees = \csc \dfrac {5 \pi} 6 = \sqrt 2$	0
Let [[Definition:Topological Subspace|subspace]] $T'$ be [[Definition:Closed Set (Topology)|closed]] in $T$. Then $V \subseteq T'$ is [[Definition:Closed Set (Topology)|closed]] in $T'$ {{iff}} $V$ is [[Definition:Closed Set (Topology)|closed]] in $T$.	0
: $\dbinom {p^n k} {p^n} \equiv k \pmod p$ where $\dbinom {p^n k} {p^n}$ is a [[Definition:Binomial Coefficient|binomial coefficient]].	0
Let $H = S \setminus \left\{{p}\right\}$ where $\setminus$ denotes [[Definition:Set Difference|set difference]]. By definition, $H$ is an [[Definition:Uncountable Discrete Topology|uncountable discrete space]]. The result follows from [[Uncountable Discrete Space is not Second-Countable]]. {{qed}}	0
By definition, a [[Definition:Cunningham Chain of the First Kind|Cunningham chain of the first kind]] is a [[Definition:Integer Sequence|sequence]] of [[Definition:Prime Number|prime numbers]] $\tuple {p_1, p_2, \ldots, p_n}$ such that: : $p_{k + 1} = 2 p_k + 1$ : $\dfrac {p_1 - 1} 2$ is not [[Definition:Prime Number|prime]] : $2 p_n + 1$ is not [[Definition:Prime Number|prime]]. Thus each [[Definition:Term of Sequence|term]] except the last is a [[Definition:Sophie Germain Prime|Sophie Germain prime]]. {{:Definition:Sophie Germain Prime/Sequence}} Let $P: \Z \to \Z$ be the [[Definition:Mapping|mapping]] defined as: :$\map P n = 2 n + 1$ Applying $P$ iteratively to each of the smallest [[Definition:Sophie Germain Prime|Sophie Germain primes]] in turn: {{begin-eqn}} {{eqn | l = \map P 2 | r = 5 | c = }} {{eqn | l = \map P 5 | r = 11 | c = }} {{eqn | l = \map P {11} | r = 23 | c = }} {{eqn | l = \map P {23} | r = 47 | c = }} {{eqn | l = \map P {47} | r = 95 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} Thus $\tuple {2, 5, 11, 23, 47}$ is a [[Definition:Cunningham Chain of the First Kind|Cunningham chain of the first kind]] of [[Definition:Length of Sequence|length]] $5$. {{begin-eqn}} {{eqn | l = \map P 3 | r = 7 | c = }} {{eqn | l = \map P 7 | r = 15 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map P {29} | r = 59 | c = }} {{eqn | l = \map P {59} | r = 119 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map P {41} | r = 83 | c = }} {{eqn | l = \map P {83} | r = 167 | c = }} {{eqn | l = \map P {167} | r = 335 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map P {53} | r = 107 | c = }} {{eqn | l = \map P {107} | r = 215 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map P {83} | r = 167 | c = }} {{eqn | l = \map P {167} | r = 335 | c = which is not [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map P {89} | r = 179 | c = }} {{eqn | l = \map P {179} | r = 359 | c = }} {{eqn | l = \map P {359} | r = 719 | c = }} {{eqn | l = \map P {719} | r = 1439 | c = }} {{eqn | l = \map P {1439} | r = 2879 | c = }} {{eqn | l = \map P {2879} | r = 5759 | c = which is not [[Definition:Prime Number|prime]]. }} {{end-eqn}} It is noted that $\dfrac {89 - 1} 2 = 44$ which is not [[Definition:Prime Number|prime]]. Hence the [[Definition:Integer Sequence|sequence]] of $6$: :$\tuple {89, 179, 359, 719, 1439, 2879}$ {{qed}}	0
Let $S \subseteq \R$. Let $\sequence {f_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Real Function|real functions]]. Let $f_n$ be [[Definition:Continuous Real Function|continuous]] for all $n \in \N$. Let the [[Definition:Infinite Series|infinite series]]: :$\displaystyle \sum_{n \mathop = 1}^\infty f_n$ be [[Definition:Uniform Convergence/Infinite Series|uniformly convergent]] to a [[Definition:Real Function|real function]] $f : S \to \R$. Then $f$ is [[Definition:Continuous Real Function|continuous]].	0
By definition of [[Definition:Connected Topological Space|connected space]], $T$ admits no [[Definition:Separation (Topology)|separation]]. Therefore, [[Definition:Vacuous Truth|vacuously]], every [[Definition:Partition (Topology)|partition]] has one [[Definition:Open Set (Topology)|open]] containing $t_1, t_2 \in T$, for all $t_1, t_2 \in T$. That is, for all $t_1, t_2 \in T$, $T$ is [[Definition:Connected Between Two Points|connected between $t_1$ and $t_2$]]. {{qed}}	0
The operation of [[Definition:Complex Multiplication|multiplication]] on the [[Definition:Set|set]] of [[Definition:Complex Number|complex numbers]] $\C$ is [[Definition:Associative|associative]]: :$\forall z_1, z_2, z_3 \in \C: z_1 \paren {z_2 z_3} = \paren {z_1 z_2} z_3$	0
:$\sinh x \sinh y = \dfrac {\map \cosh {x + y} - \map \cosh {x - y} } 2$	0
{{begin-eqn}} {{eqn | l = \sec 315 \degrees | r = \map \sec {360 \degrees - 45 \degrees} | c = }} {{eqn | r = \sec 45 \degrees | c = [[Secant of Conjugate Angle]] }} {{eqn | r = \sqrt 2 | c = [[Secant of 45 Degrees|Secant of $45 \degrees$]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int x^m \sin a x \rd x = \frac {-x^m \cos a x} a + \frac {m x^{m - 1} \sin a x} {a^2} - \frac {m \paren {m - 1} } {a^2} \int x^{m - 2} \sin a x \rd x$	0
From [[Real Numbers form Ordered Field]], the [[Definition:Real Number|real numbers]] form an [[Definition:Ordered Field|ordered field]]. The result follows from [[Order of Squares in Ordered Field]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \csc 15^\circ | r = \frac 1 {\sin 15^\circ} | c = [[Cosecant is Reciprocal of Sine]] }} {{eqn | r = \frac 1 {\frac {\sqrt 6 - \sqrt 2} 4} | c = [[Sine of 15 Degrees]] }} {{eqn | r = \frac 4 {\sqrt 6 - \sqrt 2} | c = multiplying top and bottom by $4$ }} {{eqn | r = \frac {4 \left({\sqrt 6 + \sqrt 2}\right)} {\left({\sqrt 6 - \sqrt 2}\right) \left({\sqrt 6 + \sqrt 2}\right)} | c = multiplying top and bottom by $\sqrt 6 + \sqrt 2$ }} {{eqn | r = \frac {4 \left({\sqrt 6 + \sqrt 2}\right)} {6 - 2} | c = [[Difference of Two Squares]] }} {{eqn | r = \sqrt 6 + \sqrt 2 | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $\struct {X, \tau}$ be a [[Definition:Separable Space|separable space]]. Let $\mathcal F$ be a [[Definition:Point Finite|point finite]] [[Definition:Set of Sets|set]] of [[Definition:Open Set (Topology)|open sets]] of $X$. Then $\mathcal F$ is [[Definition:Countable Set|countable]].	0
This will be proved by demonstrating the [[Definition:Contrapositive Statement|contrapositive]]: :$S$ is not [[Definition:Complete Metric Space|complete]] {{iff}} $S$ is not [[Definition:Closed Set (Metric Space)|closed]]. === Necessary Condition === Suppose that $S$ is not [[Definition:Complete Metric Space|complete]]. Then there exists a [[Definition:Cauchy Sequence in Metric Space|Cauchy sequence]] $\sequence {x_n}$ in $S$ such that the [[Definition:Limit of Sequence (Metric Space)|limit]] $\displaystyle x = \lim_{n \mathop \to \infty} x_n$, which exists in the [[Definition:Complete Metric Space|complete metric space]] $M$, is not a member of $S$. For all $\epsilon > 0$, there exists an $N \in \N$ such that for all $n \ge N$: :$\map d {x, x_n} < \epsilon$ Hence $M \setminus S$ is not [[Definition:Open Set (Metric Space)|open]]. Therefore, $S$ is not [[Definition:Closed Set (Metric Space)|closed]]. {{qed|lemma}} === Sufficient Condition === Suppose that $S$ is not [[Definition:Closed Set (Topology)|closed]]. Then $M \setminus S$ is not [[Definition:Open Set (Metric Space)|open]]. Therefore, there exists a $x \in M \setminus S$ such that for all $\epsilon > 0$, there exists a $y \in S$ such that $\map d {x, y} < \epsilon$. So there exists a [[Definition:Sequence|sequence]] $\sequence {y_n}$ in $S$ such that for all $n \in \N$: :$\map d {x, y_n} < \dfrac 1 n$ Now, we show that $\sequence {y_n}$ is a [[Definition:Cauchy Sequence in Metric Space|Cauchy sequence]]. Let $N \in \N$ be such that for all $n \ge N$: :$\map d {x, y_n} < \dfrac \epsilon 2$ Let $m, n \ge N$. Then, by the [[Definition:Triangle Inequality|triangle inequality]]: :$\map d {y_m, y_n} \le \map d {x, y_m} + \map d {x, y_n} < \epsilon$ Hence $\sequence {y_n}$ is a [[Definition:Cauchy Sequence in Metric Space|Cauchy sequence]]. Because $\struct {M, d}$ is a [[Definition:Complete Metric Space|complete metric space]] by assumption, the [[Definition:Limit of Sequence (Metric Space)|limit]] $\displaystyle \lim_{n \mathop \to \infty} y_n$ exists and is in $M$. Denote this [[Definition:Limit of Sequence (Metric Space)|limit]] by $y$. By the definition of $\sequence {y_n}$: :$\displaystyle \lim_{n \mathop \to \infty} \map d {x, y_n} = 0$ From [[Distance Function of Metric Space is Continuous]] and [[Continuity of Composite Mapping]]: :$\map d {x, y} = 0$ {{explain|Which two mappings are being composed here?}} By definition of a [[Definition:Metric|metric]], this implies that $x = y$. Since $y \notin S$, $S$ is not [[Definition:Complete Metric Space|complete]]. {{qed}} [[Category:Metric Subspaces]] [[Category:Complete Metric Spaces]] tsfu0pbjo82jwtsxn4b3m4i6dzxdk7w	0
{{ProofWanted}} {{Namedfor|Hermann Minkowski|cat = Minkowski}}	0
Let $\mathcal{O}^n$, $\mathcal{C}^n$ and $\mathcal{K}^n$ be the collections of [[Definition:Open Set (Topology)|open]], [[Definition:Closed Set (Topology)|closed]] and [[Definition:Compact (Real Analysis)|compact]] [[Definition:Subset|subsets]] of the [[Definition:Euclidean Space|Euclidean space]] $\left({\R^n, \tau}\right)$, respectively. Let $\mathcal{J}_{ho}^n$ be the collection of [[Definition:Half-Open Rectangle|half-open rectangles]] in $\R^n$. Let $\mathcal{J}^n_{ho, \text{rat}}$ be the collection of [[Definition:Half-Open Rectangle|half-open rectangles]] in $\R^n$ with [[Definition:Rational Number|rational]] endpoints. Then the [[Definition:Borel Sigma-Algebra|Borel $\sigma$-algebra]] $\mathcal B \left({\R^n}\right)$ satisfies: :$\mathcal B \left({\R^n}\right) = \sigma \left({\mathcal{O}^n}\right) = \sigma \left({\mathcal{C}^n}\right) = \sigma \left({\mathcal{K}^n}\right) = \sigma \left({\mathcal{J}_{ho}^n}\right) = \sigma \left({\mathcal{J}^n_{ho, \text{rat}}}\right)$ where $\sigma$ denotes [[Definition:Sigma-Algebra Generated by Collection of Subsets|generated $\sigma$-algebra]].	0
According to {{AuthorRef|Leonhard Paul Euler}}: {{begin-eqn}} {{eqn | l = \sum_{n \mathop = 0}^\infty \paren {-1}^n n! | r = \int_0^\infty \dfrac {e^{-u} } {1 + u} \rd u | c = }} {{eqn | r = G | c = the [[Definition:Euler-Gompertz Constant|Euler-Gompertz constant]] }} {{eqn | o = \approx | r = 0 \cdotp 59634 \, 73623 \, 23194 \, 07434 \, 10784 \, 99369 \, 27937 \, 6074 \ldots | c = }} {{end-eqn}} {{explain|Clarify meaning of this equality. Using naive manipulations (carelessly swapping integration/sum and using geometric series results) it's quite straightforward to see why someone might think this is true but we should establish the sense in which it is, since the sum on the LHS diverges.<br/>What we need to do is go back to Euler's original statement of this and see what he meant. He did lots of stuff like this, plugging values into formulae that they weren't applicable to, like e.g. $1 + 2 + 3 + \ldots {{=}} \dfrac 1 {12}$, I presume it's like one of those.}}	0
By definition, the [[Definition:Bernoulli Numbers/Generating Function|Bernoulli numbers]] are given by: :$\displaystyle \frac x {e^x - 1} = \sum_{n \mathop = 0}^\infty \frac {B_n x^n} {n!}$ We have: {{begin-eqn}} {{eqn | l = \frac x {e^x - 1} | r = \frac x 2 \left({\frac 2 {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x - e^x + 2} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {\left({e^x + 1}\right) - \left({e^x - 1}\right)} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x + 1} {e^x - 1} - 1}\right) | c = }} {{eqn | r = -\frac x 2 + \frac x 2 \left({\frac {e^x + 1} {e^x - 1} }\right) | c = }} {{end-eqn}} Take $f \left({x}\right) := \dfrac x 2 \left({\dfrac {e^x + 1} {e^x - 1} }\right)$, and note that: {{begin-eqn}} {{eqn | l = f \left({-x}\right) | r = \frac {-x} 2 \left({\dfrac {e^{-x} + 1} {e^{-x} - 1} }\right) | c = }} {{eqn | r = -\frac {-x} 2 \left({\dfrac {1 + e^x} {1 - e^x} }\right) | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $e^x$ }} {{eqn | r = -\frac {-x} 2 \left({\dfrac {e^x + 1} {-\left({e^x - 1}\right)} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x + 1} {e^x - 1} }\right) | c = }} {{eqn | r = f \left({x}\right) | c = }} {{end-eqn}} and so $f \left({x}\right) := \dfrac x 2 \left({\dfrac {e^x + 1} {e^x - 1} }\right)$ is an [[Definition:Even Function|even function]]. Thus $f \left({x}\right)$ can be seen to take the form: :$\displaystyle f \left({x}\right) = 1 + \sum_{n \mathop = 2}^\infty \frac {B_n} {n!} x^n$ and so: :$\displaystyle f \left({x}\right) = 1 + \sum_{n \mathop = 2}^\infty \frac {B_n} {n!} x^{-n}$ That is: :$\forall n \in \N: n > 1: \dfrac {B_n} {n!} x^n = \dfrac {B_n} {n!} x^{-n}$ and so for [[Definition:Odd Number|odd]] $n$ where $n > 1$ it follows that $B_n = 0$ {{qed}}	0
For $\struct {S, +, \times}$ to be a [[Definition:Ring (Abstract Algebra)|ring]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is a [[Definition:Semigroup|semigroup]]. For $\struct {S, \times}$ to be a [[Definition:Semigroup|semigroup]], it is a [[Definition:Necessary Condition|necessary condition]] that $\struct {S, \times}$ is [[Definition:Closed Algebraic Structure|closed]]. That is: :$\forall x, y \in S: x \times y \in S$ Let $x = \dfrac 1 2$ and $y = \dfrac 3 2$. Both $x$ and $y$ are in $S$, as both are [[Definition:Rational Number|rational numbers]] expressed in [[Definition:Canonical Form of Rational Number|canonical form]] whose [[Definition:Denominator|denominators]] are not [[Definition:Divisor of Integer|divisible]] by $4$. But then: :$x \times y = \dfrac 1 2 \times \dfrac 3 2 = \dfrac 3 4$ which is a [[Definition:Rational Number|rational numbers]] expressed in [[Definition:Canonical Form of Rational Number|canonical form]] whose [[Definition:Denominator|denominator]] ''is'' [[Definition:Divisor of Integer|divisible]] by $4$. Hence $x \times y \notin S$ and so $\struct {S, \times}$ is not [[Definition:Closed Algebraic Structure|closed]]. The result follows. {{qed}}	0
By the definition of the [[Definition:Limit of Real Function|limit of a real function]], we have to prove that: : $\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \left({\left|{x - a}\right| < \delta \implies \left|{f \left({x}\right) - L}\right| < \epsilon}\right)$ Let $\epsilon \in \R_{>0}$ be given. We have: : $\displaystyle \lim_{x \mathop \to a} \ g \left({x}\right) = \lim_{x \mathop \to a} \ h \left({x}\right)$ Hence by [[Sum Rule for Limits of Functions]]: :$\displaystyle \lim_{x \mathop \to a} \ h \left({x}\right) - g \left({x}\right) = 0$ By the definition of the [[Definition:Limit of Real Function|limit of a real function]]: : $(1): \quad \forall \epsilon' \in \R_{>0}: \exists \delta \in \R_{>0}: \left({\left|{x - a}\right| < \delta \implies \left|{h \left({x}\right) - L}\right| < \epsilon'}\right)$ : $(2): \quad \forall \epsilon' \in \R_{>0}: \exists \delta \in \R_{>0}: \left({\left|{x - a}\right| < \delta \implies \left|{g \left({x}\right) - L}\right| < \epsilon'}\right)$ : $(3): \quad \forall \epsilon' \in \R_{>0}: \exists \delta \in \R_{>0}: \left({\left|{x - a}\right| < \delta \implies \left|{h \left({x}\right) - g \left({x}\right)}\right| < \epsilon'}\right)$ Take $\epsilon' = \dfrac {\epsilon} 3$ in $(1)$, $(2)$, $(3)$. Then there exists $\delta_1, \delta_2, \delta_3$ that satisfies $(1)$, $(2)$, $(3)$ with $\epsilon' = \dfrac \epsilon 3$. Take $\delta = \min\{\delta_1, \delta_2, \delta_3\}$. Then: {{begin-eqn}} {{eqn | l = \left\vert{x - a}\right\vert | o = < | r = \delta | c = }} {{eqn | ll= \implies | l = \left\vert{h \left({x}\right) - L}\right\vert | o = < | r = \frac {\epsilon} 3 | c = }} {{eqn | lo= \land | l = \left\vert{g \left({x}\right) - L}\right\vert | o = < | r = \frac {\epsilon} 3 | c = }} {{eqn | lo= \land | l = \left\vert{h \left({x}\right) - g \left({x}\right)}\right\vert | o = < | r = \frac {\epsilon} 3 | c = }} {{end-eqn}} So, if $\left\vert{x - a}\right\vert<\delta$: {{begin-eqn}} {{eqn | l = \left\vert{f \left({x}\right) - L}\right\vert | r = \left\vert{f \left({x}\right) - g \left({x}\right) + h \left({x}\right) - L + g \left({x}\right) - h \left({x}\right)}\right\vert | c = }} {{eqn | o = \le | r = \left\vert{f \left({x}\right) - g \left({x}\right)}\right\vert + \left\vert{h \left({x}\right) - L}\right\vert + \left\vert{h \left({x}\right) - g \left({x}\right)}\right\vert | c = }} {{eqn | o = \le | r = \left\vert{h \left({x}\right) - g \left({x}\right)}\right\vert + \left\vert{h \left({x}\right) - L}\right\vert + \left\vert{h \left({x}\right) - g \left({x}\right)}\right\vert | c = }} {{eqn | o = \le | r = \frac {\epsilon} 3 + \frac {\epsilon} 3 + \frac {\epsilon} 3 | c = }} {{eqn | r = \epsilon | c = }} {{end-eqn}} {{qed}}	0
$(1)$ can be expressed as: {{begin-eqn}} {{eqn | l = x \rd y | r = -y \rd x | c = }} {{eqn | ll= \leadsto | l = y | r = C x^{-1} | c = [[First Order ODE/x dy = k y dx|First Order ODE: $x \rd y = k y \rd x$]] }} {{eqn | ll= \leadsto | l = x y | r = C | c = multiplying through by $x$ }} {{end-eqn}} {{qed}} [[Category:Examples of First Order ODEs]] [[Category:Examples of Separation of Variables]] 6ezpoyg7g40z9d75edixh9xyydjql0n	0
Let $b = \circ^m a$. Let $h: \N_{>0} \to S$ be the [[Definition:Mapping|mapping]] defined as: :$\forall n \in \N_{>0}: h \paren n = \circ^{n m} a$ Let the [[Definition:Mapping|mapping]] $f_b: \N_{>0} \to S$ be [[Definition:Recursively Defined Mapping/Natural Numbers|recursively defined]] as: :$\forall n \in \N_{>0}: f_b \paren n = \circ^n b$ From the [[Principle of Recursive Definition]]: :$f_b$ is the [[Definition:Unique|unique]] [[Definition:Mapping|mapping]] which satisfies: :$\forall n \in \N_{>0}: f_b \paren n = \begin{cases} b & : n = 1 \\ f_b \paren r \circ b & : n = r \circ 1 \end{cases}$ But $h \paren 1 = \circ^{1 \times m} a = \circ^m a = b$. So: {{begin-eqn}} {{eqn | lo= \forall n \in \N_{>0}: | l = h \paren {n + 1} | r = \circ^{\paren {n + 1} m} a | c = }} {{eqn | r = \circ^{\paren {n m} + m} a | c = [[Natural Number Multiplication Distributes over Addition]] }} {{eqn | r = \paren {\circ ^{n \ast m} } \circ \paren {\circ^m a} | c = [[Index Laws for Semigroup/Sum of Indices|Index Laws for Semigroup: Sum of Indices]] }} {{eqn | r = h \paren n) \circ b | c = }} {{end-eqn}} Thus $h = f_b$, and so: :$\forall n, m \in \N_{>0}: \circ^{n m} = \circ^n \paren {\circ^m a}$ From [[Natural Number Multiplication is Commutative]]: :$\forall n, m \in \N_{>0}: \circ^m \paren {\circ^n a} = \circ^{m n} = \circ^{n m}$ {{qed}}	0
Let $G$ be the [[Definition:Graph of Mapping|graph]] of the [[Definition:Real Function|function]] $y = \sin \left({\dfrac 1 x}\right)$ for $x > 0$. Let $J$ be the [[Definition:Line Segment|line segment]] joining the points $\left({0, -1}\right)$ and $\left({0, 1}\right)$ in $\R^2$. Then while $G \cup J$ is [[Definition:Connected Set (Topology)|connected]], it is '''not''' [[Definition:Path-Connected Set (Topology)|path-connected]].	0
The [[Definition:Differential of Functional|differential]] of a [[Definition:Differentiable Functional|differentiable functional]] is [[Definition:Unique|unique]].	0
Define $\map {S_N} x = \displaystyle \sum_{n \mathop = 1}^N \map {f_n} x$. We have: {{begin-eqn}} {{eqn | l = \size {\int_a^b \map f x \rd x - \sum_{n \mathop = 1}^N \int_a^b \map {f_n} x \rd x} | r = \size {\int_a^b \paren {\map f x - \map {S_N} x} \rd x} | c = }} {{eqn | o = \le | r = \paren {b - a} \sup_{x \mathop \in \closedint a b} \size {\map f x - \map {S_N} x} | c = }} {{eqn | o = \to | r = 0 | c = as $N \to +\infty$ }} {{end-eqn}} {{qed}}	0
:$\displaystyle \pi^2 \map {\sec^2} {\pi z} = 8 \sum_{n \mathop = 0}^\infty \frac {\paren {2 n + 1} + 4 z^2} {\paren {\paren {2 n + 1}^2 - 4 z^2}^2}$	0
{{begin-eqn}} {{eqn | l = \paren {\sin x + \cos x} \paren {\tan x + \cot x} | r = \paren {\sin x + \cos x} \paren {\sec x \csc x} | c = [[Sum of Tangent and Cotangent]] }} {{eqn | r = \frac {\sin x + \cos x} {\sin x \cos x} | c = {{Defof|Secant Function}} and {{Defof|Cosecant}} }} {{eqn | r = \frac 1 {\cos x} + \frac 1 {\sin x} }} {{eqn | r = \sec x + \csc x | c = {{Defof|Secant Function}} and {{Defof|Cosecant}} }} {{end-eqn}} {{qed}} [[Category:Trigonometric Identities]] mfhi8x6zpj7jjyn9usxkwmft0vebngd	0
Let $\map f t := \map \Si t = \displaystyle \int_0^t \dfrac {\sin u} u \rd u$. Then: :$\map f 0 = 0$ and: {{begin-eqn}} {{eqn | l = \map \Si t | r = \int_0^t \dfrac {\sin u} u \rd u | c = {{Defof|Sine Integral Function}} }} {{eqn | r = \int_0^t \dfrac 1 u \paren {u - \dfrac {u^3} {3!} + \dfrac {u^5} {5!} - \dfrac {u^7} {7!} + \dotsb} \rd u | c = {{Defof|Real Sine Function}} }} {{eqn | r = t - \dfrac {t^3} {3 \times 3!} + \dfrac {t^5} {5 \times 5!} - \dfrac {t^7} {7 \times 7!} + \dotsb | c = [[Primitive of Power]] }} {{eqn | ll= \leadsto | l = \laptrans {\map \Si t} | r = \laptrans {t - \dfrac {t^3} {3 \times 3!} + \dfrac {t^5} {5 \times 5!} - \dfrac {t^7} {7 \times 7!} + \dotsb} | c = }} {{eqn | r = \dfrac 1 {s^2} - \dfrac 1 {3 \times 3!} \dfrac {3!} {s^4} + \dfrac 1 {5 \times 5!} \dfrac {5!} {s^6} - \dfrac 1 {7 \times 7!} \dfrac {7!} {s^8} + \dotsb | c = [[Laplace Transform of Positive Integer Power]] }} {{eqn | r = \dfrac 1 {s^2} - \dfrac 1 {3 s^4} + \dfrac 1 {5 s^6} - \dfrac 1 {7 s^8} + \dotsb | c = simplifying }} {{eqn | r = \dfrac 1 s \paren {\dfrac {\paren {1 / s} } 1 - \dfrac {\paren {1 / s}^3} 3 + \dfrac {\paren {1 / s}^5} 5 - \dfrac {\paren {1 / s}^7} 7 + \dotsb} | c = rearranging }} {{eqn | r = \dfrac 1 s \arctan \dfrac 1 s | c = [[Power Series Expansion for Real Arctangent Function]] }} {{end-eqn}} {{qed}}	0
:$\left({w f}\right)' \left({z}\right) = w f' \left({z}\right)$	0
{{:Euclid:Proposition/V/21}} That is, let: :$a : b = e : f$ :$b : c = d : e$ Then: :$a > c \implies d > f$ :$a = c \implies d = f$ :$a < c \implies d < f$	0
The [[Definition:Cross-Relation|cross-relation]] $\boxtimes$ is an [[Definition:Equivalence Relation|equivalence relation]] on $\left({S \times C, \oplus}\right)$.	0
{{begin-eqn}} {{eqn | l = x^6 - y^6 | r = \paren {x^3}^2 - \paren {y^3}^2 | c = }} {{eqn | r = \paren {x^3 - y^3} \paren {x^3 + y^3} | c = }} {{eqn | r = \paren {x - y} \paren {x^2 + x y + y^2} \paren {x^3 + y^3} | c = [[Difference of Two Cubes]] }} {{eqn | r = \paren {x - y} \paren {x^2 + x y + y^2} \paren {x + y} \paren {x^2 - x y + y^2} | c = [[Sum of Two Cubes]] }} {{end-eqn}} {{qed}}	0
Let $\epsilon > 0$. We need to find $N$ such that: : $\forall n > N: \cmod {\lambda z_n - \lambda c} < \epsilon$ If $\lambda = 0$ the result is trivial. So, assume $\lambda \ne 0$. Then $\cmod \lambda > 0$ from the definition of the [[Definition:Complex Modulus|modulus]] of $\lambda$. Hence $\dfrac \epsilon {\cmod \lambda} > 0$. We have that $z_n \to c$ as $n \to \infty$. Thus it follows that: : $\exists N: \forall n > N: \cmod {z_n - c} < \dfrac \epsilon {\cmod \lambda}$ That is: : $\forall n > N: \cmod \lambda \cmod {z_n - c} < \epsilon$ But we have: {{begin-eqn}} {{eqn | l = \cmod \lambda \cmod {z_n - c} | r = \cmod {\lambda \paren {z_n - c} } | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | r = \cmod {\lambda x_n - \lambda l} | c = }} {{end-eqn}} Hence: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\lambda x_n} = \lambda c$ {{qed}} [[Category:Combination Theorems for Sequences]] tfbqx2zl15xj1mnq5sky0k6294531dj	0
Let $T_n$ denote the $n$th [[Definition:Triangular Number|triangular number]]. Then the [[Definition:Generating Function|generating function]] for $\sequence {T_n}$ is given as: :$\displaystyle \map G z = \frac z {\paren {1 - z}^3}$	0
Let $\C$ denote the [[Definition:Complex Number|set of complex numbers]]. Let $N: \C \to \R_{\ge 0}$ denote the [[Definition:Field Norm of Complex Number|field norm on complex numbers]]: :$\forall z \in \C: \map N z = \cmod z^2$ where $\cmod z$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z$. Then $N$ is a [[Definition:Multiplicative Function on Ring|multiplicative function]] on $\C$.	0
:$\displaystyle \int_0^\infty e^{-a x} \sin b x \rd x = \frac b {a^2 + b^2}$	0
:[[File:Euclid-X-114.png|350px]] Let the [[Definition:Rectangle|rectangle]] $AB \cdot CD$ be [[Definition:Containment of Rectangle|contained]] by the [[Definition:Apotome|apotome]] $AB$ and the [[Definition:Binomial (Euclidean)|binomial straight line]] $CD$. Let $CE$ be the greater [[Definition:Term of Binomial|term]] of $CD$. Let: :$CE$ be [[Definition:Commensurable in Length|commensurable in length]] with $AF$ :$ED$ be [[Definition:Commensurable in Length|commensurable in length]] with $FB$ :$CE : ED = AF : FB$ Let the [[Definition:Square Root|"side"]] of the [[Definition:Rectangle|rectangle]] $AB \cdot CD$ be $G$. It is to be demonstrated that $G$ is [[Definition:Rational Line Segment|rational]]. Let $H$ be a [[Definition:Rational Line Segment|rational straight line]]. Let a [[Definition:Rectangle|rectangle]] equal to $H^2$ be applied to $CD$ which produces $KL$ as [[Definition:Breadth|breadth]]. By definition, $KL$ is an [[Definition:Apotome|apotome]]. Let the [[Definition:Term of Apotome|terms]] of $KL$ be $KM$ and $ML$. Let $KM$ and $ML$ be [[Definition:Commensurable in Length|commensurable]] with the [[Definition:Term of Binomial|terms]] $CE$ and $ED$ of the [[Definition:Binomial (Euclidean)|binomial straight line]] $CD$. But from {{EuclidPropLink|book = X|prop = 112|title = Square on Rational Straight Line applied to Binomial Straight Line}}: :$CE$ and $ED$ are [[Definition:Commensurable in Length|commensurable]] with the [[Definition:Term of Apotome|terms]] $AF$ and $FB$ of the [[Definition:Apotome|apotome]] $CD$. Therefore: :$AF : FB = KM : ML$ and so: :$AF : KM = BF : LM$ and so by {{EuclidPropLink|book = V|prop = 19|title = Proportional Magnitudes have Proportional Remainders}}: :$AB : KL = AF : KM$ But from {{EuclidPropLink|book = X|prop = 12|title = Commensurability is Transitive Relation}}: :$AF$ is [[Definition:Commensurable in Length|commensurable in length]] with $KM$. Therefore from {{EuclidPropLink|book = X|prop = 11|title = Commensurability of Elements of Proportional Magnitudes}}: :$AB$ is [[Definition:Commensurable in Length|commensurable in length]] with $KL$. From {{EuclidPropLink|book = VI|prop = 1|title = Areas of Triangles and Parallelograms Proportional to Base}}: :$AB : KL = CD \cdot AB : CD \cdot KL$ Therefore from {{EuclidPropLink|book = X|prop = 11|title = Commensurability of Elements of Proportional Magnitudes}}: :$CD \cdot AB$ is [[Definition:Commensurable|commensurable]] with $CD \cdot KL$. But $CD \cdot KL = H^2$. Therefore $CD \cdot AB$ is [[Definition:Commensurable|commensurable]] with $H^2$. But $G^2 = CD \cdot AB$. Therefore $G^2$ is [[Definition:Commensurable|commensurable]] with $H^2$. But $H^2$ is [[Definition:Rational Area|rational]]. Therefore $G^2$ is also [[Definition:Rational Area|rational]]. Therefore $G$ is [[Definition:Rational Line Segment|rational straight line]]. But $G^2$ is the [[Definition:Square Root|"side"]] of the [[Definition:Rectangle|rectangle]] $AB \cdot CD$. Hence the result. {{qed}} {{Euclid Note|114|X}}	0
Let $X^n$ be an [[Definition:Dimension (Topology)|$n$-dimensional]] [[Definition:Topological Manifold|manifold]]. Let $U$ be a [[Definition:Neighborhood (Topology)|neighborhood]] of a point $p \in X^n$. Then there exist [[Definition:Local Coordinates|local coordinates]] on $U$.	0
A [[Definition:Regular Space|regular space]] is a [[Definition:Topological Space|topological space]] which is both a [[Definition:Kolmogorov Space|$T_0$ (Kolmogorov) space]] and a [[Definition:T3 Space|$T_3$ space]]. Hence from [[T0 Property is Hereditary|$T_0$ Property is Hereditary]] and [[T3 Property is Hereditary|$T_3$ Property is Hereditary]] it follows that the property of being a [[Definition:Regular Space|regular space]] is also [[Definition:Hereditary Property (Topology)|hereditary]]. A [[Definition:Tychonoff Space|Tychonoff (completely regular) space]] is a [[Definition:Topological Space|topological space]] which is both a [[Definition:Kolmogorov Space|$T_0$ (Kolmogorov) space]] and a [[Definition:T3 Space|$T_3 \frac 1 2$ space]]. Hence from [[T0 Property is Hereditary|$T_0$ Property is Hereditary]] and [[T3 1/2 Property is Hereditary|$T_3 \frac 1 2$ Property is Hereditary]] it follows that the property of being a [[Definition:Tychonoff Space|Tychonoff (completely regular) space]] is also [[Definition:Hereditary Property (Topology)|hereditary]]. A [[Definition:Completely Normal Space|completely normal space]] is a [[Definition:Topological Space|topological space]] which is both a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]] and a [[Definition:T5 Space|$T_5$ space]]. Hence from [[T1 Property is Hereditary|$T_1$ Property is Hereditary]] and [[T5 Property is Hereditary|$T_5$ Property is Hereditary]] it follows that the property of being a [[Definition:Completely Normal Space|completely normal space]] is also [[Definition:Hereditary Property (Topology)|hereditary]]. {{qed}}	0
Define: :$\displaystyle L = \inf_{n \mathop \in \N} \size x^n$ By the [[Continuum Property]], such an $L$ exists in $\R$. Clearly, $L \ge 0$. Suppose that $L > 0$. Then, by the definition of the [[Definition:Infimum of Set|infimum]], we can choose $n \in \N$ such that $\size x^n < L \size x^{-1}$. But then $\size x^{n + 1} < L$, which [[Definition:Contradiction|contradicts]] the definition of $L$. Therefore, $L = 0$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. By the definition of the [[Definition:Infimum of Set|infimum]], there exists an $N \in \N$ such that $\size x^N < \epsilon$. It follows that: :$\forall n \in \N: n \ge N \implies \size {x^n} = \size x^n \le \size x^N < \epsilon$ where either [[Absolute Value Function is Completely Multiplicative]] is applied. Hence the result, by the definition of a [[Definition:Limit of Sequence (Number Field)|limit]]. {{qed}}	0
{{ProofWanted|Lots of background work needed.}}	0
By [[Characterization of Sigma-Algebra Generated by Collection of Mappings]], we have that: :$\sigma \left({f_i: i \in I}\right) = \sigma \left({\displaystyle \bigcup_{i \mathop \in I} f_i^{-1} \left({\Sigma_i}\right)}\right)$ where the second is a [[Definition:Sigma-Algebra Generated by Collection of Subsets|$\sigma$-algebra generated by a collection of subsets]]. The result follows from applying [[Existence and Uniqueness of Sigma-Algebra Generated by Collection of Subsets]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \mu_{a b} | r = x \paren {a b} | c = Definition of $\mu$ }} {{eqn | r = \paren {x a} b | c = }} {{eqn | r = \paren {\map {\mu_a} x} b | c = Definition of $\mu$ }} {{eqn | r = \map {\mu_b} {\map {\mu_a} x} | c = Definition of $\mu$ }} {{eqn | r = \map {\paren {\mu_b \circ \mu_a} } x | c = {{Defof|Composition of Mappings}} }} {{end-eqn}} {{qed}}	0
Let $f: \R \to \Z$ be an [[Definition:Integer-Valued Function|integer-valued function]] which satisfies both of the following: :$(1): \quad \map f {x + 1} = \map f x + 1$ :$(2): \quad \forall n \in \Z_{> 0}: \map f x = \map f {\dfrac {\map f {n x} } n}$ Then either: :$\forall x \in \Q: \map f x = \floor x$ or: :$\forall x \in \Q: \map f x = \ceiling x$	0
Throughout, the complex variable $s$ is $s = \sigma + it$. We have, for $\sigma > 1$, {{begin-eqn}} {{eqn|l= -\frac{\zeta'(s)}{\zeta(s)} |r= \sum_{n\geq 1} \Lambda(n) n^{-s} |c= By [[Logarithmic Derivative of Riemann Zeta Function]] }} {{eqn|l= |r=\sum_{n\geq 1} \Lambda(n) n^{-\sigma} \exp\left( -it\log n\right) |c=Where $s = \sigma + it$ }} {{eqn|l= |r=\sum_{n\geq 1} \Lambda(n) n^{-\sigma} \left( \cos(t\log n) - i \sin(t\log n) \right) |c=By [[Euler's Formula]] }} {{end-eqn}} Therefore, :$\displaystyle -\Re\left( \frac{\zeta'(s)}{\zeta(s)} \right) = \sum_{n\geq 1} \Lambda(n) n^{-\sigma} \cos(t\log n) \qquad (1)$ Now observe that :$3 + 4\cos\theta + \cos(2\theta) = 2(1+ \cos\theta)^2 \geq 0$ Because for all $n \geq 1$ we have $\Lambda(n)n^{-\sigma} \geq 0$, we have {{begin-eqn}} {{eqn|l=0 |r=\sum_{n\geq 1} \Lambda(n) n^{-\sigma} \left\{ 3 + 4\cos(t\log n) + \cos(2t\log n) \right\} |c= |o=\leq }} {{eqn|l= |r=-\Re\left( 3\frac{\zeta'(\sigma)}{\zeta(\sigma)} + 4\frac{\zeta'(\sigma + it)}{\zeta(\sigma + it)} + \frac{\zeta'(\sigma + 2it)}{\zeta(\sigma + 2it)} \right) |c=By $(1)$ }} {{end-eqn}} Now let :$\eta(s) = \zeta(s)^3\cdot \zeta(s+it)^4\cdot \zeta(s+2it)$ Then the above computation has shown that :$\displaystyle\Re\left( \frac{\eta'(s)}{\eta(s)} \right) \leq 0$ By [[Poles of Riemann Zeta Function]] we know that $\zeta$ has a [[Definition:Simple Pole|simple pole]] at $s=1$ with [[Definition:Residue (Complex Analysis)|residue]] $1$. Suppose that $1+it$ is a zero of $\zeta$ of [[Definition:Order of a Zero|order]] $d \geq 1$. Therefore, at $s = 1$, $\eta$ has a zero of order $4d - 3 \geq 0$, that is, :$\displaystyle \eta(s) \sim (s-1)^{4d -3}$ as $s \to 1^+$, where $\sim$ indicates asymptotic equality, and superscript $+$ denotes a limit from the right along the real line. Therefore :$\displaystyle \frac{\eta'(s)}{\eta(s)} \sim \frac{4d -3}{s-1}$ as $s \to 1^+$. Since $\displaystyle \Re\left( \frac{4d -3}{s-1}\right) \to + \infty$ as $s \to 1^+$, it follows that :$\displaystyle \Re\left(\frac{\eta'(s)}{\eta(s)}\right) \to \infty$ as $s \to 1^+$. But we have already shown that :$\displaystyle\Re\left( \frac{\eta'(s)}{\eta(s)} \right) \leq 0$ a contradiction. {{qed}} [[Category:Riemann Zeta Function]] fo6o84yn30601r2nq1x3org4ioua0l4	0
==== [[Change of Base of Logarithm/Base 10 to Base e/Form 1|Form 1]] ==== {{:Change of Base of Logarithm/Base 10 to Base e/Form 1}} ==== [[Change of Base of Logarithm/Base 10 to Base e/Form 2|Form 2]] ==== {{:Change of Base of Logarithm/Base 10 to Base e/Form 2}}	0
Let $\left({X, \Sigma, \mu}\right)$ be a [[Definition:Measure Space|measure space]]. Let $E \in \Sigma$ be a [[Definition:Measurable Set|measurable set]], and let $\chi_E: X \to \R$ be its [[Definition:Characteristic Function of Set|characteristic function]]. Then $I_\mu \left({\chi_E}\right) = \mu \left({E}\right)$, where $I_\mu \left({\chi_E}\right)$ is the [[Definition:Integral of Positive Simple Function|$\mu$-integral of $\chi_E$]].	0
The following sets of $4$ consecutive triplets of [[Definition:Integer|integers]], with one [[Definition:Integer|integer]] between each triplet, are [[Definition:Square-Free|square-free]]: :$29, 30, 31; 33, 34, 35; 37, 38, 39; 41, 42, 43$ :$101, 102, 103; 105, 106, 107; 109, 110, 111; 113, 114, 115$	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|strictly positive integer]]. Let $\struct {\C_{\ne 0}, \times}$ be the [[Definition:Multiplicative Group of Complex Numbers|multiplicative group of complex numbers]]. Let $f_n: \C_{\ne 0} \to \C_{\ne 0}$ be the [[Definition:Mapping|mapping]] from the [[Definition:Complex Number|set of complex numbers less zero]] to itself defined as: :$\forall z \in \C_{\ne 0}: \map {f_n} z = z^n$ Then $f_n: \struct {\C_{\ne 0}, \times} \to \struct {\C_{\ne 0}, \times}$ is a [[Definition:Group Epimorphism|group epimorphism]]. The [[Definition:Kernel of Group Homomorphism|kernel]] of $f_n$ is the set of [[Definition:Complex Roots of Unity|complex $n$th roots of unity]].	0
Let $m, n \in \Z$ be [[Definition:Integer|integers]]. Then: :$\displaystyle \int_0^\pi \sin m x \cos n x \rd x = \begin{cases} 0 & : m + n \text { even} \\ \dfrac {2 m} {m^2 - n^2} & : m + n \text { odd} \end{cases}$	0
This is an instance of [[Linear First Order ODE/y' - (y over x) = k x|Linear First Order ODE: $y' - \dfrac y x = k x$]]. Its [[Definition:General Solution of Differential Equation|solution]] is: :$\dfrac y x = k x + C$ or: :$y = k x^2 + C x$ from which the [[Definition:General Solution of Differential Equation|solution]] to $(1)$ is found by substituting $3$ for $k$. {{qed}} [[Category:Examples of Linear First Order ODEs]] 5zl5xx9teqim55grzediaigsppbf6nk	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $F, F'$ be [[Definition:F-Sigma Set|$F_\sigma$ sets]] of $T$. Then their [[Definition:Set Intersection|intersection]] $F \cap F'$ is also a [[Definition:F-Sigma Set|$F_\sigma$ set]] of $T$.	0
By definition of [[Definition:Modulo Operation|modulo operation]]: :$x \bmod y := x - y \floor {\dfrac x y}$ for $y \ne 0$. We have: {{begin-eqn}} {{eqn | l = \dfrac {0 \cdotp 11} {-0 \cdotp 1} | r = \dfrac {1 \cdotp 1} {-1} | c = }} {{eqn | r = -1 \cdotp 1 | c = }} {{end-eqn}} and so: :$\floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } = -2$ Thus: {{begin-eqn}} {{eqn | l = 0 \cdotp 11 \bmod -0 \cdotp 1 | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } | c = }} {{eqn | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \paren {-2} | c = }} {{eqn | r = 0 \cdotp 11 - 0 \cdotp 2 | c = }} {{eqn | r = -0 \cdotp 09 | c = }} {{end-eqn}} {{qed}}	0
Follows directly from: :[[Constant Function is Uniformly Continuous/Metric Space|Constant Function is Uniformly Continuous: Metric Space]] :[[Real Number Line is Metric Space]]. {{qed}}	0
:$\map \erf 0 = 0$	0
Let $T = \struct {S, \tau_{\bar p} }$ be a [[Definition:Excluded Point Topology|excluded point space]] such that $S$ is not a [[Definition:Singleton|singleton]]. Then $T$ is not a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]].	0
:$f * g: \struct {S, \tau_{_S} } \to \struct {R, \tau_{_R} }$ is [[Definition:Continuous Mapping on Set|continuous]].	0
Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $\map \sigma n$ be the [[Definition:Sigma Function|sigma function]] of $n$. Then $\map \sigma n = n + 1$ {{iff}} $n$ is [[Definition:Prime Number|prime]].	0
{{:Euclid:Proposition/X/114/Porism}}	0
Let $x_1, x_2, \dots ,x_n \in S$ for some $n \in \N_{>0}$. Then: :$\max \set {x_1, x_2, \dotsc, x_n} = \sup \set {x_1, x_2, \dotsc, x_n}$	0
Let $r \in \Q_{>0}$ be a [[Definition:Strictly Positive Rational Number|strictly positive rational number]]. Let $\sequence {x_n}$ be the [[Definition:Real Sequence|sequence in $\R$]] defined as: : $x_n = \dfrac 1 {n^r}$ Then $\sequence {x_n}$ is a [[Definition:Null Sequence (Analysis)|null sequence]].	0
{{begin-eqn}} {{eqn | l = 1 | o = \divides | r = n }} {{eqn | l = -1 | o = \divides | r = n }} {{end-eqn}}	0
Let $\R$ be the [[Definition:Real Number Line|real number line]] considered as an [[Definition:Euclidean Space|Euclidean space]]. Let $\openint a b \subset \R$ be an [[Definition:Open Real Interval|open interval]] of $\R$. Then $\openint a b$ is an [[Definition:Open Set (Metric Space)|open set]] of $\R$.	0
The [[Integers form Integral Domain|integral domain of integers]] $\struct {\Z, +, \times}$ forms a [[Definition:Subdomain|subdomain]] of the [[Definition:Field of Real Numbers|field of real numbers]].	0
The operation of [[Definition:Rational Multiplication|multiplication]] on the [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] $\Q$ is [[Definition:Well-Defined Operation|well-defined]] and [[Definition:Closed Algebraic Structure|closed]]: :$\forall x, y \in \Q: x \times y \in \Q$	0
Let $f$, $g$ be [[Definition:Function|functions]] from $\left [{0 \,.\,.\, \to} \right ) \to \mathbb F$ of a [[Definition:Independent Variable|real variable]] $t$, where $\mathbb F \in \left\{ {\R, \C}\right\}$. Further let $f$ and $g$ be [[Definition:Continuity|continuous]] everywhere on their [[Definition:Domain of Mapping|domains]]. Let $f$ and $g$ both admit [[Definition:Laplace Transform|Laplace transforms]]. Suppose that the Laplace transforms $\mathcal L \left\{{f}\right\}$ and $\mathcal L \left\{{g}\right\}$ satisfy: :$\forall t \ge 0: \mathcal L \left\{{f\left({t}\right)}\right\} = \mathcal L \left\{{g\left({t}\right)}\right\}$ Then $f = g$ everywhere on $\left[{0 \,.\,.\, \to}\right)$.	0
There exists at least one example of a [[Definition:Topological Space|topological space]] which is a [[Definition:T2 Space|$T_2$ (Hausdorff) space]], but is not also a [[Definition:Completely Hausdorff Space|completely Hausdorff space]].	0
From [[Hilbert Sequence Space is Metric Space]], $\ell^2$ is a [[Definition:Metric Space|metric space]]. Let $x = \sequence {x_i} \in A$ be a point of $\ell^2$. Consider the [[Definition:Closed Ball|closed $\epsilon$-ball]] of $x$ in $\ell^2$: :$\map { {B_\epsilon}^-} x := \set {y \in A: \map {d_2} {x, y} \le \epsilon}$ for some $\epsilon \in \R_{>0}$. Consider the point: :$\sequence {y_n} = \tuple {x_1, x_2, \ldots, x_{n - 1}, x_n + \epsilon, x_{n + 1}, \ldots}$ We have that $\sequence {y_n} \in \map { {B_\epsilon}^-} x$. But for $m \ne n$ we have that: :$\map {d_2} {y_m, y_n} = \epsilon \sqrt 2$ and so $\sequence {y_n}$ has no [[Definition:Convergent Sequence (Metric Space)|convergent]] [[Definition:Subsequence|subsequence]]. Thus $\map { {B_\epsilon}^-} x$ is not [[Definition:Compact Topological Subspace|compact]]. So $x$ has no [[Definition:Compact Topological Subspace|compact]] [[Definition:Neighborhood of Point|neighborhood]]. {{qed}}	0
We have by definition of [[Definition:Vacuous Summation|vacuous summation]] that: :$\displaystyle \forall n \in \Z: n < 0: \sum_{i \mathop = 0}^n \binom n 1 = 0$ Then from [[Zero Choose Zero]]: :$\displaystyle \sum_{i \mathop = 0}^0 \binom 0 0 = 1$ Let $n > 0$. The assertion can be expressed: :$\displaystyle \sum_{i \mathop \le n} \left({-1}\right)^i \binom n i = 0$ for all $n > 0$ as $\dbinom n i = 0$ when $i < 0$ by definition of [[Definition:Binomial Coefficient|binomial coefficient]]. From [[Alternating Sum and Difference of r Choose k up to n]] we have: :$\displaystyle \sum_{i \mathop \le n} \left({-1}\right)^i \binom r i = \left({-1}\right)^n \binom {r - 1} n$ Putting $r = n$ we have: :$\displaystyle \sum_{i \mathop \le n} \left({-1}\right)^i \binom n i = \left({-1}\right)^n \binom {n - 1} n$ As $n - 1 < n$ it follows from the definition of [[Definition:Binomial Coefficient|binomial coefficient]] that: :$\dbinom {n - 1} n = 0$ {{qed}}	0
Let the four [[Definition:Natural Number|(natural) numbers]] $A, B, C, D$ be [[Definition:Proportion|proportional]] so that $A : B = C : D$. We need to show that $A : C = B : D$. :[[File:Euclid-VII-13.png|250px]] We have that $A : B = C : D$. So from {{EuclidDefLink|VII|20|Proportional}} we have that whatever [[Definition:Aliquot Part|aliquot part]] or [[Definition:Aliquant Part|aliquant part]] $A$ is of $B$, the same [[Definition:Aliquot Part|aliquot part]] or [[Definition:Aliquant Part|aliquant part]] is $C$ of $D$. So from {{EuclidPropLink|book = VII|prop = 10|title = Multiples of Alternate Ratios of Equal Fractions}}, whatever [[Definition:Aliquot Part|aliquot part]] or [[Definition:Aliquant Part|aliquant part]] $A$ is of $C$, the same [[Definition:Aliquot Part|aliquot part]] or [[Definition:Aliquant Part|aliquant part]] is $B$ of $D$. Therefore from {{EuclidDefLink|VII|20|Proportional}} $A : C = B : D$. {{Qed}} {{Euclid Note|13|VII}}	0
Let $J$ be a [[Definition:Twice Differentiable Functional|twice differentiable functional]]. Let $J$ have an [[Definition:Extremum of Functional|extremum]] for $y=\hat y$. Let the [[Definition:Second Variation of Functional|second variation]] $\delta^2 J \sqbrk {\hat y; h}$ be [[Definition:Strongly Positive Quadratic Functional|strongly positive]] {{WRT}} $h$. Then $J$ acquires the [[Definition:Minimum Value of Functional|minimum]] for $y = \hat y$ .	0
Let $d := \gcd \set {a, b}$. Then by definition of the [[Definition:Greatest Common Divisor of Integers|GCD]], there exist $j_1, j_2 \in \Z$ such that $a = d j_1$ and $b = d j_2$. Because $d$ [[Definition:Divisor of Integer|divides]] both $a$ and $b$, it must [[Definition:Divisor of Integer|divide]] their [[Definition:Integer Multiplication|product]]: :$\exists l \in \Z$ such that $a b = d l$ Then we have: {{begin-eqn}} {{eqn | l = d l | m = \paren {d j_1} b | mo= = | r = a \paren {d j_2} }} {{eqn | ll= \leadsto | l = l | m = j_1 b | mo= = | r = a j_2 }} {{end-eqn}} showing that $a \divides l$ and $b \divides l$. That is, $l$ is a [[Definition:Common Multiple|common multiple]] of $a$ and $b$. Now it must be shown that $l$ is the least such number. Let $m$ be any [[Definition:Common Multiple|common multiple]] of $a$ and $b$. Then there exist $k_1, k_2 \in \Z$ such that $m = a k_1 = b k_2$. By [[Bézout's Lemma]]: :$\exists x, y \in \Z: d = a x + b y$ So: {{begin-eqn}} {{eqn | l = m d | r = m a x + m b y }} {{eqn | r = \paren {b k_2} a x + \paren {a k_1} b y }} {{eqn | r = a b \paren {b k_2 + a k_1} }} {{eqn | r = d l \paren {b k_2 + a k_1} }} {{end-eqn}} Thus: :$m = l \paren {b k_2 + a k_1}$ that is, $l \divides m$. Hence by definition of the [[Definition:Lowest Common Multiple of Integers|LCM]]: :$\lcm \set {a, b} = l$ In conclusion: :$a b = d l = \gcd \set {a, b} \cdot \lcm \set {a, b}$ {{qed}}	0
From the definition of the [[Definition:Sine|sine function]], we have: :$\displaystyle \sin x = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!}$ From [[Radius of Convergence of Power Series over Factorial]], this series converges for all $x$. From [[Power Series is Differentiable on Interval of Convergence]]: {{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\sin x} | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!} | c = }} {{end-eqn}} The result follows from the definition of the [[Definition:Cosine|cosine function]]. {{qed}}	0
Let $x, y \in \R$ be [[Definition:Real Number|real numbers]]. Let $x \bmod y$ denote the [[Definition:Modulo Operation|modulo operation]]: :$x \bmod y := \begin{cases} x - y \floor {\dfrac x y} & : y \ne 0 \\ x & : y = 0 \end{cases}$ where $\floor {\dfrac x y}$ denotes the [[Definition:Floor Function|floor]] of $\dfrac x y$. Let $y > 0$. Then: :$0 \le x \bmod y < y$	0
Let $k$ be a [[Definition:Strictly Positive Integer|strictly positive integer]]. Let $X \sim t_k$ where $t_k$ is the [[Definition:Student's t-Distribution|$t$-distribution]] with $k$ degrees of freedom. Then: :$X^2 \sim F_{1, k}$ where $F_{1, k}$ is the [[Definition:F-Distribution|$F$-distribution]] with $\tuple {1, k}$ degrees of freedom.	0
Let $D$ be an [[Definition:Integral Domain|integral domain]]. Let $D \sqbrk X$ be the [[Definition:Ring of Polynomial Forms|ring of polynomial forms]] in $X$ over $D$. Let $\map P D$ be the [[Definition:Ring of Polynomial Functions|ring of polynomial functions]] over $D$. The mapping $\kappa: D \sqbrk X \to \map P D$ given by: :$\displaystyle \map \kappa {\sum_{k \mathop = 0}^n {a_k \circ X^k} } = f$ where $\displaystyle f = \sum_{k \mathop = 0}^n {a_k \circ x^k}, x \in D$ is a [[Definition:Ring Epimorphism|ring epimorphism]].	0
For all $n \in \N$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{i \mathop = 0}^n \binom n i^2 = \binom {2 n} n$ $P(0)$ is true, as this just says: :$\dbinom 0 0^2 = 1 = \dbinom {2 \times 0} 0$ This holds by [[Definition:Binomial Coefficient|definition]]. === Basis for the Induction === $P(1)$ is true, as this just says: :$\dbinom 1 0^2 + \dbinom 1 1^2 = 1^2 + 1^2 = 2 = \dbinom 2 1$ This also holds by [[Definition:Binomial Coefficient|definition]]. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{i \mathop = 0}^k \binom k i^2 = \binom {2 k} k$ Then we need to show: :$\displaystyle \sum_{i \mathop = 0}^{k + 1} \binom {k + 1} i^2 = \binom {2 \left({k + 1}\right)} {k + 1}$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 0}^{k + 1} \binom {k + 1} i^2 | r = \binom {k + 1} 0^2 + \sum_{i \mathop = 1}^k \binom {k + 1} i^2 + \binom {k + 1} {k + 1}^2 | c = }} {{eqn | r = 1 + \sum_{i \mathop = 1}^k \left({\binom k {i - 1} + \binom k i}\right)^2 + 1 | c = [[Pascal's Rule]] }} {{eqn | r = 1 + \sum_{i \mathop = 1}^k \left({\binom k {i - 1}^2 + \binom k i^2 + 2 \binom k {i - 1} \binom k i}\right) + 1 | c = }} {{eqn | r = \left({\sum_{i \mathop = 0}^{k - 1} \binom k i^2 + 1}\right) + \left({1 + \sum_{i \mathop = 1}^k \binom k i^2}\right) + 2 \sum_{i \mathop = 1}^k \left({\binom k {i - 1} \binom k i}\right) | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \left({\sum_{i \mathop = 0}^{k - 1} \binom k i^2 + \binom k k^2}\right) + \left({\binom k 0^2 + \sum_{i \mathop = 1}^k \binom k i^2}\right) + 2 \sum_{i \mathop = 1}^k \left({\binom k {i - 1} \binom k i}\right) | c = }} {{eqn | r = \sum_{i \mathop = 0}^k \binom k i^2 + \sum_{i \mathop = 0}^k \binom k i^2 + 2 \sum_{i \mathop = 1}^k \left({\binom k {i-1} \binom k i}\right) | c = }} {{eqn | r = \binom {2 k} k + \binom {2 k} k + 2 \sum_{i \mathop = 1}^k \left({\binom k {i - 1} \binom k i}\right) | c = [[Sum of Squares of Binomial Coefficients/Inductive Proof#Induction Hypothesis|Induction Hypothesis]] }} {{end-eqn}} Now we look at $\displaystyle 2 \sum_{i \mathop = 1}^k \left({\binom k {i - 1} \binom k i}\right)$. Using the [[Chu-Vandermonde Identity]]: :$\displaystyle \sum_i \binom r i \binom s {n - i} = \binom {r + s} n$ From the [[Symmetry Rule for Binomial Coefficients]], this can be written: :$\displaystyle \sum_i \binom r i \binom s {s - n + i} = \binom {r + s} n$ Putting $r = k, s = k, s - n = -1$ from whence $n = k + 1$: :$\displaystyle \sum_i \binom k i \binom k {i - 1} = \binom {2 k} {k + 1}$ So: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 0}^{k + 1} \binom {k + 1} i^2 | r = 2 \binom {2 k} k + 2 \sum_i \left({\binom k {i-1} \binom k i}\right) | c = because when $i \le 0$ and $i > k$ we have $\dbinom k {i-1} \dbinom k i = 0$ }} {{eqn | r = 2 \binom {2 k} k + 2 \binom {2 k} {k + 1} | c = }} {{eqn | r = 2 \binom {2 k + 1} {k + 1} | c = [[Pascal's Rule]] }} {{eqn | r = \binom {2 k + 1} k + \binom {2 k + 1} {k + 1} | c = [[Symmetry Rule for Binomial Coefficients]] }} {{eqn | r = \binom {2 k + 2} {k + 1} | c = [[Pascal's Rule]] }} {{eqn | r = \binom {2 \left({k + 1}\right)} {k + 1} | c = }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \N: \sum_{i \mathop = 0}^n \binom n i^2 = \binom {2 n} n$ {{qed}}	0
:[[File:Complex-Addition-as-Parallelogram.png|400px]] Let $a = a_x + i a_y$ and $b = b_x + i b_y$. Then by definition of [[Definition:Complex Addition|complex addition]]: :$a + b = \paren {a_x + b_x} + i \paren {a_y + b_y}$ Thus $\mathbf a + \mathbf b$ is the [[Definition:Complex Number as Vector|vector]] whose [[Definition:Component of Vector|components]] are $a_x + b_x$ and $a_y + b_y$. Similarly, we have: :$b + a = \paren {b_x + a_x} + i \paren {b_y + a_y}$ Thus $\mathbf b + \mathbf a$ is the [[Definition:Complex Number as Vector|vector]] whose [[Definition:Component of Vector|components]] are $b_x + a_x$ and $b_y + a_y$. It follows that both $\mathbf a + \mathbf b$ and $\mathbf b + \mathbf a$ both correspond to the [[Definition:Diagonal of Parallelogram|diagonal]] $OC$ of $OACB$. {{qed}}	0
Let $U$ be an [[Definition:Open Set (Metric Space)|open set]] of $M$ which has $x$ as an [[Definition:Element|element]]. Then by definition of an [[Definition:Open Set (Metric Space)|open set]]: :$\exists \epsilon \in \R_{>0}: \map {B_\epsilon} x \subseteq U$ From [[Open Ball of Metric Space is Open Set]], $\BB_x$ is a [[Definition:Set|set]] of [[Definition:Open Set (Metric Space)|open set]] which have $x$ as an [[Definition:Element|element]]. By definition of a [[Definition:Local Basis|local basis]], $\BB_x$ is a [[Definition:Local Basis|local basis]] of $x$. {{qed}} [[Category:Open Balls]] n2gebf0i8dwl8gaa0qxbfvgnozetg1q	0
Let $a, b \in \set {x \in \R_{>0} : x^2 \in \Q}$ be the lengths of [[Definition:Rational Line Segment|rational line segments]]. Furthermore, let $\dfrac a b \in \Q$. Then, $a b \in \Q$. {{:Euclid:Proposition/X/19}}	0
Let $\dfrac p q$ be a positive [[Definition:Rational Number|rational number]] with $p < q$. Then: :$\displaystyle \psi \left({\frac p q}\right) = -\gamma - \ln 2 q - \frac \pi 2 \cot \left({\frac p q \pi}\right) + 2 \sum_{n \mathop = 1}^{\left\lceil{q / 2}\right\rceil - 1} \cos \left({\frac {2 \pi p n} q}\right) \ln\left({\sin \left({\frac {\pi n} q}\right)}\right)$ where: :$\psi$ is the [[Definition:Digamma Function|digamma function]] :$\cot$ is the [[Definition:Real Cotangent Function|cotangent function]] :$\ln$ is the [[Definition:Real Natural Logarithm|natural logarithm]].	0
From the [[Binomial Theorem/Integral Index|binomial theorem]]: {{begin-eqn}} {{eqn | l = \paren {x + y}^n | r = \sum_{k \mathop = 0}^n \binom n k y^k x^{n - k} }} {{eqn | ll = \leadsto | l = 1 | r = \sum_{k \mathop = 0}^n \binom n k t^k \paren {1 - t}^{n - k} | c = $y = t, ~x = 1 - t$ }} {{eqn | l = 0 | r = \sum_{k \mathop = 0}^n \binom n k \paren {k t^{k - 1} \paren {1 - t}^{n - k} - t^k \paren{n - k} \paren {1 - t}^{n - k - 1} } | c = [[Definition:Derivative of Real Function|Derivative]] {{WRT}} $t$ }} {{eqn | r = \sum_{k \mathop = 0}^n k p_{n,k} \paren {\frac 1 t + \frac 1 {1 - t} } - \frac n {1 - t} }} {{eqn | ll = \leadsto | l = n t | r = \sum_{k \mathop = 0}^n k p_{n,k} }} {{end-eqn}} {{qed}}	0
Let $S$ be a [[Definition:Set|set]] of [[Definition:Real Number|real numbers]]. Let $S$ have a [[Definition:Supremum of Subset of Real Numbers|supremum]]. Let $T$ be a [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subset]] of $S$. Then $\sup T$ exists and: :$\sup T \le \sup S$	0
Let $P = \tuple {x, y}$ be a [[Definition:Point|point]] on the [[Definition:Circumference of Circle|circumference]] of a [[Definition:Unit Circle|unit circle]] whose [[Definition:Center of Circle|center]] is at the [[Definition:Origin|origin]] of a [[Definition:Cartesian Plane|cartesian plane]]. From [[Sine of Angle in Cartesian Plane]] and [[Cosine of Angle in Cartesian Plane]]: :$P = \tuple {\cos \theta, \sin \theta}$ The [[Definition:Graph of Relation|graph]] of the [[Definition:Unit Circle|unit circle]] is the [[Definition:Locus|locus]] of: :$x^2 + y^2 = 1$ as given by [[Equation of Circle]]. Substituting $x = \cos \theta$ and $y = \sin \theta$ yields: :$\cos^2 \theta + \sin^2 \theta = 1$ {{qed}}	0
Let $C_N$ be the square with vertices $\left({N + \frac 1 2}\right) \left({\pm 1 \pm i}\right)$ for $N \in \N$. Then there exists a constant $A$ independent of $N$ such that: :$\displaystyle \left\vert{\cot \left({\pi z}\right)}\right\vert < A$ for all $z$ on $C_N$.	0
All elements of a [[Definition:Group|group]] are [[Definition:Invertible Element|invertible]], so we can directly use the result from [[Index Laws for Monoids/Sum of Indices|Index Laws for Monoids: Sum of Indices]]: :$\forall m, n \in \Z: g^m \circ g^n = g^{m + n}$ {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Trivial Topological Space|trivial topological space]]. Then $T$ is [[Definition:Non-Meager Space|non-meager]].	0
Let $\zeta$ be the [[Definition:Riemann Zeta Function|Riemann zeta function]]. Then for all $t \in \R$, $\zeta(1 + it) \neq 0$	0
The total number of cards in a standard deck is $52$. The number of cards in a single bridge hand is $13$. Thus $N$ is equal to the number of ways $13$ things can be chosen from $52$. Thus: {{begin-eqn}} {{eqn | l = N | r = \dbinom {52} {23} | c = [[Cardinality of Set of Subsets]] }} {{eqn | r = \frac {52!} {13! \left({52 - 13}\right)!} | c = {{Defof|Binomial Coefficient}} }} {{eqn | r = \frac {52!} {13! \, 39!} | c = }} {{eqn | r = 635 \ 013 \ 559 \ 600 | c = after calculation }} {{end-eqn}} {{qed}}	0
=== Necessary Condition === Suppose that $n = 0$ or $m = 0$. Then from [[Zero is Zero Element for Natural Number Multiplication]]: :$m \times n = 0$ {{qed|lemma}} === Sufficient Condition === Let $m \times n = 0$. Suppose [[Definition:WLOG|WLOG]] that $n \ne 0$. {{begin-eqn}} {{eqn | l = n | o = \ne | r = 0 | c = }} {{eqn | ll= \implies | l = 1 | o = \le | r = n | c = Definition of [[Definition:One|One]] }} {{eqn | ll= \implies | l = m \times n | r = m \times \left({\left({n - 1}\right) + 1}\right) | c = Definition of [[Definition:Difference (Natural Numbers)|Difference]] }} {{eqn | r = m \times \left({n - 1}\right) + m | c = [[Natural Number Multiplication Distributes over Addition]] }} {{eqn | ll= \implies | l = 0 \le m | o = \le | r = m \times \left({n - 1}\right) + m | c = }} {{end-eqn}} But as: : $m \times \left({n - 1}\right) \circ m = m \times n = 0$ it follows that: :$0 \le m \le 0$ and so as $\le$ is [[Definition:Antisymmetric Relation|antisymmetric]], it follows that $m = 0$. {{qed}}	0
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]]. Let $F$ be a [[Definition:Closed Set (Metric Space)|closed set]] of $M$. Let $H \subseteq F$ be a [[Definition:Subset|subset]] of $F$. Let $H^-$ denote the [[Definition:Closure (Metric Space)|closure]] of $H$. Then $H^- \subseteq F$.	0
:$\mathrm d \left({\arctan \dfrac x y}\right) = \dfrac{y \, \mathrm d x - x \, \mathrm d y} {x^2 + y^2}$	0
:$\forall \alpha \in \R: \map T {\alpha f} = \alpha \map T f$	0
Let $P = \left({A, d}\right)$ be a [[Definition:Pseudometric Space|pseudometric space]]. Then $\varnothing$ and $A$ are both [[Definition:Open Set (Pseudometric Space)|open]] in $P$.	0
Let $n \in \N_{>0}$. Let $f: \R \to \R$ be the [[Definition:Real Function|real function]] defined as $\map f x = x^{1 / n}$. Then: :$\map {f'} x = n x^{n - 1}$ everywhere that $\map f x = x^n$ is defined. When $x = 0$ and $n = 0$, $\map {f'} x$ is undefined.	0
Let $\map f t = \sin \sqrt t$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = \dfrac {\cos \sqrt t} {2 \sqrt t} | c = }} {{eqn | l = \map f 0 | r = 0 | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \laptrans {\map {f'} t} | r = \dfrac 1 2 \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | c = }} {{eqn | r = s \, \map F s - \map f 0 | c = [[Laplace Transform of Derivative]] }} {{eqn | r = \dfrac {\sqrt \pi} {2 s^{1/2} } \map \exp {-\dfrac 1 {4 s} } | c = [[Laplace Transform of Sine of Root]] }} {{eqn | ll= \leadsto | l = \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | r = \sqrt {\dfrac \pi s} \, \map \exp {-\dfrac 1 {4 s} } | c = }} {{end-eqn}} {{qed}}	0
:$\tan 330^\circ = \tan \dfrac {11 \pi} 6 = -\dfrac {\sqrt 3} 3$	0
=== [[Product Space is T3 1/2 iff Factor Spaces are T3 1/2/Product Space is T3 1/2 implies Factor Spaces are T3 1/2|Necessary Condition]] === {{:Product Space is T3 1/2 iff Factor Spaces are T3 1/2/Product Space is T3 1/2 implies Factor Spaces are T3 1/2}}{{qed|lemma}} === [[Product Space is T3 1/2 iff Factor Spaces are T3 1/2/Factor Spaces are T3 1/2 implies Product Space is T3 1/2|Sufficient Condition]] === {{:Product Space is T3 1/2 iff Factor Spaces are T3 1/2/Factor Spaces are T3 1/2 implies Product Space is T3 1/2}}{{qed}}	0
:$\ds \int \frac {\d x} {p + q \sinh a x} = \frac 1 {a \sqrt{p^2 + q^2} } \ln \size {\frac {q e^{a x} + p - \sqrt {p^2 + q^2} } {q e^{a x} + p + \sqrt {p^2 + q^2} } } + C$	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Then: {{begin-eqn}} {{eqn | l = E_{2 n} | r = \sum_{k \mathop = 0}^{n - 1} \dbinom {2 n} {2 k} E_{2 n - 2 k} | c = }} {{eqn | r = \binom {2 n} 2 E_{2 n - 2} + \binom {2 n} 4 E_{2 n - 4} + \binom {2 n} 6 E_{2 n - 6} + \cdots + 1 | c = }} {{end-eqn}} where $E_n$ denotes the $n$th [[Definition:Euler Numbers|Euler number]].	0
Let $\map S x$ be a [[Definition:Trigonometric Series|trigonometric series]]: :$\map S x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos n x + b_n \sin n x}$ Let the [[Definition:Series|series]]: :$\displaystyle \sum_{n \mathop = 1}^\infty \paren {\size {a_n} + \size {b_n} }$ be [[Definition:Convergent Series of Numbers|convergent]]. Then $S$ is a [[Definition:Convergent Series of Numbers|convergent series]].	0
=== Definition 1 implies Definition 3 === Let $G$ be an [[Definition:Arborescence/Definition 1|$r$-arborescence by definition 1]]. Let $v \in V$ such that $v \ne r$. Then there is exactly one [[Definition:Directed Walk|directed walk]] $w$ from $r$ to $v$. Since $v \ne r$, either: :$w = \tuple {r, v}$ or: :$\exists m \in V: w = \tuple {r, \ldots, m, v}$ Thus $v$ is the [[Definition:Final Vertex of Arc|final vertex]] of the [[Definition:Arc of Digraph|arc]] $r v$ or the [[Definition:Arc of Digraph|arc]] $m v$. {{AimForCont}} that $v$ is the [[Definition:Final Vertex of Arc|final vertex]] of [[Definition:Distinct|distinct]] [[Definition:Arc of Digraph|arc]] $x v$ and $y v$. Then there exist [[Definition:Directed Walk|directed walks]] $w_1$ and $w_2$ from $r$ to $x$ and $r$ to $y$ respectively. But appending $v$ to $w_1$ and to $w_2$ yields [[Definition:Distinct|distinct]] [[Definition:Directed Walk|directed walks]] from $r$ to $v$. This [[Definition:Contradiction|contradicts]] the fact that there is [[Definition:Unique|exactly one]] such [[Definition:Directed Walk|directed walk]]. Thus $v$ is the [[Definition:Final Vertex of Arc|final vertex]] of [[Definition:Unique|exactly one]] [[Definition:Arc of Digraph|arc]]. {{qed|lemma}} {{AimForCont}} that $r$ is the [[Definition:Final Vertex of Arc|final vertex]] of an [[Definition:Arc of Digraph|arc]] $x r$. By [[Definition:Arborescence/Definition 1|Definition 1]], there is a [[Definition:Directed Walk|directed walk]] $w$ from $r$ to $x$. But then $w$ appended to $w$ is a [[Definition:Directed Walk|directed walk]] from $r$ to $x$ which is not equal to $w$. This [[Definition:Contradiction|contradicts]] [[Definition:Arborescence/Definition 1|definition 1]]. Thus we conclude that $r$ is not the [[Definition:Final Vertex of Arc|final vertex]] of any [[Definition:Arc of Digraph|arc]]. It follows immediately from [[Definition:Arborescence/Definition 1|definition 1]] that there is a [[Definition:Directed Walk|directed walk]] from $r$ to each [[Definition:Vertex of Graph|vertex]] $v \ne r$. Thus $G$ is an [[Definition:Arborescence/Definition 3|$r$-arborescence by Definition 3]]. {{qed|lemma}} === Definition 3 implies Definition 1 === Suppose that $G$ is an $r$-arborescence by [[Definition:Arborescence/Definition 3|Definition 3]]. Let $v \in V$. We must show that there is a [[Definition:Unique|unique]] [[Definition:Directed Walk|directed walk]] from $r$ to $v$. If $v = r$, then $\tuple r$ is a [[Definition:Directed Walk|directed walk]] from $r$ to $v$. Since $r$ is not the [[Definition:Final Vertex of Arc|final vertex]] of any [[Definition:Arc of Digraph|arc]], $\tuple r$ is the only such [[Definition:Directed Walk|directed walk]]. If $v \ne r$, then there exists some [[Definition:Directed Walk|directed walk]] $w$ from $r$ to $v$. Suppose that $z$ is a [[Definition:Directed Walk|directed walk]] from $r$ to $v$. Since $v \ne r$, $v$ is the [[Definition:Final Vertex of Arc|final vertex]] of exactly one [[Definition:Arc of Digraph|arc]] $x v$. If $x = r$, then $z$ must end with $\tuple {r, v}$. But $r$ is not the [[Definition:Final Vertex of Arc|final vertex]] of any [[Definition:Arc of Digraph|arc]]. So in fact: :$z = \tuple {r, v}$ If $x \ne r$, then: :$z = \tuple {r, \ldots, x, v}$ Continuing inductively from $x$ proves that $z = w$. {{explain|set up the induction more formally, or less formally, or something. This is messy.}} Thus for each $v \in V$ there is [[Definition:Unique|exactly one]] [[Definition:Directed Walk|directed walk]] from $r$ to $v$. So $G$ is an [[Definition:Arborescence/Definition 1|$r$-arborescence by Definition 1]]. {{qed|lemma}} === Definitions 1 and 3 imply Definition 2 === Let $G$ be an [[Definition:Arborescence/Definition 1|$r$-arborescence by Definition 1]]. From the above, $G$ is then also an [[Definition:Arborescence/Definition 3|$r$-arborescence by Definition 3]]. Let $T = \struct {V, E}$ be the [[Definition:Simple Graph|simple graph]] corresponding to $G$. That is, for $x, y \in V$, let $\set {x, y} \in E$ {{iff}} $\tuple {x, y} \in A$ or $\tuple {y, x} \in A$. We will show that $G$ is an [[Definition:Orientation (Graph Theory)|orientation]] of $T$ and that $T$ is a [[Definition:Tree (Graph Theory)|tree]]. By [[Definition:Arborescence/Definition 1|Definition 1]], there is [[Definition:Unique|exactly one]] [[Definition:Directed Walk|directed walk]] from $r$ to each [[Definition:Vertex (Graph Theory)|vertex]] $v$. Let $x, y \in V$ and suppose [[Proof by Contradiction|for the sake of contradiction]] that $\tuple {x, y} \in A$ and $\tuple {y, x} \in A$. Let $w_x$ be the [[Definition:Directed Walk|directed walk]] from $r$ to $x$. Then appending $y$ to $w_x$ yields a [[Definition:Directed Walk|directed walk]] $w_x + y$ from $r$ to $y$. But then appending $x$ to $w_x + y$ yields a [[Definition:Directed Walk|directed walk]] $w_x + y + x$ from $r$ to $x$, contradicting the fact that $w_x$ is unique. Thus $A$ is [[Definition:Asymmetric Relation|asymmetric]]. So $G$ is an [[Definition:Orientation (Graph Theory)|orientation]] of $T$. We must now show that $T$ is a [[Definition:Tree (Graph Theory)|tree]]. By [[Equivalence of Definitions of Reachable]], each [[Definition:Vertex of Graph|vertex]] $v$ of $G$ is [[Definition:Reachable|reachable]] from $r$. Let $x, y \in V$. By [[Definition:Arborescence/Definition 1|Definition 1]], there is [[Definition:Unique|exactly one]] [[Definition:Directed Walk|directed walk]] from $r$ to $x$ and [[Definition:Unique|exactly one]] [[Definition:Directed Walk|directed walk]] from $r$ to $y$. Thus there is a [[Definition:Walk (Graph Theory)|walk]] $w_x$ in $T$ from $r$ to $x$ and a [[Definition:Walk (Graph Theory)|walk]] $w_y$ from $r$ to $y$. Reversing $w_x$ and then appending $w_y$ to it (eliding the duplicate $r$) yields a [[Definition:Walk (Graph Theory)|walk]] from $x$ to $y$. Thus $T$ is [[Definition:Connected Graph|connected]]. Next we show that $G$ has no [[Definition:Directed Walk|directed]] [[Definition:Cycle (Graph Theory)|cycle]]. {{AimForCont}} that $x_0, x_1, \dots, x_n$ is a [[Definition:Directed Walk|directed walk]] with $n \ge 2$ and $x_0 = x_n$. By [[Definition:Arborescence/Definition 1|Definition 1]], there is a [[Definition:Unique|unique]] [[Definition:Directed Walk|directed walk]] $w$ from $r$ to $x_0$. But then $w + \tuple {x_1, \dots, x_n}$ is another [[Definition:Directed Walk|directed walk]] from $r$ to $x_0$, contradicting [[Definition:Unique|uniqueness]]. Thus $G$ has no [[Definition:Directed Walk|directed]] [[Definition:Cycle (Graph Theory)|cycles]]. {{AimForCont}} that $T$ has a [[Definition:Cycle (Graph Theory)|cycle]] $\tuple {x_0, x_1, \dots, x_n}$, where $x_0 = x_n$ and $n \ge 2$. Since $G$ has no [[Definition:Directed Walk|directed]] [[Definition:Cycle (Graph Theory)|cycles]], the [[Definition:Arc of Digraph|arcs]] corresponding to the [[Definition:Edge of Graph|edges]] in this [[Definition:Cycle (Graph Theory)|cycle]] cannot all go in the same direction around the [[Definition:Cycle (Graph Theory)|cycle]]. But this implies that some [[Definition:Vertex of Graph|vertex]] of $x_0, x_1, \dots, x_n$ is the [[Definition:Final Vertex of Arc|final vertex]] of two different [[Definition:Arc of Digraph|arcs]], contradicting [[Definition:Arborescence/Definition 3|Definition 3]]. {{explain}} Thus $T$ has no [[Definition:Cycle (Graph Theory)|cycles]]. As $T$ is [[Definition:Connected Graph|connected]], it is a [[Definition:Tree (Graph Theory)|tree]]. Thus we have shown that $G$ is an [[Definition:Orientation (Graph Theory)|orientation]] of $T$ and that $T$ is a [[Definition:Tree (Graph Theory)|tree]]. {{qed|lemma}} === Definition 2 implies Definition 1 === Let $G$ be an [[Definition:Arborescence/Definition 2|$r$-arborescence by Definition 2]]. That is, let $G$ be an [[Definition:Orientation (Graph Theory)|orientation]] of a [[Definition:Tree (Graph Theory)|tree]] $T$ and that every [[Definition:Vertex of Graph|vertex]] of $G$ is [[Definition:Reachable|reachable]] from $r$. Let $v \in V$. By the definition of [[Definition:Reachable/Definition 2|reachable]], there exists a [[Definition:Directed Walk|directed walk]] $w$ from $r$ to $v$. We must show that $w$ is the [[Definition:Unique|only]] [[Definition:Directed Walk|directed walk]] from $r$ to $v$. {{AimForCont}} that $z$ is a [[Definition:Directed Walk|directed walk]] from $r$ to $v$ and $z \ne v$. Then either $z$ extends $w$, $w$ extends $z$, or there is some $k$ such that $w_k \ne z_k$. First suppose that $z$ extends $w$. So $w = \tuple {x_0, \ldots, x_n}$ and $z = \tuple {x_0, \ldots, x_n, \ldots, x_m}$. Then $p = \left({x_n, \dots, x_m}\right)$ is a [[Definition:Directed Walk|directed walk]] from $v$ to $v$ with more than one [[Definition:Vertex of Graph|vertex]]. By [[Directed Circuit in Simple Digraph forms Circuit]], the [[Definition:Vertex of Graph|vertices]] of $p$ form a [[Definition:Circuit|circuit]], contradicting the fact that $T$ is a [[Definition:Tree (Graph Theory)|tree]]. Thus there exists $k$ such that $w_k \ne z_k$. It follows from the [[Well-Ordering Principle]] that there must be some [[Definition:Smallest Element|smallest]] $k$ such that $w_k \ne z_k$. Since $w$ and $z$ both end at $v$, it follows from the [[Well-Ordering Principle]] that: :there must be some [[Definition:Smallest Element|smallest]] $n > k$ such that there exists $m > k$ such that $w_n = z_m$ :and that there exists a [[Definition:Smallest Element|smallest]] such $m$. Then $\tuple {w_{k - 1}, w_k, \ldots, w_n, z_{m - 1}, \dots, z_k, z_{k - 1} }$ forms a [[Definition:Cycle (Graph Theory)|cycle]], contradicting the fact that $T$ is a [[Definition:Tree (Graph Theory)|tree]]. {{explain}} {{qed}} [[Category:Arborescences]] 3313oj1kghruvj5qbb8ynvtui1llsm8	0
=== [[Complex Sequence is Cauchy iff Convergent/Lemma 1|Lemma]] === {{:Complex Sequence is Cauchy iff Convergent/Lemma 1}} Let $\sequence {x_n}$ be a [[Definition:Real Sequence|real sequence]] where: :$x_n = \Re \paren {z_n}$ for every $n$ :$\Re \paren {z_n}$ is the [[Definition:Real Part|real part]] of $z_n$ Let $\sequence {y_n}$ be a [[Definition:Real Sequence|real sequence]] where :$y_n = \Im \paren {z_n}$ for every $n$ :$\Im \paren {z_n}$ is the [[Definition:Imaginary Part|imaginary part]] of $z_n$ We find: :$\sequence {z_n}$ is a [[Definition:Complex Cauchy Sequence|Cauchy sequence]] :$\iff \sequence {x_n}$ and $\sequence {y_n}$ are [[Definition:Real Cauchy Sequence|Cauchy sequences]] by [[Complex Sequence is Cauchy iff Convergent/Lemma 1|Lemma]] :$\iff \sequence {x_n}$ and $\sequence {y_n}$ are [[Definition:Convergent Real Sequence|convergent]] by [[Real Sequence is Cauchy iff Convergent]] :$\iff \sequence {z_n}$ is [[Definition:Convergent Complex Sequence|convergent]] by definition of [[Definition:Convergent Complex Sequence|convergent complex sequence]] {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {\cos a t} | r = \laptrans {\frac {e^{i a t} + e^{-i a t} } 2} | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 1 2 \paren {\laptrans {e^{i a t} } + \laptrans {e^{-i a t} } } | c = [[Linear Combination of Laplace Transforms]] }} {{eqn | r = \frac 1 2 \paren {\frac 1 {s - i a} + \frac 1 {s + i a} } | c = [[Laplace Transform of Exponential]] }} {{eqn | r = \frac 1 2 \paren {\frac {s + i a + s - i a} {s^2 + a^2} } | c = simplifying }} {{eqn | r = \frac s {s^2 + a^2} | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int_0^\infty e^{-a x} \cos b x \rd x | r = \intlimits {\frac {e^{-a x} \paren {-a \cos b x + b \sin b x} } {a^2 + b^2} } 0 \infty | c = [[Primitive of Exponential of a x by Cosine of b x|Primitive of $e^{a x} \cos b x$]] }} {{eqn | r = \lim_{x \mathop \to \infty} \paren {\frac {e^{-a x} \paren {-a \cos b x + b \sin b x} } {a^2 + b^2} } + \frac {e^0 \paren {a \cos 0 - b \sin 0} } {a^2 + b^2} }} {{eqn | r = \lim_{x \mathop \to \infty} \paren {\frac {e^{-a x} \paren {-a \cos b x + b \sin b x} } {a^2 + b^2} } + \frac a {a^2 + b^2} | c = [[Exponential of Zero]], [[Cosine of Zero is One]], [[Sine of Zero is Zero]] }} {{end-eqn}} Note that we have, by [[Linear Combination of Sine and Cosine]]: :$\displaystyle 0 \le \size {\frac {e^{-a x} \paren {-a \cos b x + b \sin x} } {a^2 + b^2} } \le \frac {e^{-a x} \sqrt {a^2 + b^2} } {a^2 + b^2} = \frac {e^{-a x} } {\sqrt {a^2 + b^2} }$ By [[Exponential Tends to Zero and Infinity]]: :$\displaystyle \lim_{x \mathop \to \infty} \paren {\frac {e^{-a x} } {\sqrt {a^2 + b^2} } } = 0$ So by the [[Squeeze Theorem]]: :$\displaystyle \lim_{x \mathop \to \infty} \paren {\frac {e^{-a x} \paren {-a \cos b x + b \sin x} } {a^2 + b^2} } = 0$ So: :$\displaystyle \int_0^\infty e^{-a x} \cos b x \rd x = \frac a {a^2 + b^2}$ {{qed}}	0
From [[Sum of Powers of Positive Integers]]: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n i^p | r = 1^p + 2^p + \cdots + n^p | c = }} {{eqn | r = \frac {n^{p + 1} } {p + 1} + \sum_{k \mathop = 1}^p \frac {B_k \, p^{\underline {k - 1} } \, n^{p - k + 1} } {k!} | c = }} {{end-eqn}} where $B_k$ are the [[Definition:Bernoulli Numbers|Bernoulli numbers]]. Setting $p = 3$: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n i^3 | r = \frac {n^{3 + 1} } {3 + 1} + \sum_{k \mathop = 1}^3 \frac {B_k \, 3^{\underline {k - 1} } \, n^{3 - k + 1} } {k!} | c = }} {{eqn | r = \frac {n^4} 4 + \frac {B_1 \, 3^{\underline 0} \, n^3} {1!} + \frac {B_2 \, 3^{\underline 1} \, n^2} {2!} + \frac {B_3 \, 3^{\underline 2} \, n^1} {3!} | c = }} {{eqn | r = \frac {n^4} 4 + \frac 1 2 \frac {n^3} {1!} + \frac 1 6 \frac {3 n^2} {2!} + 0 \frac {3 \times 2 n} {3!} | c = {{Defof|Bernoulli Numbers}} and {{Defof|Falling Factorial}} }} {{eqn | r = \frac {n^4} 4 + \frac {n^3} 2 + \frac {n^2} 4 | c = simplifying }} {{eqn | r = \frac {n^2 \left({n + 1}\right)^2} 4 | c = after algebra }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = \lim_{x \mathop \to 0} \frac {\map \ln {1 + x} } x | r = \lim_{x \mathop \to 0} \frac {\map \ln {1 + x} - \ln 1} x | c = subtract $\ln 1 = 0$ from the numerator, from [[Logarithm of 1 is 0]] }} {{eqn | r = \intlimits {\dfrac {\d} {\d x} \ln x} {x \mathop = 1} {} | c = {{Defof|Derivative of Real Function at Point}} }} {{eqn | r = \frac 1 1 | c = [[Derivative of Natural Logarithm Function]] }} {{eqn | r = 1 }} {{end-eqn}} {{qed}}	0
Follows directly from [[Subset of Real Numbers is Path-Connected iff Interval]]. {{qed}}	0
:$\displaystyle \int x \arctan \frac x a \ \mathrm d x = \frac {x^2 + a^2} 2 \arctan \frac x a - \frac {a x} 2 + C$	0
Note that although the [[Definition:Summation|summation]] given in the statement of the theorem is given as an [[Definition:Infinite Summation|infinite sum]], in fact it terminates after a [[Definition:Finite Set|finite number]] of terms (because when $p^k > n$ we have $0 < n/p^k < 1$). From [[Number of Multiples less than Given Number]], we have that $\floor{\dfrac n {p^k} }$ is the number of [[Definition:Integer|integers]] $m$ such that $0 < m \le n$ which are multiples of $p^k$. We look more closely at $n!$: :$n! = 1 \times 2 \times \ldots \times \paren {n - 1} \times n$ We see that any [[Definition:Integer|integer]] $m$ such that $0 < m \le n$ which is [[Definition:Divisor of Integer|divisible]] by $p^j$ and not $p^{j + 1}$ must be counted exactly $j$ times. That is: :once in $\floor {\dfrac n p}$ :once in $\floor {\dfrac n {p^2} }$ $\ldots$ :once in $\floor {\dfrac n {p^j} }$ And that is all the occurrences of $p$ as a factor of $n!$. Thus: :$\mu = \floor {\dfrac n p} + \floor {\dfrac n {p^2} } + \dotsb + \floor {\dfrac n {p^j} }$ Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = 6 \div 1 | r = 6 | c = }} {{eqn | l = 36 \div 6 | r = 6 | c = }} {{end-eqn}} Hence the [[Definition:Common Ratio|common ratio]] is $6$. {{qed}}	0
=== $(1)$ implies $(2)$ === Suppose: :$\forall p, q \in S: \neg p \vee \neg q = \neg \left({p \wedge q}\right)$ Then applying this to $\neg p$ and $\neg q$: :$\neg \neg p \vee \neg \neg q = \neg \left({\neg p \wedge \neg q}\right)$ By [[Complement of Complement in Uniquely Complemented Lattice]], $\neg \neg p = p$ and $\neg \neg q = q$. Thus: :$p \vee q = \neg \left({\neg p \wedge \neg q}\right)$. Taking [[Definition:Complement (Lattice Theory)|complements]] of both sides: :$\neg \left({p \vee q}\right) = \neg \neg \left({\neg p \wedge \neg q}\right)$ Again applying [[Complement of Complement in Uniquely Complemented Lattice]]: :$\neg \left({p \vee q}\right) = \neg p \wedge \neg q$ {{qed|lemma}} === $(2)$ implies $(1)$ === By [[Dual Pairs (Order Theory)]], $\wedge$ and $\vee$ are [[Definition:Dual Statement (Order Theory)|dual]]. Thus this implication follows from the above by [[Duality Principle (Order Theory)|Duality]]. {{qed|lemma}} === $(1)$ implies $(3)$ === By the definition of a [[Definition:Lattice/Definition 3|lattice]]: :$p \preceq q \iff p \vee q = q$ Applying this to $\neg q$ and $\neg p$: :$\neg q \preceq \neg p \iff \neg q \vee \neg p = \neg p$ By $(1)$: :$\neg q \vee \neg p = \neg \left({q \wedge p}\right)$ So: :$\neg q \preceq \neg p \iff \neg \left({q \wedge p}\right) = \neg p$ Taking the [[Definition:Complement (Lattice Theory)|complements]] of both sides of the equation on the right, and applying [[Complement of Complement in Uniquely Complemented Lattice]]: :$\neg q \preceq \neg p \iff q \wedge p = p$ But the right side is equivalent to $p \preceq q$ {{explain|We define the ordering on a lattice (definition 3) based on joins. We need an equivalent one based on meets, or maybe we have it somewhere already.}} Therefore: :$\neg q \preceq \neg p \iff p \preceq q$ {{qed|lemma}} === $(3)$ implies $(1)$ === {{improve|This is ugly}} {{MissingLinks}} Suppose that $p \preceq q \iff \neg q \preceq \neg p$ By the definition of join: :$\neg p, \neg q \preceq \neg p \vee \neg q$ Thus $\neg \left({\neg p \vee \neg q}\right) \preceq p, q$. By the definition of meet: :$\neg \left({\neg p \vee \neg q}\right) \preceq p \wedge q$ Thus: :$\neg\left({p \wedge q}\right) \preceq \neg\neg \left({\neg p \vee \neg q}\right)$ By [[Complement of Complement in Uniquely Complemented Lattice]]: $*\quad \neg\left({p \wedge q}\right) \preceq \neg p \vee \neg q$ Dually: :$\neg x \wedge \neg y \preceq \neg \left({x \vee y}\right)$ Letting $x = \neg p$ and $y = \neg q$: :$\neg \neg p \wedge \neg \neg q \preceq \neg \left({\neg p \vee \neg q}\right)$ By [[Complement of Complement in Uniquely Complemented Lattice]]: :$p \wedge q \preceq \neg \left({\neg p \vee \neg q}\right)$ By the premise and [[Complement of Complement in Uniquely Complemented Lattice]], then: $**\quad \neg p \vee \neg q \preceq \neg \left({p \wedge q}\right)$ By $*$ and $**$: $\quad \neg\left({p \wedge q}\right) = \neg p \vee \neg q$ {{qed|lemma}} === $(1)$, $(2)$, and $(3)$ together imply $(4)$ === $b, c \preceq b \vee c$, so :$a \wedge b \preceq a \wedge \left({b \vee c}\right)$ :$a \wedge c \preceq a \wedge \left({b \vee c}\right)$ By the definition of join: :$\left({a \wedge b}\right) \vee \left({a \wedge c}\right) \preceq a \wedge \left({b \vee c}\right)$ {{finish}}	0
Let $\triangle ABC$ be a [[Definition:Triangle (Geometry)|triangle]]. === Case 1: $AC$ greater than $AB$ === Using $AC$ as the radius, we construct a [[Definition:Circle|circle]] whose [[Definition:Center of Circle|center]] is $A$. Now we extend: : $CB$ to $D$ : $AB$ to $F$ : $BA$ to $G$ : $CA$ to $E$. $D$ is joined with $E$, thus: :[[File:CosineRule.png|300px]] Using the [[Intersecting Chord Theorem]] we have: : $GB \cdot BF = CB \cdot BD$ $AF$ is a radius, so $AF = AC = b = GA$ and thus: : $GB = GA + AB = b + c$ : $BF = AF - AB = b - c$ Thus: {{begin-eqn}} {{eqn | l = \left({b + c}\right) \left({b - c}\right) | r = a \cdot BD }} {{eqn | ll= \implies | l = \frac {b^2 - c^2} a | r = BD }} {{end-eqn}} Next: {{begin-eqn}} {{eqn | l = CD | r = CB + BD }} {{eqn | r = a + \frac {b^2 - c^2} a }} {{eqn | r = \frac {a^2 + b^2 - c^2} a }} {{end-eqn}} As $CA$ is a radius, $CE$ is a diameter. By [[Thales' Theorem]], it follows that $\angle CDE$ is a [[Definition:Right Angle|right angle]]. Then using the definition of [[Definition:Cosine of Angle|cosine]], we have {{begin-eqn}} {{eqn | l = \cos C | r = \frac {CD} {CE} }} {{eqn | r = \frac {\left({\dfrac{a^2 + b^2 - c^2} a}\right)} {2 b} }} {{eqn | r = \frac {a^2 + b^2 - c^2} {2 a b} }} {{eqn | ll= \implies | l = c^2 | r = a^2 + b^2 - 2 a b \cos C }} {{end-eqn}} {{qed|lemma}} === Case 2: $AC$ less than $AB$ === When $AC$ is less than $AB$, the point $B$ lies outside the circle and so the diagram needs to be modified accordingly: :[[File:CosineRule2.png|300px]] {{expand|Draw a diagram for the case where $\angle ACB$ is a right angle and where it is a convex angle to show that the formula will be the same.}} Now we extend: : $BA$ to $G$ : $CA$ to $E$. Then we construct: :$D$ as the point at which $CB$ [[Definition:Intersection (Geometry)|intersects]] the [[Definition:Circle|circle]] :$F$ as the point at which $AB$ [[Definition:Intersection (Geometry)|intersects]] the [[Definition:Circle|circle]]. Finally $D$ is joined to $E$. Using the [[Secant Secant Theorem]] we have: : $GB \cdot BF = CB \cdot BD$ $AF$ is a radius, so $AF = AC = b = GA$ and thus: : $GB = GA + AB = b + c$ : $BF = AB - AF = b - c$ Thus: {{begin-eqn}} {{eqn | l = \left({b + c}\right) \left({b - c}\right) | r = CB \cdot BD | c = [[Secant Secant Theorem]] }} {{eqn | l = \left({b + c}\right) \left({b - c}\right) | r = a \cdot BD }} {{eqn | ll= \implies | l = \frac {b^2 - c^2} a | r = BD }} {{end-eqn}} Next: {{begin-eqn}} {{eqn | l = CD | r = CB - BD }} {{eqn | r = a - \frac {b^2 - c^2} a }} {{eqn | r = \frac {a^2 - b^2 + c^2} a }} {{end-eqn}} As $CA$ is a radius, $CE$ is a diameter. By [[Thales' Theorem]], it follows that $\angle CDE$ is a [[Definition:Right Angle|right angle]]. Then using the definition of [[Definition:Cosine of Angle|cosine]], we have {{begin-eqn}} {{eqn | l = \cos C | r = \frac {CD} {CE} }} {{eqn | r = \frac {\left({\dfrac {a^2 - b^2 + c^2} a}\right)} {2 b} }} {{eqn | r = \frac {a^2 - b^2 + c^2} {2 a b} | c = }} {{eqn | ll= \implies | l = c^2 | r = a^2 + b^2 - 2 a b \cos C }} {{end-eqn}} {{qed|lemma}} === Case 3: $AC = AB$ === When $AC = AB$ the points $B$, $D$ and $F$ coincide on the [[Definition:Circumference of Circle|circumference]] of the [[Definition:Circle|circle]]: :[[File:CosineRule3.png|300px]] We extend: : $BA$ to $G$ : $CA$ to $E$ and immediately: : $GB = CB$ {{finish}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan 330^\circ | r = \tan \left({360^\circ - 30^\circ}\right) | c = }} {{eqn | r = -\tan 30^\circ | c = [[Tangent of Conjugate Angle]] }} {{eqn | r = -\frac {\sqrt 3} 3 | c = [[Tangent of 30 Degrees]] }} {{end-eqn}} {{qed}}	0
Let $S \subset \Z$ be a [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subset]] of the [[Definition:Integer|set of integers]]. Let $S$ be [[Definition:Bounded Below Subset of Real Numbers|bounded below]] in the [[Definition:Real Number|set of real numbers]]. Then its [[Definition:Infimum of Subset of Real Numbers|infimum]] $\inf S$ is an [[Definition:Integer|integer]].	0
Let $T_1, T_2, T_3$ be [[Definition:Topological Space|topological spaces]]. Let $f: T_1 \to T_2$ and $g: T_2 \to T_3$ be [[Definition:Continuous Mapping (Topology)|continuous mappings]]. Then the [[Definition:Composition of Mappings|composite mapping]] $g \circ f: T_1 \to T_3$ is [[Definition:Continuous Mapping (Topology)|continuous]].	0
{{begin-eqn}} {{eqn | n = 1 | l = 2 \sin a \cos b | r = \sin \paren {a + b} + \sin \paren {a - b} | c = [[Simpson's Formulas/Sine by Cosine/Proof 2|Simpson's Formula for Sine by Cosine: Proof 2]] }} {{eqn | n = 2 | l = 2 \cos a \sin b | r = \sin \paren {a + b} - \sin \paren {a - b} | c = [[Simpson's Formulas/Cosine by Sine/Proof 2|Simpson's Formula for Cosine by Sine: Proof 2]] }} {{eqn | ll= \leadsto | l = 2 \sin \paren {a + b} | r = 2 \sin a \cos b + 2 \cos a \sin b | c = $(1) + (2)$ }} {{eqn | ll= \leadsto | l = \sin \paren {a + b} | r = \sin a \cos b + \cos a \sin b | c = }} {{end-eqn}} {{qed}}	0
The [[Definition:Omega Constant|omega constant]] is [[Definition:Transcendental Number|transcendental]].	0
The operation of [[Definition:Real Multiplication|real multiplication]] is defined on all [[Definition:Real Number|real numbers]]. Thus: :$\forall x \in \R: \exists y \in \R: x^2 = y$ Hence the result by definition of [[Definition:Domain of Real Function|domain]]. {{qed}}	0
:$f + g: \struct {S, \tau_{_S} } \to \struct {R, \tau_{_R} }$ is [[Definition:Continuous Mapping on Set|continuous]].	0
Note that this proof does not presuppose [[Derivative of Natural Logarithm Function]]. {{begin-eqn}} {{eqn | l = \lim_{x \mathop \to 0} \frac {\map \ln {1 + x} } x | r = \lim_{n \mathop \to \infty} \frac {\map \ln {1 + \frac 1 n} } {\frac 1 n} | c = }} {{eqn | r = \lim_{n \mathop \to \infty} n \, \map \ln {1 + \frac 1 n} | c = }} {{eqn | r = \lim_{n \mathop \to \infty} \map \ln {\paren {1 + \frac 1 n}^n} | c = }} {{eqn | r = \ln e | c = {{Defof|Euler's Number|subdef = Limit of Sequence|Euler's Number as Limit of Sequence}} }} {{eqn | r = 1 | c = [[Natural Logarithm of e is 1]] }} {{end-eqn}} {{qed}}	0
Let $z_1 \in S$ and $z_2 \in S$ be joined by a [[Definition:Polygonal Path|polygonal path]] $P$. Then there are points of $P$ which are not in $S$. Hence, by definition, $S$ is not [[Definition:Connected Set (Complex Analysis)|connected]]. {{qed}}	0
Let $S = \map {B_\epsilon} x$ be an [[Definition:Open Ball|open $\epsilon$-ball at $x$]]. Let $y = \tuple {y_1, y_2} \in \map {B_\epsilon} x$. Then: {{begin-eqn}} {{eqn | l = y | o = \in | m = \map {B_\epsilon} x | c = }} {{eqn | ll= \leadstoandfrom | l = \map d {y, x} | o = < | m = \epsilon | c = {{Defof|Open Ball|Open $\epsilon$-Ball}} }} {{eqn | ll= \leadstoandfrom | l = \sqrt {\paren {y_1 - x_1}^2 + \paren {y_2 - x_2}^2} | o = < | m = \epsilon | c = {{Defof|Real Number Plane with Euclidean Metric}} }} {{eqn | ll= \leadstoandfrom | l = \paren {y_1 - x_1}^2 + \paren {y_2 - x_2}^2 | o = < | m = \epsilon^2 }} {{end-eqn}} But from [[Equation of Circle]]: :$\paren {y_1 - x_1}^2 + \paren {y_2 - x_2}^2 = \epsilon^2$ is the equation of a [[Definition:Circle|circle]] whose [[Definition:Center of Circle|center]] is $\tuple {x_1, x_2}$ and whose [[Definition:Radius of Circle|radius]] is $\epsilon$. The result follows by definition of [[Definition:Interior of Region|interior]] and [[Open Ball of Point Inside Open Ball]]. {{qed}}	0
=== Forward Implication === Proved in [[Exponential Function Inequality]]. {{qed|lemma}} === Reverse Implication === Consider $f_a \left({x}\right) = a^x - x - 1$. Then we need to prove: :$a = e \impliedby \forall x \in \R: f_a \left({x}\right) \ge 0$ By [[Linear Combination of Derivatives]], [[Derivative of Power of Constant]], [[Derivative of Identity Function]], and [[Derivative of Constant]], we have: :$f_a' \left({x}\right) = a^x \ln a - 1$ By [[Linear Combination of Derivatives]], [[Derivative of Power of Constant]], [[Derivative of Constant Multiple]], and [[Derivative of Constant]], we have: :$f_a'' \left({x}\right) = a^x \left({\ln a}\right)^2$ Now, we divide the theorem into two cases: ==== Case 1: $0 < a \le 1$ ==== Consider $x = 1$. Then: {{begin-eqn}} {{eqn | l = f_a \left({1}\right) | r = a^1 - 1 - 1 }} {{eqn | r = a - 2 }} {{eqn | o = \le | r = 1 - 2 }} {{eqn | r = -1 }} {{eqn | o = < | r = 0 }} {{end-eqn}} Thus the right hand side is not true. {{qed|lemma}} ==== Case 2: $a > 1$ ==== We have: {{begin-eqn}} {{eqn | l = \ln a | o = > | r = 0 | c = [[Logarithm of 1 is 0]] and [[Logarithm is Strictly Increasing]] }} {{eqn | n = 1 | ll = \implies | l = \left({\ln a}\right)^2 | o = > | r = 0 | c = [[Square of Non-Zero Real Number is Strictly Positive]] }} {{eqn | n = 2 | l = a^x | o = > | r = 0 | c = [[Power of Positive Real Number is Positive/Real Number|Power of Positive Real Number is Positive over Reals]] }} {{eqn | ll = \vdash | l = a^x \left({\ln a}\right)^2 | o = > | r = 0 | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R O2$]]: [[Definition:Relation Compatible with Operation|compatibility]] with [[Definition:Real Multiplication|multiplication]], using $(1)$ and $(2)$ }} {{eqn | l = f_a'' \left({x}\right) | o = > | r = 0 }} {{end-eqn}} Hence, by [[Second Derivative of Strictly Convex Real Function is Strictly Positive]], we have that $f_a$ is strictly convex. Trying to find its minimum, we solve: :$f_a' \left({x}\right) = 0$ Therefore: {{begin-eqn}} {{eqn | l = a^x \ln a - 1 | r = 0 }} {{eqn | l = a^x \ln a | r = 1 }} {{eqn | l = a^x | r = \frac 1 {\ln a} }} {{eqn | l = x | r = \log_a \left({\frac 1 {\ln a} }\right) }} {{eqn | l = x | r = - \log_a \left({\ln a}\right) }} {{end-eqn}} At this point: {{begin-eqn}} {{eqn | l = f_a \left({x}\right) | r = f_a \left({- \log_a \left({\ln a}\right)}\right) | c = }} {{eqn | r = \frac 1 {\ln a} + \log_a \left({\ln a}\right) - 1 | c = }} {{eqn | r = \frac 1 {\ln a} + \frac {\ln \left({\ln a}\right)} {\ln a} - 1 | c = [[Change of Base of Logarithm]] }} {{eqn | r = \frac {1 + \ln \left({\ln a}\right)} {\ln a} - 1 | c = }} {{end-eqn}} It is to be noted that when $a = e$, the minimum is: : $\dfrac {1 + \ln \left({\ln e}\right)} {\ln e} - 1 = 0$ meaning that the only solution to $f_e \left({x}\right) = 0$ is $x = 0$. Also, from [[Exponential Function Inequality]], we have: :$\forall x \in \R: e^x \ge x + 1$ Substituting $x = \ln \left({\ln a}\right)$: :$\ln a \ge \ln \left({\ln a}\right) + 1$ From [[Logarithm is Strictly Increasing]] and [[Logarithm of 1 is 0]]: :$\ln a > 0$ Thus: :$\dfrac {\ln a} {\ln a} \ge \dfrac {\ln \left({\ln a}\right) + 1} {\ln a}$ Therefore: :$\dfrac {1 + \ln \left({\ln a}\right)} {\ln a} - 1 \le 0$ Then, we solve the case of equality: {{begin-eqn}} {{eqn | l = \frac {1 + \ln \left({\ln a}\right)} {\ln a} - 1 | r = 0 }} {{eqn | l = \frac {1 + \ln \left({\ln a}\right)} {\ln a} | r = 1 }} {{eqn | l = 1 + \ln \left({\ln a}\right) | r = \ln a }} {{end-eqn}} From earlier, we have that the only solution is $\ln \left({\ln a}\right) = 0$, when $a = e$. Therefore, for other values of $a$, the minimum is negative. {{qed}} [[Category:Euler's Number]] 3yo1vwh450sokmb6cg2rhbk0uv4xjbe	0
From [[Index Laws for Semigroup/Product of Indices|Index Laws for Semigroup: Product of Indices]] we have: :$+^{z \times y} x = +^z \left({+^y x}\right)$ By definition of [[Definition:Natural Number Multiplication|multiplication]], this amounts to: :$x \times \left({z \times y}\right) = \left({x \times y}\right) \times z$ From [[Natural Number Multiplication is Commutative]], we have: :$x \times \left({z \times y}\right) = x \times \left({y \times z}\right)$ {{qed}}	0
Let $a, b \in \Z$ such that $a$ and $b$ are not both [[Definition:Zero (Number)|zero]]. Let $J$ be the [[Definition:Set|set]] of all [[Definition:Integer Combination|integer combinations]] of $a$ and $b$: :$J = \set {x: x = m a + n b: m, n \in \Z}$ First we show that $J$ is an ideal of $\Z$ Let $\alpha = m_1 a + n_1 b$ and $\beta = m_2 a + n_2 b$, and let $c \in \Z$ Then $\alpha,\beta \in J$ and : {{begin-eqn}} {{eqn | l = \alpha + \beta | r = m_1 a + n_1 b + m_2 a + n_2 b }} {{eqn | r = \paren {m_1 + m_2} a + \paren {n_1 + n_2} b }} {{eqn | ll= \leadsto | l = \alpha + \beta | o = \in | r = J }} {{end-eqn}} {{begin-eqn}} {{eqn | l = c \alpha | r = c \paren {m_1 a + n_1 b} }} {{eqn | r = \paren {c m_1} a + \paren {c n_1} b }} {{eqn | ll= \leadsto | l = c \alpha | o = \in | r = J }} {{end-eqn}} Thus $J$ is an [[Definition:Integral Ideal|integral ideal]]. We have that: {{begin-eqn}} {{eqn | l = a | r = 1 a + 0 b | c = }} {{eqn | lo= \land | l = b | r = 0 a + 1 b | c = }} {{eqn | ll= \leadsto | l = a, b | o = \in | r = J | c = }} {{end-eqn}} $a$ and $b$ are not both [[Definition:Zero (Number)|zero]], thus: :$J \ne \set 0$ By the something {theorem about ideals}: :$\exists x_0 > 0 : J = x_0 \Z$ :$a \in J \land \set {J = x_0 \Z} \implies x_0 \divides a$ :$b \in J \land \set {J = x_0 \Z} \implies x_0 \divides b$ :$x_0 \divides a \land x_0 \divides b \implies x_0 \in \map D {a, b}$ {{explain|What is $\map D {a, b}$?}} Furthermore: :$x_0 \in J \implies \exists r, s \in \Z : x_0 = r a + s b$ Let $x_1 \in \map D {a, b}$. Then: {{begin-eqn}} {{eqn | l = x_1 \in \map D {a, b} | o = \leadsto | r = x_1 \divides a \land x_1 \divides b }} {{eqn | o = \leadsto | r = x_1 \divides \paren {r a + s b} }} {{eqn | o = \leadsto | r = x_1 \vert x_0 }} {{eqn | o = \leadsto | r = \size {x_1} \le \size {x_0} = x_0 }} {{end-eqn}} Thus: :$x_0 = \max \set {\map D {a, b} } = \gcd \set {a, b} = r a + s b$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \wp\left({-z; \omega_1, \omega_2}\right) | r = \frac 1 {\left({-z}\right)^2} + \sum_{\left({n, m}\right) \mathop \in \Z^2 \setminus \left({0, 0}\right)} \left({ \frac 1 {\left({-z - 2 m \omega_1 - 2 n \omega_2}\right)^2} - \frac 1 {\left({2 m \omega_1 + 2 n \omega_2}\right)^2} }\right) | c = {{Defof|Weierstrass's Elliptic Function}} }} {{eqn | r = \frac 1 {z^2} + \left({\sum_{\left({n, m}\right) \mathop \in \Z_{\ge 0}^2 \setminus \left({0, 0}\right)} + \sum_{\left({n, m}\right) \mathop \in \Z_{<0}^2} }\right) \left({ \frac 1 {\left({z - 2 m \omega_1 - 2 n \omega_2}\right)^2} - \frac 1 {\left({-2 m \omega_1 - 2 n \omega_2}\right)^2} }\right) }} {{eqn | r = \frac 1 {z^2} + \left({ \sum_{\left({n, m}\right) \mathop \in \Z_{\le 0}^2 \setminus \left({0, 0}\right)} } + \sum_{\left({n, m}\right) \mathop \in \Z_{> 0}^2 }\right) \left({ \frac 1 {\left({z - 2 m \omega_1 - 2 n \omega_2}\right)^2} - \frac 1 {\left({2 m \omega_1 + 2 n \omega_2}\right)^2} }\right) | c = letting $\left({n, m}\right) \to \left({-n, -m}\right)$ }} {{eqn | r = \frac 1 {z^2} + {\sum_{\left({n, m}\right) \mathop \in \Z^2 \setminus \left({0, 0}\right) } } \left({ \frac 1 {\left({z - 2 m \omega_1 - 2 n \omega_2}\right)^2} - \frac 1 {\left({2 m \omega_1 + 2 n \omega_2}\right)^2} }\right) }} {{eqn | r = \wp\left({z; \omega_1, \omega_2}\right) | c = {{Defof|Weierstrass's Elliptic Function}} }} {{end-eqn}} {{qed}}	0
From [[Square of Real Number is Non-Negative]], the [[Definition:Image of Mapping|image]] of $f$ is $\R_{\ge 0}$. From [[Positive Real has Real Square Root]]: :$\forall x \in \R: \exists y \in \R: x^2 = y$ Hence the result by definition of [[Definition:Image of Mapping|image]]. {{qed}}	0
By [[Definition:Euler Form of Gamma Function|Euler's form of the Gamma function]]: {{begin-eqn}} {{eqn | l = \frac {\map \Gamma {z + 1} } {\map \Gamma z} | r = \paren {\frac 1 {z + 1} \lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^m \frac {\paren {1 + \frac 1 n}^{z + 1} } {1 + \frac {z + 1} n} } \div \paren {\frac 1 z \lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^m \frac {\paren {1 + \frac 1 n}^z} {1 + \frac z n} } | c = }} {{eqn | r = \frac z {z + 1} \lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^m \paren {\frac {\paren {1 + \frac 1 n}^{z + 1} \paren {1 + \frac z n} } {\paren {1 + \frac 1 n}^z \paren {1 + \frac {z + 1} n} } } | c = }} {{eqn | r = \frac z {z + 1} \lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^m \paren {\frac {\paren {1 + \frac 1 n} \paren {z + n} } {z + n + 1} } | c = }} {{eqn | r = z \lim_{m \mathop \to \infty} \frac {m + 1} {z + m + 1} = z | c = }} {{end-eqn}} {{qed}}	0
Recall that [[Real Numbers form Field]] under the [[Definition:Binary Operation|operations]] of [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]]. By definition of a [[Definition:Field (Abstract Algebra)|field]], the [[Definition:Algebraic Structure|algebraic structure]] $\left({\R_{\ne 0}, \times}\right)$ is a [[Definition:Group|group]]. Thus, by definition, $\times$ is [[Definition:Closed Algebraic Structure|closed]] in $\left({\R_{\ne 0}, \times}\right)$. {{qed}}	0
:$\map \tan {\pi - \theta} = -\tan \theta$ where $\tan$ denotes [[Definition:Tangent Function|tangent]]. That is, the [[Definition:Tangent Function|tangent]] of an [[Definition:Angle|angle]] is the negative of its [[Definition:Supplement of Angle|supplement]].	0
{{begin-eqn}} {{eqn | l = \map \cos {\frac \pi 2 - \theta} | r = \cos \frac \pi 2 \cos \theta + \sin \frac \pi 2 \sin \theta | c = [[Cosine of Difference]] }} {{eqn | r = 0 \times \cos \theta + 1 \times \sin \theta | c = [[Cosine of Right Angle]] and [[Sine of Right Angle]] }} {{eqn | r = \sin \theta }} {{end-eqn}} {{qed}}	0
From [[Strictly Monotone Real Function is Bijective]], $f$ is a [[Definition:Bijection|bijection]]. From [[Inverse of Strictly Monotone Function]], $f^{-1} : J \to I$ exists and is [[Definition:Strictly Monotone Real Function|strictly monotone]]. From [[Surjective Monotone Function is Continuous]], $f^{-1}$ is [[Definition:Continuous on Interval|continuous]]. Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = 1 - \cos x | r = \cos 0 - \cos x | c = [[Cosine of Zero is One]] }} {{eqn | r = -2 \sin \left({\dfrac {0 + x} 2}\right) \sin \left({\dfrac {0 - x} 2}\right) | c = [[Prosthaphaeresis Formula for Cosine minus Cosine]] }} {{eqn | r = -2 \sin \left({\dfrac x 2}\right) \sin \left({\dfrac {-x} 2}\right) | c = simplifying }} {{eqn | r = 2 \sin \left({\dfrac x 2}\right) \sin \left({\dfrac x 2}\right) | c = [[Sine Function is Odd]] }} {{eqn | r = 2 \sin^2 \left({\frac x 2}\right) | c = simplifying }} {{eqn | ll= \implies | l = \frac 1 {1 - \cos x} | r = \frac 1 2 \csc^2 \left({\frac x 2}\right) | c = {{Defof|Cosecant}} }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {x^3 \left({x^2 + a^2}\right)} = -\frac 1 {2 a^2 x^2} - \frac 1 {2 a^4} \ln \left({\frac {x^2 + a^2} {x^2} }\right) + C$	0
{{begin-eqn}} {{eqn | l = z^m G \left({z}\right) | r = z^m \sum_{n \mathop \ge 0} a_n z^n | c = {{Defof|Generating Function}} }} {{eqn | r = \sum_{n \mathop \ge 0} a_n z^{n + m} | c = }} {{eqn | r = \sum_{n + m \mathop \ge 0} a_{n - m} z^n | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \sum_{n \mathop \ge m} a_{n - m} z^n | c = }} {{end-eqn}} By letting $a_n = 0$ for all $n < 0$: :$z^m G \left({z}\right) = \displaystyle \sum_{n \mathop \ge 0} a_{n - m} z^n$ Hence the result. {{qed}}	0
Let $a, b, c, d$ be [[Definition:Physical Quantity|quantities]]. Let $a : b = c : d$ where $a : b$ denotes the [[Definition:Ratio|ratio]] between $a$ and $b$. Then for any [[Definition:Number|numbers]] $m$ and $n$: :$m a : n b = m c : n d$ {{:Euclid:Proposition/V/4}}	0
Let $\size x < a$. Let: {{begin-eqn}} {{eqn | l = u | r = \tanh^{-1} {\frac x a} | c = {{Defof|Real Inverse Hyperbolic Tangent}}, which is defined where $\size {\dfrac x a} < 1$ }} {{eqn | ll=\leadsto | l = x | r = a \tanh u | c = }} {{eqn | ll=\leadsto | l = \frac {\d x} {\d u} | r = a \sech^2 u | c = [[Derivative of Hyperbolic Tangent]] }} {{eqn | ll=\leadsto | l = \int \frac {\rd x} {a^2 - x^2} | r = \int \frac {a \sech^2 u} {a^2 - a^2 \tanh^2 u} \rd u | c = [[Integration by Substitution]] }} {{eqn | r = \frac a {a^2} \int \frac {\sech^2 u} {1 - \tanh^2 u} \rd u | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 a \int \frac {\sech^2 u} {\sech^2 u} \rd u | c = [[Sum of Squares of Hyperbolic Secant and Tangent]] }} {{eqn | r = \frac 1 a \int \rd u }} {{eqn | r = \frac 1 a u + C | c = [[Integral of Constant]] }} {{eqn | r = \frac 1 a \tanh^{-1} \frac x a + C | c = Definition of $u$ }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = \int x \map \ln {x^2 + a^2} \rd x | r = \frac {x^2 \map \ln {x^2 + a^2} } 2 - \int \frac {x^3} {x^2 + a^2} \rd x + C | c = [[Primitive of Power of x by Logarithm of x squared plus a squared|Primitive of $x^m \map \ln {x^2 + a^2}$]] with $m = 1$ }} {{eqn | r = \frac {x^2 \map \ln {x^2 + a^2} } 2 - \paren {\frac {x^2} 2 - \frac {a^2} 2 \map \ln {x^2 + a^2} } + C | c = [[Primitive of x cubed over x squared plus a squared|Primitive of $\dfrac {x^3} {x^2 + a^2}$]] }} {{eqn | r = \frac {\paren {x^2 + a^2} \map \ln {x^2 + a^2} - x^2} 2 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $f$ be [[Definition:Continuous on Interval|continuous]] in a [[Definition:Closed Real Interval|closed real interval]] $\left[{a \,.\,.\, b}\right]$. Then: :$\forall x \in \left[{a \,.\,.\, b}\right]: \exists x_s \in \left[{a \,.\,.\, b}\right]: f \left({x_s}\right) \le f \left({x}\right)$ :$\forall x \in \left[{a \,.\,.\, b}\right]: \exists x_n \in \left[{a \,.\,.\, b}\right]: f \left({x_n}\right) \ge f \left({x}\right)$	0
:$\displaystyle \lim_{y \mathop \to 0_+} y^r \ln y = 0$	0
Let $\struct {\mathbb K, \norm {\, \cdot \,} }$ be a [[Definition:Valued Field|valued field]]. Let the [[Definition:Infinite Product|infinite product]] $\displaystyle \prod_{n \mathop = 1}^\infty \paren {1 + a_n}$ be [[Definition:Absolute Convergence of Product|absolutely convergent]]. Then it is not [[Definition:Divergent Product|divergent]] to $0$.	0
Each element $x$ of the [[Definition:Set|set]] of [[Definition:Real Number|non-zero real numbers]] $\R_{\ne 0}$ has an [[Definition:Inverse Element|inverse element]] $\dfrac 1 x$ under the operation of [[Definition:Real Multiplication|real number multiplication]]: :$\forall x \in \R_{\ne 0}: \exists \dfrac 1 x \in \R_{\ne 0}: x \times \dfrac 1 x = 1 = \dfrac 1 x \times x$	0
{{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\sinh a x} {e^{b x} + 1} \rd x | r = \frac 1 2 \int_0^\infty \frac {e^{-b x} \paren {e^{a x} - e^{-a x} } } {1 - \paren {-e^{-b x} } } \rd x | c = {{Defof|Hyperbolic Sine}} }} {{eqn | r = \frac 1 2 \int_0^\infty \paren {e^{\paren {a - b} x} - e^{-\paren {a + b} x} } \paren {\sum_{n \mathop = 0}^\infty \paren {-1}^n e^{-b n x} } \rd x | c = [[Sum of Infinite Geometric Sequence]] }} {{eqn | r = \frac 1 2 \sum_{n \mathop = 0}^\infty \paren {-1}^n \int_0^\infty \paren {e^{\paren {a - \paren {n + 1} b} x} - e^{-\paren {a + \paren {n + 1} b} x} } \rd x | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 2 \sum_{n \mathop = 0}^\infty \paren {-1}^n \paren {\intlimits {-\frac {e^{\paren {a - \paren {n + 1} b} x} } {\paren {n + 1} b - a} } 0 \infty - \intlimits {-\frac {e^{-\paren {a + \paren {n + 1} b} x} } {a + \paren {n + 1} b} } 0 \infty} | c = [[Primitive of Exponential of a x|Primitive of $e^{a x}$]] }} {{end-eqn}} Note that as $b > a$, we have that $a - b < 0$. As $b > 0$, we therefore have $a - \paren {n + 1} b < 0$ for all [[Definition:Positive Integer|positive integer]] $n$. We also have that as $a + \paren {n + 1} b > 0$, that $-\paren {a + \paren {n + 1} b} < 0$. So, by [[Exponential Tends to Zero and Infinity]]: :$\displaystyle \lim_{x \mathop \to \infty} \frac {e^{\paren {a - \paren {n + 1} b} x} } {a - \paren {n + 1} b} = 0$ and: :$\displaystyle \lim_{x \mathop \to \infty} \frac {e^{-\paren {a + \paren {n + 1} b} x} } {a + \paren {n + 1} b} = 0$ We therefore have: {{begin-eqn}} {{eqn | l = \frac 1 2 \sum_{n \mathop = 0}^\infty \paren {-1}^n \paren {\intlimits {-\frac {e^{\paren {a - \paren {n + 1} b} x} } {\paren {n + 1} b - a} } 0 \infty - \intlimits {-\frac {e^{-\paren {a + \paren {n + 1} b} x} } {a + \paren {n + 1} b} } 0 \infty} | r = \frac 1 2 \sum_{n \mathop = 0}^\infty \paren {-1}^n \paren {\frac 1 {\paren {n + 1} b - a} - \frac 1 {\paren {n + 1} b + a} } | c = [[Exponential of Zero]] }} {{eqn | r = \frac 1 2 \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {2 a} {\paren {n + 1}^2 b^2 - a^2} | c = [[Difference of Two Squares]] }} {{eqn | r = \frac 1 b \sum_{n \mathop = 1}^\infty \paren {-1}^{n - 1} \frac {\paren {\frac a b} } {n^2 - \paren {\frac a b}^2} | c = shifting the index, extracting factors }} {{eqn | r = \frac 1 b \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\paren {\frac a b} } {\paren {\frac a b}^2 - n^2} }} {{end-eqn}} We have by [[Mittag-Leffler Expansion for Cosecant Function]]: :$\displaystyle \pi \map \csc {\pi z} = \frac 1 z + 2 \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac z {z^2 - n^2}$ Setting $z = \dfrac a b$ we have: :$\displaystyle \pi \map \csc {\frac {\pi a} b} = \frac b a + 2 \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\paren {\frac a b} } {\paren {\frac a b}^2 - n^2}$ Rearranging gives: :$\displaystyle \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\paren {\frac a b} } {\paren {\frac a b}^2 - n^2} = \frac \pi 2 \map \csc {\frac {\pi a} b} - \frac b {2 a}$ Therefore: {{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\sinh a x} {e^{b x} + 1} \rd x | r = \frac 1 b \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\paren {\frac a b} } {\paren {\frac a b}^2 - n^2} }} {{eqn | r = \frac \pi {2 b} \map \csc {\frac {\pi a} b} - \frac 1 {2 a} }} {{end-eqn}} {{qed}}	0
Let $\R$ be the [[Definition:Real Number Line|real number line]] considered as an [[Definition:Euclidean Space|Euclidean space]]. Let $\left[{a \,.\,.\, b}\right) \subset \R$ be a [[Definition:Half-Open Real Interval|half-open interval]] of $\R$. Let $c < a$. Then $\left[{a \,.\,.\, b}\right)$ is a [[Definition:Closed Set (Metric Space)|closed set]] of $\left({c \,.\,.\, b}\right)$. Similarly, let $d > b$. Then the [[Definition:Half-Open Real Interval|half-open interval]] $\left({a \,.\,.\, b}\right]$ is a [[Definition:Closed Set (Metric Space)|closed set]] of $\left({a \,.\,.\, d}\right)$.	0
Let $n \ge 1$ be a [[Definition:Natural Number|natural number]]. Then the $2^n$th [[Definition:Cyclotomic Polynomial|cyclotomic polynomial]] is: :$\map {\Phi_{2^n} } x = x^{2^{n - 1} } + 1$	0
:$\sin 4 \theta = 4 \sin \theta \cos \theta - 8 \sin^3 \theta \cos \theta$	0
Let $m \in \Z_{>0}$ be a [[Definition:Positive Integer|positive integer]]. Let $z \in \C$ be a [[Definition:Complex|complex number]]. Let $\exp z$ be the [[Definition:Complex Exponential Function|exponential of $z$]]. Then: : $\displaystyle \exp \paren {m z} = \paren {\exp z}^m$	0
Let: :$\displaystyle S_n = \sum_{i \mathop = 1}^n f_i$ Let: :$\displaystyle f = \lim_{n \mathop \to \infty} S_n$ To show the [[Definition:Partial Sum|partial sums]] [[Definition:Uniform Convergence|converge uniformly]] to $f$, we must show that: :$\displaystyle \lim_{n \mathop \to \infty} \sup_{x \mathop \in D} \size {f - S_n} = 0$ But: {{begin-eqn}} {{eqn | l = \sup_{x \mathop \in D} \size {f - S_n} | r = \sup_{x \mathop \in D} \size {\paren {f_1 + f_2 + \dotsb} - \paren {f_1 + f_2 + \dotsb + f_n} } | c = }} {{eqn | r = \sup_{x \mathop \in D} \size {f_{n + 1} + f_{n + 2} + \dotsc} | c = }} {{end-eqn}} By the [[Triangle Inequality]], this value is less than or equal to: :$\displaystyle \sum_{i \mathop = n + 1}^\infty \sup_{x \mathop \in D} \size {\map {f_i} x} \le \sum_{i \mathop = n + 1}^\infty M_i$ We have that: :$\displaystyle 0 \le \sum_{i \mathop = 1}^\infty M_n < \infty$ It follows from [[Tail of Convergent Series tends to Zero]]: :$\displaystyle 0 \le \lim_{n \mathop \to \infty} \sum_{i \mathop = n + 1}^\infty \sup_{x \mathop \in D} \size {\map {f_i} x} \le \lim_{n \mathop \to \infty} \sum_{i \mathop = n + 1}^\infty M_i = 0$ So: :$\displaystyle \lim_{n \mathop \to \infty} \sup_{x \mathop \in D} \size {f - S_n} = 0$ Hence the series [[Definition:Uniform Convergence|converges uniformly]] on the domain. {{qed}} {{expand|Establish how broadly this can be applied - it's defined for real functions but should also apply to complex ones and possibly a general metric space.}}	0
By [[Cardinal Number Less than Ordinal/Corollary|Cardinal Number Less than Ordinal: Corollary]]: :$\left\vert{\omega}\right\vert \le \omega$ Moreover, for any $n \in \omega$, by [[Cardinal of Finite Ordinal]]: :$\left\vert{n}\right\vert < \left\vert{n+1}\right\vert \le \left\vert{\omega}\right\vert$ Thus by [[Cardinal of Finite Ordinal]]: : $n \in \left\vert{\omega}\right\vert$ Therefore: : $\omega = \left\vert{\omega}\right\vert$ By [[Cardinal of Cardinal Equal to Cardinal/Corollary|Cardinal of Cardinal Equal to Cardinal: Corollary]]: : $\omega \in \mathcal N’$ {{qed}} [[Category:Cardinals]] [[Category:Minimal Infinite Successor Set]] encs8nywgu8r7y5ek5ygnq7v3r0hw6l	0
Let $\mathcal L$ be the [[Definition:Straight Line|straight line]] defined by the [[Equation of Straight Line in Plane/General Equation|equation]]: :$a x - b y = c$ Let $p_1$ and $p_2$ be [[Definition:Lattice Point|lattice points]] on $\mathcal L$. Then the shortest possible [[Definition:Distance (Linear Measure)|distance]] $d$ between $p_1$ and $p_2$ is: :$d = \dfrac {\sqrt {a^2 + b^2} } {\gcd \set {a, b} }$ where $\gcd \set {a, b}$ denotes the [[Definition:Greatest Common Divisor|greatest common divisor]] of $a$ and $b$.	0
{{begin-eqn}} {{eqn | l = \map \cos {x + \frac \pi 2} | r = \cos x \cos \frac \pi 2 - \sin x \sin \frac \pi 2 | c = [[Cosine of Sum]] }} {{eqn | r = \cos x \cdot 0 - \sin x \cdot 1 | c = [[Cosine of Right Angle]] and [[Sine of Right Angle]] }} {{eqn | r = -\sin x | c = }} {{end-eqn}} {{qed}}	0
Let $f: \Bbb E \to \Z$ be the [[Definition:Mapping|mapping]] defined as: :$\forall x \in \Bbb E: \map f x = \dfrac x 2$ $f$ is [[Definition:Well-Defined Mapping|well-defined]] as $x$ is [[Definition:Even Integer|even]] and so $\dfrac x 2 \in \Z$. Let $x, y \in \Bbb E$ such that $\map f x = \map f y$. Then: {{begin-eqn}} {{eqn | l = \map f x | r = \map f y | c = }} {{eqn | ll= \leadsto | l = \dfrac x 2 | r = \dfrac y 2 | c = Definition of $f$ }} {{eqn | ll= \leadsto | l = x | r = y | c = }} {{end-eqn}} Thus $f$ is [[Definition:Injection|injective]] by definition. Consider the [[Definition:Inverse of Mapping|inverse]] $f^{-1}$. By inspection: :$\forall x \in \Z: \map {f^{-1} } x = 2 x$ $f^{-1}$ is [[Definition:Well-Defined Mapping|well-defined]], and $2 x$ is [[Definition:Even Integer|even]]. Thus $f^{-1}$ is a [[Definition:Mapping|mapping]] from $\Z$ to $\Bbb E$. Then: {{begin-eqn}} {{eqn | l = \map {f^{-1} } x | r = \map {f^{-1} } y | c = }} {{eqn | ll= \leadsto | l = 2 x | r = 2 y | c = Definition of $f^{-1}$ }} {{eqn | ll= \leadsto | l = x | r = y | c = }} {{end-eqn}} Thus $f^{-1}$ is [[Definition:Injection|injective]] by definition. It follows by the [[Cantor-Bernstein-Schröder Theorem]] that there exists a [[Definition:Bijection|bijection]] between $\Z$ and $\Bbb E$. {{qed}}	0
Let: : $w := r^{1 / n} \paren {\map \cos {\dfrac {\theta + 2 \pi k} n} + i \, \map \sin {\dfrac {\theta + 2 \pi k} n} }$ for $k \in \Z_{>0}$. Then: {{begin-eqn}} {{eqn | l = w^n | r = \paren {r^{1 / n} \paren {\map \cos {\dfrac {\theta + 2 \pi k} n} + i \, \sin {\dfrac {\theta + 2 \pi k} n} } }^n | c = }} {{eqn | r = \paren {r^{1 / n} }^n \paren {\cos n \paren {\dfrac {\theta + 2 \pi k} n} + i \sin n \paren {\dfrac {\theta + 2 \pi k} n} } | c = [[De Moivre's Formula]] }} {{eqn | r = r \paren {\map \cos {\theta + 2 \pi k} + i \, \map \sin {\theta + 2 \pi k} } | c = }} {{eqn | r = r \paren {\cos \theta + i \, \map \sin {\theta + 2 \pi k} } | c = [[Cosine of Angle plus Multiple of Full Angle]] }} {{eqn | r = r \paren {\cos \theta + i \sin \theta} | c = [[Sine of Angle plus Multiple of Full Angle]] }} {{end-eqn}} Now let $m = k + n$. Then: {{begin-eqn}} {{eqn | l = \frac {\theta + 2 m \pi} n | r = \frac {\theta + 2 \paren {k + n} \pi} n | c = }} {{eqn | r = \frac {\theta + 2 k \pi} n + \frac {2 n \pi} n | c = }} {{eqn | r = \frac {\theta + 2 k \pi} n + 2 \pi | c = }} {{eqn | r = \frac {\theta + 2 k \pi} n + 2 \pi | c = }} {{eqn | ll= \leadsto | l = \cos \frac {\theta + 2 m \pi} n + i \sin \frac {\theta + 2 m \pi} n | r = \cos \frac {\theta + 2 k \pi} n + i \sin \frac {\theta + 2 k \pi} n | c = from above }} {{end-eqn}} exploiting the fact that [[Sine and Cosine are Periodic on Reals]]. The result follows. {{qed}}	0
:$\sec 210 \degrees = \sec \dfrac {7 \pi} 6 = -2 \dfrac {\sqrt 3} 3$	0
:$\map \Si 0 = 0$	0
{{begin-eqn}} {{eqn | l = 1 | r = \cos^2 x + \sin^2 x | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | ll= \leadsto | l = \map {D_x} 1 | r = \map {D_x} {\cos^2 x + \sin^2 x} | c = differentiating both sides }} {{eqn | ll= \leadsto | l = 0 | r = 2 \map {D_x} {\cos x} \cos x + 2 \sin x \cos x | c = [[Product Rule for Derivatives]] or [[Chain Rule for Derivatives]] and [[Derivative of Sine Function]] }} {{eqn | ll= \leadsto | l = \map {D_x} {\cos x} | r = -\sin x | c = provided $\cos x \ne 0$ }} {{end-eqn}} {{qed}}	0
We have that $\Z_{>0}$ contains only [[Definition:Strictly Positive Integer|(strictly) positive integers]]. So it follows from [[Reciprocal of Strictly Positive Real Number is Strictly Positive]] that $S$ contains only [[Definition:Strictly Positive Real Number|(strictly) positive real numbers]]. Hence $0$ is a [[Definition:Lower Bound of Subset of Real Numbers|lower bound]] for $S$. {{AimForCont}} that $0$ is not the [[Definition:Infimum of Subset of Real Numbers|infimum]] of $S$. Then $\exists h \in \R_{>0}$ such that $h$ is a [[Definition:Lower Bound of Subset of Real Numbers|lower bound]] for $S$. Then: :$\forall n \in \Z_{>0}: \dfrac 1 n \ge h$ Hence from [[Reciprocal Function is Strictly Decreasing]]: :$\forall n \in \Z_{>0}: n \le \dfrac 1 h$ where $\dfrac 1 h \in \R$. But from the [[Archimedean Principle]]: :$\exists n \in \Z_{>0}: n > \dfrac 1 h$ This is a [[Definition:Contradiction|contradiction]]. It follows by [[Proof by Contradiction]] that no such $h > 0$ exists. Hence $0$ is the [[Definition:Infimum of Subset of Real Numbers|infimum]] of $S$. {{qed}}	0
Let $\CC$ be a [[Definition:Open Cover|open cover]] for $W$. Define $Q := \set {\openint a b: a, b \in \Q}$ Define a [[Definition:Mapping|mapping]] $h: Q \to \Q \times \Q$: :$\forall \openint a b \in Q: \map h {\openint a b} = \tuple {a, b}$ It is easy to see by definition that :$h$ is an [[Definition:Injection|injection]]. By [[Injection iff Cardinal Inequality]]: :$\card Q \le \card {\Q \times \Q}$ where $\card Q$ deontes the [[Definition:Cardinality|cardinality]] of $Q$. By [[Rational Numbers are Countably Infinite]]: :$\Q$ is [[Definition:Countably Infinite Set|countably infinite]]. By definition of [[Definition:Countably Infinite Set|countably infinite]]: :there exists a [[Definition:Bijection|bijection]] $\Q \to \N$ By definitions of [[Definition:Set Equality|set equality]] and [[Definition:Cardinality|cardinality]]: :$\card \Q = \card \N$ By [[Aleph Zero equals Cardinality of Naturals]]: :$\card \Q = \aleph_0$ By [[Cardinal Product Equal to Maximum]]: :$\card {\Q \times \Q} = \max \set {\aleph_0, \aleph_0} = \aleph_0$ By [[Countable iff Cardinality not greater than Aleph Zero]]: :$Q$ is [[Definition:Countable Set|countable]]. By definition of [[Definition:Cover of Set|cover]]: :$W \subseteq \bigcup \CC$ By definition of [[Definition:Union of Set of Sets|imion]]: :$\forall x \in W: \exists U \in \CC: x \in U$ By [[Axiom:Axiom of Choice|Axiom of Choice]] define a [[Definition:Mapping|mapping]] $f: W \to \CC$: :$\forall x \in W: x \in \map f x$ We will prove that :$\forall x \in W: \exists A \in Q: x \in A \land A \cap W \subseteq \map f x$ Let $x \in W$. By definition of [[Definition:Open Cover|open cover]]: :$\map f x$ is [[Definition:Open Set (Topology)|open]] in $R_W$. By definition of [[Definition:Topological Subspace|topological subspace]]: :there exists U a [[Definition:Subset|subset]] of $\R$ such that ::$U$ is [[Definition:Open Set (Topology)|open]] in $R$ and $U \cap W = \map f x$ By definition of $f$: :$x \in \map f x$ By definition of [[Definition:Open Set of Metric Space|open set in metric space]]: :$\exists r > 0: \map {B_r} x \subseteq U$ :$\openint {x - r} {x + r} \subseteq U$ By [[Between two Real Numbers exists Rational Number]]: :$\exists q \in \Q: x - r < q < x$ and $\exists p \in \Q: x < p < x + r$ By definition of $Q$: :$\openint q p \in Q$ Thus by definition of [[Definition:Open Real Interval|open real interval]]: :$x \in \openint q p \subseteq \openint {x - r} {x + r}$ By [[Subset Relation is Transitive]]: :$\openint q p \subseteq U$ Thus by [[Set Intersection Preserves Subsets/Corollary]]: :$\openint q p \cap W \subseteq \map f x$ By [[Axiom:Axiom of Choice|Axiom of Choice]] define a [[Definition:Mapping|mapping]] $f_1: W \to Q$: :$\forall x \in W: x \in \map {f_1} x \land \map {f_1} x \cap W \subseteq \map f x$ By definitions of [[Definition:Image of Subset under Relation|image of set]] and [[Definition:Image of Mapping|image of mapping]]: :$\forall A \in \Img {f_1}: \exists x \in W: x \in f_1^{-1} \sqbrk {\set A}$ By [[Axiom:Axiom of Choice|Axiom of Choice]] define a [[Definition:Mapping|mapping]] $c: \Img {f_1} \to W$: :$\forall A \in \Img {f_1}: \map c A \in f_1^{-1} \sqbrk {\set A}$ Define a [[Definition:Mapping|mapping]] $g: \Img {f_1} \to \CC$: :$g := f \circ c$ Define $\GG = \Img g$. Thus $\GG \subseteq \CC$ By definition of [[Definition:Image of Mapping|image of mapping]]: :$\Img {f_1} \subseteq Q$ By [[Subset of Countable Set is Countable]]; :$\Img {f_1}$ is [[Definition:Countable Set|countable]]. By [[Surjection iff Cardinal Inequality]]: :$\size {\Img g} \le \size {\Img {f_1} }$ Thus by [[Countable iff Cardinality not greater than Aleph Zero]]: :$\GG$ is [[Definition:Countable Set|countable]]. It remains to prove that :$\GG$ is a [[Definition:Cover of Set|cover]] for $W$. Let $x \in W$. By definition of $f_1$: :$ x \in \map {f_1} x \land \map {f_1} x \cap W \subseteq \map f x$ By definition of [[Definition:Image of Mapping|image of mapping]]: :$\map {f_1} x \in \Img {f_1}$ Then by definition of $c$: :$y := \map c {\map {f_1} x} \in f_1^{-1} \sqbrk {\set {\map {f_1} x} }$ By definition of $f_1$: :$y \in \map {f_1} y \land \map {f_1} y \cap W \subseteq \map f y$ By definition of [[Definition:Image of Subset under Relation|image of set]]: :$\map {f_1} y = \map {f_1} x$ Then by definitions of [[Definition:Subset|subset]] and [[Definition:Set Intersection|intersection]]: :$x \in \map f y$ By definition of [[Definition:Composition of Mappings|composition of mappings]]: :$\map f y = \map g {\map {f_1} x}$ By definition of [[Definition:Image of Mapping|image of mapping]]: :$\map g {\map {f_1} x} \in \GG$ Thus by definition of [[Definition:Union of Set of Sets|union]]: :$x \in \bigcup \GG$ Thus the result. {{qed}}	0
From [[Identity Function is Odd Function]], $\map f x$ is a [[Definition:Odd Function|odd function]]. By [[Fourier Series for Odd Function over Symmetric Range]], we have: :$\displaystyle \map f x \sim \sum_{n \mathop = 1}^\infty b_n \sin \frac {n \pi x} \lambda$ where: {{begin-eqn}} {{eqn | l = b_n | r = \frac 2 \lambda \int_0^\pi \map f x \sin \frac {n \pi x} \lambda \rd x | c = }} {{eqn | r = \frac 2 \lambda \int_0^\pi x \sin \frac {n \pi x} \lambda \rd x | c = Definition of $\map f x$ }} {{eqn | r = \frac {2 \lambda} {n \pi} \paren {-1}^{n + 1} | c = [[Half-Range Fourier Sine Series for Identity Function]] }} {{end-eqn}} Substituting for $b_n$ in $(1)$: :$\displaystyle x = \dfrac {2 \lambda} \pi \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n + 1} } n \sin \frac {n \pi x} \lambda$ as required. {{qed}} [[Category:Identity Mappings]] [[Category:Fourier Series for Identity Function]] 3zk880ik4ng176ofyrpm9guf5wtma7n	0
{{begin-eqn}} {{eqn | l = \frac 1 {x \times y} \times \paren {x \times y} | r = 1 | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 4$: Inverses]] }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} \times \paren {x \times y} \times \frac 1 y | r = 1 \times \frac 1 y | c = as $y \ne 0$ }} {{eqn | ll= \leadsto | l = \paren {\frac 1 {x \times y} \times x} \times \paren {y \times \frac 1 y} | r = 1 \times \frac 1 y | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 1$: Associativity]] }} {{eqn | ll= \leadsto | l = \paren {\frac 1 {x \times y} \times x} \times 1 | r = 1 \times \frac 1 y | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 4$: Inverse]] }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} \times x | r = \frac 1 y | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 3$: Identity]] }} {{eqn | ll= \leadsto | l = \paren {\frac 1 {x \times y} \times x} \times \frac 1 x | r = \frac 1 y \times \frac 1 x | c = as $x \ne 0$ }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} \times \paren {x \times \frac 1 x} | r = \frac 1 y \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 1$: Associativity]] }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} \times 1 | r = \frac 1 y \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 4$: Inverse]] }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} | r = \frac 1 y \times \frac 1 x | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 3$: Identity]] }} {{eqn | ll= \leadsto | l = \frac 1 {x \times y} | r = \frac 1 x \times \frac 1 y | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 2$: Commutativity]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = 18 \times \tau \left({18}\right) | r = 18 \times 6 | c = [[Tau Function of 18]] }} {{eqn | r = \left({2 \times 3^2}\right) \times \left({2 \times 3}\right) | c = }} {{eqn | r = 2^2 \times 3^3 | c = }} {{eqn | r = 108 | c = }} {{eqn | l = 27 \times \tau \left({27}\right) | r = 27 \times 4 | c = [[Tau Function of 27]] }} {{eqn | r = 2^2 \times 3^3 | c = }} {{eqn | r = 108 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 24 \times \tau \left({24}\right) | r = 24 \times 8 | c = [[Tau Function of 24]] }} {{eqn | r = \left({2^3 \times 3}\right) \times 2^3 | c = }} {{eqn | r = 2^6 \times 3 | c = }} {{eqn | r = 192 | c = }} {{eqn | l = 32 \times \tau \left({32}\right) | r = 32 \times 6 | c = [[Tau Function of 32]] }} {{eqn | r = 2^5 \times \left({2 \times 3}\right) | c = }} {{eqn | r = 192 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 56 \times \tau \left({56}\right) | r = 56 \times 8 | c = [[Tau Function of 56]] }} {{eqn | r = \left({2^3 \times 7}\right) \times 2^3 | c = }} {{eqn | r = 2^6 \times 7 | c = }} {{eqn | r = 448 | c = }} {{eqn | l = 64 \times \tau \left({64}\right) | r = 64 \times 7 | c = [[Tau Function of 64]] }} {{eqn | r = 2^6 \times 7 | c = }} {{eqn | r = 448 | c = }} {{end-eqn}} {{qed}}	0
From [[Reduction Formula for Primitive of Power of x by Power of a x + b/Decrement of Power of x|Reduction Formula for Primitive of Power of $x$ by Power of $a x + b$: Decrement of Power of $x$]]: :$\displaystyle \int x^m \paren {a x + b}^n \rd x = \frac {x^m \paren {a x + b}^{n + 1} } {\paren {m + n + 1} a} - \frac {m b} {\paren {m + n + 1} a} \int x^{m - 1} \paren {a x + b}^n \rd x$ Let $m = 1$ and $n = -1$. Then: {{begin-eqn}} {{eqn | l = \int \frac {x \rd x} {a x + b} | r = \int x^1 \paren {a x + b}^{-1} \rd x | c = }} {{eqn | r = \frac {x^1 \paren {a x + b}^0} {\paren 1 a} - \frac {1 b} {\paren 1 a} \int x^0 \paren {a x + b}^{-1} \rd x | c = }} {{eqn | r = \frac x a - \frac b a \int \frac {\d x} {a x + b} | c = simplifying }} {{eqn | r = \frac x a - \frac b {a^2} \ln \size {a x + b} + C | c = [[Primitive of Reciprocal of a x + b|Primitive of Reciprocal of $a x + b$]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = 1 \times 1 | o = > | r = 0 | c = [[Square of Non-Zero Real Number is Strictly Positive]] }} {{eqn | ll= \leadsto | l = 1 | o = > | r = 0 | c = [[Definition:Real Number Axioms|Real Number Axioms: $\R \text M 3$: Identity]] }} {{eqn | ll= \leadsto | l = 0 | o = < | r = 1 | c = {{Defof|Dual Ordering}} }} {{end-eqn}} {{qed}}	0
Assume that $w$ is [[Definition:Wholly Real|wholly real]]. Then: {{begin-eqn}} {{eqn | l = w z | r = \map \Re w \, \map \Re z - \map \Im w \, \map \Im z + i \paren {\map \Re w \, \map \Im z + \map \Im w \, \map \Re z} | c = {{Defof|Complex Multiplication}} }} {{eqn | r = w \, \map \Re z + i w \, \map \Im z | c = as $\map \Re w = w$ and $\map \Im w = 0$ }} {{end-eqn}} This equation shows that $\map \Re {w z} = w \, \map \Re z$, and $\map \Im {w z} = w \, \map \Im z$. This proves $(1)$. Now, assume that $w$ is [[Definition:Wholly Imaginary|wholly imaginary]]. Then: {{begin-eqn}} {{eqn | l = w z | r = \map \Re w \, \map \Re z - \map \Im w \, \map \Im z + i \paren {\map \Re w \, \map \Im z + \map \Im w \, \map \Re z} }} {{eqn | r = -\map \Im w \, \map \Im z) + i \, \map \Im w \, \map \Re z | c = as $\map \Re w = 0$ }} {{end-eqn}} This equation shows that $\map \Re {w z} = -\map \Im w \, \map \Im z$, and $\map \Im {w c} = \map \Im w \, \map \Re z$. This proves $(2)$. {{qed}} [[Category:Complex Multiplication]] 9aoaf7jbh49ghocleog40chvkz0qv08	0
By definition of the [[Definition:Set of Residue Classes||set of residue classes modulo $m$]], $Z_m$ is the [[Definition:Quotient Set|quotient set]] of [[Definition:Congruence Modulo Integer|congruence modulo $m$]]: :$\Z_m = \dfrac \Z {\mathcal R_m}$ where $\mathcal R_m$ is the [[Definition:Congruence Modulo Integer|congruence relation modulo $m$]] on the [[Definition:Set|set]] of all $a, b \in \Z$: :$\mathcal R_m = \set {\tuple {a, b} \in \Z \times \Z: \exists k \in \Z: a = b + k m}$ By the [[Fundamental Theorem on Equivalence Relations]], $Z_m$ is a [[Definition:Set Partition|partition]] of $\Z$. Thus: :$\eqclass a m = \eqclass b m$ {{iff}}: :$x \in \eqclass a m \iff x \in \eqclass b m$ {{iff}}: :$a \equiv b \pmod m$ {{qed}}	0
:$\csc 225 \degrees = \csc \dfrac {5 \pi} 4 = -\sqrt 2$	0
:$\displaystyle \int_0^{\frac \pi 2} \cos^2 x \rd x = \frac \pi 4$	0
Let $w, z \in \C$ be [[Definition:Complex Number|complex numbers]]. $(1)$ If $w$ is [[Definition:Wholly Real|wholly real]], then: :$\map \Re {w z} = w \, \map \Re z$ and: :$\map \Im {w z} = w \, \map \Im z$ $(2)$ If $w$ is [[Definition:Wholly Imaginary|wholly imaginary]], then: :$\map \Re {w z} = -\map \Im w \, \map \Im z$ and: :$\map \Im {w z} = \map \Im w \, \map \Re z$ Here, $\map \Re z$ denotes the [[Definition:Real Part|real part]] of $z$, and $\map \Im z$ denotes the [[Definition:Imaginary Part|imaginary part]] of $z$.	0
Let us define $\eqclass {\tuple {a, b} } \boxtimes$ as in the [[Definition:Integer/Formal Definition|formal definition of integers]]. That is, $\eqclass {\tuple {a, b} } \boxtimes$ is an [[Definition:Equivalence Class|equivalence class]] of [[Definition:Ordered Pair|ordered pairs]] of [[Definition:Natural Numbers|natural numbers]] under the [[Definition:Congruence Relation|congruence relation]] $\boxtimes$. $\boxtimes$ is the [[Definition:Congruence Relation|congruence relation]] defined on $\N \times \N$ by $\tuple {x_1, y_1} \boxtimes \tuple {x_2, y_2} \iff x_1 + y_2 = x_2 + y_1$. In order to streamline the notation, we will use $\eqclass {a, b} {}$ to mean $\eqclass {\tuple {a, b} } \boxtimes$, [[Definition:Integer/Formal Definition/Notation|as suggested]]. From [[Construction of Inverse Completion|the method of construction]], $\eqclass {c + 1, c} {}$, where $c$ is any element of the [[Definition:Natural Numbers|natural numbers]] $\N$, is [[Definition:Isomorphic Copy|the isomorphic copy]] of $1 \in \N$. To ease the algebra, we will take $\eqclass {1, 0} {}$ as a canonical instance of this [[Definition:Equivalence Class|equivalence class]]. Thus it is to be shown that: :$\forall a, b \in \N: \eqclass {a, b} {} \times \eqclass {1, 0} {} = \eqclass {a, b} {} = \eqclass {1, 0} {} \times \eqclass {a, b} {}$ From [[Natural Numbers form Commutative Semiring]], we take it for granted that: :[[Definition:Natural Number Addition|addition]] and [[Definition:Natural Number Multiplication|multiplication]] are [[Definition:Commutative Operation|commutative]] and [[Definition:Associative Operation|associative]] on the [[Definition:Natural Numbers|natural numbers]] $\N$ :[[Definition:Natural Number Multiplication|natural number multiplication]] is [[Definition:Distributive Operation|distributive]] over [[Definition:Natural Number Addition|natural number addition]]. So: {{begin-eqn}} {{eqn | l = \eqclass {a, b} {} \times \eqclass {1, 0} {} | r = \eqclass {1 \times a + 0 \times b, 0 \times a + 1 \times b} {} | c = }} {{eqn | r = \eqclass {a, b} {} | c = from [[Construction of Inverse Completion/Equivalence Relation/Members of Equivalence Classes|Construction of Inverse Completion: Members of Equivalence Classes]] }} {{end-eqn}} So: :$\eqclass {a, b} {} \times \eqclass {1, 0} {} = \eqclass {a, b} {}$ The identity $\eqclass {a, b} {} = \eqclass {1, 0} {} \times \eqclass {a, b} {}$ is demonstrated similarly. {{qed}}	0
Let $\epsilon \in \R_{>0}$, and let $z \in \C$. Suppose that $\cmod {z - \xi} = R - \epsilon$. By definition of [[Definition:Radius of Convergence of Complex Power Series|radius of convergence]], it follows that $S \paren z$ is [[Definition:Absolutely Convergent Series|absolutely convergent]]. From the [[Nth Root Test|$n$th Root Test]]: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n \paren {z - \xi}^n}^{1/n} \le 1$ By [[Multiple Rule for Complex Sequences]], this inequality can be rearranged to obtain: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n}^{1/n} \le \dfrac 1 {\cmod {z - \xi} } = \dfrac 1 {R - \epsilon}$ As $\epsilon > 0$ was arbitrary, it follows that: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n}^{1/n} \le \dfrac 1 R$ {{qed|lemma}} Now, suppose that $\cmod {z - \xi} = R + \epsilon$. Then $S \paren z$ is [[Definition:Divergent Series|divergent]], so the [[Nth Root Test|$n$th Root Test]] shows that: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n \paren {z - \xi}^n}^{1/n} \ge 1$ which we can rearrange to obtain: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n}^{1/n} \ge \dfrac 1 {\cmod {z - \xi} } = \dfrac 1 {R - \epsilon}$ As $\epsilon > 0$ was arbitrary, it follows that: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n}^{1/n} \ge \dfrac 1 R$ {{qed|lemma}} Thus: :$\displaystyle \limsup_{n \mathop \to \infty} \cmod {a_n}^{1/n} = \dfrac 1 R$ Hence the result. {{qed}}	0
Let $m, n \in \Z_{> 0}$ be [[Definition:Strictly Positive Integer|(strictly) positive integers]]. Let $\struct {m \Z, +}$ and $\struct {n \Z, +}$ be the corresponding [[Definition:Additive Group of Integer Multiples|additive groups of integer multiples]]. Then: :$\struct {m \Z, +} \cap \struct {n \Z, +} = \struct {\lcm \set {m, n} \Z, +}$	0
{{begin-eqn}} {{eqn | o = | r = -2 \, \map \sin {\frac {\alpha + \beta} 2} \, \map \sin {\frac {\alpha - \beta} 2} | c = }} {{eqn | r = -2 \frac {\map \cos {\dfrac {\alpha + \beta} 2 - \dfrac {\alpha - \beta} 2} - \map \cos {\dfrac {\alpha + \beta} 2 + \dfrac {\alpha - \beta} 2} } 2 | c = [[Simpson's Formula for Sine by Sine]] }} {{eqn | r = -\paren {\cos \frac {2 \beta} 2 - \cos \frac {2 \alpha} 2} | c = }} {{eqn | r = \cos \alpha - \cos \beta | c = }} {{end-eqn}} {{qed}}	0
{{:Euclid:Proposition/V/7}} That is: :$a = b \implies a : c = b : c$ :$a = b \implies c : a = c : b$	0
Let $\Bbb O$ be the [[Definition:Set|set]] of [[Definition:Odd Integer|odd integers]]. Then $\Bbb O$ is [[Definition:Countably Infinite Set|countably infinite]].	0
The [[Definition:Hyperbolic Cotangent|hyperbolic cotangent function]] has a [[Definition:Taylor Series|Taylor series expansion]]: {{begin-eqn}} {{eqn | l = \coth x | r = \sum_{n \mathop = 0}^\infty \frac {2^{2 n} B_{2 n} \, x^{2 n - 1} } {\left({2 n}\right)!} | c = }} {{eqn | r = \frac 1 x + \frac x 3 - \frac {x^3} {45} + \frac {2 x^5} {45} + \cdots | c = }} {{end-eqn}} where $B_{2 n}$ denotes the [[Definition:Bernoulli Numbers|Bernoulli numbers]]. This [[Definition:Convergent Series|converges]] for $0 < \left|{x}\right| < \pi$.	0
Let $\R$ be the [[Definition:Real Number Line|real number line]] considered as an [[Definition:Euclidean Space|Euclidean space]]. Let $\openint a b \subset \R$ be an [[Definition:Open Real Interval|open interval]] of $\R$. Then $\openint a b$ is an [[Definition:Open Set (Metric Space)|open set]] of $\R$.	0
$x$ is [[Definition:Finite Set|finite]] {{iff}} $x \sim \N_n$ for some $n \in \N$, by definition. But $x$ is an [[Definition:Ordinal|ordinal]], and by definition, it is equal to its [[Definition:Initial Segment|initial segment]]. By definition of the [[Definition:Von Neumann Construction of Natural Numbers|von Neumann construction of natural numbers]], it follows that $x \sim n$ for some $n$. By [[Finite Ordinal is equal to Natural Number]], it follows that $x$ is equal to $n$. Thus, $x$ is an [[Definition:Element|element]] of the [[Definition:Minimal Infinite Successor Set|minimal infinite successor set]]. {{qed}}	0
Let $n$ be composite such that $n \ge 0$. From [[Composite Number has Two Divisors Less Than It]], we can write $n = a b$ where $a, b \in \Z$ and $1 < a, b < n$. {{WLOG}}, suppose that $a \le b$. Let $a > \sqrt n$. Then $b \ge a > \sqrt n$. However, if $b \ge a > \sqrt n$ is true, then: :$n = a b > \sqrt n \sqrt n > n$ This is clearly a [[Definition:Contradiction|contradiction]]. So: :$a \le \sqrt n$ From [[Positive Integer Greater than 1 has Prime Divisor]] it follows that there is some [[Definition:Prime Number|prime]] $p$ which divides $a$. From [[Absolute Value of Integer is not less than Divisors]], we have that $p \le a$ and so: :$p \le \sqrt n$ From [[Divisor Relation on Positive Integers is Partial Ordering]]: : $p \divides n$ Hence the result. {{qed}}	0
:$\displaystyle \int \cos^n a x \sin a x \rd x = \frac {-\cos^{n + 1} a x} {\paren {n + 1} a} + C$	0
Recall: {{:Definition:Hexagonal Number/Sequence}} Hence: {{begin-eqn}} {{eqn | l = 11 | r = 6 + 1 + 1 + 1 + 1 + 1 | c = }} {{eqn | l = 26 | r = 6 + 6 + 6 + 6 + 1 + 1 | c = }} {{end-eqn}} {{qed}}	0
Note that the [[Definition:Integrand|integrand]] is of the form: :$\displaystyle \int_0^\infty \frac {\map f {p x} - \map f {q x} } x \rd x$ where: :$\map f x = \arctan x$ We have, by [[Derivative of Arctangent Function]]: :$\map {f'} x = \dfrac 1 {1 + x^2}$ which is [[Definition:Continuous Mapping|continuous]] on $\R$. By [[Limit to Infinity of Arctangent Function]]: :$\displaystyle \lim_{x \mathop \to \infty} \map f x = \lim_{x \mathop \to \infty} \arctan x = \frac \pi 2$ As $f$ is [[Definition:Continuously Differentiable|continuously differentiable]] and $\displaystyle \lim_{x \mathop \to \infty} \map f x$ exists and is finite, we may apply [[Frullani's Integral]], giving: {{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\arctan p x - \arctan q x} x \rd x | r = \paren {\lim_{x \mathop \to \infty} \arctan x - \arctan 0} \ln \frac p q }} {{eqn | r = \frac \pi 2 \ln \frac p q | c = [[Arctangent of Zero is Zero]] }} {{end-eqn}} {{qed}}	0
Let $x \in \R_{\ge 0}$ be represented in the [[Definition:Golden Mean Number System|golden mean number system]]. Let $S$ be the representation for $x$ in its [[Definition:Simplest Form of Number in Golden Mean Number System|simplest form]]. Then $S$ is unique in the sense that there exists no other representation of $x$ in [[Definition:Simplest Form of Number in Golden Mean Number System|simplest form]].	0
Let $D \subset \C$ be [[Definition:Open Set (Complex Analysis)|open]]. Let $f, g: D \to \C$ be [[Definition:Analytic Function|analytic]]. Let $z \in D$ with $f \left({z}\right) \ne 0 \ne g \left({z}\right)$. Then: :$\dfrac{\left({f g}\right)' \left({z}\right)} {\left({f g}\right) \left({z}\right)} = \dfrac{f' \left({z}\right)} {f \left({z}\right)} + \dfrac {g' \left({z}\right)} {g \left({z}\right)}$ {{explain|Link to a clarifying definition of what $\left({f g}\right)$ is -- presumably it's the pointwise product, but in the context there's nothing to say it can't be $\left({f \circ g}\right)$, that is, $f$ of $g$ of $z$.}}	0
:$\cos^4 x = \dfrac {3 + 4 \cos 2 x + \cos 4 x} 8$	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x \left({x^3 + a^3}\right)^2} | r = \int \frac {a^3 \ \mathrm d x} {a^3 x \left({x^3 + a^3}\right)^2} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $a^3$ }} {{eqn | r = \int \frac {\left({x^3 + a^3 - x^3}\right) \ \mathrm d x} {a^3 x \left({x^3 + a^3}\right)^2} | c = }} {{eqn | r = \frac 1 {a^3} \int \frac {\left({x^3 + a^3}\right) \ \mathrm d x} {x \left({x^3 + a^3}\right)^2} - \frac 1 {a^3} \int \frac {x^3 \ \mathrm d x} {x \left({x^3 + a^3}\right)^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^3} \int \frac {\mathrm d x} {x \left({x^3 + a^3}\right)} - \frac 1 {a^3} \int \frac {x^2 \ \mathrm d x} {\left({x^3 + a^3}\right)^2} | c = simplifying }} {{eqn | r = \frac 1 {a^3} \left({\frac 1 {3 a^3} \ln \left\vert{\frac {x^3} {x^3 + a^3} }\right\vert}\right) - \frac 1 {a^3} \int \frac {x^2 \ \mathrm d x} {\left({x^3 + a^3}\right)^2} | c = [[Primitive of Reciprocal of x by x cubed plus a cubed|Primitive of $\dfrac 1 {x \left({x^3 + a^3}\right)}$]] }} {{eqn | r = \frac 1 {a^3} \left({\frac 1 {3 a^3} \ln \left\vert{\frac {x^3} {x^3 + a^3} }\right\vert}\right) - \frac 1 {a^3} \left({\frac {-1} {3 \left({x^3 + a^3}\right)} }\right) | c = [[Primitive of x squared over x cubed plus a cubed squared|Primitive of $\dfrac {x^2} {\left({x^3 + a^3}\right)^2}$]] }} {{eqn | r = \frac 1 {3 a^3 \left({x^3 + a^3}\right)} + \frac 1 {3 a^6} \ln \left\vert{\frac {x^3} {x^3 + a^3} }\right\vert | c = simplifying }} {{end-eqn}} {{qed}}	0
{{:Euclid:Proposition/II/1}} :[[File:Euclid-II-1.png|400px]] Let $A$ and $BC$ be two [[Definition:Straight Line|straight lines]]. Let $BC$ be cut at random at points $D$ and $E$. Then the [[Definition:Containment of Rectangle|rectangle contained]] by $A$ and $BC$ is equal to the sum of the [[Definition:Containment of Rectangle|rectangles contained]] by $A$ and $BD$, by $A$ and $DE$, and by $A$ and $EC$, as follows: [[Construction of Perpendicular Line|Construct $BF$ perpendicular]] to $BC$. [[Construction of Equal Straight Lines from Unequal|Construct $BG$ on $BF$ equal to $A$]]. [[Construction of Parallel Line|Construct $GH$ through $G$ parallel to $BC$]]. [[Construction of Parallel Line|Construct $DK, EL, CH$ through $D, E, C$ parallel to $BG$]]. Then we have that: : $\Box BCHG = \Box BDKG + \Box DELK + \Box ECHL$ Now $\Box BCHG$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $A$ and $BC$, because it is contained by $BC$ and $BG$, and $BG = A$. Similarly, from [[Opposite Sides and Angles of Parallelogram are Equal]]: : $\Box BDKG$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $A$ and $BD$, because it is contained by $BD$ and $BG = A$ : $\Box DEKL$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $A$ and $DE$, because it is contained by $DE$ and $DK$, and $DK = A$ : $\Box ECHL$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $A$ and $EC$, because it is contained by $EC$ and $EL$, and $EL = A$. Hence the result. {{qed}}	0
Let $n$ be a [[Definition:Finite Ordinal|finite ordinal]]. Let $x$ be a [[Definition:Transfinite Ordinal|transfinite ordinal]]. Then: : $n + x = x$	0
Firstly, we will prove that $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$, where $n \in \N$. Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: : $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ === Basis for the Induction === $\map P 1$ is true, as this says $\displaystyle \frac {\sin z} z = \frac {\sin z} z$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 0$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i}$ Then we need to show: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Sine of X over X as Infinite Product#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {\frac {2^k} z} \paren {2 \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Double Angle Formulas/Sine|Double Angle Formula for Sine]] }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ And then: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } \prod_{i \mathop = 1}^n \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } } \prod_{i \mathop = 1}^{\infty} \cos \frac z {2^i} | c = }} {{eqn | r = \paren 1 \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = [[Limit of Sine of X over X]] }} {{eqn | r = \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \lg x | r = \frac {\ln x} {\ln 2} | c = [[Change of Base of Logarithm]] }} {{eqn | r = \frac {\log_{10} x} {\log_{10} 2} | c = [[Change of Base of Logarithm]] }} {{eqn | ll= \leadsto | l = \lg x | r = \frac {\ln x + \log_{10} x} {\ln 2 + \log_{10} 2} | c = }} {{eqn | r = \frac {\ln x + \log_{10} x} {\ln 2 + \frac {\ln 2} {\ln 10} } | c = [[Change of Base of Logarithm]] }} {{eqn | r = \frac {\ln x + \log_{10} x} {\ln 2 \left({1 + \frac 1 {\ln 10} }\right)} | c = }} {{eqn | r = \frac {\ln x + \log_{10} x} {\ln 2 \left({1 + \log_{10} e}\right)} | c = [[Reciprocal of Logarithm]] }} {{end-eqn}} We have that: :$\ln 2 = 0 \cdotp 69347 \ 1805 \ldots$ :$\log_{10} e = 0 \cdotp 43429 \ 44819 \ldots$ Thus: {{begin-eqn}} {{eqn | l = \ln 2 \left({1 + \log_{10} e}\right) | r = 0 \cdotp 69347 \ 1805 \ldots \left({1 + 0 \cdotp 43429 \ 44819 \ldots}\right) | c = }} {{eqn | r = 0 \cdotp 99462 \ldots | c = }} {{end-eqn}} Hence: :$\lg x + \log_{10} x \approx 0 \cdotp 994 \ln x$ That is, the approximation is $99.4 \%$ accurate. {{qed}}	0
Let $T_A = \struct {A, \tau_A}$ and $T_B = \struct {B, \tau_B}$ be [[Definition:Topological Space|topological spaces]]. Let $\phi: T_A \to T_B$ be a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]. If $T_A$ is a [[Definition:First-Countable Space|first-countable space]], then it does not necessarily follow that $T_B$ is also [[Definition:First-Countable Space|first-countable]].	0
:$\displaystyle \int \frac {\d x} {\paren {\sqrt {x^2 - a^2} }^3} = \frac {-x} {a^2 \sqrt {x^2 - a^2} } + C$	0
{{begin-eqn}} {{eqn | l = \sqrt {30 \, 739} | o = \approx | r = 175 \cdotp 32541 \, 17349 | c = }} {{eqn | l = \sqrt [3] {30 \, 739} | o = \approx | r = 31 \cdotp 32539 \, 66116 | c = }} {{eqn | ll= \leadsto | l = \sqrt {30 \, 739} - \sqrt [3] {30 \, 739} | o = \approx | r = 144 \cdotp 00001 \, 5123 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \sqrt {62 \, 324} | o = \approx | r = 249 \cdotp 64775 \, 18425 | c = }} {{eqn | l = \sqrt [3] {62 \, 324} | o = \approx | r = 39 \cdotp 64774 \, 02668 | c = }} {{eqn | ll= \leadsto | l = \sqrt {62 \, 324} - \sqrt [3] {62 \, 324} | o = \approx | r = 210 \cdotp 00001 \, 1576 | c = }} {{end-eqn}} {{finish|This could be turned into a page where the sequence of $n$ for which this difference is smaller than for any smaller $n$.}}	0
Let $I$ be a [[Definition:Real Interval|real interval]]. Let $f: I \to \R$ be an [[Definition:Injection|injective]] [[Definition:Continuous on Interval|continuous real function]]. Then $f$ is [[Definition:Strictly Monotone Real Function|strictly monotone]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x \paren {x^2 + a^2} } | r = \int \frac {a^2 \rd x} {a^2 x \paren {x^2 + a^2} } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $a^2$ }} {{eqn | r = \int \frac {\paren {x^2 + a^2 - x^2} \rd x} {a^2 x \paren {x^2 + a^2} } | c = adding and subtracting $x^2$ }} {{eqn | r = \frac 1 {a^2} \int \frac {\paren {x^2 + a^2} \rd x} {x \paren {x^2 + a^2} } - \frac 1 {a^2} \int \frac {x^2 \rd x} {x \paren {x^2 + a^2} } | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\d x} x - \frac 1 {a^2} \int \frac {x \rd x} {x^2 + a^2} | c = simplifying }} {{eqn | r = \frac 1 {a^2} \ln \size x - \frac 1 {a^2} \int \frac {x \rd x} {x^2 + a^2} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^2} \ln \size x - \frac 1 {a^2} \paren {\frac 1 2 \map \ln {x^2 + a^2} } + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{eqn | r = \frac 1 {2 a^2} \map \ln {\frac {x^2} {x^2 + a^2} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
From [[Gamma Difference Equation]]: :$\map \Gamma {z + 1} = z \map \Gamma z$ Hence: {{begin-eqn}} {{eqn | l = \frac {\d} {\d z} \map \Log {\map \Gamma {z + 1} } | r = \frac {\d} {\d z} \map \Log {z \map \Gamma z} | c = }} {{eqn | r = \frac {\d} {\d z} \map \Log {\map \Gamma z} + \map \Log z | c = }} {{eqn | r = \frac {\d} {\d z} \map \Log {\map \Gamma z} + \frac {\d} {\d z} \map \Log z | c = [[Sum Rule for Derivatives]] }} {{eqn | r = \frac {\d} {\d z} \map \Log {\map \Gamma z} + \frac 1 z | c = [[Derivative of Natural Logarithm Function]] }} {{end-eqn}} Successive use of this identity gives us {{begin-eqn}} {{eqn | l = \frac {\d} {\d z} \map \Log {\map \Gamma z} | r = \frac {\d} {\d z} \map \Log {\map \Gamma {z + M} } - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} | c = }} {{end-eqn}} and thus from the [[Sum Rule for Derivatives]]: {{begin-eqn}} {{eqn | l = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma z} | r = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma {z + M} } + \sum_{k \mathop = 0}^{M - 1} \frac {\paren {-1}^n \paren {n - 1}!} {\paren {z + k}^n} }} {{end-eqn}} From [[Stirling's Formula for Gamma Function]]: {{begin-eqn}} {{eqn | l = \map \Log {\map \Gamma {z + 1} } | r = z \map \Log {\map \Gamma z} - z + \frac 1 2 \map \Log z + \map \OO {\frac 1 z} | c = }} {{eqn | l = \frac {\d} {\d z} \map \Log {\map \Gamma {z + 1} } | r = \map \Log z + \map \OO {\frac 1 z} | c = }} {{eqn | l = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma {z + 1} } | r = \map \OO {\frac 1 z} | c = }} {{eqn | l = \frac {\d} {\d z} \map \Log {\map \Gamma {z + M} } - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} | r = \map \Log {z + M} - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} + \map \OO {\frac 1 {z + M} } | c = }} {{eqn | l = \lim_{M \to \infty} \frac {\d} {\d z} \map \Log {\map \Gamma {z + M} } - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} | r = \lim_{M \to \infty} \map \Log {z + M} - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} + \lim_{M \to \infty} \map \OO {\frac 1 {z + M} } }} {{eqn | r = \lim_{M \to \infty} \map \Log {z + M} - \sum_{k \mathop = 0}^{M - 1} \frac 1 {z + k} }} {{end-eqn}} which by definition of the [[Definition:Euler-Mascheroni Constant|Euler-Mascheroni constant]]: :$\displaystyle \frac {\d} {\d z} \map \Log {\map \Gamma 1} = \lim_{M \to \infty} \frac {\d} {\d z} \map \Log {\map \Gamma {1 + M} } - \sum_{k \mathop = 0}^{M - 1} \frac 1 {1 + k} = -\gamma$ Also: :$\dfrac {\d^{1 + k} } {\d z^{1 + k}} \map \Log {\map \Gamma {z + 1} } = \map \OO {\frac 1 z}$ shows that: :$\displaystyle \lim_{M \to \infty} \frac {\d^{1 + k} } {\d z^{1 + k} } \map \Log {\map \Gamma {M + 1} } = 0$ thus for $n > 1$: {{begin-eqn}} {{eqn | l = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma 1} | r = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma {1 + M} } + \sum_{k \mathop = 1}^M \frac {\paren {-1}^n \paren {n - 1}!} {k^n} | c = }} {{eqn | l = \frac {\d^n} {\d z^n} \map \Log {\map \Gamma 1} | r = \lim_{M \to \infty} \frac {\d^n} {\d z^n} \map \Log {\map \Gamma {1 + M} } + \sum_{k \mathop = 1}^M \frac {\paren {-1}^n \paren {n - 1}!} {k^n} }} {{eqn | r = \map \zeta n \paren {n - 1}! \paren {-1}^n | c = }} {{end-eqn}} Thus by definition of [[Definition:Taylor Series|Taylor series]]: {{begin-eqn}} {{eqn | l = \map \Log {\map \Gamma z} | r = \map \Log {\map \Gamma 1} - \gamma \paren {z - 1} + \sum_{k \mathop = 2}^\infty \frac {\paren {-1}^k \map \zeta k \paren {k - 1}!} {k!} \paren {z - 1}^k }} {{eqn | r = -\gamma \paren {z - 1} + \sum_{k \mathop = 2}^\infty \frac{\paren {-1}^k \map \zeta k} k \paren {z - 1}^k }} {{end-eqn}} From [[Zeroes of Gamma Function]], we see that $\map \Gamma z$ is non-zero everywhere. Thus $\map \Log {\map \Gamma z}$ has poles only where $\Gamma$ does, that is, the [[Definition:Negative Integer|negative integers]]. Since the [[Definition:Radius of Convergence|radius of convergence]] of a [[Definition:Power Series|power series]] is equal to the distance of its center to the closest point where the function is not [[Definition:Analytic Function|analytic]]: The [[Definition:Radius of Convergence|radius of convergence]] of $\map \Log {\map \Gamma z}$ is $\cmod {1 - 0} = 1$. {{qed}} [[Category:Gamma Function]] [[Category:Natural Logarithms]] bw2a974j6jwrkswpkzjbml5mgy5zseg	0
We have by definition that $\Z \sqbrk i \subseteq \C$. Let $a, b \in \Z \sqbrk i$. We have from [[Modulus of Product]] that: :$\cmod a \cdot \cmod b = \cmod {a b}$ From [[Complex Modulus is Norm]] we have that: :$\forall a \in \C: \cmod a \ge 0$ :$\cmod a = 0 \iff a = 0$ Suppose $a = x + i y \in \Z \sqbrk i$ and $a \ne 0$. Then either $x \ne 0$ or $y \ne 0$, so either $x^2 \ge 1$ or $y^2 \ge 1$. So $\cmod a^2 \ge 1$. If also $b \in \Z \sqbrk i, b \ne 0$, then: :$\map \nu {a b} = \cmod {a b}^2 \ge \cmod a^2 = \map \nu a$ Now, consider $x, y \in Z \sqbrk i$ with $y \ne 0$. We want to find $q, r \in Z \sqbrk i$ such that $x = q y + r$ and $\map \nu r < \map \nu y$. Note that this means we want $r = y \paren {\dfrac x y - q}$ where $\dfrac x y$ is [[Definition:Complex Number|complex]] but not necessarily [[Definition:Gaussian Integer|Gaussian]]. Consider the [[Definition:Complex Number|complex number]] $p = \dfrac x y = p_r + i p_i$. We [[Definition:Extension of Mapping|extend]] $\nu$ to the [[Definition:Complex Number|complex numbers]] and define $\nu: \C \to \C$ as: :$\forall z \in \C: \map \nu z = \cmod z^2$ Let $q = q_r + i q_i$ be the [[Definition:Gaussian Integer|Gaussian integer]] such that: :$\map \nu {p - q} = \cmod {p - q}^2 = \paren {p_r - q_r}^2 + \paren {p_i - q_i}^2$ is [[Definition:Minimal Element|minimal]]. That is, $q_r$ is the nearest [[Definition:Integer|integer]] to $p_r$ and $q_i$ is the nearest integer to $p_i$. A given real number can be at a [[Definition:Distance between Real Numbers|distance]] at most $1/2$ from an integer, so it follows that: :$(1): \quad \map \nu {p - q} \le \paren {\dfrac 1 2}^2 + \paren {\dfrac 1 2}^2 = \dfrac 1 2$ Now by [[Complex Modulus is Norm]], for any two complex numbers $z_1, z_2$ we have: :$\map \nu {z_1 z_2} = \map \nu {z_1} \, \map \nu {z_2}$ Thus we obtain: {{begin-eqn}} {{eqn | l = \map \nu {y \paren {p - q} } | r = \map \nu y \, \map \nu {p - q} }} {{eqn | r = \dfrac {\map \nu y} 2 | o = \le | c = by $(1)$ }} {{eqn | r = \map \nu y | o = < | c = since $\map \nu y \ne 0$ }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | l = \map \nu {y \paren {p - q} } | r = \map \nu {y \paren {\frac x y - q} } }} {{eqn | r = \map \nu {x - y q} }} {{end-eqn}} So letting $r = x - y q$ we have $\map \nu r < \map \nu y$. Moreover we trivially have $x = q y + r$. Thus $\Z \sqbrk i$ is a [[Definition:Euclidean Domain|Euclidean domain]]. {{qed}}	0
Let $j$ be the largest [[Definition:Integer|integer]] such that: :$j m \le x y$ Let $p$ be the largest [[Definition:Integer|integer]] such that: :$p m \le y z$ By definition of [[Definition:Modulo Multiplication|multiplication modulo $m$]]: :$x \cdot_m y = x y - j m$ :$y \cdot_m z = y z - p m$ Let $k$ be the largest [[Definition:Integer|integer]] such that: :$k m \le \paren {x y - j m} z$ Let $q$ be the largest [[Definition:Integer|integer]] such that: :$q m \le x \paren {y z - p m}$ Then: :$\paren {j z + k} m \le \paren {x y} z$ :$\paren {q + x p} m \le x \paren {y z}$ Thus: {{begin-eqn}} {{eqn | l = \paren {x \cdot_m y} \cdot_m z | r = \paren {x y - j m} z - k m | c = {{Defof|Modulo Multiplication}} }} {{eqn | l = x \cdot_m \paren {y \cdot_m z} | r = x \left({y z - p m}\right) - q m | c = {{Defof|Modulo Multiplication}} }} {{end-eqn}} But suppose that there exists an [[Definition:Integer|integer]] $s$ such that: :$s m \le \paren {x y} z$ and: :$j z + k < s$ Then: :$\paren {j z + k + 1} m \le \paren {x y} z$ from which: :$\paren {k + 1} m \le \paren {x y - j m} z$ But this contradicts the definition of $k$. Thus $j z + k$ is the largest of those [[Definition:Integer|integers]] $i$ such that $i m \le \paren {x y} z$. Similarly, $q + x p$ is the largest of those [[Definition:Integer|integers]] $i$ such that $i m \le x \paren {y z}$. From [[Integer Multiplication is Associative]]: :$\paren {x y} z = x \paren {y z}$ Thus $j z + k = q + x p$ and so: {{begin-eqn}} {{eqn | l = \paren {x \cdot_m y} \cdot_m z | r = \paren {x y - j m} z - k m | c = {{Defof|Modulo Multiplication}} }} {{eqn | r = x y z - \paren {j z + k} m | c = }} {{eqn | r = x y z - \paren {q + x p} m | c = }} {{eqn | r = x \paren {y z - p m} - q m | c = }} {{eqn | r = x \cdot_m \paren {y \cdot_m z} | c = {{Defof|Modulo Multiplication}} }} {{end-eqn}} {{qed}}	0
By definition, the [[Definition:Free Commutative Monoid|free commutative monoid]] $M$ on $\left\{{x}\right\}$ is: :$M = \left\{{e, x, x^2, x^3, \ldots}\right\}$ where $e$ denotes the [[Definition:Null Sequence|null sequence]] of elements of $X$. Let $\phi$ denote the [[Definition:Mapping|mapping]] from $M$ to $\N$ as: :$\forall a \in M: \phi \left({a}\right) = \begin{cases} 0 & : a = e \\ n & : a = x^n \end{cases}$ By definition of $\phi$: :$\phi$ is [[Definition:Injection|injective]]: $\phi \left({a}\right) = \phi \left({b}\right) \implies a = b$ :$\phi$ is [[Definition:Surjection|surjective]]: $\forall a \in \N: \exists b \in M: \phi \left({b}\right) = a$ :$\phi$ is a [[Definition:Monoid Homomorphism|monoid homomorphism]]: $\phi \left({a b}\right) = \phi \left({a + b}\right) = \phi \left({a}\right) + \phi \left({b}\right)$ Hence the result, by definition of [[Definition:Monoid Isomorphism|isomorphism]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\cos a x \rd x} x | r = \ln \size x + \sum_{k \mathop \ge 1} \frac {\paren {-1}^k \paren {a x}^{2 k} } {\paren {2 k} \paren {2 k}!} + C }} {{eqn | r = \ln \size x - \frac {\paren {a x}^2} {2 \times 2!} + \frac {\paren {a x}^4} {4 \times 4!} - \frac {\paren {a x}^6} {6 \times 6!} - \cdots + C }} {{end-eqn}}	0
Let $\varepsilon$ be the [[Definition:Identity Arithmetic Function|identity arithmetic function]]. By [[Dirichlet Series of Identity Arithmetic Function]], $\varepsilon$ has [[Definition:Dirichlet Series|Dirichlet series]] $E(s) = 1$. By [[Dirichlet Series of Convolution of Arithmetic Functions]], $F(s)G(s) = 1$. {{qed}} [[Category:Dirichlet Series]] b8l8wup13ftuuhffklh4wu1yb17bdtn	0
Let $L$ be the rectangular [[Definition:Contour (Complex Plane)|contour]] with the vertices $c \pm iR$, $-N - \frac 1 2 \pm iR$. We will take the [[Definition:Contour Integral/Complex|Contour Integral]] of $\Gamma \left({t}\right) y^{-t}$ about the rectangular [[Definition:Contour (Complex Plane)|contour]] $L$. Note from [[Poles of Gamma Function]], that the poles of this function are located at the non-positive integers. Thus, by the [[Residue Theorem]], we have: :$\displaystyle \oint_L \Gamma \left({t}\right) y^{-t} \, \mathrm d z = 2 \pi i \displaystyle\sum_{k\mathop = 0}^N \operatorname{Res} \left({-k}\right)$ Thus, we obtain: :$\displaystyle \lim_{N \to \infty} \displaystyle \lim_{R \to \infty} \displaystyle \oint_L \Gamma \left({t}\right) y^{-t} \mathrm d z = 2 \pi i\displaystyle \sum_{k\mathop = 0}^{\infty} \operatorname{Res} \left({-k}\right)$ From [[Residues of Gamma Function]], we see that: :$\operatorname{Res} \left({-k}\right) = \dfrac{ \left({-1}\right)^k y^{k} }{k!} $ Which gives us: {{begin-eqn}} {{eqn | l = \lim_{N \to \infty} \lim_{R \to \infty} \oint_L \Gamma \left({t}\right) y^{-t} \, \mathrm d t | r = 2 \pi i\displaystyle \sum_{k\mathop = 0}^{\infty} \operatorname{Res} \left({-k}\right) }} {{eqn | r = 2 \pi i \sum_{k\mathop = 0}^{\infty} \frac{ \left({-1}\right)^k y^{k} }{k!} }} {{eqn | r = 2 \pi i e^{-y} | c =[[Power Series Expansion for Exponential Function]] }} {{end-eqn}} We aim to show that the all but the righthand side of the rectangular [[Definition:Contour (Complex Plane)|contour]] go to 0 as we take these limits, as our result follows readily from this. {{explain|This should be made clearer}} The top and bottom portions of the [[Definition:Contour (Complex Plane)|contour]] can be parameterized by: :$\gamma\left({t}\right)= c\pm iR - t$ where $0 < t < c + N + \frac 1 2$. {{explain|A labelled diagram here would help}} The [[Definition:Modulus of Complex Number|modulus]] of the contour integral is therefore given by: {{begin-eqn}} {{eqn | l = \left\vert \int_0^{c +N + \frac 1 2} \Gamma \left({\gamma\left(t\right)}\right) y^{-\gamma\left(t\right)} \gamma'\left(t\right) \rd t \right\vert | r = \left\vert \int_0^{c +N + \frac 1 2} \Gamma \left({c\pm iR - t}\right) y^{-\left(c\pm iR - t\right)} \rd t \right\vert }} {{eqn | r = \left\vert y^{-\left({c\pm iR}\right)} \right\vert \left\vert \int_0^{c + N + \frac 1 2} \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert }} {{eqn | r = y^{-c} \left\vert \int_0^{c + N + \frac 1 2} \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert }} {{end-eqn}} From [[Bound on Complex Values of Gamma Function]], we have that: {{begin-eqn}} {{eqn |l = \left\vert \Gamma \left({c\pm iR - t}\right) y^t \right\vert |o = \leq |r= \dfrac{\left\vert c-t+i \right\vert } {\left\vert c-t+iR \right\vert}\left\vert \Gamma \left({c-t+i}\right) y^t \right\vert |c= (1) }} {{end-eqn}} for all $|R|\geq 1$. Because $|R|\geq 1$, we have that {{begin-eqn}} {{eqn |l = \frac{\left\vert c-t+i\right\vert}{\left\vert c-t+iR \right\vert} |o = \leq |r = 1 }} {{end-eqn}} :Combining the two inequalities we obtain: {{begin-eqn}} {{eqn |l = \left\vert \Gamma \left({c\pm iR - t}\right) y^t \right\vert |o = \leq |r = \left\vert \Gamma \left({c-t+i}\right) y^t \right\vert |c = (2) }} {{end-eqn}} We see that: :$\displaystyle \int_0^{c + N + \frac 1 2 } \left\vert \Gamma \left({c- t+i}\right) y^t \right\vert \rd t < \infty$ as the poles of Gamma are at the nonpositive integers, which means that the integral is a definite integral of a continuous function. {{explain|I know you're referring to convergence here, but you should make it clearer.}} The above is enough to allow for the interchange of limits by the [[Lebesgue's Dominated Convergence Theorem|Dominated Convergence Theorem]], thus we have: {{begin-eqn}} {{eqn |l = \lim_{N \to \infty} \lim_{R \to \infty} y^{-c} \left\vert \displaystyle \int_0^{c+N+\frac{1}{2} } \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert |r = \lim_{N \to \infty} y^{-c} \left\vert \displaystyle \int_0^{c+N+\frac{1}{2} } \lim_{R \to \infty} \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert }} {{end-eqn}} But using Equation $(1)$ from above we see: {{begin-eqn}} {{eqn | l=0 | o= \leq | r= \lim_{R \to \infty} \left\vert \Gamma \left({c\pm iR - t}\right) y^t \right\vert }} {{eqn |o= \leq |r= \lim_{R \to \infty} \dfrac{\left\vert c-t+i \right\vert } {\left\vert c-t+iR \right\vert}\left\vert \Gamma \left({c-t+i}\right) y^t \right\vert }} {{eqn |r=0 }} {{end-eqn}} Thus by the [[Squeeze Theorem/Functions|Squeeze Theorem]] we have: :$\displaystyle \lim_{R \to \infty} \left\vert \Gamma \left({c\pm iR - t}\right) \right\vert = 0$ Which means we have: {{begin-eqn}} {{eqn |l = \lim_{N \to \infty} \lim_{R \to \infty} y^{-c} \left\vert \displaystyle \int_0^{c+N+\frac{1}{2} } \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert |r = \lim_{N \to \infty} y^{-c} \left\vert \displaystyle \int_0^{c+N+\frac{1}{2} } \lim_{R \to \infty} \Gamma \left({c\pm iR - t}\right) y^t \rd t \right\vert }} {{eqn |r= \lim_{N \to \infty} y^{-c} \left\vert \displaystyle \int_0^{c+N+\frac{1}{2} } 0 y^t \rd t \right\vert }} {{eqn |r= \lim_{N \to \infty} 0 }} {{eqn |r= 0 }} {{end-eqn}} Thus we have that the top and bottom of the [[Definition:Contour (Complex Plane)|contour]] go to $0$ in the limit. The {{LHS}} of the [[Definition:Contour (Complex Plane)|contour]] may be parameterized by: :$\gamma\left({t}\right) = N - \dfrac 1 2 - it$ where $t$ runs from $-R$ to $R$. Thus the absolute value of integral of the left-hand side is given as: {{begin-eqn}} {{eqn | l = \left\vert \int_{-R}^R \Gamma \left({\gamma\left(t\right)}\right) y^{-\gamma\left(t\right)} \gamma'\left(t\right) \rd t \right\vert | r = \left\vert \int_{-R}^R \Gamma \left({-N-\dfrac 1 2 -it}\right) y^{-\left(-N-\dfrac 1 2 -it\right)} \left( -i \right) \rd t \right\vert }} {{eqn | r = \left\vert \int_{-R}^R \dfrac{ \Gamma \left({-N+\dfrac 3 2 -it}\right)}{\left(-N-\dfrac 1 2 -it\right)\left(-N+\dfrac 1 2-it\right)} y^{-\left(-N-\dfrac 1 2 -it\right)} \left( -i \right) \rd t \right\vert | c = [[Gamma Difference Equation]] }} {{eqn | o = \leq | r = \int_{-R}^R \left\vert \dfrac{ \Gamma \left({-N+\dfrac 3 2 -it}\right)}{\left(-N-\dfrac 1 2 -it\right)\left(-N+\dfrac 1 2-it\right)} y^{-\left(-N-\dfrac 1 2 -it\right)} \left( -i \right) \right\vert \rd t | c = [[Modulus of Complex Integral]] }} {{eqn | r = \int_{-R}^R \left\vert \dfrac{ \Gamma \left({-N+\dfrac 3 2 -it}\right)}{\left(-N-\dfrac 1 2 -it\right)\left(-N+\dfrac 1 2-it\right)} y^{N+\dfrac 1 2} \right\vert \rd t }} {{eqn | o = \leq | r = \int_{-R}^R \dfrac{\left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert}{\left\vert \left(-N-\dfrac 1 2 -it\right)\left(-N+\dfrac 1 2-it\right)\right\vert} y^{N+\dfrac 1 2} \rd t | c = See equation (2) above }} {{eqn | r = \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \int_{-R}^R \dfrac 1 {\left\vert \left(-N-\dfrac 1 2 -it\right)\left(-N+\dfrac 1 2-it\right)\right\vert} \rd t }} {{eqn | o = \leq | r = \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \int_{-R}^R \dfrac 1 {\left\vert\left(-N+\dfrac 1 2-it\right)\right\vert^2} \rd t }} {{eqn | r = \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \int_{-R}^R \dfrac 1 {\left(-N+\dfrac 1 2\right)^2+t^2} \rd t | c = [[Definition:Complex Modulus|Definition of Complex Modulus]] }} {{eqn | r = \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \dfrac {\arctan\left(\dfrac{R}{-N+ \frac 1 2}\right) - \arctan\left(\dfrac{-R}{-N+ \frac 1 2}\right)}{-N+\frac 1 2} | c = [[Derivative of Arctangent Function/Corollary]] }} {{end-eqn}} Thus we have: {{begin-eqn}} {{eqn |l = 0 |o = \leq |r = \lim_{N \to \infty} \lim_{R \to \infty} \left\vert \displaystyle \int_{-R}^{R } \Gamma \left({-N-\dfrac 1 2 -it}\right) y^{-\left(-N-\dfrac 1 2 -it\right)} \left(-i\right) \rd t \right\vert }} {{eqn |o=\leq |r= \lim_{N \to \infty} \lim_{R \to \infty} \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \dfrac {\arctan\left(\dfrac{R}{-N+ \frac 1 2}\right) - \arctan\left(\dfrac{-R}{-N+ \frac 1 2}\right)}{-N+\frac 1 2} }} {{eqn| r= \lim_{N \to \infty} \left\vert \Gamma \left({-N+\dfrac 3 2}\right)\right\vert y^{N+\dfrac 1 2} \dfrac {\pi}{-N+\frac 1 2} }} {{eqn| r= \lim_{N \to \infty} \frac {2^{2N-2} \left(N-1\right)!} {\left({2N-2}\right)!} \sqrt \pi y^{N+\dfrac 1 2} \dfrac {\pi}{-N+\frac 1 2} | c= [[Gamma Function of Negative Half-Integer]] }} {{eqn|r= \lim_{N \to \infty} \frac{2^{2N-2} \sqrt {2 \pi \left(N-1\right)} \left({\dfrac {N-1}{e} }\right)^{N-1} } {\sqrt{2 \pi \left(2N-2\right)} \left({\dfrac {2\left(N-1\right)}{e} }\right)^{2N-2} } \sqrt \pi y^{N+\dfrac 1 2} \dfrac {\pi}{-N+\frac 1 2} |c= [[Stirling's Formula]] }} {{eqn|r= \lim_{N \to \infty} \frac{2^{2N-2} \left({\dfrac {N-1}{e} }\right)^{N-1} } { 2^{2N-2} \sqrt{2} \left({\dfrac {N-1}{e} }\right)^{2N-2} } \sqrt \pi y^{N+\dfrac 1 2} \dfrac {\pi}{-N+\frac 1 2} }} {{eqn|r= \lim_{N \to \infty} \frac{1} {\sqrt{2} \left({\dfrac {N-1}{e} }\right)^{N-1} } \sqrt \pi y^{N+\dfrac 1 2} \dfrac {\pi}{-N+\frac 1 2} }} {{eqn|r= 0 }} {{end-eqn}} Which gives us: {{begin-eqn}} {{eqn| l= \lim_{N \to \infty} \lim_{R \to \infty} \left\vert \displaystyle \int_{-R}^{R } \Gamma \left({-N-\dfrac 1 2 -it}\right) y^{-\left(-N-\dfrac 1 2 -it\right)} \left( -i \right) \rd t \right\vert | r= 0 | c= [[Squeeze Theorem/Functions|Squeeze Theorem]] }} {{end-eqn}} Thus we have the left, top, and bottom of the rectangular [[Definition:Contour (Complex Plane)|contour]] go to 0 in the limit, which gives us: {{begin-eqn}} {{eqn|l= \dfrac{1}{2\pi i} \displaystyle \int_{c-i\infty}^{c+i\infty} \Gamma \left({t}\right) y^{-t} \rd t |r= \dfrac{1}{2\pi i} \lim_{N \to \infty} \lim_{R \to \infty} \oint_L \Gamma \left({t}\right) y^{-t} \, \mathrm d t }} {{eqn| r = e^{-y} }} {{end-eqn}} {{qed}} [[Category:Gamma Function]] [[Category:Complex Analysis]] 3g44dfic49983k70ynlfr685wrahckp	0
{{begin-eqn}} {{eqn | l = {}_2 \map {F_1} {-p, 1; 1; -x} | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-p}^{\overline n} 1^{\overline n} } {1^{\overline n} } \frac {\paren {-x}^n} {n!} | c = {{Defof|Gaussian Hypergeometric Function}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \paren {\prod_{j \mathop = 0}^{n - 1} \paren {-p + j} } \frac {x^n} {n!} | c = {{Defof|Rising Factorial}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\prod_{j \mathop = 0}^{n - 1} \paren {-1} } \paren {\prod_{j \mathop = 0}^{n - 1} \paren {-p + j} } \frac {x^n} {n!} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\prod_{j \mathop = 0}^{n - 1} \paren {p - j} } \frac {x^n} {n!} | c = [[Product of Products]] }} {{eqn | r = \paren {1 + x}^p | c = [[General Binomial Theorem]] }} {{end-eqn}} {{qed}}	0
Consider the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\C, +, \times}$, where: :$\C$ is the set of all [[Definition:Complex Number|complex numbers]] :$+$ is the operation of [[Definition:Complex Addition|complex addition]] :$\times$ is the operation of [[Definition:Complex Multiplication|complex multiplication]] Then $\struct {\C, +, \times}$ forms a [[Definition:Field (Abstract Algebra)|field]].	0
First note that $m \times 0 \in m \Z$ whatever $m$ may be. Thus $m \Z \ne \O$. Let $a, b \in m \Z$. Then: {{begin-eqn}} {{eqn | l = a + b | r = m j + m k | c = for some $j, k \in \Z$ by definition of $m \Z$ }} {{eqn | r = m \paren {j + k} | c = }} {{eqn | o = \in | r = m \Z | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = a - b | r = m j - m k | c = for some $j, k \in \Z$ by definition of $m \Z$ }} {{eqn | r = m \paren {j - k} | c = }} {{eqn | o = \in | r = m \Z | c = }} {{end-eqn}} Let $r \in \Z$. Then: {{begin-eqn}} {{eqn | l = r a | r = r \paren {m j} | c = for some $j \in \Z$ by definition of $m \Z$ }} {{eqn | r = m \paren {r j} | c = }} {{eqn | o = \in | r = m \Z | c = }} {{end-eqn}} Thus the conditions for $m \Z$ to be an [[Definition:Integral Ideal|integral ideal]] are fulfilled. Hence the result. {{qed}}	0
:$\dfrac 1 {1 + \sin x} = \dfrac 1 2 \map {\sec^2} {\dfrac \pi 4 - \dfrac x 2}$	0
:$\sec 135 \degrees = \sec \dfrac {3 \pi} 4 = -\sqrt 2$	0
Let $D \subseteq \C$ be an [[Definition:Open Set (Complex Analysis)|open set]]. Let $f: D \to \C$ be a [[Definition:Continuous Complex Function|continuous function]]. Suppose that $F: D \to \C$ is an [[Definition:Complex Primitive|antiderivative]] of $f$. Let $\gamma: \closedint a b \to D$ be a [[Definition:Contour (Complex Plane)|contour]] in $D$. Then the [[Definition:Complex Contour Integral|contour integral]]: :$\displaystyle \int_\gamma \map f z \rd z = \map F {\map \gamma b} - \map F {\map \gamma a}$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u}{\mathrm d x} | r = 1 | c = [[Derivative of Identity Function]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v}{\mathrm d x} | r = \cos^2 a x | c = }} {{eqn | ll= \implies | l = v | r = \frac x 2 + \frac {\sin 2 a x} {4 a} | c = [[Primitive of Square of Cosine of a x|Primitive of $\cos^2 a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \cos^2 a x \ \mathrm d x | r = x \left({\frac x 2 + \frac {\sin 2 a x} {4 a} }\right) - \int \left({\frac x 2 + \frac {\sin 2 a x} {4 a} }\right) \times 1 \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 + \frac {x \sin 2 a x} {4 a} - \frac 1 2 \int x \ \mathrm d x - \frac 1 {4 a} \int \sin 2 a x \ \mathrm d x + C | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {x^2} 2 + \frac {x \sin 2 a x} {4 a} - \frac {x^2} 4 - \frac 1 {4 a} \int \sin 2 a x \ \mathrm d x + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {x^2} 2 + \frac {x \sin 2 a x} {4 a} - \frac {x^2} 4 - \frac 1 {4 a} \frac {\left({-\cos 2 a x}\right)} {2 a} + C | c = [[Primitive of Sine of a x|Primitive of $\sin a x$]] }} {{eqn | r = \frac {x^2} 4 + \frac {x \sin 2 a x} {4 a} + \frac {\cos 2 a x} {8 a^2} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
As $y_n \to m$ as $n \to \infty$, it follows from [[Modulus of Limit]] that $\size {y_n} \to \size m$ as $n \to \infty$. As $m \ne 0$, it follows from the definition of the [[Definition:Complex Modulus|modulus]] of $m$ that $\size m > 0$. As the statement is given, it is possible that $y_n = 0$ for some $n$. At such $n$, the [[Definition:Term of Sequence|terms]] $\dfrac {x_n} {y_n}$ are not defined. However, from [[Sequence Converges to Within Half Limit]], we have: : $\exists N: \forall n > N: \size {y_n} > \dfrac {\size m} 2$ Hence for all $n > N$ we have that $y_n \ne 0$. Thus we may restrict our attention to the [[Definition:Domain of Mapping|domain]] of $\sequence {y_n}$ such that $n > N$, knowing that $\dfrac {x_n} {y_n}$ will be defined in that [[Definition:Domain of Mapping|domain]]. So, for $n > N$, consider: {{begin-eqn}} {{eqn | l = \size {\frac {x_n} {y_n} - \frac l m} | r = \size {\frac {m x_n - y_n l} {m y_n} } | c = }} {{eqn | o = < | r = \frac 2 {\size m^2} \size {m x_n - y_n l} | c = }} {{end-eqn}} By the above, $m x_n - y_n l \to m l - m l = 0$ as $n \to \infty$. The result follows by the [[Squeeze Theorem for Real Sequences]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \sin b x} a - \frac b a \int e^{a x} \cos b x \rd x | c = [[Primitive of Exponential of a x by Sine of b x/Lemma|Primitive of $e^{a x} \sin b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} \sin b x} a - \frac b a \paren {\frac {e^{a x} \cos b x} a + \frac b a \int e^{a x} \sin b x \rd x} | c = [[Primitive of Exponential of a x by Cosine of b x/Lemma|Primitive of $e^{a x} \cos b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} a \sin b x - e^{a x} b \cos b x} {a^2} - \frac {b^2} {a^2} \int e^{a x} \sin b x \rd x | c = simplifying }} {{eqn | ll= \leadsto | l = \paren {1 + \frac {b^2} {a^2} } \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \left({a \sin b x - b \cos b x}\right)} {a^2} | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {a^2 + b^2} {a^2} \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2} | c = common denominator }} {{eqn | ll= \leadsto | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2 + b^2} | c = multiplying by $\dfrac {a^2} {a^2 + b^2}$ }} {{end-eqn}} {{qed}}	0
Define: :$\displaystyle L = \inf_{n \mathop \in \N} \size x^n$ By the [[Continuum Property]], such an $L$ exists in $\R$. Clearly, $L \ge 0$. Suppose that $L > 0$. Then, by the definition of the [[Definition:Infimum of Set|infimum]], we can choose $n \in \N$ such that $\size x^n < L \size x^{-1}$. But then $\size x^{n + 1} < L$, which [[Definition:Contradiction|contradicts]] the definition of $L$. Therefore, $L = 0$. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. By the definition of the [[Definition:Infimum of Set|infimum]], there exists an $N \in \N$ such that $\size x^N < \epsilon$. It follows that: :$\forall n \in \N: n \ge N \implies \size {x^n} = \size x^n \le \size x^N < \epsilon$ where either [[Absolute Value Function is Completely Multiplicative]] is applied. Hence the result, by the definition of a [[Definition:Limit of Sequence (Number Field)|limit]]. {{qed}}	0
For any [[Definition:Triangle (Geometry)|triangle]] $\triangle ABC$: :$\dfrac a {\sin A} = \dfrac b {\sin B} = \dfrac c {\sin C} = 2 R$ where: :$a$, $b$, and $c$ are the [[Definition:Opposite (in Triangle)|sides opposite]] $A$, $B$ and $C$ respectively :$R$ is the [[Definition:Circumradius of Triangle|circumradius]] of $\triangle ABC$.	0
From [[Sum of Cubes of Three Indeterminates Minus 3 Times their Product]]: :$x^3 + y^3 + z^3 - 3 x y z = \paren {x + y + z} \paren {x + \omega y + \omega^2 z} \paren {x + \omega^2 y + \omega z}$ Setting $x \gets 0, y \gets a, z \gets b$: :$0^3 + a^3 + b^3 - 3 \times 0 \times a b = \paren {0 + a + b} \paren {0 + \omega a + \omega^2 b} \paren {0 + \omega^2 a + \omega b}$ The result follows. {{qed}}	0
:$\map {\dfrac \d {\d x} } {\coth^{-1} u} = \dfrac {-1} {u^2 - 1} \dfrac {\d u} {\d x}$ where $\size u > 1$	0
After the [[Definition:Prime Gap|prime gap]] of $14$ between the [[Definition:Ordered Pair|pairs]] of consecutive [[Definition:Prime Number|prime numbers]]: :$\left({113, 127}\right), \left({293, 307}\right), \left({317, 331}\right), \ldots$ the next [[Definition:Prime Gap|prime gap]] which is greater than $14$ is between the [[Definition:Ordered Pair|pair]] of consecutive [[Definition:Prime Number|prime numbers]]: :$\left({523, 541}\right)$ for a [[Definition:Prime Gap|prime gap]] of $18$.	0
From [[Between two Squares exists one Mean Proportional]]: :$\tuple {a^2, ab, b^2}$ is a [[Definition:Geometric Sequence of Integers|geometric sequence]]. Let $a, b \in \Z$ such that $a^2 \divides b^2$. Then from [[First Element of Geometric Sequence that divides Last also divides Second]]: :$a^2 \divides a b$ Thus: {{begin-eqn}} {{eqn | l = a^2 | o = \divides | r = a b | c = }} {{eqn | ll= \leadsto | lo= \exists k \in \Z: | l = k a^2 | r = a b | c = {{Defof|Divisor of Integer}} }} {{eqn | ll= \leadsto | l = k a | r = b | c = }} {{eqn | ll= \leadsto | l = a | o = \divides | r = b | c = {{Defof|Divisor of Integer}} }} {{end-eqn}} {{qed|lemma}} Let $a \divides b$. Then: {{begin-eqn}} {{eqn | l = a | o = \divides | r = b | c = }} {{eqn | ll= \leadsto | lo= \exists k \in \Z: | l = k a | r = b | c = {{Defof|Divisor of Integer}} }} {{eqn | ll= \leadsto | l = k a^2 | r = a b | c = }} {{eqn | ll= \leadsto | l = k a b | r = b^2 | c = {{Defof|Geometric Sequence of Integers}} }} {{eqn | ll= \leadsto | l = k^2 a^2 | r = b^2 | c = }} {{eqn | ll= \leadsto | l = a^2 | o = \divides | r = b^2 | c = {{Defof|Divisor of Integer}} }} {{end-eqn}} {{qed}} {{Euclid Note|14|VIII}}	0
{{begin-eqn}} {{eqn | l = \int x \csch a x \rd x | r = \frac 1 {a^2} \int \theta \csch \theta \rd \theta | c = [[Integration by Substitution|Substitution of $a x \to \theta$]] }} {{eqn | r = \frac 1 {a^2} \int \theta \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} \, \theta^{2 n - 1} } {\paren {2 n}!} \rd \theta | c = [[Power Series Expansion for Cosecant Function]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} } {\paren {2 n}!} \int \theta^{2 n} \rd \theta | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} \paren {a x}^{2 n + 1} } {\paren {2 n + 1}!} + C | c = [[Integration by Substitution|Substituting back $\theta \to ax$]] }} {{end-eqn}} {{qed}}	0
This proof assumes the definition of $\exp$ [[Definition:Exponential Function/Real/Sum of Series|as a series]]. Then: {{begin-eqn}} {{eqn | l = \map \exp {x + y} | r = \sum_{n \mathop = 0}^\infty \frac 1 {n!} \paren {x + y}^n }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac 1 {n!} \sum_{k \mathop = 0}^n \frac {n!} {k! \paren {n - k}!} x^k y^{n - k} | c = [[Binomial Theorem]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \sum_{k \mathop = 0}^n \paren {\frac 1 {k!} x^k} \paren {\frac 1 {\paren {n - k}!} y^{n - k} } }} {{eqn | r = \paren {\sum_{n \mathop = 0}^\infty \frac {x^n} {n!} } \paren {\sum_{n \mathop = 0}^\infty \frac {y^n} {n!} } | c = {{Defof|Cauchy Product}} }} {{eqn | r = \map \exp x \, \map \exp y }} {{end-eqn}} {{qed}}	0
Let $a, b \in \Z$ be [[Definition:Integer|integers]] such that $a \ge 0$ and $b \ne 0$. Let $r$ be the [[Definition:Remainder|remainder]] resulting from the operation of [[Definition:Integer Division|integer division]] of $a$ by $b$: $a = q b + r, 0 \le r < \size b$ Then $r$ is equal to the [[Definition:Least Positive Residue|least positive residue]] of $a \pmod b$.	0
{{begin-eqn}} {{eqn | l = \map G {w, z} | r = \sum_{m, \, n \mathop \ge 0} a_{m n} w^m z^n | c = {{Defof|Generating Function for Doubly Subscripted Sequence}} }} {{eqn | r = \sum_{m, \, n \mathop \ge 0} \dbinom n m w^m z^n | c = }} {{eqn | r = \sum_{n \mathop \ge 0} \paren {1 + w}^n z^n | c = [[Binomial Theorem]] }} {{eqn | r = \sum_{n \mathop \ge 0} \paren {\paren {1 + w} z}^n | c = }} {{eqn | r = \dfrac 1 {1 - \paren {1 + w} z} | c = [[Sum of Infinite Geometric Sequence]] }} {{eqn | r = \dfrac 1 {1 - z - w z} | c = }} {{end-eqn}} {{qed}}	0
For $x \in \R$ and $n \in \N_{> 0}$, let: :$\map {f_n} x = \dfrac 1 2 \paren {\paren {1 + \dfrac x n}^n - \paren {1 - \dfrac x n}^n }$ Then $\map {f_n} x = 0$ {{iff}}: {{begin-eqn}} {{eqn | l = \paren {1 + \frac x n}^n | r = \paren {1 - \frac x n}^n | c = }} {{eqn | ll= \leadstoandfrom | l = 1 + \frac x n | r = \paren {1 - \frac x n} e^{2 \pi i \frac k n} | c = for $k \in \Z$ }} {{eqn | ll= \leadstoandfrom | l = x | r = n \frac {e^{2 \pi i \frac k n} - 1} {e^{2 \pi i \frac k n} + 1} | c = }} {{eqn | r = n i \, \map \tan {\frac {k \pi} n } | c = [[Tangent Exponential Formulation]] }} {{end-eqn}} Hence the [[Definition:Root of Polynomial|roots]] of $\map {f_{2 m + 1} } x$ are: :$\paren {2 m + 1} i \, \map \tan {\dfrac {k \pi} {2 m + 1} }$ for $-m \le k \le m$. Observe that $\map {f_{2 m + 1} } x$ is a [[Definition:Polynomial over Real Numbers|polynomial]] of [[Definition:Degree of Polynomial|degree]] $2 m + 1$. Then for some [[Definition:Constant|constant]] $C$, we have: {{begin-eqn}} {{eqn | l = \map {f_{2 m + 1} } x | r = C x \prod_{\substack {k \mathop = - m \\ k \mathop \ne 0} }^m \paren {1 - \frac x {\paren {2 m + 1} i \, \map \tan {k \pi / \paren {2 m + 1} } } } | c = [[Polynomial Factor Theorem]] }} {{eqn | r = C x \prod_{k \mathop = 1}^m \paren {1 + \frac {x^2} {\paren {2 m + 1}^2 \map {\tan^2} {k \pi / \paren {2 m + 1} } } } | c = [[Tangent Function is Odd]] }} {{end-eqn}} It can be seen from the [[Binomial Theorem/Integral Index|Binomial Theorem]] that the coefficient of $x$ in $\map {f_{2 m + 1} } x$ is $1$. Hence $C = 1$, and we obtain: :$\displaystyle \map {f_{2 m + 1} } x = x \prod_{k \mathop = 1}^m \paren {1 + \frac {x^2} {\paren {2 m + 1}^2 \map {\tan^2} {k \pi / \paren {2 m + 1} } } }$ Let $l < m$. Then: :$\displaystyle x \prod_{k \mathop = 1}^l \paren {1 + \frac {x^2} {\paren {2 m + 1}^2 \map {\tan^2} {k \pi / \paren {2 m + 1} } } } \le \map {f_{2 m + 1} } x$ Taking the limit as $m \to \infty$ we have: {{begin-eqn}} {{eqn | l = \lim_{m \mathop \to \infty} x \prod_{k \mathop = 1}^l \paren {1 + \frac {x^2} {k^2 \pi^2} \paren {\frac {k \pi / \paren {2 m + 1} } {\map \tan {k \pi / \paren {2 m + 1} } } }^2 } | o = \le | r = \frac 1 2 \paren {e^x - e^{-x} } | c = {{Defof|Exponential Function/Real|subdef = Limit of Sequence|Exponential Function}} }} {{eqn | ll= \leadsto | o = \le | l = x \prod_{k \mathop = 1}^l \paren {1 + \frac {x^2} {k^2 \pi^2} } | r = \sinh x | c = [[Limit of Tan X over X]] and [[Definition:Hyperbolic Sine|Definition of Hyperbolic Sine]] }} {{end-eqn}} By [[Tangent Inequality]], we have: :$\map \tan {\dfrac {k \pi} {2 m + 1} } \ge \dfrac {k \pi} {2 m + 1}$ and hence: :$\displaystyle \map {f_{2 l + 1} } x \le x \prod_{k \mathop = 1}^l \paren {1 + \frac {x^2} {k^2 \pi^2} } \le \sinh x$ Taking the limit as $l \to \infty$, we have by the [[Squeeze Theorem]]: :$\displaystyle x \prod_{k \mathop = 1}^\infty \paren {1 + \frac {x^2} {k^2 \pi^2} } = \sinh x$ Substituting $x \mapsto i x$, we obtain: :$\displaystyle \sin x = x \prod_{k \mathop = 1}^\infty \paren {1 - \frac {x^2} {k^2 \pi^2} }$ {{qed}}	0
Let $a x^2 + b x + c = 0$. Then: {{begin-eqn}} {{eqn | l = 4 a^2 x^2 + 4 a b x + 4 a c | r = 0 | c = multiplying through by $4 a$ }} {{eqn | ll= \leadsto | l = \paren {2 a x + b}^2 - b^2 + 4 a c | r = 0 | c = [[Completing the Square]] }} {{eqn | ll= \leadsto | l = \paren {2 a x + b}^2 | r = b^2 - 4 a c }} {{eqn | ll= \leadsto | l = x | r = \frac {-b \pm \sqrt {b^2 - 4 a c} } {2 a} }} {{end-eqn}} {{qed}}	0
Let $r \in \R_{\ge 0}$ be a [[Definition:Positive Real Number|positive real number]]. Then: :$\exists y_1 \in \R_{\ge 0}: {y_1}^2 = r$ :$\exists y_2 \in \R_{\le 0}: {y_2}^2 = r$	0
{{ProofWanted|The proof will boil down to the fact that any real number can be expressed as the limit of a sequence of strictly increasing / decreasing rationals. In that way any interval whose endpoints are irrational can be expressed as the limit of the union of an infinite sequence of intervals with rational endpoints. // Perhaps. Another approach would be to show that each element of an open interval has an open interval around it with rational endpoints that lies within the given interval.}}	0
From [[Number of Representations as Sum of Two Primes]], the number of ways an [[Definition:Integer|integer]] $n$ can be represented as the [[Definition:Integer Addition|sum]] of two [[Definition:Prime Number|primes]] is no greater than the number of [[Definition:Prime Number|primes]] in the [[Definition:Closed Real Interval|interval]] $\closedint {\dfrac n 2} {n - 2}$. The [[Definition:Closed Real Interval|interval]] $\closedint {\dfrac {210} 2} {210 - 2}$ is $\closedint {105} {208}$. The [[Definition:Prime Number|primes]] in this [[Definition:Closed Real Interval|interval]] can be enumerated: :$107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199$ It can be seen there are exactly $19$ of them. We have: {{begin-eqn}} {{eqn | l = 11 + 199 | r = 210 }} {{eqn | l = 13 + 197 | r = 210 }} {{eqn | l = 17 + 193 | r = 210 }} {{eqn | l = 19 + 191 | r = 210 }} {{eqn | l = 29 + 181 | r = 210 }} {{eqn | l = 31 + 179 | r = 210 }} {{eqn | l = 37 + 173 | r = 210 }} {{eqn | l = 43 + 167 | r = 210 }} {{eqn | l = 47 + 163 | r = 210 }} {{eqn | l = 53 + 157 | r = 210 }} {{eqn | l = 59 + 151 | r = 210 }} {{eqn | l = 61 + 149 | r = 210 }} {{eqn | l = 71 + 139 | r = 210 }} {{eqn | l = 73 + 137 | r = 210 }} {{eqn | l = 79 + 131 | r = 210 }} {{eqn | l = 83 + 127 | r = 210 }} {{eqn | l = 97 + 113 | r = 210 }} {{eqn | l = 101 + 109 | r = 210 }} {{eqn | l = 103 + 107 | r = 210 }} {{end-eqn}} and as can be seen, there are $19$ such representations, one for each [[Definition:Prime Number|prime]] in $\closedint {105} {208}$. {{ProofWanted|It remains to be shown that $210$ is the largest number that can be represented by the maximum of these.}}	0
Let $a, b, z \in \R$. Let $a$ be [[Definition:Congruence (Number Theory)|congruent to $b$ modulo $z$]], that is: :$a \equiv b \pmod z$ Then: :$\forall m \in \Z: m a \equiv m b \pmod z$	0
Recall the definition of the [[Definition:Complex Sine Function|sine function]]: {{begin-eqn}} {{eqn | l = \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = }} {{eqn | r = z - \frac {z^3} {3!} + \frac {z^5} {5!} - \frac {z^7} {7!} + \cdots + \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} + \cdots | c = }} {{end-eqn}} Recall the definition of the [[Definition:Exponential Function/Complex/Sum of Series|exponential as a power series]]: {{begin-eqn}} {{eqn | l = \exp z | r = \sum_{n \mathop = 0}^\infty \frac {z^n} {n!} | c = }} {{eqn | r = 1 + \frac z {1!} + \frac {z^2} {2!} + \frac {z^3} {3!} + \cdots + \frac {z^n} {n!} + \cdots | c = }} {{end-eqn}} Then, starting from the {{RHS}}: {{begin-eqn}} {{eqn | l = \frac {\exp \paren {i z} - \exp \paren {-i x} } {2 i} | r = \frac 1 {2 i} \paren {\sum_{n \mathop = 0}^\infty \frac {\paren {i z}^n} {n!} - \sum_{n \mathop = 0}^\infty \frac {\paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^n - \paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^{2 n} - \paren {-i z}^{2 n} } {\paren {2 n}!} + \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = split into [[Definition:Even Integer|even]] and [[Definition:Odd Integer|odd]] $n$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-i z}^{2 n} = \paren {i z}^{2 n}$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {2 \paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-1}^{2 n + 1} = -1$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = cancel $2$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {i \paren {-1}^n z^{2 n + 1} } {\paren {2 n + 1}!} | c = as $i^{2 n + 1} = i \paren {-1})^n $ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1!} } | c = cancel $i$ }} {{eqn | r = \sin z }} {{end-eqn}} {{qed}}	0
Let $\displaystyle \map f s = \sum_{n \mathop = 1}^\infty \frac {a_n} {n^s}$ be a [[Definition:Dirichlet Series|Dirichlet series]]. Suppose that $f$ [[Definition:Absolutely Convergent Series|converges absolutely]] at $s_0 = \sigma_0 + i t_0 \in \C$. Then $f$ [[Definition:Absolutely Convergent Series|converges absolutely]] at all points $s = \sigma + i t \in \C$ with $\sigma \ge \sigma_0$.	0
:$f + g: \struct {S, \tau_{_S} } \to \struct{R, \tau_{_R} }$ is [[Definition:Continuous Mapping on Set|continuous]].	0
:$\displaystyle \int \frac {\left({a x + b}\right)^m} {\left({p x + q}\right)^n} \rd x = \frac {-1} {\left({n - 1}\right) \left({b p - a q}\right)} \left({\frac {\left({a x + b}\right)^{m + 1} } {\left({p x + q}\right)^{n - 1} } + \left({n - m - 2}\right) a \int \frac {\left({a x + b}\right)^m} { \left({p x + q}\right)^{n - 1} } \rd x}\right)$	0
Suppose $f$ has no zeroes in $U$. Then the set described in the theorem is the [[Definition:Empty Set|empty set]], and we're done. So we suppose $\exists z_0 \in U$ such that $\map f {z_0} = 0$. Since $f$ is analytic, there is a [[Taylor's Theorem|Taylor series]] for $f$ at $z_0$ which converges for $\cmod {z - z_0} < R$. Now, since $\map f {z_0} = 0,$ we know $a_0 =0$. Other $a_j$ may be $0$ as well. So let $k$ be the least number such that $a_j = 0$ for $0 \le j < k$, and $a_k \ne 0$. Then we can write the Taylor series for $f$ about $z_0$ as: :$\displaystyle \sum_{n \mathop = k}^\infty a_n \paren {z - z_0}^n = \paren {z - z_0}^k \sum_{n \mathop = 0}^\infty a_{n + k} \paren {z - z_0}^n$ where $a_k \ne 0$ (otherwise, we'd just start at $k + 1$). Now we define a new function $\map g z$, as the sum on the right hand side, which is clearly analytic in $\cmod {z - z_0} < R$. Since it is analytic here, it is also [[Definition:Continuous Complex Function|continuous]] here. Since $\map g {z_0} = a_k \ne 0, \exists \epsilon >0$ so that $\forall z$ such that $\cmod {z - z_0} < \epsilon, \cmod {\map g z - a_k} < \dfrac {\cmod {a_k} } 2$. But then $\map g z $ cannot possibly be $0$ in that disk. Hence the result. {{qed}} [[Category:Complex Analysis]] jf8s5xpcz1mubifqc80p9wynul3b3f5	0
From [[Real Numbers form Ordered Integral Domain]], $\struct {\R, +, \times, \le}$ forms an [[Definition:Ordered Integral Domain|ordered integral domain]]. Thus: {{begin-eqn}} {{eqn | l = a | o = < | r = b | c = }} {{eqn | ll= \leadsto | l = b - a | o = > | r = 0 | c = {{Defof|Positivity Property}} }} {{eqn | ll= \leadsto | l = c \times \paren {b - a} | o = < | r = 0 | c = [[Product of Strictly Negative Element with Strictly Positive Element is Strictly Negative]] }} {{eqn | ll= \leadsto | l = b \times c - a \times c | o = < | r = 0 | c = {{Ring-axiom|D}} }} {{eqn | ll= \leadsto | l = a \times c | o = > | r = b \times c | c = {{Defof|Positivity Property}} }} {{end-eqn}} {{Qed}}	0
{{begin-eqn}} {{eqn | l = \frac {\map \ln {1 + x} } {1 + x} | r = \sum_{n \mathop = 1}^\infty \paren {-1}^{n + 1} H_n x^n | c = }} {{eqn | r = x - H_2 x^2 + H_3 x^3 - H_4 x^4 + \cdots | c = }} {{end-eqn}} where $H_n$ denotes the $n$th [[Definition:Harmonic Number|harmonic number]]: :$H_n = \ds \sum_{r \mathop = 1}^n \dfrac 1 r = 1 + \dfrac 1 2 + \dfrac 1 3 \cdots + \dfrac 1 r$ valid for all $x \in \R$ such that $\size x < 1$.	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $g \in G$. Let $m, n \in \N_{>0}$. Then: :$\forall m, n \in \N_{>0}: g^n \circ g^m = g^m \circ g^n$	0
From [[Condition for Alexandroff Extension to be T2 Space|Condition for Alexandroff Extension to be $T_2$ Space]], $T^*$ is a [[Definition:Hausdorff Space|$T_2$ space]] {{iff}} $\struct {\Q, \tau_d}$ is a [[Definition:Locally Compact Hausdorff Space|locally compact Hausdorff Space]]. But from [[Rational Number Space is not Locally Compact Hausdorff Space]], $\struct {\Q, \tau_d}$ is not a [[Definition:Locally Compact Hausdorff Space|locally compact Hausdorff Space]]. Hence the result. {{qed}}	0
Let $a, b, c, d \in \Z_{>0}$ be [[Definition:Positive Integer|positive integers]]. Let $\dfrac a b$ be in [[Definition:Canonical Form of Rational Number|canonical form]]. Let $\dfrac a b = \dfrac c d$. Then: :$a \divides c$ and: :$b \divides d$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. {{:Euclid:Proposition/VII/20}}	0
:$\displaystyle \int \frac {\d x} {p + q e^{a x} } = \frac x p - \frac 1 {a p} \ln \size {p + q e^{a x} } + C$	0
Let $T_\alpha = \left({S_\alpha, \tau_\alpha}\right)$ and $T_\beta = \left({S_\beta, \tau_\beta}\right)$ be [[Definition:Topological Space|topological spaces]]. Let $f: T_\alpha \to T_\beta$ be a [[Definition:Bijection|bijection]]. {{TFAE|def = Homeomorphism (Topological Spaces)|view = homeomorphism}}	0
From [[Cauchy's Mean Theorem]]: :$(1): \quad \displaystyle \paren {\prod_{k \mathop = 1}^n a_k}^{1/n} \le \frac 1 n \paren {\sum_{k \mathop = 1}^n a_k}$ for $r_1, r_2, \ldots, r_n$. Setting: :$a_1 = a_2 = \ldots = a_{n - 1} := 1 + \dfrac x {n - 1}$ and: :$a_n = 1$ Substituting for $a_1, a_2, \ldots, a_n$ into $(1)$ gives: {{begin-eqn}} {{eqn | l = \paren {1 + \dfrac x {n - 1} }^{\frac {n - 1} n} | o = \le | r = \dfrac {\paren {n - 1} \paren {1 + \frac x {n - 1} } + 1} n | c = }} {{eqn | r = 1 + \dfrac x n | c = }} {{eqn | ll= \leadsto | l = \paren {1 + \dfrac x {n - 1} }^{n - 1} | o = \le | r = \paren {1 + \dfrac x n}^n | c = }} {{end-eqn}} The above is valid only if $a_1, a_2, \ldots, a_n$ are [[Definition:Positive Real Number|positive]]. Hence we have shown that $\sequence {s_n}$ is [[Definition:Increasing Real Sequence|increasing]] when: :$1 + \dfrac x {n - 1} \ge 0$ that is, when: :$n \ge 1 - x$ From [[Equivalence of Definitions of Real Exponential Function/Limit of Sequence implies Sum of Series|Equivalence of Definitions of Real Exponential Function: Limit of Sequence implies Sum of Series]], we have: :$(2): \quad \paren {1 + \dfrac x n}^n \le 1 + \size x + \dfrac {\size x^2} {2!} + \dotsb + \dfrac {\size x^n} {n!}$ Since there exists $N$ such that: :$\forall n > N: \dfrac {\size x^n} {n!} \le \paren {\dfrac 1 2}^n$ it follows from $(2)$ that: {{begin-eqn}} {{eqn | l = \paren {1 + \dfrac x n}^n | o = \le | r = 1 + \size x + \dfrac {\size x^2} {2!} + \dotsb + \dfrac {\size x^N} {N!} + \paren {\dfrac 1 2}^{N + 1} + \dotsb + \paren {\dfrac 1 2}^n | c = }} {{eqn | o = \le | r = C + \dfrac {1 + \paren {\frac 1 2}^{n + 1} } {1 + \frac 1 2} | c = where $C$ is some constant }} {{eqn | o = < | r = C + 2 | c = }} {{end-eqn}} Hence we have that $\sequence {s_n}$ is [[Definition:Strictly Increasing Real Sequence|strictly increasing]] and [[Definition:Bounded Above Real Sequence|bounded above]]. So by the [[Monotone Convergence Theorem (Real Analysis)]], $\sequence {s_n}$ is [[Definition:Convergent Real Sequence|convergent]]. As $1 + \dfrac x n$ is [[Definition:Positive Real Number|positive]] when $n$ is large enough, it follows that the [[Definition:Limit of Real Sequence|limit]] of $\sequence {s_n}$ is [[Definition:Positive Real Number|positive]]. {{qed}}	0
Let $\N_{>0}$ be the set of [[Definition:Natural Numbers|natural numbers]] without [[Definition:Zero (Number)|zero]], that is: :$\N_{>0} = \N \setminus \set 0$ Let $+$ denote [[Definition:Natural Number Addition|natural number addition]]. The [[Definition:Algebraic Structure|structure]] $\struct {\N_{>0}, +}$ forms a [[Definition:Semigroup|semigroup]].	0
From [[Difference of Two Powers]]: :$\displaystyle a^n - b^n = \paren {a - b} \paren {a^{n - 1} + a^{n - 2} b + a^{n - 3} b^2 + \ldots + a b^{n - 2} + b^{n - 1} } = \paren {a - b} \sum_{j \mathop = 0}^{n - 1} a^{n - j - 1} b^j$ Set $a = x$ and $b = 1$: :$\displaystyle x^n - 1 = \paren {x - 1} \paren {x^{n - 1} + x^{n - 2} + \cdots + x + 1} = \paren {x - 1} \sum_{j \mathop = 0}^{n - 1} x^j$ from which the result follows directly. {{qed}}	0
The following [[Definition:Cube Number|cube numbers]] can be expressed as the [[Definition:Integer Addition|sum]] of $3$ [[Definition:Positive Integer|positive]] [[Definition:Cube Number|cube numbers]]: :$6^3, 9^3, 12^3, 18^3, 19^3, 20^3, 24^3, 25^3, \ldots$ {{OEIS|A066890}} with [[Definition:Cube Root|cube roots]]: :$6, 9, 12, 18, 19, 20, 24, 25, \ldots$ {{OEIS|A023042}} {{WIP|Add these sequences to the separate pages for the numbers themselves}}	0
Let $X$ be one of the [[Definition:Standard Number Field|standard number fields]] $\Q, \R, \C$. Let $\left \langle {x_n} \right \rangle$ be a [[Definition:Sequence|sequence in $X$]] which [[Definition:Convergent Sequence|converges]] to $l$. That is: : $\displaystyle \lim_{n \mathop \to \infty} x_n = l$ Then: : $\displaystyle \lim_{n \mathop \to \infty} \left|{x_n - l}\right| = 0$	0
=== Proof for Complex Analysis === Suppose $L' \ne L$ is another limit of $f \left({z}\right)$ at $z_0$. Let us take $\epsilon = \dfrac {\left\vert{L - L'}\right\vert} 2$. Then we can find $\delta_1 > 0, \delta_2 > 0$ such that: * $z \in S, 0 < \left\vert{z - z_0}\right\vert < \delta_1 \implies \left\vert{f \left({z}\right) - L}\right\vert < \epsilon$ * $z \in S, 0 < \left\vert{z - z_0}\right\vert < \delta_2 \implies \left\vert{f \left({z}\right) - L'}\right\vert < \epsilon$ Because $z_0$ is a limit point: : $\exists z^* \in S: 0 < \left\vert{z - z_0}\right\vert < \min \left\{{\delta_1, \delta_2}\right\}$ Then: {{begin-eqn}} {{eqn | l=\left\vert{L - L'}\right\vert | r=\left\vert{L - f \left({z^*}\right) + f \left({z^*}\right) - L'}\right\vert | c= }} {{eqn | o=\le | r=\left\vert{L - f \left({z^*}\right)}\right\vert + \left\vert{f \left({z^*}\right) - L'}\right\vert | c=[[Triangle Inequality]] }} {{eqn | o=< | r=\epsilon + \epsilon | c= }} {{eqn | r=2 \epsilon | c= }} {{end-eqn}} This contradicts the choice we made of $\epsilon$. {{qed}} {{wtd|Expand the context to include the more general result for metric spaces etc.}} [[Category:Complex Analysis]] 711tzlnlrulnvtl4yuph2knofq1ko50	0
We have that: :$\omega^+ = \omega \cup \set \omega$ and so: :$\omega \subseteq \omega^+$ {{qed|lemma}} By definition: :$\bigcup \omega^+ = \set {x: \exists y \in \omega^+: x \in y}$ Thus: :$x \in \bigcup \omega^+ \implies x \in \omega$ {{qed|lemma}} So by definition of [[Definition:Set Equality|set equality]]: :$\bigcup \omega^+ = \omega$ {{qed}}	0
Performing the calculation using long division: <pre> 0.03225806451612903... ----------------------- 31)1.00000000000000000000 93 155 -- --- 70 50 62 31 -- -- 80 190 62 186 -- --- 180 40 155 31 --- -- 250 90 248 62 --- -- 200 280 186 279 --- --- 140 100 124 93 --- --- 160 ... 155 --- </pre> Therefore $\dfrac 1 {31} = 0 \cdotp \dot 03225 \, 80645 \, 1612 \dot 9$, with an [[Definition:Odd Integer|odd]] [[Definition:Period of Recurrence|period]] of $15$. The [[Definition:Prime Number|prime numbers]] less than $31$ are $2, 3, 5, 7, 11, 13, 17, 19, 23, 29$. We have: :$\dfrac 1 2 = 0 \cdotp 5$: not recurring. :$\dfrac 1 3 = 0 \cdotp \dot 3$: recurring with [[Definition:Period of Recurrence|period]] $1$. :$\dfrac 1 5 = 0 \cdotp 2$: not recurring. :$\dfrac 1 7 = 0 \cdotp \dot 14285 \dot 7$: recurring with [[Definition:Period of Recurrence|period]] $6$. :$\dfrac 1 {11} = 0 \cdotp \dot 0 \dot 9$: recurring with [[Definition:Period of Recurrence|period]] $2$. :$\dfrac 1 {13} = 0 \cdotp \dot 07692 \dot 3$: recurring with [[Definition:Period of Recurrence|period]] $6$. :$\dfrac 1 {17} = 0 \cdotp \dot 058823529411764 \dot 7$: recurring with [[Definition:Period of Recurrence|period]] $16$. :$\dfrac 1 {19} = 0 \cdotp \dot 05263157894736842 \dot 1$: recurring with [[Definition:Period of Recurrence|period]] $18$. :$\dfrac 1 {23} = 0 \cdotp \dot 043478260869565217391 \dot 3$: recurring with [[Definition:Period of Recurrence|period]] $22$. :$\dfrac 1 {29} = 0 \cdotp \dot 034482758620689655172413793 \dot 1$: recurring with [[Definition:Period of Recurrence|period]] $28$. All of the above either terminates, has [[Definition:Period of Recurrence|period]] $1$ or has [[Definition:Even Integer|even]] [[Definition:Period of Recurrence|periods]]. {{qed}}	0
:$\forall x \in \R_{>0}: 1 - \dfrac 1 x \le \ln x$ where $\ln x$ denotes the [[Definition:Natural Logarithm|natural logarithm]] of $x$.	0
The set of [[Definition:Natural Numbers|natural numbers]] under [[Definition:Natural Number Addition|addition]] can be denoted $\left ({\N, +}\right)$. From [[Natural Numbers under Addition form Commutative Monoid]], the [[Definition:Algebraic Structure|algebraic structure]] $\left ({\N, +}\right)$ is a [[Definition:Commutative Monoid|commutative monoid]]. Therefore by definition of [[Definition:Commutative Monoid|commutative monoid]], $\left ({\N, +}\right)$ is a [[Definition:Commutative Semigroup|commutative semigroup]]. From [[Natural Number Addition is Cancellable]], all of the elements of $\left ({\N, +}\right)$ are [[Definition:Cancellable Element|cancellable]]. The result follows from the [[Inverse Completion Theorem]]. {{qed}} [[Category:Inverse Completions]] [[Category:Natural Numbers]] q7fkui5qs8edlrc74tqw2t2cnac9r61	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \cos^{m - 1} a x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = -\paren {m - 1} a \cos^{m - 2} a x \sin a x | c = [[Derivative of Cosine of a x|Derivative of $\cos a x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac {\cos a x} {\sin^n a x} | c = }} {{eqn | r = \sin^{-n} a x \cos a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {\sin^{-n + 1} a x} {a \paren {-n + 1} } | c = [[Primitive of Power of Sine of a x by Cosine of a x|Primitive of $\sin^n a x \cos a x$]] }} {{eqn | r = \frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\cos^m a x} {\sin^n a x} \rd x | r = \int \cos^{m - 1} a x \frac {\cos a x} {\sin^n a x} \rd x | c = }} {{eqn | r = \paren {\cos^{m - 1} a x} \paren {\frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} } | c = [[Integration by Parts]] }} {{eqn | o = | ro= - | r = \int \paren {\frac {-1} {a \paren {n - 1} \sin^{n - 1} a x} } \paren {-\paren {m - 1} a \cos^{m - 2} a x \sin a x } \rd x + C | c = }} {{eqn | r = \frac {-\cos^{m - 1} a x} {a \paren {n - 1} \sin^{n - 1} a x} - \frac {m - 1} {n - 1} \int \frac {\cos^{m - 2} a x} {\sin^{n - 2} a x} \rd x + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $\sequence {f_n}_n$ be the [[Definition:Sequence|sequence]] of [[Definition:Real Function|real functions]] $f_n: \R_{>0} \to \R$ defined as: :$\map {f_n} x = n \paren {\sqrt [n] x - 1}$ Let $k \in \N$. Let $J = \closedint {\dfrac 1 k} k$. Then the [[Definition:Sequence|sequence]] of [[Definition:Derivative of Real Function|derivatives]] $\sequence { {f_n}'}_n$ [[Definition:Uniform Convergence|converges uniformly]] to some [[Definition:Real Function|real function]] $g: J \to \R$.	0
$(1)$ can be expressed as: {{begin-eqn}} {{eqn | l = x \rd y | r = -y \rd x | c = }} {{eqn | ll= \leadsto | l = y | r = C x^{-1} | c = [[First Order ODE/x dy = k y dx|First Order ODE: $x \rd y = k y \rd x$]] }} {{eqn | ll= \leadsto | l = x y | r = C | c = multiplying through by $x$ }} {{end-eqn}} {{qed}} [[Category:Examples of First Order ODEs]] [[Category:Examples of Separation of Variables]] 6ezpoyg7g40z9d75edixh9xyydjql0n	0
:$\displaystyle \int \frac {\sin a x \ \mathrm d x} {\sin \left({a x + \phi}\right)} = \frac x {\cos \phi} - \tan \phi \int \frac {\cos a x \ \mathrm d x} {\sin \left({a x + \phi}\right)} + C$	0
:$\displaystyle \int \frac {x \rd x} {1 + \cos a x} = \frac x a \tan \frac {a x} 2 + \frac 2 {a^2} \ln size {\cos \frac {a x} 2} + C$	0
:$\displaystyle \int \tanh a x \rd x = \frac {\ln \size {\cosh a x} } a + C$	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\left({a x + b}\right)^3} | r = \int \frac 1 a \frac {u - b} {a u^3} \ \mathrm d u | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\mathrm d u} {u^2} - \frac b {a^2} \int \frac {\mathrm d u} {u^3} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \frac {-1} {u} - \frac b {a^2} \frac {-1} {2 u^2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {a^2 \left({a x + b}\right)} + \frac b {2 a^2 \left({a x + b}\right)^2} + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = v | r = \sqrt x | c = }} {{eqn | ll= \implies | l = \frac {\d v} {\d x} | r = \frac 1 {2 \sqrt x} | c = [[Power Rule for Derivatives]] }} {{eqn | l = u | r = \frac {2 \sqrt {a x + b} } {\sqrt x} | c = }} {{eqn | ll= \implies | l = \frac {\d u} {\d x} | r = \frac {\frac {\sqrt x \cdot 2 a} {2 \sqrt{a x + b} } - \frac {2 \sqrt {a x + b} } {2 \sqrt x} } x | c = [[Quotient Rule for Derivatives]] etc. }} {{eqn | r = \frac {-b} {x^{3/2} \sqrt {a x + b} } | c = simplifying }} {{end-eqn}} From [[Integration by Parts]]: : $\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \ \frac {\d u} {\d x} \rd x$ from which: {{begin-eqn}} {{eqn | l = \int \frac {\sqrt{a x + b} } x \rd x | r = \int \frac {2 \sqrt{a x + b} }{\sqrt x} \frac 1 {2 \sqrt x} \rd x | c = }} {{eqn | r = \frac {2 \sqrt{a x + b} } {\sqrt x} \sqrt x - \int {\sqrt x} \frac {-b} {x^{3/2} \sqrt {a x + b} } \rd x | c = }} {{eqn | r = 2 \sqrt {a x + b} + b \int \frac {\d x} {x \sqrt {a x + b} } | c = simplification }} {{end-eqn}} {{qed}}	0
It can be seen that $(1)$ is a [[Definition:Nonhomogeneous Linear Second Order ODE|nonhomogeneous linear second order ODE]] with [[Definition:Constant|constant]] [[Definition:Coefficient|coefficients]] in the form: :$y'' + p y' + q y = \map R x$ where: :$p = 10$ :$q = 25$ :$\map R x = 14 e^{-5 x}$ First we establish the solution of the corresponding [[Definition:Constant Coefficient Homogeneous Linear Second Order ODE|constant coefficient homogeneous linear second order ODE]]: :$(2): \quad y'' + 10 y' + 25 y = 0$ From [[Linear Second Order ODE/y'' + 10 y' + 25 y = 0|Linear Second Order ODE: $y'' + 10 y' + 25 y = 0$]], this has the [[Definition:General Solution to Differential Equation|general solution]]: :$y_g = C_1 e^{-5 x} + C_2 x e^{-5 x}$ We have that: :$\map R x = 14 e^{-5 x}$ and it is noted that $14 e^{-5 x}$ is a [[Definition:Particular Solution to Differential Equation|particular solution]] of $(2)$. So from the [[Method of Undetermined Coefficients/Exponential|Method of Undetermined Coefficients for the Exponential function]]: :$y_p = A x^2 e^{-5 x}$ where: :$A = \dfrac {14} 2$ So from [[General Solution of Linear 2nd Order ODE from Homogeneous 2nd Order ODE and Particular Solution]]: :$y = y_g + y_p = C_1 e^{-5 x} + C_2 x e^{-5 x} + 7 x^2 e^{-5 x}$ {{qed}}	0
Let $f: D \to \R$ be $n+1$ times [[Definition:Differentiable Real Function|differentiable]] in an interval $I \subset \R$. Let $x_0, \dotsc, x_n \in I$ be pairwise distinct points. Let $P$ be the [[Lagrange Interpolation Formula]] of [[Definition:Degree|degree]] at most $n$ such that $P \left({x_i}\right) = f \left({x_i}\right)$ for all $i = 0, \dotsc, n$. Let $R \left({x}\right) = f \left({x}\right) - P \left({x}\right)$. Then, for every $x \in I$, there exists $\xi$ in the [[Definition:Interval/Ordered_Set/Closed|interval]] spanned by $x$ and $x_i$, $i = 0, \dotsc, n$, such that: :$R \left({x}\right) = \dfrac{\left({x - x_0}\right) \left({x - x_1}\right) \dotsm \left({x - x_n}\right) f^{\left({n + 1}\right)} \left({\xi}\right)} {\left({n + 1}\right)!}$	0
:$\displaystyle \int \frac {\mathrm d x} {x^2 \left({x^4 - a^4}\right)} = \frac 1 {a^4 x} + \frac 1 {4 a^5} \ln \left\vert{\frac {x - a} {x + a} }\right\vert + \frac 1 {2 a^5} \arctan \frac x a + C$	0
From [[Euler's Reflection Formula]]: :$\map \Gamma {\dfrac 1 n} \, \map \Gamma {1 - \dfrac 1 n} = \pi \, \map \csc {\dfrac \pi n}$ Then: {{begin-eqn}} {{eqn | l = \map \Gamma {\frac 1 n} \, \map \Gamma {1 - \frac 1 n} | r = \frac {\map \Gamma {\frac 1 n} \, \map \Gamma {1 - \frac 1 n} } {\map \Gamma {1 - \frac 1 n + \frac 1 n} } | c = $\map \Gamma 1 = 1$ }} {{eqn | r = \map \Beta {\frac 1 n, 1 - \frac 1 n} | c = {{Defof|Beta Function|index = 3}} }} {{eqn | r = 2 \int_0^{\pi / 2} \paren {\sin \theta}^{\frac 2 n - 1} \paren {\cos \theta}^{1 - \frac 2 n} \rd \theta | c = {{Defof|Beta Function|index = 2}} }} {{eqn | r = 2 \int_0^{\pi / 2} \paren {\frac {\sin \theta} {\cos \theta} }^{\frac 2 n - 1} \rd \theta }} {{eqn | r = 2 \int_0^{\pi / 2} \paren {\tan \theta}^{\frac 2 n - 1} \rd \theta | c = {{Defof|Tangent Function}} }} {{end-eqn}} Note we have: {{begin-eqn}} {{eqn | l = \frac {\map \d {\paren {\tan \theta}^{\frac 2 n} } } {\d \theta} | r = \frac {\map \d {\tan \theta} } {\d \theta} \cdot \frac {\map \d {\paren {\tan \theta}^{\frac 2 n} } } {\map \d {\tan \theta} } | c = [[Chain Rule for Derivatives]] }} {{eqn | r = \sec^2 \theta \cdot \frac 2 n \paren {\tan \theta}^{\frac 2 n - 1} | c = [[Derivative of Tangent Function]], [[Derivative of Power]] }} {{eqn | r = \frac 2 n \paren {1 + \tan^2 \theta} \paren {\tan \theta}^{\frac 2 n - 1} | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = \frac 2 n \paren {1 + \paren {\tan^{\frac 2 n} \theta}^n} \cdot \paren {\tan \theta}^{\frac 2 n - 1} }} {{end-eqn}} As $\theta \nearrow \dfrac \pi 2$, $\tan \theta \to \infty$ and $\tan 0 = 0$, so making a substitution of $x = \paren {\tan \theta}^{\frac 2 n}$ to our original integral: {{explain|$\theta \nearrow \dfrac \pi 2$ needs to be explained, as it does not exist anywhere else on {{ProofWiki}}.}} {{begin-eqn}} {{eqn | l = 2 \int_0^{\pi / 2} \paren {\tan \theta}^{\frac 2 n - 1} \rd \theta | r = 2 \int_0^\infty \paren {\tan \theta}^{\frac 2 n - 1} \frac {\rd x} {\frac 2 n \paren {1 + \paren {\tan^{\frac 2 n} \theta}^n} \cdot \paren {\tan \theta}^{\frac 2 n - 1} } | c = [[Integration by Substitution]] }} {{eqn | r = \frac {2 n} 2 \int_0^\infty \frac {\paren {\tan\theta}^{\frac 2 n - 1} } {\paren {\tan \theta}^{\frac 2 n - 1} } \frac {\rd x} {1 + x^n} }} {{eqn | r = n \int_0^\infty \frac 1 {1 + x^n} \rd x }} {{end-eqn}} So we have: :$\displaystyle \pi \, \map \csc {\frac \pi n} = n \int_0^\infty \frac 1 {1 + x^n} \rd x$ Hence: :$\displaystyle \int_0^\infty \frac 1 {1 + x^n} \rd x = \frac \pi n \, \map \csc {\frac \pi n}$ {{qed}}	0
:$\ds \int \frac {\d x} {p + q \sinh a x} = \frac 1 {a \sqrt{p^2 + q^2} } \ln \size {\frac {q e^{a x} + p - \sqrt {p^2 + q^2} } {q e^{a x} + p + \sqrt {p^2 + q^2} } } + C$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\ds \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = 1 | c = [[Derivative of Identity Function]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {1 - \sin a x} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} | c = [[Primitive of Reciprocal of 1 minus Sine of a x]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \rd x} {1 - \sin a x} | r = x \paren {\frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} } - \int \paren {\frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} } \times 1 \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac x a \map \tan {\frac \pi 4 + \frac {a x} 2} - \frac 1 a \int \map \tan {\frac \pi 4 + \frac {a x} 2} \rd x + C | c = simplifying }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = z | r = \frac \pi 4 + \frac {a x} 2 }} {{eqn | l = \frac {\d z} {\d x} | r = \frac a 2 | c = [[Derivative of Power]] }} {{eqn | ll= \implies | l = \frac 1 a \int \map \tan {\frac \pi 4 + \frac {a x} 2} \rd x | r = \frac 1 a \int \frac 2 a \tan z \rd z | c = [[Integration by Substitution]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\cos z} + C | c = [[Primitive of Tangent Function/Cosine Form|Primitive of $\tan z$: Cosine Form]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \cos {\frac \pi 4 + \frac {a x} 2} } + C | c = substituting back for $z$ }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \sin {\frac \pi 2 - \paren {\frac \pi 4 + \frac {a x} 2} } } + C | c = [[Sine of Complement equals Cosine]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \sin {\frac \pi 4 - \frac {a x} 2} } + C | c = }} {{end-eqn}} Putting it all together: :$\ds \int \frac {x \rd x} {1 - \sin a x} = \frac x a \map \tan {\frac \pi 4 + \frac {a x} 2} + \frac 2 {a^2} \ln \size {\map \sin {\frac \pi 4 - \frac {a x} 2} } + C$ {{qed}}	0
The [[Definition:Second Order ODE|second order ODE]]: :$(1): \quad \paren {x - 1} y'' - x y' + y = 0$ has the [[Definition:General Solution of Differential Equation|general solution]]: :$y = C_1 x + C_2 e^x$	0
:$\displaystyle \int x \cos^2 a x \ \mathrm d x = \frac {x^2} 4 + \frac {x \sin 2 a x} {4 a} + \frac {\cos 2 a x} {8 a^2} + C$	0
{{begin-eqn}} {{eqn | l = \int \sinh p x \cosh q x \ \mathrm d x | r = \int \left({\frac {\sinh \left({p x + q x}\right) + \sinh \left({p x - q x}\right)} 2}\right) \ \mathrm d x | c = [[Simpson's Formula for Hyperbolic Sine by Hyperbolic Cosine]] }} {{eqn | r = \frac 1 2 \int \sinh \left({p + q}\right) x \ \mathrm d x + \frac 1 2 \int \sinh \left({p - q}\right) x \ \mathrm d x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 2 \frac {\cosh \left({p + q}\right) x} {p + q} + \frac 1 2 \frac {\cosh \left({p - q}\right) x} {p - q} + C | c = [[Primitive of Hyperbolic Sine of a x|Primitive of $\sinh a x$]] }} {{eqn | r = \frac {\cosh \left({p + q}\right) x} {2 \left({p + q}\right)} + \frac {\cosh \left({p - q}\right) x} {2 \left({p - q}\right)} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{ProofWanted}} {{Namedfor|Jacques Philippe Marie Binet|cat = Binet}}	0
:$\displaystyle \int \frac {\d x} {x^2 \paren {x^2 + a^2}^2} = -\frac 1 {a^4 x} - \frac x {2 a^4 \paren {x^2 + a^2} } - \frac 3 {2 a^5} \arctan \frac x a + C$	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\cos x} | r = \int \sec x \rd x | c = {{Defof|Real Secant Function}} }} {{eqn | r = \ln \size {\map \tan {\frac \pi 4 + \frac x 2} } + C | c = [[Primitive of Secant Function/Tangent plus Angle Form|Primitive of $\sec x$: Tangent plus Angle Form]] }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {\cos a x} | r = \frac 1 a \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \frac x {\sin^{n - 2} a x} | c = }} {{eqn | r = x \sin^{-n + 2} a x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = x \map {\frac {\d} {\d x} } {\sin^{-n + 2} a x} + \paren {\frac {\d} {\d x} x} \paren {\sin^{-n + 2} a x} | c = [[Product Rule]] }} {{eqn | r = a x \paren {-n + 2} \sin^{-n + 1} a x \cos a x + \sin^{-n + 2} a x | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{eqn | r = \frac {-a x \paren {n - 2} \cos a x} {\sin^{n - 1} a x} + \frac 1 {\sin^{n - 2} a x} | c = simplifying }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {\sin^2 a x} | c = }} {{eqn | r = \csc^2 a x | c = [[Cosecant is Reciprocal of Sine|Cosecant is $\dfrac 1 \sin$]] }} {{eqn | ll= \leadsto | l = v | r = \frac {-\cot a x} a | c = [[Primitive of Square of Cosecant of a x|Primitive of $\csc^2 a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \d x} {\sin^n a x} | r = \paren {\frac x {\sin^{n - 2} a x} } \paren {\frac {-\cot a x} a} | c = [[Integration by Parts]] }} {{eqn | o = | ro= - | r = \int \paren {\frac {-\cot a x} a} \paren {\frac {-a x \paren {n - 2} \cos a x} {\sin^{n - 1} a x} + \frac 1 {\sin^{n - 2} a x} } \rd x | c = }} {{eqn | r = \frac {-x \cos a x} {a \sin^{n - 1} a x} - \int \paren {\frac {x \paren {n - 2} \cos^2 a x} {\sin^n a x} - \frac {\cos a x} {a \sin^{n - 1} a x} } \rd x | c = simplifying }} {{eqn | r = \frac {-x \cos a x} {a \sin^{n - 1} a x} - \paren {n - 2} \int \frac {x \cos^2 a x} {\sin^n a x} \rd x + \frac 1 a \int \frac {\cos a x} {\sin^{n - 1} a x} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {-x \cos a x} {a \sin^{n - 1} a x} | c = }} {{eqn | o = | ro= - | r = \paren {n - 2} \int \frac {x \paren {1 - \sin^2 a x} } {\sin^n a x} \rd x | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | o = | ro= + | r = \frac 1 a \paren {\frac {-1} {\paren {n - 2} a \sin^{n - 2} a x} } | c = [[Primitive of Power of Sine of a x by Cosine of a x|Primitive of $\sin^n a x \cos a x$]] }} {{eqn | r = \frac {-x \cos a x} {a \sin^{n - 1} a x} | c = }} {{eqn | o = | ro= - | r = \paren {n - 2} \int \frac {x \rd x} {\sin^n a x} + \paren {n - 2} \int \frac {x \rd x} {\sin^{n - 2} a x} | c = [[Linear Combination of Integrals]] and simplifying }} {{eqn | o = | ro= + | r = \frac 1 a \paren {\frac {-1} {\paren {n - 2} a \sin^{n - 2} a x} } | c = }} {{end-eqn}} This leads to: {{begin-eqn}} {{eqn | l = \paren {n - 1} \int \frac {x \rd x} {\sin^n a x} | r = \frac {-x \cos a x} {a \sin^{n - 1} a x} + \paren {n - 2} \int \frac {x \rd x} {\sin^{n - 2} a x} - \frac 1 {a^2 \paren {n - 2} \sin^{n - 2} a x} | c = }} {{eqn | ll= \leadsto | l = \int \frac {x \rd x} {\sin^n a x} | r = \frac {-x \cos a x} {a \paren {n - 1} \sin^{n - 1} a x} - \frac 1 {a^2 \paren {n - 1} \paren {n - 2} \sin^{n - 2} a x} + \frac {n - 2} {n - 1} \int \frac {x \rd x} {\sin^{n - 2} a x} | c = }} {{end-eqn}} {{qed}}	0
Let us introduce the variable $z = \dfrac {dy}{dx}$. Then the initial ODE can be written: :$\begin{cases} \dfrac {dy}{dx} = z & : y \left({x_0}\right) = y_0 \\ & \\ \dfrac {dz}{dx} = - P \left({x}\right) z - Q \left({x}\right) y + R \left({x}\right) & : z \left({x_0}\right) = y_0' \end{cases}$ The converse is also true. The result follows from [[Existence of Solution to System of First Order ODEs]]. {{qed}} [[Category:Differential Equations]] 6vhz0eqftyu4brtbvbujanu266sw2gu	0
:$\displaystyle \int \frac {\d x} {\csc a x} = \frac {-\cos a x} a + C$	0
Let $\openint a b \subset \R$ be an [[Definition:Open Real Interval|open interval]]. Let $\xi$ be a point in $\openint a b$. {{TFAE|def = Differentiable Real Function at Point|view = differentiable real function at a point}}	0
Let $\map f z, \map j z, \map k z$ be single-valued [[Definition:Continuous Complex Function|continuous complex functions]] in a [[Definition:Domain of Mapping|domain]] $D \subseteq \C$, where $D$ is [[Definition:Open Set (Complex Analysis)|open]]. Let $f$, $j$, and $k$ be [[Definition:Complex-Differentiable Function|complex-differentiable]] at all points in $D$. Let $\map f z = \map j z \, \map k z$. Then: :$\forall z \in D: \map {f'} z = \map j z \, \map {k'} z + \map {j'} z \, \map k z$	0
{{begin-eqn}} {{eqn | l = \int_0^\infty \map \exp {-\paren {a x^2 + bx + c} } \rd x | r = \int_0^\infty \map \exp {-a \paren {x + \frac b {2 a} }^2 + \frac {b^2} {4 a} - c} \rd x | c = [[Completing the Square]] }} {{eqn | r = \map \exp {\frac {b^2 - 4 a c} {4 a} } \int_0^\infty \map \exp {-a \paren {x + \frac b {2 a} }^2} \rd x | c = [[Exponential of Sum]] }} {{eqn | r = \map \exp {\frac {b^2 - 4 a c} {4 a} } \int_0^\infty \map \exp {-\paren {\sqrt a x + \frac b {2 \sqrt a} }^2} \rd x }} {{eqn | r = \frac 1 {\sqrt a} \map \exp {\frac {b^2 - 4 a c} {4 a} } \int_{\frac b {2 \sqrt a} }^\infty \map \exp {-u^2} \rd u | c = [[Integration by Substitution|substituting]] $u = \sqrt a x + \dfrac b {2 \sqrt a}$ }} {{eqn | r = \frac 1 2 \sqrt {\frac \pi a} \map \exp {\frac {b^2 - 4 a c} {4 a} } \paren {\frac 2 {\sqrt \pi} \int_{\frac b {2 \sqrt a} }^\infty \map \exp {-u^2} \rd u} }} {{eqn | r = \frac 1 2 \sqrt {\frac \pi a} \map \exp {\frac {b^2 - 4 a c} {4 a} } \map \erfc {\frac b {2 \sqrt a} } | c = {{Defof|Complementary Error Function}} }} {{end-eqn}} {{qed}}	0
Let [[Definition:Euler's Equation for Vanishing Variation|Euler's equation]] be :$\map {F_y} {x, \hat y, \hat y'} - \dfrac \d {\d x} \map {F_{y'} } {x, \hat y, \hat y'} = 0$ which is derived from: :$\displaystyle \int_a^b \paren {\map {F_y} {x, \hat y, \hat y'} - \frac \d {\d x} \map {F_{y'} } {x, \hat y, \hat y'} } \rd x = 0$ Let $\map {\hat y} x = \map y x$ and $\map {\hat y} x = \map y x + \map h x$ be solutions of [[Definition:Euler's Equation for Vanishing Variation|Euler's equation]]. By [[Taylor's Theorem]]: {{begin-eqn}} {{eqn | r = \map {F_y} {x, y + h, y' + h'} - \frac \d {\d x} \map {F_{y'} } {x, y + h, y' + h'} | o = }} {{eqn | r = F_y + F_{yy} h + F_{yy'} h' + \map \OO {h^2, h'^2} - \map {\frac \d {\d x} } {F_{y'} + F_{y'y} h + F_{y'y'} h' + \map \OO {h^2, h'^2} } }} {{eqn | r = \paren {F_y - \frac \d {\d x} F_{y'} } + F_{yy} h + \map \OO {h^2, h h', h'^2} - F_{y'y}' h - F_{y'y'}' h' - F_{y'y'} h'' - \frac \d {\d x} \map \OO {h^2, h h', h'^2} }} {{eqn | r = \paren {F_{yy} - \frac \d {\d x} F_{y'y} } h - \map {\frac \d {\d x} } {F_{y'y'} h'} + \map \OO {h^2, h h', h'^2} }} {{end-eqn}} where the omitted [[Definition:Ordered Tuple|ordered tuple]] of [[Definition:Variable|variables]] is $\tuple {x, y, y'}$, and $\map {\hat y} x = \map y x$ has been used as a [[Definition:Solution to Differential Equation|solution]] to $F_y - \dfrac \d {\d x} F_{y'} = 0$. Therefore, [[Definition:Euler's Equation for Vanishing Variation|Euler's equation]] is to be derived from :$\displaystyle \int_a^b \paren {\paren {F_{yy} - \frac \d {\d x} F_{y'y} } h - \map {\frac \d {\d x} } {F_{y'y'} h'} + \map \OO {h^2, h h', h'^2} } \rd x = 0$ By [[Integration by Parts|integration by parts]], :$\displaystyle \int_a^b \map \OO {h^2, h h', h'^2} \rd x = \int_a^b \map \OO {h^2} \rd x$ Thus, the equivalent [[Definition:Differential Equation|differential equation]] is: :$\paren {F_{yy} - \dfrac \d {\d x} F_{y'y} } h - \map {\dfrac \d {\d x} } {F_{y'y'} h'} + \map \OO {h^2} = 0$ Omission of $\map \OO {h^2}$ and multiplication of equation by $\frac 1 2$ yields [[Definition:Jacobi's Equation of Functional|Jacobi's equation]]. {{qed}}	0
:$\displaystyle \int \frac {\paren {\sqrt {x^2 - a^2} }^3} {x^3} \rd x = \frac {-\paren {\sqrt {x^2 - a^2} }^3} {2 x^2} + \frac {3 \sqrt {x^2 - a^2} } 2 - \frac {3 a} 2 \arcsec \size {\frac x a} + C$	0
Let $f: \R \to \R$ or $\R \to \C$ be a [[Definition:Continuous|continuous]] [[Definition:Function|function]], twice [[Definition:Differentiable on Interval|differentiable]] on any [[Definition:Closed Real Interval|closed interval]] $\closedint 0 a$. Let $\laptrans f = F$ denote the [[Definition:Laplace Transform|Laplace transform]] of $f$. Then, everywhere that $\dfrac {\d^2} {\d s^2} \laptrans f$ exists: :$\dfrac {\d^2} {\d s^2} \laptrans {\map f t} = \laptrans {t^2 \, \map f t}$	0
We will use the notation $D f \left({x}\right) = f^{\prime} \left({x}\right)$ as it is convenient. Let $n = 0$. Then $\forall x \in \R: x^n = 1$. Thus $f \left({x}\right)$ is the [[Definition:Constant Mapping|constant function]] $f_1 \left({x}\right)$ on $\R$. Thus from [[Derivative of Constant]], $D f \left({x}\right) = D \left({x^0}\right) = 0 x^{-1}$, except where $x = 0$. So the result holds for $n = 0$. Let $n = 1$. Then: : $\forall x \in \R: f \left({x}\right) = x^n = x$ Then from [[Derivative of Identity Function]]: : $D \left({x}\right) = 1 = 1 \cdot x^{1-1}$ So the result holds for $n = 1$. Now assume $D \left({x^k}\right) = k x^{k-1}$. Then by the [[Product Rule for Derivatives]]: : $D \left({x^{k+1}}\right) = D \left({x^k x}\right) = x^k D \left({x}\right) + D \left({x^k}\right) x = x^k \cdot 1 + k x^{k-1} x = \left({k+1}\right) x^k$ The result follows by [[Principle of Mathematical Induction|induction]]. {{qed}}	0
Let $P$ be a [[Definition:Polygon|polygon]] embedded in the [[Definition:Complex Plane|complex plane]] $\C$. Denote the [[Definition:Boundary (Geometry)|boundary]] of $P$ as $\partial P$. Then there exists a [[Definition:Simple Contour (Complex Plane)|simple]] [[Definition:Closed Contour (Complex Plane)|closed contour]] $C$ such that: : $\operatorname{Im} \left({C}\right) = \partial P$ where $\operatorname{Im} \left({C}\right)$ denotes the [[Definition:Image of Contour (Complex Plane)|image]] of $C$.	0
The [[Definition:Volume|volume]] $V$ of a [[Definition:Right Circular Cone|right circular cone]] is given by: :$V = \dfrac 1 3 \pi r^2 h$ where: : $r$ is the [[Definition:Radius of Circle|radius]] of the [[Definition:Base of Cone|base]] : $h$ is the [[Definition:Height of Cone|height]] of the cone, that is, the [[Definition:Linear Measure|distance]] between the [[Definition:Apex of Cone|apex]] and the [[Definition:Center of Circle|center]] of the [[Definition:Base of Cone|base]].	0
Let $f \in A$ be an arbitrary [[Definition:Continuous Real Function|continuous real function]] $f: \Bbb I \to \R$. From [[Continuous Real Function is Darboux Integrable]], $\map {\paren {\map h f} } x$ exists and is [[Definition:Continuous Real Function|continuous]] on $\Bbb I$. Let $x = a$. Then we have: {{begin-eqn}} {{eqn | l = \map {\paren {\map h f} } x | r = \int_a^x \map f t \rd t | c = Definition of $h$ }} {{eqn | ll= \leadsto | l = \map {\paren {\map h f} } a | r = \int_a^a \map f t \rd t | c = }} {{eqn | r = 0 | c = [[Definite Integral on Zero Interval]] }} {{end-eqn}} Thus $h$ is a [[Definition:Continuous Real Function|continuous real function]] on $\Bbb I$ such that $\map h a = 0$. Further, by definition, $\int \map f t \rd t$ is the [[Definition:Primitive|primitive]] of $f$, and exists by dint of the above. It follows from the [[Fundamental Theorem of Calculus]] that $f = \map {\dfrac \d {\d x} } h$ and so $\map h x$ is [[Definition:Differentiable Real Function on Closed Interval|functions differentiable]] on $\Bbb I$. Thus it is shown that $C \subseteq B \subseteq A$ and the result follows. {{qed}}	0
From [[Definite Integral from 0 to 1 of Arctangent of x over x|Definite Integral from $0$ to $1$ of $\dfrac {\arctan x} x$]], we have: :$\displaystyle \int_0^1 \frac {\arctan x} x \rd x = G$ Let: :$x = \tan \theta$ By [[Derivative of Tangent Function]], we have: :$\displaystyle \frac {\d x} {\d \theta} = \sec^2 \theta$ We have, by [[Arctangent of Zero is Zero]]: :as $x \to 0$, $\theta \to 0$. We also have, by [[Arctangent of One]]: :as $x \to 1$, $\theta \to \dfrac \pi 4$ We therefore have: {{begin-eqn}} {{eqn | l = \int_0^1 \frac {\arctan x} x \rd x | r = \int_0^{\pi/4} \frac {\sec^2 \theta \map \arctan {\tan \theta} } {\tan \theta} \rd \theta | c = [[Integration by Substitution|substituting]] $x = \tan \theta$ }} {{eqn | r = \int_0^{\pi/4} \frac \theta {\cos^2 \theta} \times \frac {\cos \theta} {\sin \theta} \rd \theta | c = {{Defof|Real Secant Function}}, {{Defof|Real Tangent Function}} }} {{eqn | r = \int_0^{\pi/4} \frac \theta {\sin \theta \cos \theta} \rd \theta }} {{eqn | r = \int_0^{\pi/4} \frac \theta {\frac 1 2 \sin 2 \theta} \rd \theta | c = [[Double Angle Formula for Sine]] }} {{eqn | r = \frac 1 2 \int_0^{\pi/2} \frac \phi {\sin \phi} \rd \phi | c = [[Integration by Substitution|substituting]] $\phi = 2 \theta$ }} {{end-eqn}} giving: :$\displaystyle \int_0^{\pi/2} \frac \phi {\sin \phi} \rd \phi = 2 G$ {{qed}}	0
Let the [[Definition:First Order Ordinary Differential Equation|first order ordinary differential equation]]: :$(1): \quad \map M {x, y} + \map N {x, y} \dfrac {\d y} {\d x} = 0$ be non-[[Definition:Homogeneous Differential Equation|homogeneous]] and not [[Definition:Exact Differential Equation|exact]]. Let $\map \mu {x, y}$be an [[Definition:Integrating Factor|integrating factor]] for $(1)$. If one of these is the case: :$\mu$ is a function of $x$ only :$\mu$ is a function of $y$ only :$\mu$ is a function of $x + y$ :$\mu$ is a function of $x y$ then: :$\mu = e^{\int \map f w \rd w}$ where $w$ depends on the nature of $\mu$.	0
{{begin-eqn}} {{eqn | l = \map \Ei x | r = -\gamma - \ln x + \int_0^x \frac {1 - e^{-u} } u \rd u | c = [[Characterization of Exponential Integral Function]] }} {{eqn | r = -\gamma - \ln x + \int_0^x \frac 1 u \paren {1 - \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {u^n} {n!} } \rd u | c = {{Defof|Real Exponential Function}} }} {{eqn | r = -\gamma - \ln x + \int_0^x \frac 1 u \paren {\sum_{n \mathop = 1}^\infty \paren {-1}^{n + 1} \frac {u^n} {n!} } \rd u }} {{eqn | r = -\gamma - \ln x + \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n + 1} } {n!} \paren {\int_0^x u^{n - 1} \rd u} | c = [[Power Series is Termwise Integrable within Radius of Convergence]] }} {{eqn | r = -\gamma - \ln x + \sum_{n \mathop = 1}^\infty \paren {-1}^{n + 1} \frac {x^n} {n \times n!} | c = [[Primitive of Power]] }} {{end-eqn}} {{qed}}	0
From [[Continuously Differentiable Curve has Finite Arc Length]], $s$ exists and is given by: {{begin-eqn}} {{eqn | l = s | r = \int_P^x \sqrt {1 + \paren {\frac {\d y} {\d u} }^2} \rd u }} {{eqn | ll= \leadsto | l = \frac {\d s} {\d x} | r = \frac {\d} {\d x} \int_P^x \sqrt {1 + \paren {\frac {\d y} {\d u} } ^2} \rd u | c = [[Definition:Differentiation|differentiating]] both sides {{WRT|Differentiation}} $x$ }} {{eqn | r = \sqrt {1 + \paren {\frac {\d y} {\d x} }^2} | c = [[Fundamental Theorem of Calculus/First Part|Fundamental Theorem of Calculus: First Part]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \int \frac {\paren {p - q \cos a x} \rd x} {\paren {p + q \cos a x} \paren {p - q \cos a x} } | c = }} {{eqn | r = \int \frac {\paren {p - q \cos a x} \rd x} {p^2 - q^2 \cos^2 a x} | c = }} {{eqn | r = \int \frac {\paren {p - q \cos a x} \rd x} {\paren {p^2 - q^2} + q^2 \sin^2 a x} | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} Let $p^2 > q^2$. Thus, let $p^2 - q^2 = d^2$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \int \frac {\paren {p - q \cos a x} \rd x} {d^2 + q^2 \sin^2 a x} | c = }} {{eqn | r = \int \frac {p \rd x} {d^2 + q^2 \sin^2 a x} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac p {a d \sqrt{d^2 + q^2} } \map \arctan {\frac {\sqrt {d^2 + q^2} \tan a x} d} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Primitive of Reciprocal of p squared plus square of q by Sine of a x|Primitive of $\dfrac 1 {p^2 + q^2 \sin^2 a x}$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 a \int \frac {q \paren {\sin a x}' \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 a \int \frac {q \rd u} {d^2 + q^2 u^2} | c = letting $u = \sin a x$ }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a q} \int \frac {\d u} {\frac {d^2} {q^2} + u^2} | c = }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a q} \frac q d \map \arctan {\frac {q u} d} + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a d} \map \arctan {\frac {q \sin a x} d} + C | c = substituting $u = \sin a x$ }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \paren {\map \arctan {\frac {p \tan a x} {\sqrt {p^2-q^2} } } - \map \arctan {\frac {q \sin a x} {\sqrt {p^2 - q^2} } } } + C | c = substituting $d = \sqrt{p^2 - q^2}$ }} {{eqn-intertext|While this would usually be considered as an acceptable form to leave such an expression, there is some way to go to obtain the result requested.}} {{eqn | r = \frac 1 {a \sqrt{p^2 - q^2} } \map \arctan {\frac {\frac {p \tan a x} {\sqrt {p^2 - q^2} } - \frac {q \sin a x} {\sqrt {p^2 - q^2} } } {1 + \frac {p \tan a x} {\sqrt {p^2 - q^2} } \frac {q \sin a x} {\sqrt {p^2 - q^2} } } } + C | c = [[Difference of Arctangents]] }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \tan a x - q \sin a x} } {p^2 - q^2 + p q \tan a x \sin a x} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \paren {\frac {2 \tan \frac {a x} 2} {1 - \paren {\tan \frac {a x} 2}^2 } } - q \paren {\frac {2 \tan \frac {a x} 2} {1 + \paren {\tan \frac {a x} 2}^2 } } } } {p^2 - q^2 + p q \paren {\frac {2 \tan \frac {a x} 2} {1 - \paren {\tan \frac {a x} 2}^2 } } \paren {\frac {2 \tan \frac {a x} 2} {1 + \paren {\tan \frac {a x} 2}^2 } } } } + C | c = [[Tangent Half-Angle Substitution for Sine]] and [[Double Angle Formula for Tangent]] }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \paren {\frac {2 u} {1 - u^2 } } - q \paren {\frac {2 u} {1 + u^2 } } } } {p^2 - q^2 + p q \paren {\frac {2 u} {1 - u^2 } } \paren {\frac {2 u} {1 + u^2} } } } + C | c = letting $u = \tan \frac {a x} 2$ }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {2 p u \paren {1 + u^2} - 2 q u \paren {1 - u^2} } } {\paren {p^2 - q^2} \paren {1 - u^2} \paren {1 + u^2} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {2 p u + 2 p u^3 - 2 q u + 2 q u^3} } {\paren {p^2 - q^2} \paren {1 - u^4} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {2 \sqrt {p^2 - q^2} \paren {\paren {p - q} u + \paren {p + q} u^3 } } {\paren {p^2 - q^2} \paren {1 - u^4} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} \paren {m u + n u^3} } {m n \paren {1 - u^4} + \paren {n^2 - m^2} u^2} } + C | c = letting $m = p - q$ and $n = p + q$ }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m n - m n u^4 + n^2 u^2 - m^2 u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m n - m^2 u^2 + n^2 u^2 - m n u^4} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m \paren {n - m u^2} + n u^2 \paren {n - m u^2} } } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {\paren {m + n u^2} \paren {n - m u^2} } } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u} {n - m u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {\frac m n} u} {1 - \frac m n u^2} } + C | c = }} {{eqn | r = \frac 2 {a \sqrt {m n} } \map \arctan {\sqrt {\frac m n} u} + C | c = [[Sum of Arctangents]] }} {{eqn | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } u} + C | c = substituting $m = p - q$ and $n = p + q$ }} {{eqn | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } \tan \frac {a x} 2 } + C | c = substituting $u = \tan \frac {a x} 2$ }} {{end-eqn}} {{qed|lemma}} {{questionable|The derivation up till here has been done on the assumption that $p^2 > q^2$. Hence it is a failure of logic to take the expression above and extend it to $p^2 < q^2$ without having first established in the above that it is valid so to do.}} Now let $p^2 < q^2$. {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } \tan \frac {a x} 2 } | c = }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \map \arctan {i \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2 } | c = where $i$ is the [[Definition:Imaginary Unit|imaginary unit]] }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {1 + \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2} {1 - \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2} } | c = [[Arctangent of Imaginary Number]] }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {\sqrt {\frac {q + p} {q - p} } + \tan \frac {a x} 2} {\sqrt {\frac {q + p} {q - p} } - \tan \frac {a x} 2} } | c = }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {\tan \frac {a x} 2 + \sqrt {\frac {q + p} {q - p} } } {\tan \frac {a x} 2 - \sqrt {\frac {q + p} {q - p} } } } | c = }} {{eqn | r = \frac 1 {a \sqrt {q^2 - p^2} } \ln \size {\frac {\tan \frac {a x} 2 + \sqrt {\frac {q + p} {q - p} } } {\tan \frac {a x} 2 - \sqrt {\frac {q + p} {q - p} } } } | c = }} {{end-eqn}} {{qed}}	0
We have the result $f$ is [[Definition:Bounded Real-Valued Function|bounded]] by [[Continuous Real Function is Bounded]]. By [[Condition for Darboux Integrability]], it suffices to show that for all $\epsilon > 0$, there exists a [[Definition:Subdivision (Real Analysis)|subdivision]] $P$ of $\closedint a b$ such that: :$\map U P – \map L P < \epsilon$ where $\map U P$ and $\map L P$ denote the [[Definition:Upper Sum|upper sum]] and [[Definition:Lower Sum|lower sum]] of $\map f x$ on $\closedint a b$ belonging to the [[Definition:Subdivision (Real Analysis)|subdivision]] $P$. Let $\epsilon > 0$. We have the result [[Continuous Function on Closed Interval is Uniformly Continuous]]. By the definition of [[Definition:Uniformly Continuous Real Function|uniform continuity]], there exists a $\delta > 0$ such that if $x, y \in \closedint a b$ are such that $\size {x – y} < \delta$, then: :$\size {\map f x – \map f y} < \dfrac \epsilon {b - a}$ Let $P = \set {x_0, x_1, x_2, \ldots, x_n}$ be a [[Definition:Subdivision (Real Analysis)|subdivision]] of $\closedint a b$ such that: :$\displaystyle \max_{1 \mathop \le k \mathop \le n} \paren {x_k – x_{k - 1} } < \delta$ For all [[Definition:Integer|integers]] $k$ satisfying $1 \le k \le n$, it follows from the [[Heine-Borel Theorem/Real Line|Heine-Borel theorem]] that $\closedint {x_{k - 1} } {x_k}$ is [[Definition:Compact Space|compact]]. So we can apply [[Continuous Image of Compact Space is Compact/Corollary 3|Corollary 3 to Continuous Image of Compact Space is Compact]] to conclude that there exist $u_k, v_k \in \closedint {x_{k - 1} } {x_k}$ such that: {{begin-eqn}} {{eqn | l = \map f {u_k} | r = \sup \set {\map f x: x \in \closedint {x_{k - 1} } {x_k} } }} {{eqn | l = \map f {v_k} | r = \inf \set {\map f x: x \in \closedint {x_{k - 1} } {x_k} } }} {{end-eqn}} By assumption, $x_k – x_{k - 1} < \delta$, so: :$\size {u_k – v_k} < \delta$ It follows from the definition of $\delta$ that: :$\displaystyle \map f {u_k} – \map f {v_k} < \frac \epsilon {b – a}$ This gives: {{begin-eqn}} {{eqn | l = \map U P – \map L P | r = \sum_{k \mathop = 1}^n \map f {u_k} \paren {x_k – x_{k - 1} } - \sum_{k \mathop = 1}^n \map f {v_k} \paren {x_k – x_{k - 1} } }} {{eqn | r = \sum_{k \mathop = 1}^n \paren {\map f {u_k} – \map f {v_k} } \paren {x_k – x_{k - 1} } }} {{eqn | o = < | r = \frac \epsilon {b – a} \sum_{k \mathop = 1}^n \paren {x_k – x_{k - 1} } }} {{eqn | r = \frac \epsilon {b – a} \paren {x_n – x_0} }} {{eqn | r = \epsilon }} {{end-eqn}} as desired. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \sinh^2 a x \cosh^2 a x \rd x | r = \int \paren {\sinh a x \cosh a x}^2 \rd x | c = }} {{eqn | r = \int \paren {\frac {\sinh 2 a x} 2}^2 \rd x | c = [[Double Angle Formula for Hyperbolic Sine]] }} {{eqn | r = \frac 1 4 \int \sinh^2 2 a x \rd x | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 4 \paren {\frac {\sinh 2 \paren {2 a x} } {4 \paren {2 a} } - \frac x 2} + C | c = [[Primitive of Square of Hyperbolic Sine of a x/Corollary|Primitive of $\sinh^2 a x$]] }} {{eqn | r = \frac {\sinh 4 a x} {32 a} - \frac x 8 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \coth^3 a x \ \mathrm d x | r = \int \coth a x \coth^2 a x \ \mathrm d x | c = }} {{eqn | r = \int \coth a x \left({1 + \operatorname{csch}^2 a x}\right) \ \mathrm d x | c = [[Difference of Squares of Hyperbolic Cotangent and Cosecant]] }} {{eqn | r = \int \coth a x \ \mathrm d x + \int \coth a x \operatorname{csch}^2 a x \ \mathrm d x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {\ln \left\vert{\sinh a x}\right\vert} a + \int \coth a x \operatorname{csch}^2 a x \ \mathrm d x | c = [[Primitive of Hyperbolic Tangent of a x|Primitive of $\tanh a x$]] }} {{eqn | r = \frac {\ln \left\vert{\sinh a x}\right\vert} a - \frac {\coth^2 a x} {2 a} + C | c = [[Primitive of Power of Hyperbolic Cotangent of a x by Square of Hyperbolic Cosecant of a x|Primitive of $\coth^n a x \operatorname{csch}^2 a x$]]: $n = 1$ }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = u | r = \arccos \frac x a | c = }} {{eqn | n = 1 | ll= \leadsto | l = \cos u | r = \frac x a | c = {{Defof|Arccosine}} }} {{eqn | n = 2 | ll= \leadsto | l = \sin u | r = \sqrt {1 - \frac {x^2} {a^2} } | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \paren {\arccos \frac x a}^2 \rd x | r = -a \int u^2 \sin u \rd x | c = [[Primitive of Function of Arccosine]] }} {{eqn | r = -a \paren {2 u \sin u + \paren {2 - u^2} \cos u} + C | c = [[Primitive of x squared by Sine of a x|Primitive of $x^2 \sin a x$]] where $a = 1$ }} {{eqn | r = -a \paren {2 \arccos \frac x a \sqrt {1 - \frac {x^2} {a^2} } + \paren {2 - \paren {\arccos \frac x a}^2} \frac x a} + C | c = substituting for $u$, $\sin u$ and $\cos u$ }} {{eqn | r = x \paren {\arccos \frac x a}^2 - 2 x - 2 \sqrt {a^2 - x^2} \arccos \frac x a + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {x^2 \rd x} {x^4 + a^4} | r = \int \paren {\frac x {2 a \sqrt 2 \paren {x^2 - a x \sqrt 2 + a^2} } - \frac x {2 a \sqrt 2 \paren {x^2 + a x \sqrt 2 + a^2} } } \rd x | c = [[Primitive of x squared over x fourth plus a fourth/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | n = 1 | r = \frac 1 {4 a \sqrt 2} \int \frac {2 x \rd x} {x^2 - a x \sqrt 2 + a^2} - \frac 1 {4 a \sqrt 2} \int \frac {2 x \rd x} {x^2 + a x \sqrt 2 + a^2} | c = [[Linear Combination of Integrals]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {2 x \rd x} {x^2 - a x \sqrt 2 + a^2} | r = \int \frac {\paren {2 x - a \sqrt 2 + a \sqrt 2} \rd x} {x^2 - a x \sqrt 2 + a^2} | c = }} {{eqn | r = \int \frac {\paren {2 x - a \sqrt 2} \rd x} {x^2 - a x \sqrt 2 + a^2} + a \sqrt 2 \int \frac {\d x} {x^2 - a x \sqrt 2 + a^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \ln \size {x^2 - a x \sqrt 2 + a^2} + a \sqrt 2 \int \frac {\d x} {x^2 - a x \sqrt 2 + a^2} | c = [[Primitive of Function under its Derivative]] }} {{eqn | r = \ln \size {x^2 - a x \sqrt 2 + a^2} + a \sqrt 2 \paren {\frac {-\sqrt 2} a \, \map \arctan {1 - \frac {x \sqrt 2} a} } | c = [[Primitive of Reciprocal of x fourth plus a fourth/Lemma 2|Primitive of $\dfrac 1 {x^2 - a x \sqrt 2 + a^2}$]] }} {{eqn | n = 2 | r = \ln \size {x^2 - a x \sqrt 2 + a^2} - 2 \, \map \arctan {1 - \frac {x \sqrt 2} a} | c = simplifying }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = \int \frac {2 x \rd x} {x^2 + a x \sqrt 2 + a^2} | r = \int \frac {\paren {2 x + a \sqrt 2 - a \sqrt 2} \rd x} {x^2 + a x \sqrt 2 + a^2} | c = }} {{eqn | r = \int \frac {\paren {2 x + a \sqrt 2} \rd x} {x^2 + a x \sqrt 2 + a^2} - a \sqrt 2 \int \frac {\d x} {x^2 + a x \sqrt 2 + a^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \ln \size {x^2 + a x \sqrt 2 + a^2} - a \sqrt 2 \int \frac {\d x} {x^2 + a x \sqrt 2 + a^2} | c = [[Primitive of Function under its Derivative]] }} {{eqn | r = \ln \size {x^2 + a x \sqrt 2 + a^2} - a \sqrt 2 \paren {\frac {\sqrt 2} a \, \map \arctan {1 + \frac {x \sqrt 2} a} } | c = [[Primitive of Reciprocal of x fourth plus a fourth/Lemma 1|Primitive of $\dfrac 1 {x^2 + a x \sqrt 2 + a^2}$]] }} {{eqn | n = 3 | r = \ln \size {x^2 + a x \sqrt 2 + a^2} - 2 \, \map \arctan {1 + \frac {x \sqrt 2} a} | c = simplifying }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^4 + a^4} | r = \frac 1 {4 a \sqrt 2} \int \frac {2 x \rd x} {x^2 - a x \sqrt 2 + a^2} - \frac 1 {4 a \sqrt 2} \int \frac {2 x \rd x} {x^2 + a x \sqrt 2 + a^2} | c = from $(1)$ }} {{eqn | r = \frac 1 {4 a \sqrt 2} \paren {\ln \size {x^2 - a x \sqrt 2 + a^2} - 2 \, \map \arctan {1 - \frac {x \sqrt 2} a} } | c = from $(2)$ }} {{eqn | o = | ro= - | r = \frac 1 {4 a \sqrt 2} \paren {\ln \size {x^2 + a x \sqrt 2 + a^2} - 2 \, \map \arctan {1 + \frac {x \sqrt 2} a} } | c = from $(3)$ }} {{eqn | r = \frac 1 {4 a \sqrt 2} \map \ln {\frac {x^2 - a x \sqrt 2 + a^2} {x^2 + a x \sqrt 2 + a^2} } - \frac 1 {2 a \sqrt 2} \paren {\map \arctan {1 - \frac {x \sqrt 2} a} - \map \arctan {1 + \frac {x \sqrt 2} a} } | c = simplifying }} {{end-eqn}} {{qed}}	0
The [[Definition:Definite Integral|definite integral]] of $f$ from $a$ to $b$ can be evaluated by: :$\displaystyle \int_{\map \phi a}^{\map \phi b} \map f t \rd t = \int_a^b \map f {\map \phi u} \dfrac \d {\d u} \map \phi u \rd u$ where $x = \map \phi u$.	0
We have, by [[Derivative of Exponential Function/Corollary 1|Derivative of Exponential Function: Corollary 1]]: :$\dfrac \d {\d u} \paren {1 - e^{-u} } = e^{-u}$ By [[Primitive of Reciprocal]]: :$\displaystyle \int \frac {\d u} u = \ln u + C$ So: {{begin-eqn}} {{eqn | l = \int_0^x \frac {1 - e^{-u} } u \rd u | r = \intlimits {\paren {1 - e^{-u} } \ln u} 0 x - \int_0^x e^{-u} \ln u \rd u | c = [[Integration by Parts]] }} {{eqn | r = \paren {1 - e^{-x} } \ln x - \lim_{x \mathop \to 0^+} \paren {1 - e^{-x} } \ln x - \paren {\int_0^\infty e^{-u} \ln u \rd u - \int_x^\infty e^{-u} \ln u \rd u} | c = [[Sum of Integrals on Adjacent Intervals for Integrable Functions]] }} {{end-eqn}} We have: {{begin-eqn}} {{eqn | l = \lim_{x \mathop \to 0^+} \paren {1 - e^{-x} } \ln x | r = \paren {\lim_{x \mathop \to 0^+} \frac {1 - e^{-x} - \paren {1 - e^0} } x} \paren {\lim_{x \mathop \to 0^+} x \ln x} | c = [[Combination Theorem for Limits of Functions/Product Rule|Combination Theorem for Limits of Functions: Product Rule]] }} {{eqn | r = \frac \d {\d x} \paren {1 - e^{-x} }_{x = 0} \paren {\lim_{x \mathop \to 0^+} x \ln x} | c = {{Defof|Derivative}} }} {{eqn | r = e^0 \times 0 | c = [[Derivative of Exponential Function]], [[Limit of Power of x by Logarithm of x|Limit of $x^m \paren {\ln x}^n$]] }} {{eqn | r = 0 }} {{end-eqn}} Also applying [[Definite Integral to Infinity of Exponential of -x by Logarithm of x|Definite Integral to Infinity of $e^{-x} \ln x$]] gives: :$\displaystyle \int_0^x \frac {1 - e^{-u} } u \rd u = \paren {1 - e^{-x} } \ln x + \gamma + \int_x^\infty e^{-u} \ln u \rd u$ We have, by [[Primitive of Exponential of a x|Primitive of $e^{a x}$]]: :$\displaystyle \int e^{-u} \rd u = -e^{-u} + C$ We have by [[Derivative of Logarithm Function]]: :$\dfrac \d {\d u} \paren {\ln u} = \dfrac 1 u$ We therefore have: {{begin-eqn}} {{eqn | l = \paren {1 - e^{-x} } \ln x + \gamma + \int_x^\infty e^{-u} \ln u \rd u | r = \paren {1 - e^{-x} } \ln x + \gamma + \paren {\intlimits {-e^{-u} \ln u} x \infty + \int_x^\infty \frac {e^{-u} } u \rd u} | c = [[Integration by Parts]] }} {{eqn | r = \paren {1 - e^{-x} } \ln x + \gamma - \lim_{u \mathop \to \infty} \paren {e^{-u} \ln u} + e^{-x} \ln x + \map \Ei x | c = {{Defof|Exponential Integral Function}} }} {{eqn | r = \ln x + \gamma + \map \Ei x - \lim_{u \mathop \to \infty} \paren {e^{-u} \ln u} }} {{end-eqn}} It remains to evaluate the limit on the {{RHS}}. We have, for $u > 1$: :$e^{-u} \ln u > 0$ and by [[Bounds of Natural Logarithm]]: :$e^{-u} \ln u < u e^{-u} - e^{-u}$ We have, from [[Limit to Infinity of Power of x by Exponential of -a x|Limit to Infinity of $x^n e^{-a x}$]]: :$\displaystyle \lim_{u \mathop \to \infty} \paren {u e^{-u} - e^{-u} } = 0$ So, by the [[Squeeze Theorem]]: :$\displaystyle \lim_{u \mathop \to \infty} \paren {e^{-u} \ln u} = 0$ So: :$\displaystyle \int_0^x \frac {1 - e^{-u} } u \rd u = \ln x + \gamma + \map \Ei x$ Rearranging gives: :$\displaystyle \map \Ei x = -\gamma - \ln x + \int_0^x \frac {1 - e^{-u} } u \rd u$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\map \Ei x} | r = \map {\frac \d {\d x} } {-\gamma - \ln x + \int_0^x \frac {1 - e^{-t} } t \rd t} | c = [[Characterization of Exponential Integral Function]] }} {{eqn | r = -\frac 1 x + \frac 1 x - \frac {e^{-x} } x | c = [[Derivative of Constant]], [[Derivative of Natural Logarithm]], [[Fundamental Theorem of Calculus/First Part/Corollary|Fundamental Theorem of Calculus: First Part (Corollary)]] }} {{eqn | r = -\frac {e^{-x} } x }} {{end-eqn}} {{qed}} [[Category:Derivatives]] [[Category:Exponential Integral Function]] r2b3x4o3h45japoer99twgowm3463ne	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x \paren {x^2 + a^2} } | r = \int \frac {a^2 \rd x} {a^2 x \paren {x^2 + a^2} } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $a^2$ }} {{eqn | r = \int \frac {\paren {x^2 + a^2 - x^2} \rd x} {a^2 x \paren {x^2 + a^2} } | c = adding and subtracting $x^2$ }} {{eqn | r = \frac 1 {a^2} \int \frac {\paren {x^2 + a^2} \rd x} {x \paren {x^2 + a^2} } - \frac 1 {a^2} \int \frac {x^2 \rd x} {x \paren {x^2 + a^2} } | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\d x} x - \frac 1 {a^2} \int \frac {x \rd x} {x^2 + a^2} | c = simplifying }} {{eqn | r = \frac 1 {a^2} \ln \size x - \frac 1 {a^2} \int \frac {x \rd x} {x^2 + a^2} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^2} \ln \size x - \frac 1 {a^2} \paren {\frac 1 2 \map \ln {x^2 + a^2} } + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{eqn | r = \frac 1 {2 a^2} \map \ln {\frac {x^2} {x^2 + a^2} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {x \rd x} {\cos a x} | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {E_n \paren {a x}^{2 n + 2} } {\paren {2 n + 2} \paren {2 n}!} + C | c = }} {{eqn | r = \dfrac 1 {a^2} \paren {\frac {\paren {a x}^2} 2 + \frac {\paren {a x}^4} 8 + \frac {5 \paren {a x}^6} {144} + \cdots} + C | c = }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {\sinh a x} | r = \int \operatorname{csch} a x \ \mathrm d x | c = Definition of [[Definition:Hyperbolic Cosecant/Definition 2|Hyperbolic Cosecant]] }} {{eqn | r = \frac 1 a \ln \left\vert {\tanh \frac {a x} 2} \right\vert + C | c = [[Primitive of Hyperbolic Cosecant of a x|Primitive of $\operatorname{csch} a x$]] }} {{end-eqn}} {{qed}}	0
This [[Definition:First Order ODE|first order ODE]] is in the form: :$\dfrac {\d y} {\d x} + k y = 0$ where $k = 1$. From [[First Order ODE/dy = k y dx|First Order ODE: $\d y = k y \rd x$]], this has the solution: :$y = C e^{-x}$	0
Let $f$ be a [[Definition:Darboux Integrable Function|Darboux integrable]] [[Definition:Periodic Function|periodic function]] with [[Definition:Periodic Function/Period|period]] $L$. Let $\alpha \in \R$ and $n \in \Z$. Then: :$\displaystyle \int_\alpha^{\alpha + n L} \map f x \d x = n \int_0^L \map f x \d x$	0
:$\ds \int \frac {\d x} {\sin a x} = \frac 1 a \ln \size {\csc a x - \cot a x} + C$	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sin^m a x \cos^n a x} | r = \int \frac {\cos^{-n} a x \rd x} {\sin^m a x} | c = }} {{eqn | r = \frac {-\cos^{-n + 1} a x} {a \paren {m - 1} \sin^{m - 1} a x} - \frac {-n - m + 2} {m - 1} \int \frac {\cos^{-n} a x} {\sin^{m - 2} a x} \rd x + C | c = [[Primitive of Power of Cosine of a x over Power of Sine of a x/Reduction of Power of Sine|Primitive of $\dfrac {\cos^m a x} {\sin^n a x}$]] }} {{eqn | r = \frac {-1} {a \paren {n - 1} \sin^{m - 1} a x \cos^{n - 1} a x} + \frac {m + n - 2} {m - 1} \int \frac {\d x} {\sin^{m - 2} a x \cos^n a x} + C | c = }} {{end-eqn}} {{qed}}	0
:$\dfrac {\d y} {\d x} = \dfrac {\paren {\dfrac {\d y} {\d u} } } {\paren {\dfrac {\d x} {\d u} } }$ for $\dfrac {\d x} {\d u} \ne 0$.	0
Let $m \ne 0$. {{begin-eqn}} {{eqn | l = \int \cos m x \rd x | r = \frac {\sin m x} m + C | c = [[Primitive of Cosine of a x|Primitive of $\cos m x$]] }} {{eqn | ll= \leadsto | l = \int_\alpha^{\alpha + 2 \pi} \cos m x \rd x | r = \intlimits {\frac {\sin m x} m} \alpha {\alpha + 2 \pi} | c = }} {{eqn | r = \paren {\frac {\map \sin {m \paren {\alpha + 2 \pi} } } m} - \paren {\frac {\sin m \alpha} m} | c = }} {{eqn | r = \paren {\frac {\sin m \alpha} m} - \paren {\frac {\sin m \alpha} m} | c = [[Sine of Angle plus Full Angle/Corollary|Corollary to Sine of Angle plus Full Angle]] }} {{eqn | r = 0 - 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed|lemma}} Let $m = 0$. {{begin-eqn}} {{eqn | l = \int \cos 0 x \rd x | r = \int 1 \rd x | c = [[Cosine of Zero is One]] }} {{eqn | r = x + C | c = [[Primitive of Constant]] }} {{eqn | ll= \leadsto | l = \int_\alpha^{\alpha + 2 \pi} \cos 0 x \rd x | r = \bigintlimits x \alpha {\alpha + 2 \pi} | c = }} {{eqn | r = \alpha + 2 \pi - \alpha | c = }} {{eqn | r = 2 \pi | c = }} {{end-eqn}} {{qed}}	0
In the [[Definition:Summation|summation]]: :$\displaystyle \sum_{j \mathop = 0}^1 D_u^j w \sum_{\substack {\sum_{p \mathop \ge 1} k_p \mathop = j \\ \sum_{p \mathop \ge 1} p k_p \mathop = 1 \\ \forall p \ge 1: k_p \mathop \ge 0} } 1! \prod_{m \mathop = 1}^1 \dfrac {\paren {D_x^m u}^{k_m} } {k_m! \paren {m!}^{k_m} }$ we need to consider $j = 0$ and $j = 1$. Let $j = 0$. Consider the set of $k_p$ such that: :$k_1 + k_2 + \cdots = 0$ :$1 \times k_1 + 2 k_2 + \cdots = 1$ :$k_1, k_2, \ldots \ge 0$ It is apparent by inspection that no set of $k_p$ can fulfil these conditions. Therefore when $j = 0$ the [[Definition:Summation|summation]] is [[Definition:Vacuous Summation|vacuous]] Let $j = 1$. Consider the set of $k_p$ such that: :$k_1 + k_2 + \cdots = 1$ :$1 \times k_1 + 2 k_2 + \cdots = 1$ :$k_1, k_2, \ldots \ge 0$ By inspection, it is seen that these can be satisfied only by: :$k_1 = 1$ and all other $k_p = 0$. When $k_m = 0$: :$\dfrac {\paren {D_x^m u}^{k_m} } {k_m! \paren {m!}^{k_m} } = 1$ by definition of [[Definition:Zeroth Derivative|zeroth derivative]] and [[Definition:Factorial|factorial of $0$]]. Thus any contribution to the [[Definition:Summation|summation]] where $k_m = 0$ can be disregarded. Thus we have: {{begin-eqn}} {{eqn | o = | r = \sum_{j \mathop = 0}^1 \sum_{\substack {k_1 \mathop = j \\ k_1 \mathop = 1 \\ k_1 \mathop \ge 0} } D_u^j w \dfrac {1!} {k_1! \paren {1!}^{k_1} } \paren {D_x^1 u}^{k_1} | c = }} {{eqn | r = \sum_{k_1 \mathop = 1} D_u^1 w \dfrac {1!} {k_1! \paren {1!}^{k_1} } \paren {D_x^1 u}^{k_1} | c = simplifying: $j = 0$ is [[Definition:Vacuous Summation|vacuous]] }} {{eqn | r = D_u w D_x u | c = }} {{end-eqn}} {{qed}}	0
From [[Derivative of Cosine Function]]: :$\map {\dfrac \d {\d x} } {-\cos x} = \sin x$ The result follows from the definition of [[Definition:Primitive (Calculus)|primitive]]. {{qed}}	0
Let $c, d \in \closedint a b$ such that $c < d$. Then $f$ satisfies the conditions of the [[Mean Value Theorem]] on $\closedint c d$. Hence: :$\exists \xi \in \openint c d: \map {f'} \xi = \dfrac {\map f d - \map f c} {d - c}$ Let $f$ be such that: :$\forall x \in \openint a b: \map {f'} x \le 0$ Then: :$\map {f'} \xi \le 0$ and hence: :$\map f d \le \map f c$ Thus $f$ is [[Definition:Decreasing Real Function|decreasing]] on $\closedint a b$. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int_0^1 \ln x \map \ln {1 + x} \rd x | r = \int_0^1 \ln x \paren {\sum_{n \mathop = 1}^\infty \paren {-1}^{n - 1} \frac {x^n} n} \rd x | c = [[Power Series Expansion for Logarithm of 1 + x|Power Series Expansion for $\map \ln {1 + x}$]] }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n \paren {\int_0^1 x^n \ln x \rd x} | c = [[Fubini's Theorem]] }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n} n \paren {\frac {\map \Gamma 2} {\paren {n + 1}^2} } | c = [[Definite Integral from 0 to 1 of Power of x by Power of Logarithm of x|Definite Integral from $0$ to $1$ of $x^m \paren {\ln x}^n$]] }} {{eqn | r = 1! \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n} {n \paren {n + 1}^2} | c = [[Gamma Function Extends Factorial]] }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n} n - \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n} {n + 1} - \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n} {\paren {n + 1}^2} | c = partial fraction expansion }} {{eqn | r = -\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n - \sum_{n \mathop = 2}^\infty \frac {\paren {-1}^{n - 1} } n - \sum_{n \mathop = 2}^\infty \frac {\paren {-1}^{n - 1} } {n^2} | c = shifting indexes }} {{eqn | r = -\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n - \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n - 1} - \paren {\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } {n^2} - 1} }} {{eqn | r = 2 - 2 \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } n - \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n - 1} } {n^2} }} {{eqn | r = 2 - 2 \ln 2 - \frac {\pi^2} {12} | c = [[Newton-Mercator Series for ln 2|Newton-Mercator Series for $\ln 2$]], [[Sum of Reciprocals of Squares Alternating in Sign]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\d x} {\sin a x + \cos a x} = \frac 1 {a \sqrt 2} \ln \size {\map \tan {\frac {a x} 2 + \frac \pi 8} } + C$	0
Let $C$ be a [[Definition:Contour (Complex Plane)|contour in $\C$]]. Let $f, g: \Img C \to \C$ be [[Definition:Continuous Complex Function|continuous complex functions]], where $\Img C$ denotes the [[Definition:Image of Contour (Complex Plane)|image]] of $C$. Let $\lambda, \mu \in \C$ be [[Definition:Complex Number|complex]] [[Definition:Constant|constants]]. Then: :$\displaystyle \int_C \paren {\lambda \map f z + \mu \map g z} \rd z = \lambda \int_C \map f z \rd z + \mu \int_C \map g z \rd z$	0
We have that $x^2 + a^2$ is in the form $a x^2 + b x + c$, where $b^2 - 4 a c < 0$. Thus from [[Primitive of Reciprocal of a x squared plus b x plus c|Primitive of $\dfrac 1 {a x^2 + b x + c}$]] for $b^2 - 4 a c > 0$: :$\displaystyle \int \frac {\d x} {a x^2 + b x + c} = \frac 2 {\sqrt {4 a c - b^2} } \map \arctan {\frac {2 a x + b} {\sqrt {4 a c - b^2} } } + C$ setting $a := 1, b := 0, c := a^2$: {{begin-eqn}} {{eqn | l = \int \frac 1 {x^2 + a^2} \rd x | r = \dfrac 2 {\sqrt {4 a^2 - 0} } \map \arctan {\dfrac {2 x + 0} {\sqrt {4 a^2} } } + C | c = [[Primitive of Reciprocal of a x squared plus b x plus c|Primitive of $\dfrac 1 {a x^2 + b x + c}$]] setting }} {{eqn | r = \frac 1 a \arctan {\frac x a} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = z | r = x^2 }} {{eqn | ll= \implies | l = \frac {\mathrm d z} {\mathrm d x} | r = 2 x | c = [[Power Rule for Derivatives]] }} {{eqn | ll= \implies | l = \int \frac{\left({\sqrt {x^2 + a^2} }\right)^3} {x^2} \ \mathrm d x | r = \int \frac{\left({\sqrt {z + a^2} }\right)^3} {2 z \sqrt z} \ \mathrm d z | c = [[Integration by Substitution]] }} {{eqn | r = \frac {-\left({\sqrt {z + a^2} }\right)^3} {\sqrt z} + \frac 3 2 \int \frac{\sqrt {z + a^2} } {\sqrt z} \ \mathrm d z | c = [[Primitive of Power of a x + b over Power of p x + q/Formulation 3|Primitive of $\dfrac {\left({a x + b}\right)^m} {\left({p x + q}\right)^n}$: Formulation 3]] }} {{eqn | r = \frac {-\left({\sqrt {x^2 + a^2} }\right)^3} x + 3 \int \sqrt {x^2 + a^2} \ \mathrm d x | c = substituting for $z$ and simplifying }} {{eqn | r = \frac {-\left({\sqrt {x^2 + a^2} }\right)^3} x + \frac{3 x \sqrt {x^2 + a^2} } 2 + \frac {3 a^2} 2 \ln \left({x + \sqrt {x^2 + a^2} }\right) + C | c = [[Primitive of Root of x squared plus a squared|Primitive of $\sqrt {x^2 + a^2}$]] }} {{end-eqn}} {{qed}}	0
The [[Definition:Second Order ODE|second order ODE]]: :$(1): \quad y'' + 4 y' + 4 y = 0$ has the [[Definition:General Solution to Differential Equation|general solution]]: :$y = \paren {C_1 + C_2 x} e^{-2 x}$	0
The method of finite differences will be used here. Consider a [[Definition:Closed Real Interval|closed real interval]] $\closedint a b$, which is divided in $n + 1$ equal parts. Choose its [[Definition:Subdivision (Real Analysis)|subdivision]] to be [[Definition:Normal Subdivision|normal:]] :$a = x_0 < x_1 < \cdots < x_n < x_{n + 1} = b$ such that for $i \in set {0, 1, \ldots, n - 1, n}$ we have $x_{i + 1} - x_i = \Delta x$. Approximate the desired function $y$ by a polygonal line with vertices $\tuple {x_i, y_i}$ where $i \in \set {0, 1, \ldots, n, n + 1}$, where $y_i = \map y {x_i}$. Hence, the functional $J \sqbrk y$ can be approximated by the following sum: :$\displaystyle \map {\mathscr J} {y_1, y_2, \ldots, y_{n - 1}, y_n} = \sum_{i \mathop = 0}^n \map F {x_i, y_i, \frac {y_{i + 1} - y_i} {\Delta x} } \Delta x$ Note that the values $\map y {x_0} = A$ and $\map y {x_1} = B$ are fixed, and therefore not varied. Now, consider a partial derivative of $J$ with respect to $y_k$, where $k \in \set {1, 2, \ldots, n - 1, n}$. {{begin-eqn}} {{eqn | l = \frac {\partial \mathscr J} {\partial y_k} | r = \frac \partial {\partial y_k} \sum_{i \mathop = 0}^n \map F {x_i, y_i, \frac {y_{i + 1} - y_i} {\Delta x} } \Delta x | c = }} {{eqn | r = \sum_{i \mathop = 0}^n \frac \partial {\partial y_k} \map F {x_i, y_i, \frac {y_{i + 1} - y_i} {\Delta x} } \Delta x | c = }} {{eqn | r = \sum_{i \mathop = 0}^n \paren {\frac {\partial F} {\partial y_i} \paren {x_i, y_i, \frac {y_{i + 1} - y_i} {\Delta x} } \frac {\partial y_i} {\partial y_k} + \frac {\partial F} {\partial {\frac {y_{i + 1} - y_i} {\Delta x} } } \paren {x_i, y_i, \frac {y_{i + 1} - y_i} {\Delta x} } \frac {\partial {\frac {y_{i + 1} - y_i} {\Delta x} } } {\partial y_k} } \Delta x | c = }} {{end-eqn}} As all the functions $y_i$ are independent {{WRT}} each other, we have $\dfrac {\partial y_m} {\partial y_k} = \delta_{m k}$, where $\delta_{m k}$ is the [[Definition:Kronecker Delta|Kronecker Delta]]. Then the aforementioned sum simplifies to: {{explain|"aforementioned" -- reference it directly, using a label}} :$\dfrac {\partial \mathscr J} {\partial y_k} = \paren {\dfrac {\partial F} {\partial y_k} \paren {x_k, y_k, \dfrac {y_{k + 1} - y_k} {\Delta x} } + \dfrac {\partial F} {\partial {\frac {y_k - y_{k - 1} } {\Delta x} } } \paren {x_{k - 1}, y_{k - 1}, \dfrac {y_k - y_{k - 1} } {\Delta x} } \dfrac 1 {\Delta x} - \dfrac {\partial F} {\partial {\frac {y_{k + 1} - y_k} {\Delta x} } } \paren {x_k, y_k, \dfrac {y_{k + 1} - y_k} {\Delta x} } \dfrac 1 {\Delta x} } \Delta x$ In order to get a variational derivative, the denominator of the {{LHS}} has to represent an area. For this reason, divide everything by $\Delta x$, and take the limit $\Delta \to 0$. Then for all $k \in \set {1, 2, \dotsc, n - 1, n}$: {{begin-eqn}} {{eqn | l = \lim_{\Delta x \mathop \to 0} \frac {y_{k + 1} - y_k} {\Delta x} | r = \lim_{\Delta x \mathop \to 0} \frac {\map y {x_{k + 1} } - \map y {x_k} } {\Delta x} | c = }} {{eqn | r = \lim_{\Delta x \mathop \to 0} \frac {\map y {x_k + \Delta x} - \map y {x_k} } {\Delta x} | c = }} {{eqn | r = \map {y'} {x_k} | c = {{Defof|Derivative of Real Function at Point}} }} {{end-eqn}} Similarly, for $\map F {x, y, y'}$ we have {{begin-eqn}} {{eqn | r = \lim_{\Delta x \mathop \to 0} \frac {\map {F_{\map {y'} {x_k} } } {x_k, y_k, \map {y'} {x_k} } - \map {F_{\map {y'} {x_{k - 1} } } } {x_{k - 1}, y_{k - 1}, \map {y'} {x_{k - 1} } } } {\Delta x} | o = | c = }} {{eqn | r = \lim_{\Delta x \mathop \to 0} \frac {\map {F_{\map {y'} {x_{k-1} + \Delta x} } } {x_{k - 1} + \Delta x, \map y {x_{k - 1} + \Delta x}, \map {y'} {x_{k - 1} + \Delta x} } - \map {F_{\map {y'} {x_{k - 1} } } } {x_{k-1}, \map y {x_{k - 1} }, \map {y'} {x_{k - 1} } } } {\Delta x} | c = }} {{eqn | r = \frac \d {\d x} \map {F_{\map {y'} {x_{k - 1} } } } {x_{k - 1}, \map y {x_{k - 1} }, \map {y'} {x_{k - 1} } } }} {{end-eqn}} Thus: :$\displaystyle \lim_{\Delta x \mathop \to 0} \frac {\partial \mathscr J} {\partial y_k \Delta x} = \map {F_{\map y {x_k} } } {x_k, \map y {x_k}, \map {y'} {x_k} } - \frac \d {\d x} \map {F_{\map {y'} {x_{k - 1} } } } {x_{k - 1}, \map y {x_{k - 1} }, \map {y'} {x_{k - 1} } }$ Note that the denominator on the left is an area covered by a rectangle with sides $\Delta x$ and $\partial y$, and vanishes as $\Delta x \to 0$. Finally, since the distance between any two neighbouring points approaches 0 as $\Delta x \to 0$, the set of all $x_k \in \closedint a b$ can be treated as continuous, and the index $k$ dropped: :$\displaystyle \lim_{\Delta x \mathop \to 0} \frac {\partial J} {\partial y \Delta x} = \map {F_{\map y x} } {x, \map y x, \map {y'} x} - \frac \d {\d x} \map {F_{\map {y'} x} } {x, \map y x, \map {y'} x}$ The {{LHS}} by [[Definition:Variational Derivative|definition]] is a variational derivative. Suppose the {{LHS}} vanishes. Then the {{RHS}} vanishes as well. {{finish|Refine vanishing area with $\partial y$-probably introduce grid for $y$ as well; make the proof of set of $x_k$ becoming a real line more rigorous}}	0
This follows directly from the definition of [[Definition:Definite Integral|definite integral]]: From [[Continuous Image of Closed Interval is Closed Interval]] it follows that $m$ and $M$ both exist. The [[Definition:Closed Real Interval|closed interval]] $\closedint a b$ is a [[Definition:Finite Subdivision|finite subdivision]] of itself. By definition, the [[Definition:Upper Sum|upper sum]] is $M \paren {b - a}$, and the [[Definition:Lower Sum|lower sum]] is $m \paren {b - a}$. The result follows. {{qed}}	0
Substitution of $\rd s$ into $J$ results in the following [[Definition:Real Functional|functional]]: :$\ds J \sqbrk y = \int_a^b \map f {x, y, y'} \sqrt {1 + y'^2} \rd x$ We can consider this as a [[Definition:Real Functional|functional]] with the following effective $F$: :$F = \map f {x, y, y'} \sqrt {1 + y'^2}$ Find [[Definition:Euler's Equation for Vanishing Variation|Euler's Equation]]: {{begin-eqn}} {{eqn | l = F_y - \dfrac \d {\d x} F_{y'} | r = f_y \sqrt {1 + y'^2} - \dfrac \d {\d x} \paren {\map f {x, y, y'} \frac {y'} {\sqrt {1 + y'^2} } } }} {{eqn | r = f_y \sqrt {1 + y'^2} - \dfrac \d {\d x} \map f {x, y, y'} \frac {y'} {\sqrt {1 + y'^2} } - \map f {x, y, y'} \dfrac \d {\d x} \frac {y'} {\sqrt {1 + y'^2} } }} {{eqn | r = f_y \sqrt {1 + y'^2} - f_x \frac {y'} {\sqrt {1 + y'^2} } - f_y \frac {y'^2} {\sqrt {1 + y'^2} } - f_{y'} \frac {y' y''} {\sqrt {1 + y'^2} } - f \frac {y'' \sqrt {1 + y'^2} - \frac {y'^2 y''} {\sqrt {1 + y'^2} } } {1 + y'^2} }} {{eqn | r = \frac 1 {\sqrt {1 + y'^2} } \paren {f_y - f_x y' - f_{y'} y' y'' - f \frac {y''} {\paren {1 + y'^2}^{\frac 3 2} } } }} {{end-eqn}} Due to [[Definition:Assumption|assumptions]] on $y$, the prefactor does not vanish. By [[Definition:Euler's Equation for Vanishing Variation|Euler's Equation]], the last [[Definition:Expression|expression]] vanishes. {{qed}}	0
This page gathers together [[Definition:Derivative|derivatives]] of [[Definition:Inverse Trigonometric Function|inverse trigonometric functions]].	0
The [[Definition:Linear First Order ODE|linear first order ODE]]: :$(1): \quad \dfrac {\d y} {\d x} - \dfrac y x = 3 x$ has the [[Definition:General Solution of Differential Equation|general solution]]: :$\dfrac y x = 3 x + C$ or: :$y = 3 x^2 + C x$	0
From [[Primitive of Reciprocal of x by Power of x minus Power of a|Primitive of $\dfrac 1 {x \left({x^n - a^n}\right)}$]]: :$\displaystyle \int \frac {\mathrm d x} {x \left({x^n - a^n}\right)} = \frac 1 {n a^n} \ln \left\vert{\frac {x^n - a^n} {x^n} }\right\vert + C$ Setting $n = 4$: {{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x \left({x^3 + a^3}\right)} | r = \frac 1 {4 a^4} \ln \left\vert{\frac {x^4 - a^4} {x^4} }\right\vert + C | c = }} {{end-eqn}} directly. {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {\map \erf {\sqrt t} } | r = \laptrans {\frac 2 {\sqrt \pi} \int_0^{\sqrt t} \map \exp {-u^2} \rd u} | c = {{Defof|Error Function}} }} {{eqn | r = \laptrans {\frac 2 {\sqrt \pi} \int_0^{\sqrt t} \paren {\sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {u^{2 n} } {n!} \rd u} } | c = {{Defof|Exponential Function/Real|Real Exponential Function|subdef = Sum of Series}} }} {{eqn | r = \laptrans {\frac 2 {\sqrt \pi} \paren {\sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {\sqrt t}^{2 n + 1} } {\paren {2 n + 1} n!} } } | c = [[Primitive of Power]] }} {{eqn | r = \frac 2 {\sqrt \pi} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n} {\paren {2 n + 1} n!} \laptrans {t^{n + \frac 1 2} } | c = [[Linear Combination of Laplace Transforms]] }} {{eqn | r = \frac 2 {\sqrt \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\map \Gamma {n + \frac 3 2} } {\paren {2 n + 1} n! s^{n + \frac 3 2} } | c = [[Laplace Transform of Real Power]] }} {{eqn | r = \frac 2 {s^{3/2} \sqrt \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {n + \frac 1 2} \map \Gamma {n + \frac 1 2} } {\paren {2 n + 1} n! s^n} | c = [[Gamma Difference Equation]] }} {{eqn | r = \frac 1 {s^{3/2} \sqrt \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\map \Gamma {n + \frac 1 2} } {\map \Gamma {n + 1} s^n} | c = [[Gamma Function Extends Factorial]] }} {{end-eqn}} We have: {{begin-eqn}} {{eqn | l = \map \Gamma {n + \frac 1 2} | r = \frac \pi {\map \sin {\pi \paren {\frac 1 2 - n} } \map \Gamma {\frac 1 2 - n} } | c = [[Euler's Reflection Formula]] }} {{eqn | r = \frac \pi {\map \cos {-n \pi} \map \Gamma {\frac 1 2 - n} } | c = [[Sine of Complement equals Cosine]] }} {{eqn | r = \frac {\paren {-1}^n \pi} {\map \Gamma {\frac 1 2 - n} } | c = [[Cosine of Integer Multiple of Pi]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \frac 1 {s^{3/2} \sqrt \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\map \Gamma {n + \frac 1 2} } {\map \Gamma {n + 1} s^n} | r = \frac 1 {s^{3/2} \sqrt \pi} \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {-1}^n \pi} {\map \Gamma {n + 1} \map \Gamma {\frac 1 2 - n} s^n} | c = }} {{eqn | r = \frac 1 {s^{3/2} } \sum_{n \mathop = 0}^\infty \frac {\sqrt \pi} {\map \Gamma {n + 1} \map \Gamma {\frac 1 2 - n} s^n} | c = }} {{eqn | r = \frac 1 {s^{3/2} } \sum_{n \mathop = 0}^\infty \frac {\map \Gamma {-\frac 1 2 + 1} } {\map \Gamma {n + 1} \map \Gamma {-\frac 1 2 - n + 1} s^n} | c = [[Gamma Function of One Half]] }} {{eqn | r = \dfrac 1 {s^{3/2} } \sum_{n \mathop = 0}^\infty \binom {-\frac 1 2} n \frac 1 {s^n} | c = {{Defof|Binomial Coefficient/Complex Numbers|Binomial Coefficient}} }} {{eqn | r = \dfrac 1 {s^{3/2} } \paren {1 + \dfrac 1 s}^{-1/2} | c = [[General Binomial Theorem]] }} {{eqn | r = \dfrac 1 {s \sqrt {s + 1} } | c = simplification }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = z | r = x^2 }} {{eqn | ll= \implies | l = \frac {\mathrm d z} {\mathrm d x} | r = 2 x | c = [[Power Rule for Derivatives]] }} {{eqn | ll= \implies | l = \int x \sqrt {a^2 - x^2} \ \mathrm d x | r = \int \frac {\sqrt z \sqrt {a^2 - z} \ \mathrm d z} {2 \sqrt z} | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 2 \int \sqrt {a^2 - z} \ \mathrm d z | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 2 \frac {2 \left({\sqrt {a^2 - z} }\right)^3} {-3} + C | c = [[Primitive of Root of a x + b|Primitive of $\sqrt {a x + b}$]] }} {{eqn | r = \frac {-\left({\sqrt {a^2 - x^2} }\right)^3} 3 + C | c = substituting for $z$ and simplifying }} {{end-eqn}} {{qed}}	0
It can be seen that $(1)$ is a [[Definition:Constant Coefficient Homogeneous Linear Second Order ODE|constant coefficient homogeneous linear second order ODE]]. Its [[Definition:Auxiliary Equation|auxiliary equation]] is: :$(2): \quad: m^2 + 10 m + 25 = 0$ From [[Solution to Quadratic Equation with Real Coefficients]], the [[Definition:Root of Polynomial|roots]] of $(2)$ are: :$m_1 = m_2 = -5$ These are [[Definition:Real Number|real]] and equal. So from [[Solution of Constant Coefficient Homogeneous LSOODE]], the [[Definition:General Solution to Differential Equation|general solution]] of $(1)$ is: :$y = C_1 e^{-5 x} + C_2 x e^{-5 x}$ {{qed}}	0
From [[Fourier Series/Logarithm of Sine of x over 0 to Pi|Fourier Series of $\map \ln {\sin x}$ from $0$ to $\pi$]]: :$\displaystyle \map \ln {\sin x} = -\ln 2 - \sum_{n \mathop = 1}^\infty \frac {\cos 2 n x} n$ Then, by [[Parseval's Theorem]]: {{begin-eqn}} {{eqn | l = \frac 2 \pi \int_0^\pi \paren {\map \ln {\sin x} }^2 \rd x | r = 2 \paren {\ln 2}^2 + \sum_{n = 1}^\infty \frac 1 {n^2} }} {{eqn | r = 2 \paren {\ln 2}^2 + \frac {\pi^2} 6 | c = [[Basel Problem]] }} {{end-eqn}} We then have: {{begin-eqn}} {{eqn | l = \int_0^\pi \paren {\map \ln {\sin x} }^2 \rd x | r = \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x + \int_{\pi/2}^\pi \paren {\map \ln {\sin x} }^2 \rd x | c = [[Sum of Integrals on Adjacent Intervals for Integrable Functions]] }} {{eqn | r = \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x + \int_0^{\pi/2} \paren {\map \ln {\map \sin {\pi - x} } }^2 \rd x }} {{eqn | r = 2 \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x | c = [[Sine of Supplementary Angle]] }} {{end-eqn}} So we have: :$\displaystyle \frac 4 \pi \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x = 2 \paren {\ln 2}^2 + \frac {\pi^2} 6$ multiplying by $\dfrac \pi 4$ we have: :$\displaystyle \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x = \frac \pi 2 \paren {\ln 2}^2 + \frac {\pi^3} {24}$ {{qed}}	0
It can be seen that $(1)$ is an instance of the [[Definition:Cauchy-Euler Equation|Cauchy-Euler Equation]]: :$x^2 y'' + p x y' + q y = 0$ where: :$p = 2$ :$q = -2$ By [[Conversion of Cauchy-Euler Equation to Constant Coefficient Linear ODE]], this can be expressed as: :$\dfrac {\d^2 y} {\d t^2} + \paren {p - 1} \dfrac {\d y} {\d t} + q y = 0$ by making the substitution: :$x = e^t$ Hence it can be expressed as: :$(2): \quad \dfrac {\d^2 y} {\d t^2} + \dfrac {\d y} {\d t} - 2 y = 0$ From [[Linear Second Order ODE/y'' + y' - 2 y = 0|Linear Second Order ODE: $y'' + y' - 2 y = 0$]], this has the [[Definition:General Solution|general solution]]: {{begin-eqn}} {{eqn | l = y | r = C_1 e^t + C_2 e^{-2 t} | c = }} {{eqn | r = C_1 x + C_2 x^{-2} | c = substituting $x = e^t$ }} {{eqn | r = C_1 x + \frac {C_2} {x^2} | c = }} {{end-eqn}} {{qed}}	0
Let $C$ and $D$ be [[Definition:Contour (Complex Plane)|contours]]. That is, $C$ is a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Directed Smooth Curve|directed smooth curves]] $C_1, \ldots, C_n$. Let $C_i$ be [[Definition:Directed Smooth Curve/Parameterization|parameterized]] by the [[Definition:Smooth Path (Complex Analysis)|smooth path]] $\gamma_i: \left[{a_i\,.\,.\,b_i}\right] \to \C$ for all $i \in \left\{ {1, \ldots, n}\right\}$. Similarly, $D$ is a [[Definition:Finite Sequence|finite sequence]] of [[Definition:Directed Smooth Curve|directed smooth curves]] $D_1, \ldots, D_m$. Let $D_i$ be [[Definition:Directed Smooth Curve/Parameterization|parameterized]] by the [[Definition:Smooth Path (Complex Analysis)|smooth path]] $\sigma_i: \left[{c_i\,.\,.\,d_i}\right] \to \C$ for all $i \in \left\{ {1, \ldots, m}\right\}$. Suppose $\gamma_n \left({b_n}\right) = \sigma_1 \left({c_1}\right)$. Then the finite sequence: :$C_1, \ldots, C_n, D_1, \ldots, D_m$ defines a [[Definition:Contour (Complex Plane)|contour]].	0
:$\displaystyle \int x \arctan \frac x a \ \mathrm d x = \frac {x^2 + a^2} 2 \arctan \frac x a - \frac {a x} 2 + C$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = x^m | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = m x^{m - 1} | c = [[Power Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \sin a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {- \cos a x} a | c = [[Primitive of Sine of a x|Primitive of $\sin a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^m \sin a x \rd x | r = \paren {x^m} \paren {\frac {-\cos a x} a} - \int \paren {\frac {-\cos a x} a} \paren {m x^{m - 1} } \rd x + C | c = [[Integration by Parts]] }} {{eqn | n = 1 | r = \frac {- x^m \cos a x} a + \frac m a \int x^{m - 1} \cos a x \rd x + C | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} Similarly, let: {{begin-eqn}} {{eqn | l = u | r = x^{m - 1} | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \paren {m - 1} x^{m - 2} | c = [[Power Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \cos a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {\sin a x} a | c = [[Primitive of Cosine of a x|Primitive of $\cos a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^{m - 1} \cos a x \rd x | r = \paren {x^{m - 1} } \paren {\frac {\sin a x} a} - \int \paren {\frac {\sin a x} a} \paren {m - 1} x^{m - 2} \rd x + C | c = [[Integration by Parts]] }} {{eqn | n = 2 | r = \frac {x^{m - 1} \sin a x} a - \frac {m - 1} a \int x^{m - 2} \sin a x \rd x + C | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \int x^m \sin a x \rd x | r = \frac {- x^m \cos a x} a + \frac m a \int x^{m - 1} \cos a x \rd x + C | c = from $(1)$ }} {{eqn | r = \frac {- x^m \cos a x} a + \frac m a \paren {\frac {x^{m - 1} \sin a x} a - \frac {m - 1} a \int x^{m - 2} \sin a x \rd x} + C | c = from $(2)$ }} {{eqn | r = \frac {- x^m \cos a x} a + \frac {m x^{m - 1} \sin a x} {a^2} - \frac {m \paren {m - 1} } {a^2} \int x^{m - 2} \sin a x \rd x + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let: :$\map f {x, y} = \map {\tan^2} {x^2 - y^2}$ Then: {{begin-eqn}} {{eqn | l = \map {f_1} {x, y} | r = 4 x \map \tan {x^2 - y^2} \map {\sec^2} {x^2 - y^2} }} {{eqn | l = \map {f_2} {1, 2} | r = 8 \tan 3 \sec^2 3 }} {{end-eqn}}	0
From the [[Primitive of Reciprocal of x squared minus a squared/Logarithm Form 1|$1$st logarithm form]]: :$\ds \int \frac {\d x} {x^2 - a^2} = \begin {cases} \dfrac 1 {2 a} \map \ln {\dfrac {a - x} {a + x} } + C & : \size x < a\\ & \\ \dfrac 1 {2 a} \map \ln {\dfrac {x - a} {x + a} } + C & : \size x > a \\ & \\ \text {undefined} & : \size x = a \end {cases}$ From [[Primitive of Reciprocal of x squared minus a squared/Logarithm Form/Lemma|Primitive of Reciprocal of a squared minus x squared: Logarithm Form: Lemma]]: :$\map \ln {\dfrac {x - a} {x + a} }$ is defined {{iff}} $\size x > a$ :$\map \ln {\dfrac {a - x} {a + x} }$ is defined {{iff}} $\size x < a$ Let $\size x > a$. Then $\map \ln {\dfrac {x - a} {x + a} }$ is defined. We have that: {{begin-eqn}} {{eqn | l = \dfrac {x - a} {x + a} | o = > | r = 0 | c = }} {{eqn | ll= \leadsto | l = \dfrac {x - a} {x + a} | r = \size {\dfrac {x - a} {x + a} } | c = }} {{end-eqn}} So the result holds for $\size x > a$. Let $\size x < a$. Then $\map \ln {\dfrac {a - x} {a + x} }$ is defined. We have: We have that: {{begin-eqn}} {{eqn | l = \dfrac {a - x} {a + x} | r = -\dfrac {x - a} {x + a} | c = }} {{eqn | o = < | r = 0 | c = }} {{eqn | ll= \leadsto | l = \dfrac {a - x} {a + x} | r = \size {\dfrac {x - a} {x + a} } | c = }} {{end-eqn}} The result follows. {{qed}}	0
:$\displaystyle \int e^{a x} \sin b x \rd x = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2 + b^2} + C$	0
<onlyinclude> {{begin-eqn}} {{eqn | l = \int \frac {\arccsc \frac x a \rd x} x | r = -\paren {\sum_{k \mathop \ge 0} \frac {\paren {2 k + 1}!} {2^{2 k} \paren {k!}^2 \paren {2 k + 1}^3} \paren {\frac a x}^{2 k + 1} } + C | c = }} {{eqn | r = -\paren {\frac a x + \frac 1 {2 \times 3 \times 3} \paren {\frac a x}^3 + \frac {1 \times 3} {2 \times 4 \times 5 \times 5} \paren {\frac a x}^5 + \frac {1 \times 3 \times 5} {2 \times 4 \times 6 \times 7 \times 7} \paren {\frac a x}^7 + \cdots} + C | c = }} {{end-eqn}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \coth^{-1} \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {-a} {x^2 - a^2} | c = [[Derivative of Inverse Hyperbolic Cotangent of x over a|Derivative of $\coth^{-1} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^2} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {-1} x | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\coth^{-1} \dfrac x a \rd x} {x^2} | r = \paren {\coth^{-1} \frac x a} \paren {\frac {-1} x} - \int \paren {\frac {-1} x} \paren {\frac {-a} {x^2 - a^2} } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - a \int \frac {\d x} {x \paren {x^2 - a^2} } + C | c = simplifying }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - a \paren {\frac 1 {2 a^2} \map \ln {\frac {x^2 - a^2} {x^2} } } + C | c = [[Primitive of Reciprocal of x by x squared minus a squared|Primitive of $\dfrac 1 {x \paren {x^2 - a^2} }$]] }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - \frac 1 {2 a} \map \ln {\frac {x^2 - a^2} {x^2} } + C | c = simplifying }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x + \frac 1 {2 a} \map \ln {\frac {x^2} {x^2 - a^2} } + C | c = [[Logarithm of Reciprocal]] }} {{end-eqn}} {{qed}}	0
:$\ds \int \csch x \coth x \rd x = -\csch x + C$ where $C$ is an [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]].	0
Let $\map y x$ be a [[Definition:Real Function|real mapping]] in 2-dimensional [[Definition:Real Euclidean Space|real Euclidean space]]. Let $y$ pass through the [[Definition:Point|points]] $\tuple {x_0, y_0}$ and $\tuple {x_1, y_1}$. Consider a [[Definition:Surface of Revolution|surface of revolution]] constructed by rotating $y$ around the $x$-axis. Suppose this [[Definition:Surface|surface]] is [[Definition:Smooth Real Function|smooth]] for any $x$ between $x_0$ and $x_1$. Then its [[Definition:Surface|surface]] [[Definition:Area|area]] is minimized by the following [[Definition:Curve|curve]], known as a catenoid: :$y = C \map \cosh {\dfrac {x + C_1} C}$ Furthermore, its [[Definition:Area|area]] is: :$A = \paren {x_1 - x_0} C \pi + \dfrac {\pi C^2} 2 \paren {\map \sinh {\dfrac {2 \paren {x_1 + C_1} } C} - \map \sinh {\dfrac {2 \paren {x_0 + C_1} } C} }$	0
Let: :$\displaystyle f\left({z}\right) = \operatorname P \left\{ \begin{matrix} a & b & c \\ \alpha & \beta & \gamma & z \\ \alpha' & \beta' & \gamma' \end{matrix} \right\}$ where: :$\operatorname P$ is the [[Definition:Riemann P-symbol|Riemann P-symbol]] :$\alpha + \beta + \gamma + \alpha' + \beta' + \gamma' = 1$ :$\alpha - \alpha'$ is not a [[Definition:Negative Number|negative]] [[Definition:Integer|integer]]. Then: :$\displaystyle f\left({z}\right) = \left({ \frac {z - a} {z - b} }\right)^\alpha \left({ \frac {z - c} {z - b} }\right)^\gamma {}_2 \operatorname F_1 \left({ {\alpha + \beta + \gamma, \alpha + \beta' + \gamma} \atop {1 + \alpha - \alpha'} } \, \middle \vert {\, \frac {\left({z - a}\right) \left({c - b}\right)} {\left({z - b} \right) \left({c - a}\right)} }\right)$ where ${}_2 \operatorname F_1$ is the [[Definition:Gaussian Hypergeometric Function|Gaussian hypergeometric function]].	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \ln^n x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = n \ln^{n - 1} x \frac 1 x | c = [[Derivative of Natural Logarithm|Derivative of $\ln x$]], [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x^m | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^{m + 1} }{m + 1} | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^m \ln^n x \ \mathrm d x | r = \frac {x^{m + 1} }{m + 1} \ln^n x - \int \frac {x^{m + 1} }{m + 1} \left({n \ln^{n - 1} x \frac 1 x}\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^{m + 1} \ln^n x} {m + 1} - \frac n {m + 1} \int x^m \ln^{n - 1} x \ \mathrm d x + C | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int_0^\infty \frac {\sin^3 x} {x^3} \rd x = \frac {3 \pi} 8$	0
Let $\map {y_g} {x, C_1, C_2}$ be a [[Definition:General Solution to Differential Equation|general solution]] of $(2)$. Note that $C_1$ and $C_2$ are the two [[Definition:Arbitrary Constant|arbitrary constants]] that are to be expected of a [[Definition:Second Order ODE|second order ODE]]. Let $\map {y_p} x$ be a certain fixed [[Definition:Particular Solution to Differential Equation|particular solution]] of $(1)$. Let $\map y x$ be an arbitrary [[Definition:Particular Solution to Differential Equation|particular solution]] of $(1)$. Then: {{begin-eqn}} {{eqn | o = | r = \paren {y - y_p}'' + \map P x \paren {y - y_p}' + \map Q x \paren {y - y_p} | c = }} {{eqn | r = \paren {y'' + \map P x y' + \map Q x y} - \paren {y_p'' + \map P x y_p' + \map Q x y_p} | c = }} {{eqn | r = \map R x - \map R x | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} We have that $\map {y_g} {x, C_1, C_2}$ is a [[Definition:General Solution to Differential Equation|general solution]] of $(2)$. Thus: :$\map y x - \map {y_p} x = \map {y_g} {x, C_1, C_2}$ or: :$\map y x = \map {y_g} {x, C_1, C_2} + \map {y_p} x$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \sech^2 x \rd x | r = \tanh x + C | c = [[Primitive of Square of Hyperbolic Secant Function|Primitive of $\sech^2 x$]] }} {{eqn | ll= \leadsto | l = \int \sech^2 a x \rd x | r = \frac 1 a \paren {\tanh a x} + C | c = [[Primitive of Function of Constant Multiple]] }} {{eqn | r = \frac {\tanh a x} a + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\cos x} | r = \lim_{h \mathop \to 0} \frac {\map \cos {x + h} - \cos x} h | c = {{Defof|Derivative of Real Function at Point}} }} {{eqn | r = \lim_{h \mathop \to 0} \frac {\map \cos {\paren {x + \frac h 2} + \frac h 2} - \map \cos {\paren {x + \frac h 2} - \frac h 2} } h }} {{eqn | r = \lim_{h \mathop \to 0} \frac {-2 \map \sin {x + \frac h 2} \map \sin {\frac h 2} } h | c = [[Simpson's Formula for Sine by Sine]] }} {{eqn | r = -\lim_{h \mathop \to 0} \map \sin {x + \frac h 2} \lim_{h \mathop \to 0} \frac {\map \sin {\frac h 2} } {\frac h 2} | c = [[Multiple Rule for Limits of Functions]] and [[Product Rule for Limits of Functions]] }} {{eqn | r = -\sin x \times 1 | c = Continuity of Sine and [[Limit of Sine of X over X]] }} {{eqn | r = -\sin x }} {{end-eqn}} {{qed}}	0
Let $n \in \Q$. Let $f: \R \to \R$ be the [[Definition:Real Function|real function]] defined as $f \left({x}\right) = x^n$. Then: :$\map {f'} x = n x^{n-1}$ everywhere that $\map f x = x^n$ is defined. When $x = 0$ and $n = 0$, $\map {f'} x$ is undefined.	0
Let $\closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $f$ be a [[Definition:Bounded Real-Valued Function|bounded]] [[Definition:Real Function|real function]] defined on $\closedint a b$. Then $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] {{iff}}: :for every $\epsilon \in \R_{>0}$, there exists a [[Definition:Finite Subdivision|finite subdivision]] $S$ of $\closedint a b$ such that $\map U S – \map L S < \epsilon$ where :$\map U S$ is the [[Definition:Upper Sum|upper sum]] of $f$ on $\closedint a b$ with respect to $S$ :$\map L S$ is the [[Definition:Lower Sum|lower sum]] of $f$ on $\closedint a b$ with respect to $S$	0
Let: {{begin-eqn}} {{eqn | l = u | r = \sqrt{a x + b} | c = }} {{eqn | l = x | r = \frac {u^2 - b} a | c = }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = F \left({\sqrt{a x + b} }\right) | r = \frac x {\sqrt{a x + b} } | c = }} {{eqn | ll= \implies | l = F \left({u}\right) | r = \left({\frac {u^2 - b} a}\right) \frac 1 u | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\sqrt{a x + b} } | r = \frac 2 a \int u \left({\frac {u^2 - b} a}\right) \frac 1 u \ \mathrm d u | c = [[Primitive of Function of Root of a x + b|Primitive of Function of $\sqrt{a x + b}$]] }} {{eqn | r = \frac 2 {a^2} \int \left({u^2 - b}\right) \ \mathrm d u | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 2 {a^2} \left({\frac {u^3} 3 - b u}\right) + C | c = [[Primitive of Power]] and [[Primitive of Constant]] }} {{eqn | r = \frac 2 {a^2} \left({\frac {\left({a x + b}\right) \sqrt{a x + b} } 3 - b \sqrt{a x + b} }\right) + C | c = substituting for $u$ }} {{eqn | r = \frac 2 {3 a^2} \left({\left({a x + b}\right) - 3 b}\right) \sqrt{a x + b} + C | c = extracting common factors }} {{eqn | r = \frac {2 \left({a x - 2 b}\right) \sqrt{a x + b} } {3 a^2} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | o = | r = \int \frac {\d x} {x^2 \sqrt {a x^2 + b x + c} } }} {{eqn | r = \int \frac {c \d x} {c x^2 \sqrt {a x^2 + b x + c} } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $c$ }} {{eqn | r = \int \frac {\paren {a x^2 + b x + c - a x^2 - b x} \rd x} {c x^2 \sqrt {a x^2 + b x + c} } | c = adding and subtracting $a x^2 + b x$ }} {{eqn | r = \frac 1 c \int \frac {\paren {a x^2 + b x + c} \rd x} {x^2 \sqrt {a x^2 + b x + c} } - \frac a c \int \frac {x^2 \rd x} {x^2 \sqrt {a x^2 + b x + c} } - \frac b c \int \frac {x \rd x} {x^2 \sqrt {a x^2 + b x + c} } | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 c \int \frac {\sqrt {a x^2 + b x + c} \rd x} {x^2} - \frac a c \int \frac {\d x} {\sqrt {a x^2 + b x + c} } - \frac b c \int \frac {\d x} {x \sqrt {a x^2 + b x + c} } | c = simplification }} {{eqn | r = \frac 1 c \paren {-\frac {\sqrt {a x^2 + b x + c} } x + a \int \frac {\d x} {\sqrt {a x^2 + b x + c} } + \frac b 2 \int \frac {\d x} {x \sqrt {a x^2 + b x + c} } } | c = [[Primitive of Root of a x squared plus b x plus c over x squared|Primitive of $\dfrac {\sqrt {a x^2 + b x + c} } {x^2}$]] }} {{eqn | o = | ro= - | r = \frac a c \int \frac {\d x} {\sqrt {a x^2 + b x + c} } - \frac b c \int \frac {\d x} {x \sqrt {a x^2 + b x + c} } | c = }} {{eqn | r = -\frac {\sqrt {a x^2 + b x + c} } {c x} - \frac b {2 c} \int \frac {\d x} {x \sqrt {a x^2 + b x + c} } | c = gathering terms }} {{end-eqn}} {{qed}}	0
If $\forall x \in \openint a b: \map {f'} x \le 0$, then $f$ is [[Definition:Decreasing Real Function|decreasing]] on $\closedint a b$.	0
Let $\map f t := \map \Si t = \displaystyle \int_0^t \dfrac {\sin u} u \rd u$. Then: :$\map f 0 = 0$ and: {{begin-eqn}} {{eqn | l = \map {f'} t | r = \dfrac {\sin t} t | c = }} {{eqn | ll= \leadsto | l = t \map {f'} t | r = \sin t | c = }} {{eqn | ll= \leadsto | l = \laptrans {t \map {f'} t} | r = \laptrans {\sin t} | c = }} {{eqn | r = \dfrac 1 {s^2 + 1} | c = [[Laplace Transform of Sine]] }} {{eqn | ll= \leadsto | l = -\dfrac \d {\d s} \laptrans {\map {f'} t} | r = \dfrac 1 {s^2 + 1} | c = [[Derivative of Laplace Transform]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d s} } {s \laptrans {\map f t} - \map f 0} | r = -\dfrac 1 {s^2 + 1} | c = [[Laplace Transform of Derivative]] }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = -\int \dfrac 1 {s^2 + 1} \rd s | c = $\map f 0 = 0$, and integrating both sides {{WRT|Integration}} $s$ }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = -\arctan s + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{end-eqn}} By the [[Initial Value Theorem of Laplace Transform]]: :$\displaystyle \lim_{s \mathop \to \infty} s \laptrans {\map f t} = \lim_{t \mathop \to 0} \map f t = \map f 0 = 0$ which leads to: :$c = \dfrac \pi 2$ Thus: {{begin-eqn}} {{eqn | l = s \laptrans {\map f t} | r = \dfrac \pi 2 - \arctan s | c = }} {{eqn | r = \arccot s | c = [[Sum of Arctangent and Arccotangent]] }} {{eqn | r = \arctan \dfrac 1 s | c = [[Arctangent of Reciprocal equals Arccotangent]] }} {{eqn | ll= \leadsto | l = \laptrans {\map f t} | r = \dfrac 1 s \arctan \dfrac 1 s | c = }} {{end-eqn}} {{qed}}	0
Let $(1)$ be rearranged as: {{begin-eqn}} {{eqn | l = \frac {\d x} {\d y} | r = \frac {1 - x y} {y^2} | c = }} {{eqn | n = 2 | ll= \leadsto | l = \frac {\d x} {\d y} + \frac x y | r = \frac 1 {y^2} | c = }} {{end-eqn}} It can be seen that $(2)$ is a [[Definition:Linear First Order ODE|linear first order ODE]] in the form: :$\dfrac {\d x} {\d y} + \map P y x = \map Q y$ where: :$\map P y = \dfrac 1 y$ :$\map Q y = \dfrac 1 {y^2}$ Thus: {{begin-eqn}} {{eqn | l = \int \map P y \rd y | r = \int \dfrac 1 y \rd y | c = }} {{eqn | r = \ln y | c = }} {{eqn | ll= \leadsto | l = e^{\int P \rd y} | r = y | c = }} {{end-eqn}} Thus from [[Solution by Integrating Factor]], $(2)$ can be rewritten as: {{begin-eqn}} {{eqn | l = \map {\dfrac \d {\d y} } {x y} | r = \dfrac 1 y | c = }} {{eqn | ll= \leadsto | l = x y | r = \int \frac {\d y} y | c = }} {{eqn | r = \ln y + C | c = }} {{end-eqn}} {{qed}}	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\left({a x + b}\right)^2} | r = \int \frac 1 a \frac {u - b} {a u^2} \ \mathrm d u | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\mathrm d u} u - \frac b {a^2} \int \frac {\mathrm d u} {u^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \ln \left\vert{u}\right\vert + C - \frac b {a^2} \int \frac {\mathrm d u} u | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^2} \ln \left\vert{u}\right\vert - \frac b {a^2} \frac {-1} u + C | c = [[Primitive of Power]] }} {{eqn | r = \frac b {a^2 \left({a x + b}\right)} + \frac 1 {a^2} \ln \left\vert{a x + b}\right\vert + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {\left({1 - \cos a x}\right)^2} | r = \int \left({\frac 1 2 \csc^2 \frac {a x} 2}\right)^2 \ \mathrm d x | c = [[Reciprocal of One Minus Cosine]] }} {{eqn | r = \frac 1 4 \int \csc^4 \frac {a x} 2 \ \mathrm d x | c = simplifying }} {{eqn | r = \frac 1 4 \left({\frac{-\csc^2 \dfrac {a x} 2 \cot \dfrac {a x} 2} {\dfrac {3 a} 2} + \frac 2 3 \int \csc^2 \frac {a x} 2 \ \mathrm d x}\right) + C | c = [[Primitive of Power of Cosecant of a x|Primitive of $\csc^n a x$]] }} {{eqn | r = \frac {-1} {6 a} \csc^2 \frac {a x} 2 \cot \dfrac {a x} 2 + \frac 1 6 \int \csc^2 \frac {a x} 2 \ \mathrm d x + C | c = simplifying }} {{eqn | r = \frac {-1} {6 a} \csc^2 \frac {a x} 2 \cot \dfrac {a x} 2 + \frac 1 6 \left({\frac {-2} a \cot \frac {a x} 2}\right) + C | c = [[Primitive of Square of Cosecant of a x|Primitive of $\csc^2 a x$]] }} {{eqn | r = \frac {-1} {6 a} \left({1 + \cot^2 \frac {a x} 2}\right) \cot \dfrac {a x} 2 - \frac 2 {6 a} \cot \frac {a x} 2 + C | c = [[Difference of Squares of Cosecant and Cotangent]] }} {{eqn | r = \frac {-1} {2a} \cot \frac {a x} 2 - \frac 1 {6 a} \cot^3 \frac {a x} 2 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
From [[Derivative of Hyperbolic Cosecant]]: :$\dfrac \d {\d x} \csch x = -\csch x \coth x$ The result follows from the definition of [[Definition:Primitive (Calculus)|primitive]]. {{Qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x^3 \left({\sqrt {x^2 + a^2} }\right)^3} | r = \int \frac {a^2 \ \mathrm d x} {a^2 x^3 \left({\sqrt {x^2 + a^2} }\right)^3} | c = }} {{eqn | r = \int \frac {\left({x^2 + a^2 - x^2}\right) \ \mathrm d x} {a^2 x^3 \left({\sqrt {x^2 + a^2} }\right)^3} | c = }} {{eqn | r = \frac 1 {a^2} \int \frac {\left({x^2 + a^2}\right) \ \mathrm d x} {x^3 \left({\sqrt {x^2 + a^2} }\right)^3} - \frac 1 {a^2} \int \frac {x^2 \ \mathrm d x} {x^3 \left({\sqrt {x^2 + a^2} }\right)^3} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\mathrm d x} {x^3 \sqrt {x^2 + a^2} } - \frac 1 {a^2} \int \frac {\mathrm d x} {x \left({\sqrt {x^2 + a^2} }\right)^3} | c = simplifying }} {{eqn | r = \frac 1 {a^2} \left({\frac {- \sqrt {x^2 + a^2} } {2 a^2 x^2} + \frac 1 {2 a^3} \ln \left({\frac {a + \sqrt {x^2 + a^2} } x}\right)}\right) - \frac 1 {a^2} \int \frac {\mathrm d x} {x \left({\sqrt {x^2 + a^2} }\right)^3} + C | c = [[Primitive of Reciprocal of x cubed by Root of x squared plus a squared|Primitive of $\dfrac 1 {x^3 \sqrt {x^2 + a^2} }$]] }} {{eqn | r = \frac 1 {a^2} \left({\frac {- \sqrt {x^2 + a^2} } {2 a^2 x^2} + \frac 1 {2 a^3} \ln \left({\frac {a + \sqrt {x^2 + a^2} } x}\right)}\right) | c = }} {{eqn | o = | ro= - | r = \frac 1 {a^2} \left({\frac 1 {a^2 \sqrt {x^2 + a^2} } - \frac 1 {a^3} \ln \left({\frac {a + \sqrt {x^2 + a^2} } x}\right) }\right) + C | c = [[Primitive of Reciprocal of x by Root of x squared plus a squared cubed|Primitive of $\dfrac 1 {x \left({\sqrt {x^2 + a^2} }\right)^3}$]] }} {{eqn | r = \frac {-1} {2 a^2 x^2 \sqrt {x^2 + a^2} } - \frac 3 {2 a^4 \sqrt {x^2 + a^2} } + \frac 3 {2 a^5} \ln \left({\frac {a + \sqrt {x^2 + a^2} } x}\right) + C | c = simplification }} {{end-eqn}} {{qed}}	0
{{ProofWanted|Graphical approach based on dividing the area under the curve as trapezia. We need to explain rigorously what an "approximation" means, and we may also want to quantify the error.}}	0
:$\displaystyle \int \map F {e^{a x} } \rd x = \frac 1 a \int \frac {\map F u} u \rd u$ where $u = e^{a x}$.	0
Let $u = \map f {x_1, x_2, \ldots, x_m}$ be a [[Definition:Real-Valued Function|function]] of the $m$ [[Definition:Real Independent Variable|independent variables]] $x_1, x_2, \ldots, x_m$. There are $m^n$ [[Definition:Partial Derivative|partial derivatives]] of $u$ of [[Definition:Order of Partial Derivative|order $n$]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\cos a x \rd x} {\sin a x + \cos a x} | r = \int \frac {\paren {\sin a x + \cos a x - \sin a x} \rd x} {\sin a x + \cos a x} | c = }} {{eqn | r = \int \frac {\paren {\sin a x + \cos a x} \rd x} {\sin a x + \cos a x} - \int \frac {\sin a x \rd x} {\sin a x + \cos a x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \int \rd x - \int \frac {\sin a x \rd x} {\sin a x + \cos a x} | c = simplification }} {{eqn | r = x - \int \frac {\sin a x \rd x} {\sin a x + \cos a x} + C | c = [[Primitive of Constant]] }} {{eqn | r = x - \paren {\frac x 2 - \frac 1 {2 a} \ln \size {\sin a x + \cos a x} } + C | c = [[Primitive of Sine of a x over Sine of a x plus Cosine of a x|Primitive of $\dfrac {\sin a x} {\sin a x + \cos a x}$]] }} {{eqn | r = \frac x 2 + \frac 1 {2 a} \ln \size {\sin a x + \cos a x} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x^2 \left({a x + b}\right)^3} | r = \int \left({\frac {-3 a} {b^4 x} + \frac 1 {b^3 x^2} + \frac {3 a^2} {b^4 \left({a x + b}\right)} + \frac {2 a^2} {b^3 \left({a x + b}\right)^2} + \frac {a^2} {b^2 \left({a x + b}\right)^3} }\right) \ \mathrm d x | c = [[Primitive of Reciprocal of x squared by a x + b cubed/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac {-3 a} {b^4} \int \frac {\mathrm d x} x + \frac 1 {b^3} \int \frac {\mathrm d x} {x^2} + \frac {3 a^2} {b^4} \int \frac {\mathrm d x} {\left({a x + b}\right)} + \frac {2 a^2} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + \frac {a^2} {b^2} \int \frac {\mathrm d x} {\left({a x + b}\right)^3} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {-3 a} {b^4} \ln \left\vert{x}\right\vert + \frac 1 {b^3} \int \frac {\mathrm d x} {x^2} + \frac {3 a^2} {b^4} \int \frac {\mathrm d x} {\left({a x + b}\right)} + \frac {2 a^2} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + \frac {a^2} {b^2} \int \frac {\mathrm d x} {\left({a x + b}\right)^3} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac {-3 a} {b^4} \ln \left\vert{x}\right\vert + \frac 1 {b^3} \int \frac {\mathrm d x} {x^2} + \frac {3 a^2} {b^4} \ln \left\vert{a x + b}\right\vert + \frac {2 a^2} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + \frac {a^2} {b^2} \int \frac {\mathrm d x} {\left({a x + b}\right)^3} + C | c = [[Primitive of Reciprocal of a x + b]] }} {{eqn | r = \frac {-3 a} {b^4} \ln \left\vert{x}\right\vert + \frac 1 {b^3} \int \frac {\mathrm d x} {x^2} + \frac {3 a^2} {b^4} \ln \left\vert{a x + b}\right\vert + \frac {2 a^2} {b^3} \frac {-1} {a \left({a x + b}\right)} + \frac {a^2} {b^2} \int \frac {\mathrm d x} {\left({a x + b}\right)^3} + C | c = [[Primitive of Reciprocal of a x + b squared]] }} {{eqn | r = \frac {-3 a} {b^4} \ln \left\vert{x}\right\vert + \frac 1 {b^3} \int \frac {\mathrm d x} {x^2} + \frac {3 a^2} {b^4} \ln \left\vert{a x + b}\right\vert - \frac {2 a} {b^3 \left({a x + b}\right)} + \frac {a^2} {b^2} \frac {-1} {2 a \left({a x + b}\right)^2} + C | c = [[Primitive of Reciprocal of a x + b cubed]] }} {{eqn | r = \frac {-3 a} {b^4} \ln \left\vert{x}\right\vert + \frac 1 {b^3} \frac {-1} x + \frac {3 a^2} {b^4} \ln \left\vert{a x + b}\right\vert - \frac {2 a} {b^3 \left({a x + b}\right)} - \frac a {2 b^2 \left({a x + b}\right)^2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {-a} {2 b^2 \left({a x + b}\right)^2} - \frac {2 a} {b^3 \left({a x + b}\right)} - \frac 1 {b^3 x} + \frac {3 a} {b^4} \ln \left\vert{\frac {a x + b} x}\right\vert + C | c = [[Difference of Logarithms]] and rearranging }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {x^3 \ \mathrm d x} {\left({\sqrt {x^2 + a^2} }\right)^3} = \sqrt {x^2 + a^2} + \frac {a^2} {\sqrt {x^2 + a^2} } + C$	0
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\dfrac {\d^n} {\d x^n} \dfrac {\ln x} x = \paren {-1}^{n + 1} n! \dfrac {H_n - \ln x} {x^{n + 1} }$ === Basis for the Induction === $\map P 1$ is the case: :$\dfrac \d {\d x} \dfrac {\ln x} x = \paren {-1}^n n! \dfrac {H_n - \ln x} {x^{n + 1} }$ {{begin-eqn}} {{eqn | l = \dfrac \d {\d x} \dfrac {\ln x} x | r = \dfrac {1 - \ln x} {x^2} | c = [[Derivative of Logarithm over Power]] }} {{eqn | r = \paren {-1}^2 \times 1! \times \dfrac {H_1 - \ln x} {x^{1 + 1} } | c = }} {{end-eqn}} Thus $\map P 1$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\dfrac {\d^k} {\d x^k} \dfrac {\ln x} x = \paren {-1}^{k + 1} k! \dfrac {H_k - \ln x} {x^{k + 1} }$ from which it is to be shown that: :$\dfrac {\d^{k + 1} } {\d x^{k + 1} } \dfrac {\ln x} x = \paren {-1}^{k + 2} \paren {k + 1}! \dfrac {H_{k + 1} - \ln x} {x^{k + 2} }$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \dfrac {\d^{k + 1} } {\d x^{k + 1} } \dfrac {\ln x} x | r = \dfrac \d {\d x} \paren {-1}^{k + 1} k! \dfrac {H_k - \ln x} {x^{k + 1} } | c = [[Nth Derivative of Natural Logarithm by Reciprocal#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {-1}^{k + 1} k! \map {\dfrac \d {\d x} } {\dfrac {H_k} {x^{k + 1} } - \dfrac {\ln x} {x^{k + 1} } } | c = [[Linear Combination of Derivatives]] }} {{eqn | r = \paren {-1}^{k + 1} k! \paren {\dfrac {-\paren {k + 1} H_k} {x^{k + 2} } - \dfrac {1 - \paren {k + 1} \ln x} {x^{k + 2} } } | c = [[Derivative of Power]], [[Derivative of Logarithm over Power]] }} {{eqn | r = \paren {-1}^{k + 2} \paren {k + 1}! \paren {\dfrac {H_k} {x^{k + 2} } + \dfrac 1 {\paren {k + 1} x^{k + 2} } - \dfrac {\ln x} {x^{k + 2} } } | c = extracting $-\paren {k + 1}$ as a factor }} {{eqn | r = \paren {-1}^{k + 2} \paren {k + 1}! \paren {\paren {H_k + \dfrac 1 {\paren {k + 1} } } \dfrac 1 {x^{k + 2} } - \dfrac {\ln x} {x^{k + 2} } } | c = }} {{eqn | r = \paren {-1}^{k + 2} \paren {k + 1}! \paren {\dfrac {H_{k + 1} } {x^{k + 2} } - \dfrac {\ln x} {x^{k + 2} } } | c = }} {{eqn | r = \paren {-1}^{k + 2} \paren {k + 1}! \dfrac {H_{k + 1} - \ln x} {x^{k + 2} } | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{>0}: \dfrac {\d^n} {\d x^n} \dfrac {\ln x} x = \paren {-1}^{n + 1} n! \dfrac {H_n - \ln x} {x^{n + 1} }$ {{qed}} [[Category:Derivatives]] [[Category:Natural Logarithms]] cm4r7tcoruidcpnlxpy8ugbr0dga9x6	0
Let $b = 0$. Then: :$\displaystyle \int \frac {\mathrm d x} {a x^2 + b x + c} = \begin{cases} \dfrac 1 {\sqrt {a c} } \arctan \left({x \sqrt {\dfrac a c} }\right) + C & : a c > 0 \\ \dfrac 1 {2 \sqrt {-a c} } \ln \left\vert{\dfrac {a x - \sqrt {-a c} } {a x + \sqrt {-a c} } }\right\vert + C & : a c < 0 \\ \dfrac {-1} {a x} + C & : c = 0 \end{cases}$	0
{{ProofWanted|big job}}	0
{{begin-eqn}} {{eqn | l = \int \sec x \rd x | r = \ln \size {\map \tan {\frac \pi 4 + \frac x 2} } | c = [[Primitive of Secant Function/Tangent plus Angle Form|Primitive of $\sec x$: Tangent plus Angle Form]] }} {{eqn | ll= \leadsto | l = \int \sec a x \rd x | r = \frac 1 a \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \operatorname{csch}^2 x \ \mathrm d x | r = -\coth x + C | c = [[Primitive of Square of Hyperbolic Cosecant Function|Primitive of $\operatorname{csch}^2 x$]] }} {{eqn | ll= \implies | l = \int \operatorname{sech}^2 a x \ \mathrm d x | r = \frac 1 a \left({-\coth a x}\right) + C | c = [[Primitive of Function of Constant Multiple]] }} {{eqn | r = \frac {-\coth a x} a + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = \int \frac {x^2 \ \mathrm d x} {x^2 - a^2} | r = \int \frac {x^2 - a^2 + a^2} {x^2 - a^2} \ \mathrm d x | c = }} {{eqn | r = \int \frac {x^2 - a^2} {x^2 - a^2} \ \mathrm d x + \int \frac {a^2} {x^2 - a^2} \ \mathrm d x | c = }} {{eqn | r = \int \mathrm d x + a^2 \int \frac {\mathrm d x} {x^2 - a^2} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = x + a^2 \int \frac {\mathrm d x} {x^2 - a^2} + C | c = [[Primitive of Constant]] }} {{eqn | r = x + a^2 \left({-\frac 1 a \coth^{-1} {\frac x a} }\right) + C | c = [[Primitive of Reciprocal of x squared minus a squared/Inverse Hyperbolic Cotangent Form|Primitive of Reciprocal of $x^2 - a^2$: $\coth^{-1}$ form]] }} {{eqn | r = x - a \coth^{-1} {\frac x a} + C | c = simplifying }} {{end-eqn}} {{qed}} [[Category:Primitive of x squared over x squared minus a squared]] l98lpnwxoktlylc6pngqudn6q75wtyc	0
The [[Definition:Real Natural Logarithm|(real) natural logarithm function]] is [[Definition:Differentiable Real Function|differentiable]].	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \cosh^{-1} \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 {\sqrt {x^2 - a^2} } | c = [[Derivative of Inverse Hyperbolic Cosine of x over a|Derivative of $\cosh^{-1} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x^2 | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^3} 3 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^2 \cosh^{-1} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \cosh^{-1} \frac x a - \int \frac {x^3} 3 \left({\frac 1 {\sqrt {x^2 - a^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \cosh^{-1} \frac x a - \frac 1 3 \int \frac {x^3 \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = simplifying }} {{eqn | r = \frac {x^3} 3 \cosh^{-1} \frac x a - \frac 1 3 \left({\frac {\left({\sqrt {x^2 - a^2} }\right)^3} 3 + a^2 \sqrt {x^2 - a^2} }\right) + C | c = [[Primitive of x cubed over Root of x squared minus a squared|Primitive of $\dfrac {x^3} {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^3} 3 \cosh^{-1} \frac x a - \frac {\left({x^2 + 2 a^2}\right) \sqrt {x^2 - a^2} } 9 + C | c = simplifying }} {{end-eqn}} {{qed}} {{finish|Analysis to be done for the negative case. The confusion here is that $\cosh^{-1} \frac x a$ is ''defined'' as being positive.}}	0
{{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\sin a x} {\sinh b x} \rd x | r = \frac 1 i \int_0^\infty \frac {e^{-b x} \paren {e^{i a x} - e^{-i a x} } } {1 - e^{-2 b x} } \rd x | c = [[Sine Exponential Formulation]], {{Defof|Hyperbolic Sine}} }} {{eqn | r = \frac 1 i \int_0^\infty \paren {e^{\paren {i a - b} x} - e^{-\paren {i a + b} x} } \paren {\sum_{n \mathop = 0}^\infty e^{-2 n b x} } \rd x | c = [[Sum of Infinite Geometric Sequence]] }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \int_0^\infty \paren {e^{\paren {i a - \paren {2 n + 1} b} x} - e^{-\paren {i a + \paren {2 n + 1} b} x} } \rd x | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \paren {\intlimits {-\frac {e^{\paren {i a - \paren {2 n + 1} b} x} } {\paren {2 n + 1} b - i a} } 0 \infty - \intlimits {-\frac {e^{-\paren {i a + \paren {2 n + 1} b} x} } {\paren {2 n + 1} b + i a} } 0 \infty} | c = [[Primitive of Exponential of a x|Primitive of $e^{a x}$]] }} {{end-eqn}} We have, as $b, n > 0$: {{begin-eqn}} {{eqn | l = \size {\lim_{x \mathop \to \infty} e^{\paren {i a - \paren {2 n + 1} b} x} } | r = \lim_{x \mathop \to \infty} \size {e^{\paren {i a - \paren {2 n + 1} b} x} } | c = [[Modulus of Limit]] }} {{eqn | r = \lim_{x \mathop \to \infty} \size {e^{i a x} } \size {e^{-\paren {2 n + 1} b x} } | c = [[Exponential of Sum]] }} {{eqn | r = \lim_{x \mathop \to \infty} e^{-\paren {2 n + 1} b x} }} {{eqn | r = 0 | c = [[Exponential Tends to Zero and Infinity]] }} {{end-eqn}} We similarly have: {{begin-eqn}} {{eqn | l = \size {\lim_{x \mathop \to \infty} e^{-\paren {i a + \paren {2 n + 1} b} x} } | r = \lim_{x \mathop \to \infty} \size {e^{-\paren {i a + \paren {2 n + 1} b} x} } | c = [[Modulus of Limit]] }} {{eqn | r = \lim_{x \mathop \to \infty} \size {e^{-i a x} } \size {e^{-\paren {2 n + 1} b x} } | c = [[Exponential of Sum]] }} {{eqn | r = \lim_{x \mathop \to \infty} e^{-\paren {2 n + 1} b x} }} {{eqn | r = 0 | c = [[Exponential Tends to Zero and Infinity]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \frac 1 i \sum_{n \mathop = 0}^\infty \paren {\intlimits {-\frac {e^{\paren {i a - \paren {2 n + 1} b} x} } {\paren {2 n + 1} b - i a} } 0 \infty - \intlimits {-\frac {e^{-\paren {i a + \paren {2 n + 1} b} x} } {\paren {2 n + 1} b + i a} } 0 \infty} | r = \frac 1 i \sum_{n \mathop = 0}^\infty \paren {\frac 1 {\paren {2 n + 1} b - i a} - \frac 1 {\paren {2 n + 1} b + i a} } | c = [[Exponential of Zero]] }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {2 i a} {a^2 + \paren {2 n + 1}^2 b^2} | c = [[Difference of Two Squares]] }} {{eqn | r = \frac 2 b \sum_{n \mathop = 0}^\infty \frac {\paren {\frac a b} } {\paren {\frac a b}^2 + \paren {2 n + 1}^2} }} {{eqn | r = \frac 4 b \sum_{n \mathop = 0}^\infty \frac {\paren {\frac a {2 b} } } {4 \paren {\frac a {2 b} }^2 + \paren {2 n + 1}^2} }} {{end-eqn}} By [[Mittag-Leffler Expansion for Hyperbolic Tangent Function]], we have: :$\displaystyle \pi \map {\tanh} {\pi z} = 8 \sum_{n \mathop = 0}^\infty \frac z {4 z^2 + \paren {2 n + 1}^2}$ where $z$ is not a [[Definition:Half-Integer|half-integer]] multiple of $i$. Setting $z = \dfrac a {2 b}$ gives: :$\displaystyle \pi \map {\tanh} {\frac {a \pi} {2 b} } = 8 \sum_{n \mathop = 0}^\infty \frac {\paren {\frac a {2 b} } } {4 \paren {\frac a {2 b} }^2 + \paren {2 n + 1}^2}$ Therefore: {{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\sin a x} {\sinh b x} \rd x | r = \frac 4 b \sum_{n \mathop = 0}^\infty \frac {\paren {\frac a {2 b} } } {4 \paren {\frac a {2 b} }^2 + \paren {2 n + 1}^2} }} {{eqn | r = \frac \pi {2 b} \map {\tanh} {\frac {a \pi} {2 b} } }} {{end-eqn}} {{qed}}	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\d u} {\d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x^2 \rd x} {\paren {a x + b}^2} | r = \int \frac 1 a \paren {\frac {u - b} a}^2 \frac 1 {u^2} \rd u | c = [[Integration by Substitution]] }} {{eqn | r = \int \frac 1 {a^3} \paren {1 - \frac {2 b} u + \frac {b^2} {u^2} } \rd u | c = [[Square of Difference]], and simplification }} {{eqn | r = \frac 1 {a^3} \int \d u - \frac {2 b} {a^3} \int \frac {\d u} u + \frac {b^2} {a^3} \int \frac {\d u} {u^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac u {a^3} - \frac {2 b} {a^3} \int \frac {\d u} u + \frac {b^2} {a^3} \int \frac {\d u} {u^2} + C | c = [[Primitive of Constant]] }} {{eqn | r = \frac u {a^3} - \frac {2 b} {a^3} \ln \size u + \frac {b^2} {a^3} \int \frac {\d u} {u^2} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac u {a^3} - \frac {2 b} {a^3} \ln \size u + \frac {b^2} {a^3} \frac {-1} u + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {a x + b} {a^3} - \frac {b^2} {a^3 \paren {a x + b} } - \frac {2 b} {a^3} \ln \size {a x + b} + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
Let $f$ be a [[Definition:Differentiable Real Function|differentiable real function]] such that $f$ is [[Definition:Even Function|even]]. Then its [[Definition:Derivative|derivative]] $f'$ is an [[Definition:Odd Function|odd function]].	0
It is clear that for [[Definition:Step Function|step functions]] $s$ and $t$: {{handwaving|"It is clear that ..."}} {{MissingLinks}} :$\displaystyle \int_a^b \lambda \map s x + \mu \map t x \rd x = \lambda \int_a^b \map s x \rd x + \mu \int_a^b \map t x \rd x$ Under any partition, the [[Definition:Lower Sum|lower sums]] and [[Definition:Upper Sum|upper sums]] of $f$ and $g$ are [[Definition:Step Function|step functions]], so the above formula relates the lower and upper sums of $f$ and $g$ to the lower and upper sums of the linear combinations of $f$ and $g$. Because this identity is preserved for all possible partitions of $\closedint a b$, it is preserved for the supremum and infimum of all possible lower and upper sums, so the linear combinations of $f$ and $g$ are integrable. {{begin-eqn}} {{eqn | l = \int_a^b \paren {\lambda \map f t + \mu \map g t} \rd t | r = \sup \set {\sum_{\nu \mathop = 1}^n \map {m_\nu^{\paren {\lambda f + \mu g} } } {x_\nu - x_{\nu - 1} }: \forall \nu \in \closedint 1 n x_\nu > x_{\nu - 1} } | c = }} {{eqn | r = \lambda \sup \set {\sum_{\nu \mathop = 1}^n \map {m_\nu^{\paren f} } {x_\nu - x_{\nu - 1} }: \forall \nu \in \closedint 1 n x_\nu > x_{\nu - 1} } + \mu \sup \set {\sum_{\nu \mathop = 1}^n \map {m_\nu^{\paren g} } {x_\nu - x_{\nu - 1} }: \forall \nu \in \closedint 1 n x_\nu > x_{\nu - 1} } | c = }} {{eqn | r = \lambda \int_a^b \map f x \rd x + \mu \int_a^b \map g x \rd x | c = }} {{end-eqn}} {{qed}}	0
:$\dfrac {\map \d {\csch^{-1} } {\frac x a} } {\d x} = \dfrac {-a} {\size x \sqrt {a^2 + x^2} }$ where $x \ne 0$.	0
{{begin-eqn}} {{eqn | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \sin b x} a - \frac b a \int e^{a x} \cos b x \rd x | c = [[Primitive of Exponential of a x by Sine of b x/Lemma|Primitive of $e^{a x} \sin b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} \sin b x} a - \frac b a \paren {\frac {e^{a x} \cos b x} a + \frac b a \int e^{a x} \sin b x \rd x} | c = [[Primitive of Exponential of a x by Cosine of b x/Lemma|Primitive of $e^{a x} \cos b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} a \sin b x - e^{a x} b \cos b x} {a^2} - \frac {b^2} {a^2} \int e^{a x} \sin b x \rd x | c = simplifying }} {{eqn | ll= \leadsto | l = \paren {1 + \frac {b^2} {a^2} } \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \left({a \sin b x - b \cos b x}\right)} {a^2} | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {a^2 + b^2} {a^2} \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2} | c = common denominator }} {{eqn | ll= \leadsto | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2 + b^2} | c = multiplying by $\dfrac {a^2} {a^2 + b^2}$ }} {{end-eqn}} {{qed}}	0
Let $\delta J$ be a [[General Variation of Integral Functional/Dependent on N Functions|general variation of integral functional dependent on n functions]]. Suppose a following coordinate transformation is done: :$\set {x, \ldots y_i, \ldots, \ldots, y_i', \ldots, F} \to \set {x, \ldots, y_i, \ldots, \ldots p', \ldots, H}, i = \tuple {1, \ldots, n}$ Then, in [[Definition:Canonical Variable|canonical variables]]: :$\displaystyle \delta J = \int_{x_0}^{x_1} \sum_{i \mathop = 1}^n \paren {F_{y_i} - \dfrac {\d {p_i} } {\d x} } \map {h_i} x \rd x + \intlimits {\sum_{i \mathop = 1}^n p_i \delta y_i - H \delta x} {x \mathop = x_0} {x \mathop = x_1}$ where: {{begin-eqn}} {{eqn | l = \bigvalueat {\delta x} {x \mathop = x_j} | r = \delta x_j }} {{eqn | l = \bigvalueat {\delta y_i} {x \mathop = x_j} | r = \delta_i^j }} {{eqn | l = j | r = \tuple {0, 1} }} {{end-eqn}} {{explain|the meaning of $j {{=}} \tuple {0, 1}$ in this context, and hence the meaning of $x_j$ and $\delta x_j$, and also $\delta_i^j$, all of which are obscure}}	0
{{begin-eqn}} {{eqn | l = \frac {e^x - 1} x | r = \frac {e^x - e^0} x | c = [[Exponential of Zero]] }} {{eqn | o = \to | r = \intlimits {\dfrac \d {\d x} e^x} {x \mathop = 0} {} | c = {{Defof|Derivative of Real Function at Point}}, as $x \to 0$ }} {{eqn | r = \bigintlimits {e^x} {x \mathop = 0} {} | c = [[Derivative of Exponential Function]] }} {{eqn | r = 1 | c = [[Exponential of Zero]] }} {{end-eqn}} {{qed}}	0
Let $\map u x$ be a [[Definition:Differentiable Real Function|differentiable real function]] of $x$. Let $n$ be a [[Definition:Real Number|real number]] such that $n \ne -1$. Then: :$\map {\dfrac \d {\d x} } {\map u x^n} = n \map u x^{n - 1} \map {\dfrac \d {\d x} } {\map u x}$	0
By [[Derivative of Sine Integral Function]], we have: :$\displaystyle \frac \d {\d x} \paren {\map \Si x} = \frac {\sin x} x$ So: {{begin-eqn}} {{eqn | l = \int \map \Si x \rd x | r = \int 1 \times \map \Si x \rd x }} {{eqn | r = x \map \Si x - \int x \frac {\sin x} x \rd x | c = [[Integration by Parts]] }} {{eqn | r = x \map \Si x - \int \sin x \rd x }} {{eqn | r = x \map \Si x + \cos x + C | c = [[Primitive of Sine Function]] }} {{end-eqn}} {{qed}} [[Category:Primitives]] [[Category:Sine Integral Function]] r25o0orymphqj2y701esrh7ugpyc8d5	0
{{begin-eqn}} {{eqn | l = \frac {\d} {\d x} \paren {x^3 + a^3} | r = 3 x^2 | c = [[Derivative of Power]] }} {{eqn | ll= \implies | l = \int \frac {x^2 \rd x} {x^3 + a^3} | r = \frac 1 3 \ln \size {x^3 + a^3} + C | c = [[Primitive of Function under its Derivative]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = z | r = p + q \sin a x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d z} {\mathrm d x} | r = a q \sin a x | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] }} {{eqn | ll= \implies | l = \int \frac {\cos a x \ \mathrm d x} {\left({p + q \sin a x}\right)^n} | r = \int \frac {\mathrm d z} {a q z^n} | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {a q} \int z^{-n} \ \mathrm d z | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 {a q} \frac {z^{-n+1} } {-n + 1} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {a q \left({n - 1}\right) z^{n - 1} } + C | c = simplifying }} {{eqn | r = \frac {-1} {a q \left({n - 1}\right) \left({p + q \sin a x}\right)^{n - 1} } + C | c = substituting for $z$ }} {{end-eqn}} {{qed}}	0
With a view to expressing the problem in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = x }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = 1 | c = [[Power Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = \frac x {\left({\sqrt {x^2 + a^2} }\right)^3} }} {{eqn | ll= \implies | l = v | r = \frac {-1} {\sqrt {x^2 + a^2} } | c = [[Primitive of x over Root of x squared plus a squared cubed|Primitive of $\dfrac x {\left({\sqrt {x^2 + a^2} }\right)^3}$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x^2 \ \mathrm d x} {\left({\sqrt {x^2 + a^2} }\right)^3} | r = \int x \frac {x \ \mathrm d x} {\left({\sqrt {x^2 + a^2} }\right)^3} | c = }} {{eqn | r = x \frac {-1} {\sqrt {x^2 + a^2} } - \int \frac {-1} {\sqrt {x^2 + a^2} } 1 + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-x} {\sqrt {x^2 + a^2} } + \int \frac 1 {\sqrt {x^2 + a^2} } + C | c = simplifying }} {{eqn | r = \frac {-x} {\sqrt {x^2 + a^2} } + \ln \left({x + \sqrt {x^2 + a^2} }\right) + C | c = [[Primitive of Reciprocal of Root of x squared plus a squared/Logarithm Form|Primitive of $\dfrac 1 {\sqrt {x^2 + a^2} }$]] }} {{end-eqn}} {{qed}}	0
Let $f$ be a [[Definition:Continuous Real Function|continuous function]], [[Definition:Periodic Function|periodic]] in $\pi$. Then: :$\displaystyle \int_0^\infty \frac {\sin x} x f \left({x}\right) \rd x = \int_0^{\frac \pi 2} f \left({x}\right) \rd x$	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sin x} | r = \int \csc x \rd x | c = {{Defof|Real Cosecant Function}} }} {{eqn | r = \ln \size {\csc x - \cot x} + C | c = [[Primitive of Cosecant Function/Cosecant minus Cotangent Form|Primitive of $\csc x$: Cosecant minus Cotangent Form]] }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {\sin a x} | r = \frac 1 a \ln \size {\csc a x - \cot a x} + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
{{:Solution of Constant Coefficient Homogeneous LSOODE}} Let $p^2 < 4 q$. Then $(1)$ has the [[Definition:General Solution|general solution]]: :$y = e^{a x} \paren {C_1 \cos b x + C_2 \sin b x}$ where: :$m_1 = a + i b$ :$m_2 = a - i b$	0
:$\displaystyle \int \frac {\d x} {\paren {a x + b}^3} = -\frac 1 {2 a \paren {a x + b}^2} + C$	0
The [[Definition:First Order Ordinary Differential Equation|first order ODE]]: :$\dfrac {\d y} {\d x} = \map F {\dfrac {a x + b y + c} {d x + e y + f} }$ such that: :$ a e \ne b d$ can be solved by substituting: :$x := z - h$ :$y := w - k$ where: :$h = \dfrac {c e - b f} {a e - b d}$ :$k = \dfrac {a f - c d} {a e - b d}$ to obtain: :$\dfrac {\d w} {\d z} = \map F {\dfrac {a z + b w} {d z + e w} }$ which can be solved by the technique of [[Solution to Homogeneous Differential Equation]].	0
{{begin-eqn}} {{eqn | l = \map {\sinh^{-1} } {\frac x a} | r = \map \ln {\frac x a + \sqrt {\paren {\frac x a}^2 + a^2} } | c = {{Defof|Inverse Hyperbolic Sine|subdef = Real|index = 2}} }} {{eqn | r = \map \ln {\frac x a + \dfrac 1 a \sqrt {x^2 + a^2} } | c = }} {{eqn | r = \map \ln {\dfrac 1 a \paren {x + \sqrt {x^2 + a^2} } } | c = }} {{eqn | r = \map \ln {x + \sqrt {x^2 + a^2} } - \ln a | c = [[Difference of Logarithms]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d x} } {\map \ln {x + \sqrt {x^2 + a^2} } } | r = \map {\dfrac \d {\d x} } {\map {\sinh^{-1} } {\frac x a} + \ln a } | c = }} {{eqn | r = \dfrac 1 {\sqrt {x^2 + a^2} } + \map {\dfrac \d {\d x} } {\ln a} | c = [[Derivative of Inverse Hyperbolic Sine of x over a]] }} {{eqn | r = \dfrac 1 {\sqrt {x^2 + a^2} } + 0 | c = [[Derivative of Constant]] }} {{end-eqn}} We have that $\sqrt {x^2 + a^2} > x$ for all $x$. Thus: {{begin-eqn}} {{eqn | l = x + \sqrt {x^2 + a^2} | o = > | r = 0 | c = }} {{eqn | ll= \leadsto | l = \size {x + \sqrt {x^2 + a^2} } | r = x + \sqrt {x^2 + a^2} | c = {{Defof|Absolute Value}} }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d x} } {\size {x + \sqrt {x^2 + a^2} } } | r = \map {\dfrac \d {\d x} } {\map \ln {x + \sqrt {x^2 + a^2} } } | c = }} {{end-eqn}} and the result follows. {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {\int_0^t \map f u \map g {t - u} \rd u} | r = \int_{t \mathop = 0}^\infty e^{-s t} \paren {\int_{u \mathop = 0}^t \map f u \map g {t - u} \rd u} \rd t | c = {{Defof|Laplace Transform}} }} {{eqn | r = \int_{t \mathop = 0}^\infty \int_{u \mathop = 0}^t e^{-s t} \map f u \map g {t - u} \rd u \rd t | c = }} {{eqn | r = \lim_{M \mathop \to \infty} \int_{t \mathop = 0}^M \int_{u \mathop = 0}^t e^{-s t} \map f u \map g {t - u} \rd u \rd t | c = }} {{eqn | n = 1 | r = \lim_{M \mathop \to \infty} s_M | c = }} {{end-eqn}} where $s_M$ is defined to be: :$\displaystyle \int_{t \mathop = 0}^M \int_{u \mathop = 0}^t e^{-s t} \map f u \map g {t - u} \rd u \rd t$ The region in the plane over which $(1)$ is to be [[Definition:Integration|integrated]] is $\mathscr R_{t u}$ below: :[[File:ConvolutionTheorem1.png|400px]] Setting $t - u = v$, that is $t = u + v$, the shaded region above is transformed into the region $\mathscr R_{u v}$ the $u v$ plane: :[[File:ConvolutionTheorem2.png|400px]] Thus: {{begin-eqn}} {{eqn | l = s_M | r = \iint_{\mathscr R_{t u} } e^{-s t} \map f u \map g {t - u} \rd u \rd t | c = from $(1)$ above }} {{eqn | n = 2 | r = \iint_{\mathscr R_{u v} } e^{-s \paren {u + v} } \map f u \map g v \size {\dfrac {\map \partial {u, t} } {\map \partial {u, v} } } \rd u \rd v | c = }} {{end-eqn}} where $\dfrac {\map \partial {u, t} } {\map \partial {u, v} }$ is the [[Definition:Jacobian|Jacobian]] of the transformation: {{begin-eqn}} {{eqn | l = J | r = \dfrac {\map \partial {u, t} } {\map \partial {u, v} } | c = }} {{eqn | r = \begin{vmatrix} \dfrac {\partial u} {\partial u} & \dfrac {\partial u} {\partial v} \\ \dfrac {\partial t} {\partial u} & \dfrac {\partial t} {\partial v} \end{vmatrix} | c = }} {{eqn | r = \begin{vmatrix} 1 & 0 \\ 1 & 1 \end{vmatrix} | c = }} {{end-eqn}} Thus the {{RHS}} of $(2)$ is: :$(3): \quad \displaystyle s_M = \int_{v \mathop = 0}^M \int_{u \mathop = 0}^{M - v} e^{-s \paren {u + v} } \map f u \map g v \rd u \rd v$ Let $\map K {u, v}$ be the [[Definition:Function|function]] defined as: :$\map K {u, v} = \begin{cases} e^{-s \paren {u + v} } \map f u \map g v & : u + v \le M \\ 0 & : u + v > M \end{cases}$ This [[Definition:Function|function]] is defined over the [[Definition:Square (Geometry)|square]] region in the diagram below: :[[File:ConvolutionTheorem3.png|400px]] but is [[Definition:Zero (Number)|zero]] over the lighter shaded portion. Now we can write $(3)$ as: {{begin-eqn}} {{eqn | l = s_M | r = \int_{v \mathop = 0}^M \int_{u \mathop = 0}^M \map K {u, v} \rd u \rd v | c = }} {{eqn | ll= \leadsto | l = \lim_{M \mathop \to \infty} s_M | r = \int_{v \mathop = 0}^\infty \int_{u \mathop = 0}^\infty \map K {u, v} \rd u \rd v | c = }} {{eqn | r = \int_{v \mathop = 0}^\infty \int_{u \mathop = 0}^\infty e^{-s \paren {u + v} } \map f u \map g v \rd u \rd v | c = }} {{eqn | r = \paren {\int_{v \mathop = 0}^\infty e^{-s u} \map f u \rd u} \paren {\int_{v \mathop = 0}^\infty e^{-s v} \map g u \rd v} | c = }} {{eqn | r = \map F s \map G s | c = }} {{end-eqn}} Hence the result. {{qed}}	0
:$\displaystyle \int \map \Si x \rd x = x \map \Si x + \cos x + C$	0
:$\displaystyle \int F \left({\ln x}\right) \rd x = \int F \left({u}\right) e^u \rd u$ where $u = \ln x$.	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sqrt {x^2 + a^2} } | r = \sinh^{-1} {\frac x a} + C | c = [[Primitive of Reciprocal of Root of x squared plus a squared/Inverse Hyperbolic Sine Form|Primitive of $\dfrac 1 {\sqrt {x^2 - a^2} }$ in $\sinh^{-1}$ form]] }} {{eqn | r = \map \ln {x + \sqrt {x^2 + a^2} } - \ln a + C | c = [[Inverse Hyperbolic Sine of x over a in Logarithm Form|$\sinh^{-1} {\dfrac x a}$ in Logarithm Form]] }} {{eqn | r = \map \ln {x + \sqrt {x^2 + a^2} } + C | c = subsuming $-\ln a$ into [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = z | r = 2 a x + b | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d z} {\mathrm d x} | r = 2 a | c = [[Derivative of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {\left({a x^2 + b x + c}\right)^2} | r = \int \left({\frac {4 a} {\left({2 a x + b}\right)^2 + 4 a c - b^2} }\right)^2 \ \mathrm d x | c = [[Completing the Square]] }} {{eqn | r = \int \frac {\left({4 a}\right)^2} {\left({z^2 + 4 a c - b^2}\right)^2} \frac {\mathrm d z} {2 a} | c = [[Integration by Substitution]] }} {{eqn | r = 8 a \int \frac {\mathrm d z} {\left({z^2 + 4 a c - b^2}\right)^2} | c = simplifying }} {{end-eqn}} Let $u = z^2$. Let: {{begin-eqn}} {{eqn | l = u | r = z^2 | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d z} | r = 2 z | c = [[Derivative of Power]] }} {{eqn | ll= \implies | l = 8 a \int \frac {\mathrm d z} {\left({z^2 + 4 a c - b^2}\right)^2} | r = 8 a \int \frac {\mathrm d u} {2 \sqrt u \left({u + 4 a c - b^2}\right)^2} | c = [[Integration by Substitution]] }} {{eqn | r = 4 a \int \frac {\mathrm d u} {\sqrt u \left({u + 4 a c - b^2}\right)^2} | c = simplifying }} {{end-eqn}} Recall the result [[Primitive of Reciprocal of Power of p x + q by Root of a x + b|Primitive of $\dfrac 1 {\left({p x + q}\right)^n \sqrt{a x + b} }$]]: :$\displaystyle \int \frac {\mathrm d x} {\left({p x + q}\right)^n \sqrt{a x + b} } = \frac {\sqrt{a x + b} } {\left({n - 1}\right) \left({a q - b p}\right) \left({p x + q}\right)^{n-1} } + \frac {\left({2 n - 3}\right) a} {2 \left({n - 1}\right) \left({a q - b p}\right)} \int \frac {\mathrm d x} {\left({p x + q}\right)^{n-1} \sqrt{a x + b} }$ Let: {{begin-eqn}} {{eqn | l = x | o = =: | r = u }} {{eqn | l = a | o = =: | r = 1 }} {{eqn | l = b | o = =: | r = 0 }} {{eqn | l = p | o = =: | r = 1 }} {{eqn | l = q | o = =: | r = 4 a c - b^2 }} {{eqn | l = n | o = =: | r = 2 }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = 4 a \int \frac {\mathrm d u} {\sqrt u \left({u + 4 a c - b^2}\right)^2} | r = 4 a \left({\frac {\sqrt u} {\left({4 a c - b^2}\right) \left({u + 4 a c - b^2}\right)} + \frac 1 {2 \left({4 a c - b^2}\right)} \int \frac {\mathrm d u} {\left({u + 4 a c - b^2}\right) \sqrt u} }\right) | c = }} {{eqn | r = 4 a \left({\frac z {\left({4 a c - b^2}\right) \left({z^2 + 4 a c - b^2}\right)} + \frac 1 {2 \left({4 a c - b^2}\right)} \int \frac {2 z \ \mathrm d z} {\left({z^2 + 4 a c - b^2}\right) z} }\right) | c = substituting for $z$ }} {{eqn | r = 4 a \frac {2 a x + b} {\left({4 a c - b^2}\right) \left({\left({2 a x + b}\right)^2 + 4 a c - b^2}\right)} + \frac {2 a} {4 a c - b^2} \int \frac {4 a \ \mathrm d x} {\left({\left({2 a x + b}\right)^2 + 4 a c - b^2}\right)} | c = substituting for $x$ }} {{eqn | r = \frac {2 a x + b} {\left({4 a c - b^2}\right) \left({a x^2 + b x + c}\right)} + \frac {2 a} {4 a c - b^2} \int \frac {\mathrm d x} {\left({a x^2 + b x + c}\right)} | c = [[Completing the Square]] }} {{end-eqn}} {{qed}}	0
Let $\left[{a \,.\,.\, b}\right] \subseteq \R$ be a [[Definition:Closed Real Interval|closed real interval]]. Let $h: \left[{a \,.\,.\, b}\right] \to \R$ be a [[Definition:Continuous Real Function on Closed Interval|continuous]] [[Definition:Real Function|real function]] such that: :$\forall x \in \left[{a \,.\,.\, b}\right]: h \left({x}\right) \ge 0$ Let: :$\displaystyle \int_a^b h \left({x}\right) \rd x = 0$ Then: :$\forall x \in \left[{a \,.\,.\, b}\right]: h \left({x}\right) = 0$	0
We note that $(1)$ is in the form: :$\map M {x, y} \rd x + \map N {x, y} \rd y = 0$ but that $(1)$ is not [[Definition:Exact Differential Equation|exact]]. So, let: :$\map M {x, y} = x y - 1$ :$\map N {x, y} = x^2 - x y = x \paren {x - y}$ Let: :$\map P {x, y} = \dfrac {\partial M} {\partial y} - \dfrac {\partial N} {\partial x}$ Thus: {{begin-eqn}} {{eqn | l = \map P {x, y} | r = x - \paren {2 x - y} | c = }} {{eqn | r = -\paren {x - y} | c = }} {{end-eqn}} It can be observed that: {{begin-eqn}} {{eqn | l = \frac {\map P {x, y} } {\map N {x, y} } | r = \frac {-\paren {x - y} } {x \paren {x - y} } | c = }} {{eqn | r = -\frac 1 x | c = }} {{end-eqn}} Thus $\dfrac {\map P {x, y} } {\map N {x, y}}$ is a function of $x$ only. So [[Integrating Factor for First Order ODE/Function of One Variable|Integrating Factor for First Order ODE: Function of One Variable]] can be used: :$\map \mu y = e^{\int \map g x \rd x}$ Hence: {{begin-eqn}} {{eqn | l = \int \map g x \rd x | r = \int -\paren {1 / x} \rd x | c = }} {{eqn | r = -\ln x | c = }} {{eqn | r = \map \ln {x^{-1} } | c = }} {{eqn | ll= \leadsto | l = e^{\int \map g x \rd x} | r = \frac 1 x | c = }} {{end-eqn}} Thus an [[Definition:Integrating Factor|integrating factor]] for $(1)$ has been found: :$\mu = \dfrac 1 x$ which yields, when multiplying it throughout $(1)$: :$\paren {y - \dfrac 1 x} \rd x + \paren {x - y} \rd y = 0$ which is now [[Definition:Exact Differential Equation|exact]]. By [[First Order ODE/(y - 1 over x) dx + (x - y) dy = 0|First Order ODE: $\paren {y - \dfrac 1 x} \rd x + \paren {x - y} \rd y = 0$]], its solution is: :$x y - \ln x - \dfrac {y^2} 2 + C$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {p^2 \sin^2 a x - q^2 \cos^2 a x} | r = \int \frac {\sec^2 a x} {p^2 \tan^2 a x - q^2} \rd x | c = multiplying by $\dfrac {\sec^2 a x} {\sec^2 a x}$ }} {{eqn | r = \frac 1 a \int \frac 1 {p^2 t^2 - q^2} \rd t | c = [[Integration by Substitution|substituting]] $t = \tan a x$ }} {{eqn | r = \frac 1 {a p^2} \int \frac 1 {t^2 - \paren {\frac q p}^2} \rd t }} {{eqn | r = \frac 1 {2 a \frac {p^2} p q} \ln \size {\frac {t - \frac q p} {t + \frac q p} } + C | c = [[Primitive of Reciprocal of x squared minus a squared/Logarithm Form|Primitive of $\dfrac 1 {x^2 - a^2}$: Logarithm Form]] }} {{eqn | r = \frac 1 {2 a p q} \ln \size {\frac {p t - q} {p t + q} } + C }} {{eqn | r = \frac 1 {2 a p q} \ln \size {\frac {p \tan a x - q} {p \tan a x + q} } + C | c = substituting back for $t$ }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\d x} {\sin^m a x \cos^n a x} = \frac {-1} {a \paren {n - 1} \sin^{m - 1} a x \cos^{n - 1} a x} + \frac {m + n - 2} {m - 1} \int \frac {\d x} {\sin^{m - 2} a x \cos^n a x} + C$	0
From [[Integral to Infinity of Function over Argument]]: :$\displaystyle \int_0^\infty {\dfrac {\map f x} x} = \int_0^{\to \infty} \map F u \rd u$ for a [[Definition:Real Function|real function]] $f$ and its [[Definition:Laplace Transform|Laplace transform]] $\laptrans f = F$, provided they exist. Let $\map f x := \sin x$. Then from [[Laplace Transform of Sine]]: :$\laptrans {\map f x} = \dfrac 1 {s^2 + 1}$ Hence: {{begin-eqn}} {{eqn | l = \int_0^\infty \frac {\sin x} x \rd x | r = \int_0^{\to \infty} \dfrac {\d u} {u^2 + 1} | c = }} {{eqn | r = \bigintlimits {\arctan u} 0 \infty | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | r = \dfrac \pi 2 | c = }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {x^3 \left({a x + b}\right)} = \frac {2 a x - b} {2 b^2 x^2} + \frac {a^2} {b^3} \ln \left\vert{\frac x {a x + b} }\right\vert + C$	0
First: {{begin-eqn}} {{eqn | o = | r = \int \frac {\d x} {x^2 \paren {a x^2 + b x + c}^2} | c = }} {{eqn | r = \int \frac {c \rd x} {c x^2 \paren {a x^2 + b x + c}^2} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $c$ }} {{eqn | r = \frac 1 c \int \frac {c \rd x} {x^2 \paren {a x^2 + b x + c}^2} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 c \int \frac {a x^2 + b x + c - a x^2 - b x} {x^2 \paren {a x^2 + b x + c}^2} \rd x | c = adding and subtracting $a x^2 + b x$ }} {{eqn | r = \frac 1 c \int \frac {\paren {a x^2 + b x + c} \rd x} {x^2 \paren {a x^2 + b x + c}^2} - \frac a c \int \frac {x^2 \rd x} {x^2 \paren {a x^2 + b x + c}^2} | c = [[Linear Combination of Integrals]] }} {{eqn | o = | ro= - | r = \frac b c \int \frac {x \rd x} {x^2 \paren {a x^2 + b x + c}^2} | c = }} {{eqn | n = 1 | r = \frac 1 c \int \frac {\d x} {x^2 \paren {a x^2 + b x + c} } - \frac a c \int \frac {\d x} {\paren {a x^2 + b x + c}^2} | c = simplification }} {{eqn | o = | ro= - | r = \frac b c \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} | c = }} {{end-eqn}} Next, with a view to obtaining an expression in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \frac 1 {\paren {a x^2 + b x + c} } | c = }} {{eqn | r = \paren {a x^2 + b x + c}^{-1} | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = -\paren {2 a x + b} \paren {a x^2 + b x + c}^{-2} | c = [[Chain Rule for Derivatives]] and [[Derivative of Power]] }} {{eqn | r = \frac {-\paren {2 a x + b} } {\paren {a x^2 + b x + c}^2} | c = simplifying }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^2} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {-1} x | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^2 \paren {a x^2 + b x + c} } | r = \int \frac 1 {\paren {a x^2 + b x + c} } \frac 1 {x^2} \rd x | c = }} {{eqn | r = \frac 1 {\paren {a x^2 + b x + c} } \frac {-1} x - \int \frac {-1} x \frac {-\paren {2 a x + b} } {\paren {a x^2 + b x + c}^2} \rd x | c = [[Integration by Parts]] }} {{eqn | n = 2 | r = \frac {-1} {x \paren {a x^2 + b x + c} } - 2 a \int \frac {\d x} {\paren {a x^2 + b x + c}^2} | c = simplification }} {{eqn | o = | ro= - | r = b \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} | c = [[Linear Combination of Integrals]] }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | o = | r = \int \frac {\d x} {x^2 \paren {a x^2 + b x + c}^2} | c = }} {{eqn | r = \frac 1 c \int \frac {\d x} {x^2 \paren {a x^2 + b x + c} } - \frac a c \int \frac {\d x} {\paren {a x^2 + b x + c}^2} - \frac b c \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} | c = from $(1)$ }} {{eqn | r = \frac 1 c \paren {\frac {-1} {x \paren {a x^2 + b x + c} } - 2 a \int \frac {\d x} {\paren {a x^2 + b x + c}^2} - b \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} } | c = from $(2)$ }} {{eqn | o = | ro= - | r = \frac a c \int \frac {\d x} {\paren {a x^2 + b x + c}^2} - \frac b c \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} | c = }} {{eqn | r = \frac {-1} {c x \paren {a x^2 + b x + c} } - \frac {3 a} c \int \frac {\d x} {\paren {a x^2 + b x + c}^2} - \frac {2 b} c \int \frac {\d x} {x \paren {a x^2 + b x + c}^2} | c = gathering terms }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cos \left({\frac \pi 2 - \theta}\right) | r = \cos \left({\theta - \frac \pi 2}\right) | c = [[Cosine Function is Even]] }} {{eqn | r = \sin \left({\theta - \frac \pi 2 + \frac \pi 2}\right) | c = [[Sine of Angle plus Right Angle]] }} {{eqn | r = \sin \theta }} {{end-eqn}} {{qed}}	0
{{ProofWanted}} [[Category:Entire Functions]] epim9a757ti02kdsc835c98mkdrn4my	0
Let $a$ be an [[Definition:Integer|integer]] chosen at random. The [[Definition:Probability|probability]] that $a$ is [[Definition:Square-Free Integer|square-free]] is given by: :$\map \Pr {\neg \exists b \in \Z: b^2 \divides a} = \dfrac 1 {\map \zeta 2} = \dfrac 6 {\pi^2}$ where $\zeta$ denotes the [[Definition:Riemann Zeta Function|zeta function]]. The [[Definition:Decimal Expansion|decimal expansion]] of $\dfrac 1 {\map \zeta 2}$ starts: :$\dfrac 1 {\map \zeta 2} = 0 \cdotp 60792 \, 71018 \, 54026 \, 6 \ldots$ {{OEIS|A059956}}	0
We use the technique of [[Definition:Formation of Ordinary Differential Equation by Elimination|formation of ordinary differential equation by elimination]]. [[Definition:Differentiation|Differentiating]] $(1)$ {{WRT|Differentiation}} $x$ gives: :$\dfrac {\d y} {\d x} = 1 - C e^{-x}$ Eliminating $C$: {{begin-eqn}} {{eqn | l = C | o = = | r = \frac {y - x} {e^{-x} } | c = }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = 1 - \frac {y - x} {e^{-x} } e^{-x} | c = }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} + y | r = x + 1 | c = }} {{end-eqn}} Thus from [[Orthogonal Trajectories of One-Parameter Family of Curves]], the [[Definition:Orthogonal Trajectories|family of orthogonal trajectories]] is given by: :$-\dfrac {\d y} {\d x} = - x = 1 - y$ The [[Definition:Integrating Factor|integrating factor]] is $e^y$, giving: :$\displaystyle e^y x = \int y e^y - e^y \rd y$ Using [[Primitive of x by Exponential of a x|Primitive of $x e^{a x}$]]: :$\displaystyle \int y e^y \rd y = y e^y - e^y$ Thus we get: :$e^y x = y e^y - e^y - e^y + C$ which gives us: :$x = y - 2 + C e^{-y}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = y | r = \sech^{-1} x | c = }} {{eqn | ll= \leadsto | l = \sech y | r = x | c = {{Defof|Inverse Hyperbolic Secant}} }} {{eqn | ll= \leadsto | l = \map \sech {\pm \, y} | r = x | c = [[Hyperbolic Secant Function is Even]] }} {{eqn | ll= \leadsto | l = \map \sec {\pm \, i y} | r = x | c = [[Hyperbolic Secant in terms of Secant]] }} {{eqn | ll= \leadsto | l = \pm \, i y | r = \sec^{-1} x | c = {{Defof|Inverse Secant}} }} {{eqn | ll= \leadsto | l = y | r = \pm \, i \sec^{-1} x | c = multiplying both sides by $\pm \, i$ }} {{end-eqn}} {{qed}}	0
Let $\alpha \in \Q$ be a [[Definition:Rational Number|rational number]]. From [[Real Number is Closed in Real Number Line]], $\set \alpha$ is a [[Definition:Closed Set (Topology)|closed set]] of $\R$. From [[Rational Numbers are Countably Infinite]], $\displaystyle \bigcup_{\alpha \mathop \in \Q} \set \alpha$ is a [[Definition:Countable Union|countable union]]. Thus $\Q = \displaystyle \bigcup_{\alpha \mathop \in \Q} \set \alpha$ is a [[Definition:Countable Union|countable union]] of [[Definition:Closed Set (Topology)|closed sets]] of $\R$. Hence the result by definition of [[Definition:F-Sigma Set|$F_\sigma$ set]]. {{qed}}	0
Let $n \ge 1$ be a [[Definition:Positive Integer|positive integer]]. Let $\map {\Phi_n} x$ denote the $n$th [[Definition:Cyclotomic Polynomial|cyclotomic polynomial]]. Let $a \in \Z$ be an [[Definition:Integer|integer]] such that $\map {\Phi_n} a \ne 0$. Let $p$ be a [[Definition:Prime Divisor|prime divisor]] of $\map {\Phi_n} a$. Then $p \equiv 1 \pmod n$ or $p \divides n$.	0
By [[Cantor's Theorem]] there is no [[Definition:Surjection|surjection]]: : $\N \twoheadrightarrow \mathcal P \left({\N}\right)$ Additionally, we have [[Power Set of Natural Numbers is not Countable]]. Therefore, if we can show that $\mathcal P \left({\N}\right)$ [[Definition:Injection|injects]] into $\R$ then there is no [[Definition:Injection|injection]] $\R \hookrightarrow \N$ and $\R$ is uncountable. To prove the theorem we construct an [[Definition:Injection|injection]] $f: \mathcal P \left({\N}\right) \to \R$. For a subset $S \subseteq \N$, let $\chi_S$ be the [[Definition:Characteristic Function of Set|characteristic function]] of $S$, and let $d_i = \chi_S \left({i}\right)$ for all $i \in \N$. By the definition of [[Definition:Characteristic Function of Set|characteristic function]], $\left\langle{d_i}\right\rangle_{i \in \N}$ is an [[Definition:Infinite Sequence|infinite sequence]] of $1$s and $0$s. There are two cases: $\left\langle{d_i}\right\rangle_{i \in \N}$ terminates in an [[Definition:Infinite Sequence|infinite sequence]] of $1$s, or it does not. Suppose $\left\langle{d_i}\right\rangle_{i \in \N}$ does ''not'' terminate in an [[Definition:Infinite Sequence|infinite sequence]] of $1$s. Then $f \left({S}\right)$ is the [[Definition:Binary Notation|binary]] [[Definition:Basis Expansion|expansion]] of the following number in $\left[{0 \,.\,.\, 1}\right)$: :$0.d_1 d_2 d_3 d_4 \ldots$ Otherwise $\left\langle{d_i}\right\rangle_{i \in \N}$ ''does'' terminate in an [[Definition:Infinite Sequence|infinite sequence]] of $1$s. Then $f \left({S}\right)$ is the [[Definition:Integer|integer]] expressed in [[Definition:Binary Notation|binary]] as: :$1 d_1 d_2 d_3 \ldots d_k$ where $d_k$ is the last member of the sequence not equal to $1$. In either case, every [[Definition:Subset|subset]] of $\N$, that is, element of $\mathcal P \left({\N}\right)$, is mapped to an element of $\R$. That $f$ is an [[Definition:Injection|injection]] follows from the uniqueness statement of [[Existence of Base-N Representation]]. {{qed}}	0
:$\cos 18^\circ = \cos \dfrac \pi {10} = \dfrac {\sqrt{10 + 2 \sqrt 5}} 4$ where $\cos$ denotes the [[Definition:Cosine Function|cosine function]].	0
Each element $x$ of the [[Definition:Set|set]] of [[Definition:Real Number|real numbers]] $\R$ has an [[Definition:Inverse Element|inverse element]] $-x$ under the operation of [[Definition:Real Addition|real number addition]]: :$\forall x \in \R: \exists -x \in \R: x + \paren {-x} = 0 = \paren {-x} + x$	0
For $x \in \openint {-\pi} \pi$: :$\displaystyle \size x = \frac \pi 2 - \frac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\map \cos {2 n - 1} x} {\paren {2 n - 1}^2}$	0
The [[Definition:Canonical Form of Rational Number|canonical form]] of a [[Definition:Rational Number|rational number]] is [[Definition:Unique|unique]].	0
The number $1 \, 857 \, 437 \, 604$ is a [[Definition:Square Number|square number]] whose [[Definition:Sigma Function|$\sigma$ value]] is a [[Definition:Cube Number|cube]].	0
:$\displaystyle \int \frac {x \rd x} {a^2 - x^2} = -\frac 1 2 \, \map \ln {a^2 - x^2} + C$ for $x^2 < a^2$.	0
From the definition of the [[Definition:Beta Function|Beta function]]: {{begin-eqn}} {{eqn | l = \map \Beta {z_1, z_2} | r = \frac {\map \Gamma {z_1} \, \map \Gamma {z_2} } {\map \Gamma {z_1 + z_2} } | c = }} {{eqn | r = \int_0^1 u^{z_1 - 1} \paren {1 - u}^{z_2 - 1} \rd u | c = [[Equivalence of Definitions of Beta Function]] }} {{end-eqn}} Letting $z_1 = z_2 = z$ gives: {{begin-eqn}} {{eqn | l = \frac {\map \Gamma z \, \map \Gamma z} {\map \Gamma {2 z} } | r = \int_0^1 u^{z - 1} \paren {1 - u}^{z - 1} \rd u | c = }} {{eqn | r = \frac 1 2 \int_{-1}^1 \paren {\frac {1 + x} 2 }^{z - 1} \paren {\frac {1 - x} 2}^{z - 1} \rd x | c = [[Integration by Substitution|Substitute]] $u = \dfrac {1 + x} 2$ }} {{eqn | r = \frac 1 {2^{2 z - 1} } \int_{-1}^1 \paren {1 - x^2}^{z - 1} \rd x | c = }} {{eqn | ll= \leadsto | n = 1 | l = 2^{2 z - 1} \map \Gamma z \, \map \Gamma z | r = 2 \map \Gamma {2 z} \int_0^1 \paren {1 - x^2}^{z - 1} \rd x | c = by [[Definite Integral of Even Function]] }} {{end-eqn}} Now substituting $u = x^2$ into the [[Definition:Beta Function|Beta function]]: :$\displaystyle \map \Beta {z_1, z_2} = \int_0^1 x^{2 z_1 - 2} \paren {1 - x^2}^{z_2 - 1} 2 x \rd x$ Letting $z_1 = \dfrac 1 2$ and $z_2 = z$ gives: :$(2): \quad \displaystyle \map \Beta {\frac 1 2, z} = 2 \int_0^1 \paren {1 - x^2}^{z - 1} \rd x$ Combining results $(1)$ and $(2)$: {{begin-eqn}} {{eqn | l = 2^{2 z - 1} \map \Gamma z \, \map \Gamma z | r = \map \Gamma {2 z} \map \Beta {\frac 1 2, z} | c = }} {{eqn | r = \map \Gamma {2 z} \frac {\map \Gamma {\frac 1 2} \, \map \Gamma z} {\map \Gamma {\frac 1 2 + z} } | c = }} {{eqn | ll= \leadsto | l = \map \Gamma z \, \map \Gamma {z + \dfrac 1 2} | r = 2^{1 - 2 z} \map \Gamma {\frac 1 2} \, \map \Gamma {2 z} }} {{end-eqn}} From [[Gamma Function of One Half]]: :$\map \Gamma {\dfrac 1 2} = \sqrt \pi$ It follows that: :$\map \Gamma z \, \map \Gamma {z + \dfrac 1 2} = 2^{1 - 2 z} \sqrt \pi \, \map \Gamma {2 z}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = s \ln x | r = \ln {x^s} | c = [[Logarithm of Power]] }} {{eqn | o = \le | r = x^s - 1 | c = [[Upper Bound of Natural Logarithm]] }} {{eqn | o = \le | r = x^s | c = }} {{end-eqn}} The result follows by dividing both sides by $s$. {{qed}}	0
Let $x, y \in n \Z$. Then $\exists p, q \in \Z: x = n p, y = n q$. So $x + y = n p + n q = n \paren {p + q}$ where $p + q \in \Z$. Thus $x + y \in n \Z$ and so $\struct {n \Z, +}$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed}}	0
Let $\map {B_\epsilon} l$ be any [[Definition:Open Ball of Metric Space|open $\epsilon$-ball]] of $l$. Let $A = \set {n: x_n \notin \map {B_\epsilon} l}$. By assumption $A$ is [[Definition:Finite|finite]]. From [[Finite Non-Empty Subset of Totally Ordered Set has Smallest and Greatest Elements]], any [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] of $\N$ has a [[Definition:Maximum Element|maximum]]. Let $N$ be the [[Definition:Maximum Element|maximum]] of $A$. Then for every $n > N$, $x_n$ must be in the [[Definition:Open Ball|open $\epsilon$-ball]] $\map {B_\epsilon} l$.	0
Let $k$ be the order of $a$ modulo $p$. By [[Element to Power of Multiple of Order is Identity]], $k \divides p - 1$. If $k = n$, the result follows. Let $k < n$. Then by [[Product of Cyclotomic Polynomials]], there exists $d \divides k$ such that $p \divides \map {\Phi_d} a$. Consequently, $a$ is a [[Definition:Double Root (Polynomial)|double root]] of $\Phi_d \Phi_n$ modulo $p$. Again by [[Product of Cyclotomic Polynomials]], $a$ is a double root of $x^n - 1$ modulo $p$. Thus, by [[Double Root of Polynomial is Root of Derivative]], $a$ is a root of the derivative of $x^n - 1$ modulo $p$, which is the constant polynomial $n$. Thus $n \equiv 0 \pmod p$, for a nonzero constant polynomial has no roots. {{qed}}	0
Let $S \subseteq \R$. Let $\sequence {f_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Real Function|real functions]] $S \to \R$. Let $\sequence {f_n}$ be [[Definition:Uniform Convergence|uniformly convergent]] on $S$. Then $\sequence {f_n}$ is [[Definition:Uniform Cauchy Criterion|uniformly Cauchy]] on $S$.	0
From [[Bézout's Lemma]] we have: Let $a, b \in \Z$ such that $a$ and $b$ are not both [[Definition:Zero (Number)|zero]]. Let $\gcd \set {a, b}$ be the [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] of $a$ and $b$. Then: :$\exists x, y \in \Z: a x + b y = \gcd \set {a, b}$ Furthermore, $\gcd \set {a, b}$ is the [[Definition:Smallest Element|smallest]] [[Definition:Positive Integer|positive]] [[Definition:Integer Combination|integer combination]] of $a$ and $b$. In this instance, $a, b \in \Z_{>0}$ and are therefore both non-[[Definition:Zero (Number)|zero]]. The result then follows by definition of [[Definition:Greatest Common Divisor of Integers|greatest common divisor]]: $d = \gcd \set {a, b}$ {{iff}}: : $(1): \quad d \divides a \land d \divides b$ : $(2): \quad c \divides a \land c \divides b \implies c \divides d$ {{qed}}	0
First: {{begin-eqn}} {{eqn | o = | r = \int \frac {\d x} {x^m \paren {a x^2 + b x + c}^n} | c = }} {{eqn | r = \int \frac {c \rd x} {c x^m \paren {a x^2 + b x + c}^n} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $c$ }} {{eqn | r = \frac 1 c \int \frac {c \rd x} {x^m \paren {a x^2 + b x + c}^n} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 c \int \frac {a x^2 + b x + c - a x^2 - b x} {x^m \paren {a x^2 + b x + c}^n} \rd x | c = adding and subtracting $a x^2 + b x$ }} {{eqn | r = \frac 1 c \int \frac {\paren {a x^2 + b x + c} \rd x} {x^m \paren {a x^2 + b x + c}^n} - \frac a c \int \frac {x^2 \rd x} {x^m \paren {a x^2 + b x + c}^n} | c = [[Linear Combination of Integrals]] }} {{eqn | o = | ro= - | r = \frac b c \int \frac {x \rd x} {x^m \paren {a x^2 + b x + c}^n} | c = }} {{eqn | n = 1 | r = \frac 1 c \int \frac {\d x} {x^m \paren {a x^2 + b x + c}^{n - 1} } - \frac a c \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} | c = simplification }} {{eqn | o = | ro= - | r = \frac b c \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = }} {{end-eqn}} Next, with a view to obtaining an expression in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \frac 1 {\paren {a x^2 + b x + c}^{n - 1} } | c = }} {{eqn | r = \paren {a x^2 + b x + c}^{-\paren {n - 1} } | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = -\paren {n - 1} \paren {a x^2 + b x + c}^{-\paren {n - 1} - 1} \paren {2 a x + b} | c = [[Chain Rule for Derivatives]] and [[Derivative of Power]] }} {{eqn | r = \frac {-\paren {n - 1} \paren {2 a x + b} } {\paren {a x^2 + b x + c}^n} | c = simplifying }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^m} | c = }} {{eqn | r = x^-m | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^{-m + 1} } {-m + 1} | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {\paren {m - 1} x^{m - 1} } | c = simplifying }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^m \paren {a x^2 + b x + c}^{n - 1} } | r = \int \frac 1 {\paren {a x^2 + b x + c}^{n - 1} } \frac 1 {x^m} \rd x | c = Note the index is $n - 1$ }} {{eqn | r = \frac 1 {\paren {a x^2 + b x + c}^{n - 1} } \frac {-1} {\paren {m - 1} x^{m - 1} } | c = [[Integration by Parts]] }} {{eqn | o = | ro= - | r = \int \frac {-1} {\paren {m - 1} x^{m - 1} } \frac {-\paren {n - 1} \paren {2 a x + b} } {\paren {a x^2 + b x + c}^n} \rd x | c = }} {{eqn | n = 2 | r = \frac {-1} {\paren {m - 1} x^{m - 1} \paren {a x^2 + b x + c}^{n - 1} } | c = simplification }} {{eqn | o = | ro= - | r = \frac {2 a \paren {n - 1} } {m - 1} \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} | c = and [[Linear Combination of Integrals]] }} {{eqn | o = | ro= - | r = \frac {b \paren {n - 1} } {m - 1} \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | o = | r = \int \frac {\d x} {x^m \paren {a x^2 + b x + c}^n} | c = }} {{eqn | r = \frac 1 c \int \frac {\d x} {x^m \paren {a x^2 + b x + c}^{n - 1} } - \frac a c \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac b c \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = $(1)$ }} {{eqn | r = \frac 1 c \paren {\frac {-1} {\paren {m - 1} x^{m - 1} \paren {a x^2 + b x + c}^{n - 1} } - \frac {2 a \paren {n - 1} } {m - 1} \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac {b \paren {n - 1} } {m - 1} \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} } | c = $(2)$ }} {{eqn | o = | ro= - | r = \frac a c \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac b c \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = }} {{eqn | r = \frac {-1} {c \paren {m - 1} x^{m - 1} \paren {a x^2 + b x + c}^{n - 1} } - \frac {2 a \paren {n - 1} } {c \paren {m - 1} } \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac {b \paren {n - 1} } {c \paren {m - 1} } \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = }} {{eqn | o = | ro= - | r = \frac a c \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac b c \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} | c = }} {{eqn | r = \frac {-1} {\paren {m - 1} c x^{m - 1} \paren {a x^2 + b x + c}^{n - 1} } - \frac {\paren {m + 2 n - 3} a} {\paren {m - 1} c} \int \frac {\d x} {x^{m - 2} \paren {a x^2 + b x + c}^n} - \frac {\paren {m - n + 2} b} {\paren {m - 1} c} \int \frac {\d x} {x^{m - 1} \paren {a x^2 + b x + c}^n} }} {{end-eqn}} {{qed}}	0
Let $S \subset \Z$ be a [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subset]] of the [[Definition:Integer|set of integers]]. Let $S$ be [[Definition:Bounded Below Subset of Real Numbers|bounded below]] in the [[Definition:Real Number|set of real numbers]] $\R$. Then $S$ has a [[Definition:Smallest Element|smallest element]], and it is equal to the [[Definition:Infimum of Subset of Real Numbers|infimum]] $\sup S$.	0
Let $c \divides a \land c \divides b$. From [[Common Divisor Divides Integer Combination]]: :$\forall p, q \in \Z: c \divides \paren {p a + q b}$ Putting $p = 1$ and $q = -1$: :$c \divides \paren {a - b}$ {{qed}}	0
{{:Definition:Open Set/Complex Analysis/Definition 1}}	0
{{begin-eqn}} {{eqn | r = \int \frac {\sqrt {a x^2 + b x + c} } {x^2} \ \mathrm d x | o = }} {{eqn | r = \int \frac {a x^2 + b x + c} {x^2 \sqrt {a x^2 + b x + c} } \ \mathrm d x | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $\sqrt {a x^2 + b x + c}$ }} {{eqn | r = a \int \frac {x^2 \ \mathrm d x} {x^2 \sqrt {a x^2 + b x + c} } + b \int \frac {x \ \mathrm d x} {x^2 \sqrt {a x^2 + b x + c} } + c \int \frac {\mathrm d x} {x^2 \sqrt {a x^2 + b x + c} } | c = [[Linear Combination of Integrals]] }} {{eqn | r = a \int \frac {\mathrm d x} {\sqrt {a x^2 + b x + c} } + b \int \frac {\mathrm d x} {x \sqrt {a x^2 + b x + c} } + c \int \frac {\mathrm d x} {x^2 \sqrt {a x^2 + b x + c} } | c = simplifying }} {{eqn | r = c \left({-\frac {\sqrt {a x^2 + b x + c} } {c x} - \frac b {2 c} \frac {\mathrm d x} {x \sqrt {a x^2 + b x + c} } }\right) | c = [[Primitive of Reciprocal of x squared by Root of a x squared plus b x plus c|Primitive of $\dfrac 1 {x^2 \sqrt {a x^2 + b x + c} }$]] }} {{eqn | o = | ro= + | r = a \int \frac {\mathrm d x} {\sqrt {a x^2 + b x + c} } + b \int \frac {\mathrm d x} {x \sqrt {a x^2 + b x + c} } | c = }} {{eqn | r = \frac {-\sqrt {a x^2 + b x + c} } x + a \int \frac {\mathrm d x} {\sqrt {a x^2 + b x + c} } + \frac b 2 \int \frac {\mathrm d x} {x \sqrt {a x^2 + b x + c} } | c = simplifying }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \coth^{-1} \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {-a} {x^2 - a^2} | c = [[Derivative of Inverse Hyperbolic Cotangent of x over a|Derivative of $\coth^{-1} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^2} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {-1} x | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\coth^{-1} \dfrac x a \rd x} {x^2} | r = \paren {\coth^{-1} \frac x a} \paren {\frac {-1} x} - \int \paren {\frac {-1} x} \paren {\frac {-a} {x^2 - a^2} } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - a \int \frac {\d x} {x \paren {x^2 - a^2} } + C | c = simplifying }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - a \paren {\frac 1 {2 a^2} \map \ln {\frac {x^2 - a^2} {x^2} } } + C | c = [[Primitive of Reciprocal of x by x squared minus a squared|Primitive of $\dfrac 1 {x \paren {x^2 - a^2} }$]] }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x - \frac 1 {2 a} \map \ln {\frac {x^2 - a^2} {x^2} } + C | c = simplifying }} {{eqn | r = \frac {-\coth^{-1} \dfrac x a} x + \frac 1 {2 a} \map \ln {\frac {x^2} {x^2 - a^2} } + C | c = [[Logarithm of Reciprocal]] }} {{end-eqn}} {{qed}}	0
Let $X$ be a [[Definition:Topological Space|topological space]]. Let $M$ be a [[Definition:Metric Space|metric space]]. Let $(f_n)$ be a [[Definition:Sequence|sequence]] of [[Definition:Mapping|mappings]] $f_n : X\to M$. Let $f_n$ [[Definition:Locally Uniform Convergence|converge locally uniformly]] to $f:X\to M$. Then $f_n$ [[Definition:Compact Convergence|converges compactly]] to $f$.	0
Let $1$ be temporarily considered to be a [[Definition:Prime Number|prime number]]. Under that consideration, the largest [[Definition:Right-Truncatable Prime|right-truncatable]] [[Definition:Prime Number|prime numbers]] are: :$1 \, 979 \, 339 \, 333$ :$1 \, 979 \, 339 \, 339$	0
Denote the [[Definition:Open Ball|open ball]] of $0$ with [[Definition:Radius of Open Ball|radius]] $r \in \R_{>0}$ as $B_r \left({0}\right)$. Let $z \in D$. By the [[Alternative Differentiability Condition]], it follows that there exists $r \in \R_{>0}$ such that for all $h \in B_r \left({0}\right) \setminus \left\{ {0}\right\}$: :$f\left({z + h}\right) = f \left({z}\right) + h \left({f' \left({z}\right) + \epsilon_f \left({h}\right) }\right)$ :$g\left({z + h}\right) = g \left({z}\right) + h \left({g' \left({z}\right) + \epsilon_g \left({h}\right) }\right)$ where $\epsilon_f, \epsilon_g: B_r \left({0}\right) \setminus \left\{ {0}\right\} \to \C$ are [[Definition:Continuous Complex Function|continuous functions]] that [[Definition:Convergent Function|converge]] to $0$ as $h$ tends to $0$. Then: {{begin-eqn}} {{eqn | l = \left({f + g}\right) \left({z + h}\right) | r = f \left({z}\right) + h \left({f' \left({z}\right) + \epsilon_f \left({h}\right) }\right) + g \left({z}\right) + h \left({g' \left({z}\right) + \epsilon_g \left({h}\right) }\right) }} {{eqn | r = \left({f + g}\right) \left({z}\right) + h \left({f' \left({z}\right) + g' \left({z}\right) + \left({ \epsilon_f + \epsilon_g }\right) \left({h}\right) }\right) }} {{end-eqn}} From [[Sum Rule for Continuous Functions]], it follows that $\epsilon_f + \epsilon_g$ is a [[Definition:Continuous Complex Function|continuous function]]. From [[Sum Rule for Limits of Functions]], it follows that $\displaystyle \lim_{h \to 0} \left({ \epsilon_f + \epsilon_g }\right) \left({h}\right) = 0$. Then the [[Alternative Differentiability Condition]] shows that: :$\left({f + g}\right)' \left({z}\right) = f' \left({z}\right) + g' \left({z}\right)$ {{qed}}	0
We have the [[Weierstrass Factorization Theorem|Weierstrass products]]: :$\displaystyle \map \sin {\pi z} = \pi z \prod_{n \mathop \ne 0} \paren {1 - \frac z n} \map \exp {\frac z n}$ From the [[Definition:Weierstrass Form of Gamma Function|Weierstrass form of the Gamma function]]: :$\displaystyle \frac 1 {\map \Gamma z} = z e^{\gamma z} \prod_{n \mathop = 1}^\infty \paren {1 + \frac z n} \map \exp {-\frac z n}$ from which: {{begin-eqn}} {{eqn | l = \dfrac 1 {-z \, \map \Gamma z \, \map \Gamma {-z} } | r = \frac {-z^2 \, \map \exp {\gamma z} \, \map \exp {-\gamma z} } {-z} \prod_{n \mathop = 1}^\infty \paren {1 + \frac z n} \paren {1 - \frac z n} \, \map \exp {\frac z n} \, \map \exp {-\frac z n} }} {{eqn | r = z \prod_{n \mathop = 1}^\infty \paren {1 - \frac {z^2} {n^2} } }} {{eqn | r = \dfrac {\map \sin {\pi z} } \pi | c = [[Euler Formula for Sine Function]] }} {{end-eqn}} whence: {{begin-eqn}} {{eqn | l = \map \Gamma z \, \map \Gamma {1 - z} | r = -z \, \map \Gamma z \, \map \Gamma {-z} | c = [[Gamma Difference Equation]] }} {{eqn | r = \frac \pi {\map \sin {\pi z} } }} {{end-eqn}} {{qed}} {{Namedfor|Leonhard Paul Euler|cat = Euler}}	0
This proof assumes the truth of the [[Derivative of Cosine Function]]: From [[Cosine of Zero is One]]: :$\cos 0 = 1$ From [[Derivative of Cosine Function]]: :$D_x \left({\cos x}\right) = - \sin x$ and by [[Derivative of Constant]]: :$D_x \left({-1}\right) = 0$ So by [[Sum Rule for Derivatives]]: :$D_x \left({\cos x - 1}\right) = - \sin x$ By [[Sine of Zero is Zero]]: :$\sin 0 = 0$ From [[Derivative of Identity Function]]: :$D_x \left({x}\right) = 1$ Thus [[L'Hôpital's Rule]] applies and so: : $\displaystyle \lim_{x \mathop \to 0} \frac {\cos x - 1} x = \lim_{x \mathop \to 0} \frac {-\sin x} 1 = \frac {-0} 1 = 0$ {{qed}}	0
Note that a [[Definition:Sylow p-Subgroup|Sylow $2$-subgroup]] of $G$ is of [[Definition:Order of Group|order $4$]]. From [[Sylow Theorems/Examples/Sylow 3-Subgroups in Group of Order 12|Sylow $3$-Subgroups in Group of Order 12]], there are either $1$ or $4$ [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]]. Suppose there is [[Definition:Unique|exactly $1$]] [[Definition:Sylow p-Subgroup|Sylow $3$-subgroup]] $P$. Then from [[Sylow p-Subgroup is Unique iff Normal|Sylow $p$-Subgroup is Unique iff Normal]], $P$ is [[Definition:Normal Subgroup|normal]]. {{qed|lemma}} Suppose there are $4$ [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]] $P_1$, $P_2$, $P_3$ and $P_4$. Each [[Definition:Set Intersection|intersection]] $P_i \cap P_j$ for $i, j \in \set {1, 2, 3, 4}, i \ne j$ is the [[Definition:Trivial Subgroup|trivial subgroup]] of $G$: :$P_i \cap P_j = \set e$ Thus $G$ contains: :The [[Definition:Identity Element|identity element]] $e$ :$8$ [[Definition:Element|elements]] of [[Definition:Order of Group Element|order]] $3$, of which $2$ each are in $P_1$, $P_2$, $P_3$ and $P_4$ :$3$ more [[Definition:Element|elements]], which (along with $e$) must form the [[Definition:Sylow p-Subgroup|Sylow $2$-subgroup]] of [[Definition:Order of Group|order $4$]]. This [[Definition:Sylow p-Subgroup|Sylow $2$-subgroup]] $Q$ must be [[Definition:Unique|unique]]. Hence by [[Sylow p-Subgroup is Unique iff Normal|Sylow $p$-Subgroup is Unique iff Normal]], $Q$ is [[Definition:Normal Subgroup|normal]]. {{qed|lemma}} Hence the result. {{qed}}	0
:$\displaystyle \int x \arccot \frac x a \rd x = \frac {x^2 + a^2} 2 \arccot \frac x a + \frac {a x} 2 + C$	0
First, we will prove that: :$\forall x \in \R: \map f x \in \mathscr D$ Let $x \in \R$. It is to be proved that $\map f x$ is a [[Definition:Proper Subset|proper subset]] of $\Q$ such that: :$(1): \quad \forall z \in \map f x: \forall y \in \Q: y < z \implies y \in \map f x$ :$(2): \quad \forall z \in \map f x: \exists y \in \map f x: z < y$ We have that: :$x \notin \map f x$ Therefore by definition $\map f x$ is a [[Definition:Proper Subset|proper subset]] of $\Q$. Ad. $(1)$: Let $z \in \map f x, y \in \Q$ such that: :$y < z$ By definition of $\map f x$: :$z < x$ Then: :$y < x$ Thus by definition of $\map f x$: :$y \in \map f x$ Ad. $(2)$: Let $z \in \map f x$. By definition of $\map f x$: :$z < x$ By [[Between two Real Numbers exists Rational Number]]: :$\exists r \in \Q: z < r < x$ Then by definition of $\map f x$: :$r \in \map f x$ Thus: :$\exists r \in \map f x: z < r$ By definition of [[Definition:Bijection|bijection]] it suffices to prove that $f$ is an [[Definition:Injection|injection]] and a [[Definition:Surjection|surjection]]. We will show by definition that $f: \R \to \mathscr D$ is an [[Definition:Injection|injection]]. Let $x_1, x_2 \in \R$ such that :$\map f {x_1} = \map f {x_2}$ {{AimForCont}} $x_1 \ne x_2$. {{WLOG}} suppose $x_1 < x_2$. By [[Between two Real Numbers exists Rational Number]]: :$\exists r \in \Q: x_1 < r < x_2$ Then by definition of $\map f x$: :$r \notin \map f {x_1}$ and :$r \in \map f {x_2}$ This contradicts $\map f {x_1} = \map f {x_2}$. We will prove by definition that $f: \R \to \mathscr D$ is a [[Definition:Surjection|surjection]]. Let $L \in \mathscr D$. By definition of [[Definition:Dedekind Cut|Dedekind cut]]: :$L$ is a [[Definition:Proper Subset|proper subset]] of $\Q$. By definition of [[Definition:Proper Subset|proper subset]]: :$\exists r \in \Q: r \notin L$ By definition of [[Definition:Dedekind Cut|Dedekind cut]]: :$(3): \quad \forall x \in L: \forall y \in \Q: y < x \implies y \in L$ Then :$\forall x \in L: r \not < x \land r \ne x$ Hence :$\forall x \in L: r > x$ Then $L$ is [[Definition:Bounded Above Subset of Real Numbers|bounded above]] by definition. By definition of [[Definition:Supremum of Set|supremum]]: :$\map \sup L \le r$ Hence: :$\map \sup L \in \R$ By definition of [[Definition:Supremum of Set|supremum]]: :$\map \sup L$ is an [[Definition:Upper Bound of Subset of Real Numbers|upper bound]] of $L$. Then by definition of [[Definition:Upper Bound of Subset of Real Numbers|upper bound]]: :$\forall x \in L: x < \map \sup L$ We will prove that: :$\forall x \in \Q: x < \map \sup L \implies x \in L$ Let $x \in \Q$ such that: :$x < \map \sup L$ {{AimForCont}} $x \notin L$. By $(3)$: :$\forall x \in L: r \ge x$ By definition: :$r$ is an [[Definition:Upper Bound of Subset of Real Numbers|upper bound]] of $L$. By definition of [[Definition:Supremum of Set|supremum]]: :$r \ge \map \sup L$ This contradicts $x < \map \sup L$. Thus: :$L = \map f {\map \sup L}$ {{qed}} [[Category:Real Numbers]] [[Category:Dedekind Cuts]] rf1bi2w5yjvrm65r9a92qd7eo5dskgu	0
From [[Power Series Expansion for Cotangent Function]]: {{:Power Series Expansion for Cotangent Function}} {{begin-eqn}} {{eqn | l = x \cot ax | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} x^{2 n} } {\paren {2 n}!} | c = }} {{eqn | ll= \leadsto | l = \int x \cot a x \rd x | r = \int \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} x^{2 n} } {\paren {2 n}!} \rd x | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\int \frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} x^{2 n} } {\paren {2 n}!} \rd x} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} } {\paren {2 n}!} \times \int x^{2 n} \rd x} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} } {\paren {2 n}!} \times \frac {x^{2 n + 1} } {2 n + 1} + C} | c = [[Primitive of Power]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n 2^{2 n} B_{2 n} a^{2 n - 1} x^{2 n + 1} } {\paren {2 n + 1}!} + C | c = }} {{eqn | ll= \leadsto | l = \int x \cot a x \rd x | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n 2^{2 n} B_{2 n} \paren {a x}^{2 n + 1} } {\paren {2 n + 1}!} + C | c = }} {{end-eqn}} {{qed}}	0
Let $\left({X, d}\right)$ and $\left({Y, e}\right)$ be [[Definition:Metric Space|metric spaces]]. Let $f: X \to Y$ be a [[Definition:Mapping|mapping]]. Let $x \in X$. Then $f$ is [[Definition:Continuous at Point of Metric Space|continuous at $x$]] {{iff}} $f$ is [[Definition:Sequential Continuity|sequentially continuous at $x$]].	0
From [[Fourier Series for Odd Function over Symmetric Range]], $\map S x$ is the [[Definition:Fourier Series|Fourier series]] of an [[Definition:Odd Function|odd]] [[Definition:Real Function|real function]] over the [[Definition:Real Interval|interval]] $\openint 0 \lambda$. We have that $\map S x \sim \map f x$ over $\openint 0 \lambda$. Thus over $\openint {-\lambda} 0$ it follows that: :$\map S x = -\map f {-x}$ {{qed}}	0
We use the fact that the [[Definition:Real Exponential Function|exponential function]] is the [[Definition:Inverse Mapping|inverse]] of the [[Definition:Natural Logarithm|natural logarithm function]]: :$y = e^x \iff x = \ln y$ {{begin-eqn}} {{eqn | l = \dfrac {\d x} {\d y} | r = \dfrac 1 y | c = [[Derivative of Natural Logarithm Function]] }} {{eqn | ll= \leadsto | l = \dfrac {\d y} {\d x} | r = \dfrac 1 {1 / y} | c = [[Derivative of Inverse Function]] }} {{eqn | r = y }} {{eqn | r = e^x }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\csc a x} | r = \int \sin a x \rd x | c = [[Cosecant is Reciprocal of Sine]] }} {{eqn | r = \frac {-\cos a x} a + C | c = [[Primitive of Sine of a x|Primitive of $\sin a x$]] }} {{end-eqn}} {{qed}}	0
From [[Absolute Value Function on Integers induces Equivalence Relation]], $\RR$ is an [[Definition:Equivalence Relation|equivalence relation]]. Let: :$\size {x_1} = \size {x_2}$ :$\size {y_1} = \size {y_2}$ Then by definition of [[Definition:Absolute Value|absolute value]]: {{begin-eqn}} {{eqn | l = \size {x_1 y_1} | r = \size {x_1} \size {y_1} | c = }} {{eqn | r = \size {x_2} \size {y_2} | c = }} {{eqn | r = \size {x_2 y_2} | c = }} {{end-eqn}} That is: :$\paren {x_1 y_1, x_2 y_2} \in \RR$ That is, $\RR$ is a [[Definition:Congruence Relation|congruence relation]] for [[Definition:Integer Multiplication|integer multiplication]]. {{qed}}	0
By definition of [[Definition:Homeomorphism (Topological Spaces)|homeomorphism]], $f$ and $g$ are both [[Definition:Bijection|bijections]]. From [[Composite of Bijections is Bijection]] it follows that $g \circ f$ is also a [[Definition:Bijection|bijection]]. Similarly, also by definition of [[Definition:Homeomorphism (Topological Spaces)|homeomorphism]], $f$ and $g$ are both [[Definition:Everywhere Continuous Mapping (Topology)|continuous mappings]]. From [[Composite of Continuous Mappings is Continuous]] it follows that $g \circ f$ is also a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]. Hence the result, from definition of [[Definition:Homeomorphism (Topological Spaces)|homeomorphism]]. {{qed}} [[Category:Homeomorphisms]] 6wti8nffp909u0kceehmavp8m0shjq1	0
Let $u = p^2 + q^2$ and $v = q^2 - p^2$. Then: {{begin-eqn}} {{eqn | n = 1 | l = u + v | r = 2 q^2 }} {{eqn | n = 2 | l = u - v | r = 2 p ^2 }} {{end-eqn}} Also: {{begin-eqn}} {{eqn | l = u^2 - v^2 | r = \paren {u + v} \paren {u - v} }} {{eqn | l = u^2 - v^2 | r = \paren {2 q^2} \paren {2 p^2} | c = from $\paren 1$ and $\paren 2$ }} {{eqn | n = 3 | l = u^2 - v^2 | r = 4 p^2 q^2 }} {{end-eqn}} Therefore: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p^2 \sin^2 a x + q^2 \cos^2 a x} | r = \int \frac {\d x} {p^2 \paren {\frac {1 - \cos 2 a x} 2} + q^2 \cos^2 a x} | c = [[Square of Sine]] }} {{eqn | r = \int \frac {\d x} {p^2 \paren {\frac {1 - \cos 2 a x} 2} + q^2 \paren {\frac {1 + \cos 2 a x} 2} } | c = [[Square of Cosine]] }} {{eqn | r = \int \frac {2 \d x} {p^2 - p^2 \cos 2 a x + q^2 + q^2 \cos 2 a x} | c = }} {{eqn | r = \int \frac {2 \d x} {p^2 + q^2 + \paren {q^2 - p^2} \cos 2 a x} | c = }} {{eqn | r = 2 \paren { \frac 2 {\paren {2 a} \sqrt{u^2 - v^2} } } \map \arctan {\sqrt{ \frac {u - v} {u + v} } \tan \frac {\paren {2 a} x} 2} + C | c = [[Primitive of Reciprocal of p plus q by Cosine of a x|Primitive of $\dfrac 1 {p + q \cos a x}$]] }} {{eqn | r = 2 \paren {\frac 1 {a \sqrt{4 p^2 q^2} } } \map \arctan {\sqrt{ \frac {2 p^2} {2 q^2} } \tan a x} + C | c = from $\paren 3$ }} {{eqn | r = \paren {\frac 2 {2 a p q} } \map \arctan {\sqrt{ \frac {p^2} {q^2} } \tan a x} + C | c = }} {{eqn | r = \paren {\frac 1 {a p q} } \map \arctan {\frac p q \tan a x} + C | c = }} {{eqn | r = \frac 1 {a p q} \map \arctan {\frac {p \tan a x} q} + C | c = }} {{end-eqn}} {{qed}}	0
{{ProofWanted|bored with this}}	0
:$\displaystyle \int \frac {\d x} {a x + b} = \frac 1 a \ln \size {a x + b} + C$	0
Let $x \in \R$ be a [[Definition:Real Number|real number]] such that: :$1 \le x < 10$ Then: :$0 \le \log_{10} x \le 1$ where $\log_{10}$ denotes the [[Definition:Common Logarithm|common logarithm]] function.	0
We have that $\sin x$ has a [[Power Series Expansion for Sine Function|power series representation]]: :$\sin x = x - \dfrac {x^3} {3!} + \dfrac {x^5} {5!} - \dfrac {x^7} {7!} + \cdots$ The [[Zeroes of Sine and Cosine|roots of sine]] are the numbers $k \pi$, where $k$ is any [[Definition:Integer|integer]]. From the [[Polynomial Factor Theorem/Corollary|Polynomial Factor Theorem]], the following ''might'' be true: :$\displaystyle \sin x = A x \prod \paren {1 - \frac x {k \pi} }$ where the product is taken over all $n \in \Z \setminus \set 0$, and $A$ is some constant. The intuition is as follows. {{begin-eqn}} {{eqn | l = \sin x | r = \ldots \paren {1 - \frac x {2 \pi} } \paren {1 - \frac x \pi} A x \paren {1 + \frac x \pi} \paren {1 + \frac x {2 \pi} } \cdots }} {{eqn | r = A x \paren {1 - \frac {x^2} {\pi^2} } \paren {1 - \frac {x^2} {2^2 \pi^2} } \paren {1 - \frac {x^2} {3^2 \pi^2} } \cdots }} {{eqn | ll= \leadsto | l = \frac {\sin x} x | r = A \paren {1 - \frac {x^2} {\pi^2} } \paren {1 - \frac {x^2} {2^2 \pi^2} } \paren {1 - \frac {x^2} {3^2 \pi^2} } \cdots | c = for $x \ne 0$ }} {{end-eqn}} That $\dfrac {\sin x} x \to 1$ as $x \to 0$ is a [[Limit of Sine of X over X|well known limit]]. Letting $x$ tend to $0$ in the above equation implies that $A = 1$. We now formalize the above claims. {{proof wanted}}	0
Let $U$ be a [[Definition:Simply Connected|simply connected]] [[Definition:Open Set (Complex Analysis)|open]] [[Definition:Subset|subset]] of the [[Definition:Complex Number|complex plane]] $\C$. Let $a_1, a_2, \dots, a_n$ be [[Definition:Finite Set|finitely many]] points of $U$. Let $f: U \to \C$ be [[Definition:Analytic Function|analytic]] in $U \setminus \set {a_1, a_2, \dots, a_n}$. Let $L = \partial U$ be oriented counterclockwise. Then: :$\displaystyle \oint_L \map f z \rd z = 2 \pi i \sum_{k \mathop = 1}^n \Res f {a_k}$	0
Let $a \in \Z$ be an [[Definition:Integer|integer]] such that $a \ne 0$. Then: :$\gcd \left\{{a, 0}\right\} = \left\lvert{a}\right\rvert$ where $\gcd$ denotes [[Definition:Greatest Common Divisor of Integers|greatest common divisor (GCD)]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sqrt {x^2 + a^2} } | r = \map \ln {x + \sqrt {x^2 + a^2} } + C | c = [[Primitive of Reciprocal of Root of x squared plus a squared/Logarithm Form|Primitive of $\dfrac 1 {\sqrt {x^2 + a^2} }$ in Logarithm Form]] }} {{eqn | l = \int \frac {\d x} {-\sqrt {x^2 + a^2} } | r = -\map \ln {x + \sqrt {x^2 + a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \ln \size {x - \sqrt {x^2 + a^2} } + C + \map \ln {a^2} | c = [[Negative of Logarithm of x plus Root x squared plus a squared/Corollary|Negative of Logarithm of x plus Root x squared plus a squared: Corollary]] }} {{eqn | r = \ln \size {x - \sqrt {x^2 + a^2} } + C | c = subsuming $\map \ln {a^2}$ into the constant }} {{end-eqn}} {{qed}}	0
Let $\sequence {x_n}$ be [[Definition:Decreasing Real Sequence|decreasing]] and [[Definition:Bounded Below Real Sequence|bounded below]]. Then $\sequence {x_n}$ [[Definition:Convergent Real Sequence|converges]] to its [[Definition:Infimum of Sequence|infimum]].	0
{{begin-eqn}} {{eqn | l = \int_0^\pi x \, \map \ln {\sin x} \rd x | r = \int_0^\pi \paren {\pi - x} \, \map \ln {\map \sin {\pi - x} } \rd x | c = [[Integral between Limits is Independent of Direction]] }} {{eqn | r = \pi \int_0^\pi \map \ln {\sin x} - \int_0^\pi x \, \map \ln {\sin x} \rd x | c = [[Sine of Supplementary Angle]], [[Linear Combination of Definite Integrals]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = 2 \int_0^\pi x \, \map \ln {\sin x} \rd x | r = \pi \int_0^\pi \map \ln {\sin x} \rd x }} {{eqn | r = 2 \pi \int_0^{\pi/2} \map \ln {\sin x} \rd x | c = [[Definite Integral from 0 to Half Pi of Logarithm of Sine x/Lemma|Definite Integral from $0$ to $\dfrac \pi 2$ of $\map \ln {\sin x}$: Lemma]] }} {{eqn | r = -\pi^2 \ln 2 | c = [[Definite Integral from 0 to Half Pi of Logarithm of Sine x|Definite Integral from $0$ to $\dfrac \pi 2$ of $\map \ln {\sin x}$]] }} {{end-eqn}} giving: :$\displaystyle \int_0^\pi x \, \map \ln {\sin x} \rd x = -\frac {\pi^2} 2 \ln 2$ {{qed}}	0
Let $x, y \in \R$ be [[Definition:Real Number|real numbers]]. Let $a^x$ be defined as [[Definition:Power to Real Number|$a$ to the power of $x$]]. Then: :$\paren {a^x}^y = a^{x y}$	0
[[File:Primitive-of-1-over-x-squared-minus-a-squared.png|thumb|right|600px|$\color {blue} {\dfrac 1 {x^2 - a^2} } \qquad \color {green} {-\dfrac 1 a \tanh^{-1} \dfrac x a} \qquad \color {red} {-\dfrac 1 a \coth^{-1} \dfrac x a}$]] Let $a \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real]] [[Definition:Constant|constant]].	0
For $\struct {\Z_{\ge 0}, +, \times}$ to be a [[Definition:Ring (Abstract Algebra)|ring]], it is [[Definition:Necessary Condition|necessary]] for the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\Z_{\ge 0}, +}$ to form a [[Definition:Group|group]]. But from the [[Natural Numbers under Addition do not form Group/Corollary|corollary to Natural Numbers under Addition do not form Group]]: :$\struct {\Z_{\ge 0}, +}$ is not a [[Definition:Group|group]]. {{qed}}	0
The [[Definition:Reciprocal|reciprocal]] [[Definition:Real Function|function]]: :$\operatorname{recip}: \R \setminus \set 0 \to \R$, $x \mapsto \dfrac 1 x$ is [[Definition:Strictly Decreasing Real Function|strictly decreasing]]: :on the [[Definition:Open Real Interval|open interval]] $\openint 0 \to$ :on the [[Definition:Open Real Interval|open interval]] $\openint \gets 0$	0
{{begin-eqn}} {{eqn | l = \int x \sec a x \rd x | r = \frac 1 {a^2} \int \theta \sec \theta \rd \theta | c = [[Integration by Substitution|Substitution of $a x \to \theta$]] }} {{eqn | r = \frac 1 {a^2} \int \theta \sum_{n \mathop = 0}^\infty \frac{ \paren {-1}^n E_{2 n} \theta^{2 n} } {\paren {2 n}!} \rd \theta | c = [[Power Series Expansion for Secant Function]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n E_{2 n} } {\paren {2 n}!} \int \theta^{2 n + 1} \rd \theta | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n E_{2 n} \paren {a x}^{2 n + 2} } {\paren {2 n + 2} \paren {2 n}!} + C | c = [[Integration by Substitution|Substituting back $\theta \to ax$]] }} {{end-eqn}} {{qed}}	0
Let $z = a + i b$ be a [[Definition:Complex Number|complex number]], where $a, b \in \R$. Then the [[Definition:Field Norm of Complex Number|field norm]] of $z$ is the [[Definition:Field Norm|field norm]] with respect to the [[Definition:Field Extension|field extension]] $\C / \R$.	0
:$\displaystyle \int \cosh^{-1} \frac x a \ \mathrm d x = \begin{cases} x \cosh^{-1} \dfrac x a - \sqrt {x^2 - a^2} + C & : \cosh^{-1} \dfrac x a > 0 \\ x \cosh^{-1} \dfrac x a + \sqrt {x^2 - a^2} + C & : \cosh^{-1} \dfrac x a < 0 \end{cases}$	0
Let $n \in \N$ be a [[Definition:Natural Number|natural number]] such that $n \ge 2$. Let $\N_n$ be defined as: :$\N_n := \set {1, 2, \dotsc, n}$ Let $A_n = \set {a_1, a_2, \dotsc, a_n} \subseteq \Z$ be a [[Definition:Set|set]] of $n$ [[Definition:Integer|integers]]. From [[Expression for Integers as Powers of Same Primes]], let: :$\displaystyle \forall i \in \N_n: a_i = \prod_{p_j \mathop \in T} {p_j}^{e_{i j} }$ where: :$T = \set {p_j: j \in \N_r}$ such that: :$\forall j \in \N_{r - 1}: p_j < p_{j - 1}$ :$\forall j \in \N_r: \exists i \in \N_n: p_j \divides a_i$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Then: :$\displaystyle \map \gcd {A_n} = \prod_{j \mathop \in \N_r} {p_j}^{\min \set {e_{i j}: \, i \in \N_n} }$ where $\map \gcd {A_n}$ denotes the [[Definition:Greatest Common Divisor/Integers/General Definition|greatest common divisor]] of $a_1, a_2, \dotsc, a_n$.	0
We have, by [[Euler's Formula/Corollary|Euler's Formula: Corollary]]: :$\map \exp {-i a x^2} = -i \map \sin {a x^2} + \map \cos {a x^2}$ As $\map \sin {a x^2}$ and $\map \cos {a x^2}$ are both [[Definition:Real Number|real]] for real $a, x$, we therefore have: {{begin-eqn}} {{eqn | l = \int_0^\infty \map \sin {a x^2} \rd x | r = -\int_0^\infty \map \Im {\map \exp {-i a x^2} } \rd x }} {{eqn | r = -\map \Im {\int_0^\infty \map \exp {-i a x^2} \rd x} }} {{eqn | r = -\frac 1 {\sqrt a} \map \Im {\int_0^\infty \map \exp {-i t^2} \rd t} | c = [[Integration by Substitution|substituting]] $\sqrt a x = t$ }} {{eqn | r = -\frac 1 {\sqrt a} \map \Im {\frac 1 2 \sqrt {\frac \pi 2} \paren {1 - i} } | c = [[Definite Integral to Infinity of Exponential of -i x^2|Definite Integral to Infinity of $\map \exp {-i x^2}$]] }} {{eqn | r = \frac 1 2 \sqrt {\frac \pi {2 a} } }} {{end-eqn}} {{qed}}	0
From [[Sigma Function of Prime Number]], the [[Definition:Integer Addition|sum]] $\map \sigma p$ of all the [[Definition:Positive Integer|positive integer]] [[Definition:Divisor of Integer|divisors]] of a [[Definition:Prime Number|prime number]] $p$ is $p + 1$. But from [[Sigma Function of 1]], $\map \sigma 1 = 1$. If $1$ were to be classified as [[Definition:Prime Number|prime]], then $\map \sigma 1$ would be an exception to the rule that $\map \sigma p = p + 1$. {{qed}}	0
{{refactor|Extract individual proofs into their own pages and transclude them}} === Weierstrass Form equivalent to Euler Form === First it is shown that the [[Definition:Gamma Function/Weierstrass Form|Weierstrass form]] is equivalent to the [[Definition:Gamma Function/Euler Form|Euler form]]. {{begin-eqn}} {{eqn | l = \frac 1 {\map \Gamma z} | r = z e^{\gamma z} \prod_{n \mathop = 1}^\infty \paren {\paren {1 + \frac z n} e^{-z/n} } | c = [[Definition:Gamma Function/Weierstrass Form|Weierstrass Form]] of $\Gamma$ Function }} {{eqn | r = z \paren {\lim_{m \mathop \to \infty} \exp \paren {\paren {1 + \frac 1 2 + \cdots + \frac 1 m - \ln \paren m} z} } \paren {\lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^\infty \paren {\paren {1 + \frac z n} e^{-z/n} } } | c = {{Defof|Euler-Mascheroni Constant}} }} {{end-eqn}} Combining the limits: {{begin-eqn}} {{eqn | l = \frac 1 {\map \Gamma z} | r = z \lim_{m \mathop \to \infty} \paren {\exp \paren {\paren {1 + \frac 1 2 + \cdots + \frac 1 m - \ln \paren m} z} \prod_{n \mathop = 1}^m \paren {\paren {1 + \frac z n} e^{-z/n} } } | c = }} {{eqn | r = z \lim_{m \mathop \to \infty} \paren {\exp \paren {\paren {1 + \frac 1 2 + \cdots + \frac 1 m - \ln \paren m} z} \exp \paren {\frac {-z} 1 + \frac {-z} 2 + \cdots + \frac {-z} m} \prod_{n \mathop = 1}^m \paren {1 + \frac z n} } | c = }} {{eqn | r = z \lim_{m \mathop \to \infty} \paren {\exp \paren {\paren {1 - 1 + \frac 1 2 - \frac 1 2 + \cdots + \frac 1 m - \frac 1 m - \ln \paren m} z} \prod_{n \mathop = 1}^m \paren {1 + \frac z n} } | c = [[Exponential of Sum]] }} {{eqn | r = z \lim_{m \mathop \to \infty} \paren {m^{-z} \prod_{n \mathop = 1}^m \paren {1 + \frac z n} } | c = }} {{end-eqn}} But: :$(1): \quad m = \dfrac {m!} {\paren {m - 1}!} = \dfrac 2 1 \cdot \dfrac 3 2 \cdots \dfrac {x + 1} x \cdots \dfrac m {m - 1}$ Each term in $(1)$ is just $\dfrac {x + 1} x = 1 + \dfrac 1 x$, so: :$\displaystyle m = \prod_{n \mathop = 1}^{m - 1} \paren {1 + \frac 1 n}$ Thus the expression for $\dfrac 1 {\map \Gamma z}$ becomes: {{begin-eqn}} {{eqn | o = | r = z \lim_{m \mathop \to \infty} \paren {\prod_{n \mathop = 1}^{m - 1} \paren {1 + \frac 1 n}^{-z} \prod_{n \mathop = 1}^m \paren {1 + \frac z n} } | c = }} {{eqn | r = z \lim_{m \mathop \to \infty} \paren {\paren {1 + \frac 1 m}^z \prod_{n \mathop = 1}^m \paren {1 + \frac 1 n}^{-z} \paren {1 + \frac z n} } | c = }} {{eqn | r = z \lim_{m \mathop \to \infty} \paren {1 + \frac 1 m}^z \lim_{m \mathop \to \infty} \prod_{n \mathop = 1}^m \paren {1 + \frac 1 n}^{-z} \paren {1 + \frac z n} | c = [[Product Rule for Complex Sequences]] }} {{eqn | r = z \prod_{n \mathop = 1}^\infty \paren {1 + \frac 1 n}^{-z} \paren {1 + \frac z n} }} {{end-eqn}} Hence: :$\displaystyle \map \Gamma z = \frac 1 z \prod_{n \mathop = 1}^\infty \paren {1 + \frac 1 n}^z \paren {1 + \frac z n}^{-1}$ which is the [[Definition:Gamma Function/Euler Form|Euler form of the Gamma function]]. {{qed|lemma}} === Integral Form equivalent to Euler Form === This is proved in the page: :[[Integral Form of Gamma Function equivalent to Euler Form]] {{qed}}	0
:$\displaystyle \int \frac {\d x} {\cos a x \paren {1 - \sin a x} } = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {2 a} \ln \size {\map \tan {\frac {a x} 2 + \frac \pi 4} } + C$	0
By [[Factors in Absolutely Convergent Product Converge to One]], $\norm {a_n} < 1$ for $n \ge n_0$. Let $n_1 \ge n_0$. {{AimForCont}} the product [[Definition:Divergent Product|diverges]] to $0$. Then: :$\displaystyle \prod_{n \mathop = n_1}^\infty \paren {1 + a_n} = 0$ By [[Norm of Limit]]: :$\displaystyle \prod_{n \mathop = n_1}^\infty \norm {1 + a_n} = 0$ By the [[Triangle Inequality]] and [[Squeeze Theorem]]: :$\displaystyle \prod_{n \mathop = n_1}^\infty \paren {1 - \norm {a_n} } = 0$ By the [[Weierstrass Product Inequality]], we have for $N \ge n_1$: :$\displaystyle \prod_{n \mathop = n_1}^N \paren {1 - \norm {a_n} } \ge 1 - \sum_{n \mathop = n_1}^N \norm{a_n}$ Taking limits: :$0 \ge 1 - \displaystyle \sum_{n \mathop = n_1}^\infty \norm {a_n}$ Because $\displaystyle \prod_{n \mathop = 1}^\infty \paren {1 + a_n}$ is [[Definition:Absolute Convergence of Product|absolutely convergent]]: :$\displaystyle \sum_{n \mathop = n_1}^\infty \norm {a_n} < 1$ for $n_1$ [[Definition:Sufficiently Large|sufficiently large]]. This is a [[Definition:Contradiction|contradiction]]. {{qed}}	0
If $T_A$ is a [[Definition:Tychonoff Space|Tychonoff (completely regular) space]], then so is $T_B$.	0
By definition, [[Definition:Closed Set (Topology)|closed sets]] of $T$ are [[Definition:Compact Topological Space|compact sets]] of $T$. So, for example, $\left[{0 \,.\,.\, 1}\right]$ and $\left[{2 \,.\,.\, 3}\right]$ are [[Definition:Disjoint Sets|disjoint]] [[Definition:Compact Topological Space|compact sets]] and therefore [[Definition:Closed Set (Topology)|closed sets]] of $T$. Hence the result by definition of [[Definition:Ultraconnected Space|ultraconnected]]. {{qed}}	0
From [[Open Sets in Real Number Line]], $\openint a b$ is [[Definition:Open Set (Topology)|open]] in $\struct {\R, \tau_d}$. From [[Closure of Open Real Interval is Closed Real Interval]]: :$\openint a b^- = \closedint a b$ where $\openint a b^-$ denotes the [[Definition:Closure (Topology)|closure]] of $\openint a b$. From [[Interior of Closed Real Interval is Open Real Interval]]: :$\closedint a b^\circ = \openint a b$ where $\closedint a b^\circ$ denotes the [[Definition:Interior (Topology)|interior]] of $\closedint a b$. Hence the result, by definition of [[Definition:Regular Open Set|regular open]]. {{qed}}	0
<onlyinclude> For every pair of [[Definition:Integer|integers]] $a, b$ where $a \ge 0$ and $b > 0$, there exist [[Definition:Unique|unique]] integers $q, r$ such that $a = q b + r$ and $0 \le r < b$: :$\forall a, b \in \Z, a \ge 0, b > 0: \exists! q, r \in \Z: a = q b + r, 0 \le r < b$ <onlyinclude> In the above equation: * $a$ is the '''dividend''' * $b$ is the '''divisor''' * $q$ is the '''quotient''' * $r$ is the '''principal remainder''', or, more usually, just the '''remainder'''.	0
Let $a > 0$. We make no statement about $x_1$. We specify that: :$x_{n + 1} = \dfrac {x_n + \dfrac a {x_n} } 2$ Now: {{begin-eqn}} {{eqn | l = x_{n + 1} - \sqrt a | r = \frac {x_n + \dfrac a {x_n} } 2 - \sqrt a | c = }} {{eqn | r = \frac 1 {2 x_n} \paren {x_n^2 - 2 x_n \sqrt a + a} | c = }} {{eqn | r = \frac 1 {2 x_n} \paren {x_n - \sqrt a}^2 | c = }} {{eqn | r = \frac 1 {2 x_n} \paren {\dfrac {\paren {x_{n - 1} - \sqrt a}^2} {2 x_{n - 1} } }^2 | c = }} {{eqn | r = \frac 1 {2 x_n} \frac 1 {\paren {2 x_{n - 1} }^2} \paren {x_{n - 1} - \sqrt a}^4 | c = }} {{eqn | r = \frac 1 {2 x_n} \frac 1 {\paren {2 x_{n - 1} }^2} \frac 1 {\paren {2 x_{n - 2} }^4} \paren {x_{n - 2} - \sqrt a}^8 | c = }} {{eqn | r = \frac 1 {2 x_n} \frac 1 {\paren {2 x_{n - 1} }^2} \cdots \frac 1 {\paren {2 x_1}^{2 n - 1} } \paren {x_1 - \sqrt a}^{2 n} | c = }} {{end-eqn}} If we now assume that $x_1 \ge \sqrt a$, then it follows from [[Hero's Method/Lemma 2|Lemma 2]] that $x_n \ge \sqrt a$. So: {{begin-eqn}} {{eqn | l = \size {x_{n + 1} - \sqrt a} | o = \le | r = \paren {\frac 1 {2 \sqrt a} }^{1 + 2 + 2^2 + \cdots + 2^{n - 1} } \paren {x_1 - \sqrt a}^{2 n} | c = }} {{eqn | r = \paren {\frac 1 {2 \sqrt a} }^{\dfrac {2^n - 1} {2 - 1} } \paren {x_1 - \sqrt a}^{2 n} | c = }} {{eqn | r = 2 \sqrt a \paren {\frac {x_1 - \sqrt a} {2 \sqrt a} }^{2 n} | c = }} {{end-eqn}} If $\size y < 1$, then $y^n \to 0$ as $n \to \infty$ from [[Sequence of Powers of Number less than One]]. So, by [[Limit of Subsequence equals Limit of Real Sequence]]: :$y^{2^n} \to 0$ as $n \to \infty$ Thus we see that: :$x_n \to \sqrt a$ as $n \to \infty$ provided that: :$\dfrac {x_1 - \sqrt a} {2 \sqrt a} < 1$ that is, that: :$\sqrt a \le x_1 < 3 \sqrt a$ We assumed above that $x_1 \ge \sqrt a$. Now we have shown that $x_n \to \sqrt a$ as $n \to \infty$ provided that $\sqrt a \le x_1 < 3 \sqrt a$. However, we have already shown that $x_n \to \sqrt a$ as long as $x_1 \ge 0$. The advantage to this analysis is that this gives us an opportunity to determine how close $x_n$ gets to $\sqrt a$. {{Qed}}	0
Suppose $\tau''$ is a [[Definition:Topology|topology]] on $H$ such that: :$(1) \quad$ For any [[Definition:Topological Space|topological space]] $T' = \struct {A', \tau'}$, and :$(2) \quad$ For any [[Definition:Mapping|mapping]] $g: A' \to H$: $g$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau', \tau''}$-continuous]] {{iff}} $i \circ g$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau', \tau}$-continuous]]. It needs to be shown that $\tau''$ must be the same as $\tau_H$. Let $A' = H$ and $\tau' = \tau''$. Let $g$ be the [[Definition:Identity Mapping|identity mapping]] on $H$. From [[Identity Mapping is Continuous]], $g$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau'', \tau''}$-continuous]] Thus from [[Continuity of Composite Mapping]], $i \circ g$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau'', \tau}$-continuous]]. Hence for any $U \in \tau$: :$\map {\paren {i \circ g}^{-1} } U \in \tau''$ But: :$\map {\paren {i \circ g}^{-1} } U = \map {i^{-1} } U = U \cap H$ Hence $\tau_H \subseteq \tau''$. Next, take take $A' = H$ and $\tau' = \tau_H$. Let $g$ be the [[Definition:Identity Mapping|identity mapping]] on $H$. We have that $i \circ g = i$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau_H, \tau}$-continuous]], From [[Continuity of Composite with Inclusion/Inclusion on Mapping|Continuity of Composite with Inclusion: Inclusion on Mapping]], it follows that $g$ is [[Definition:Continuous Mapping (Topology)|$\tuple {\tau_H, \tau''}$-continuous]]. But by definition of [[Definition:Continuous Mapping (Topology)|continuity]], this is the same as saying $\tau'' \subseteq \tau_H$. So $\tau'' = \tau_H$, as required. {{qed}}	0
From [[Natural Logarithm of 2 is Greater than One Half]]: :$\ln 2 \ge \dfrac 1 2$ From the definition of [[Definition:Infinite Limit at Infinity|infinite limit at infinity]], our assertion is: :$\forall M \in \R_{>0} : \exists N > 0 : x > N \implies \ln x > M$. As $x \to +\infty$, we will restrict our attention to [[Definition:Sufficiently Large|sufficiently large]] $M$. From [[Logarithm is Strictly Increasing]]: :$\ln x$ is [[Definition:Strictly Increasing Real Function|strictly increasing]]. So, for [[Definition:Sufficiently Large|sufficiently large]] $M$: :$x > 2^{2 M} \implies \ln x > \ln 2^{2 M}$ From the [[Laws of Logarithms]]: {{begin-eqn}} {{eqn | l = \ln 2^{2 M} | r = 2 M \ln 2 }} {{eqn | o = \ge | r = 2 M \cdot \dfrac 1 2 }} {{eqn | r = M }} {{end-eqn}} Choosing $N = \ln 2^{2 M}$: :$\forall M \ge a: \exists N > 0: x > N \implies \ln x > M$ for some $a \in \R$. Hence the result, by the definition of [[Definition:Infinite Limit at Infinity|infinite limit at infinity]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \int \frac {\paren {p - q \cos a x} \rd x} {\paren {p + q \cos a x} \paren {p - q \cos a x} } | c = }} {{eqn | r = \int \frac {\paren {p - q \cos a x} \rd x} {p^2 - q^2 \cos^2 a x} | c = }} {{eqn | r = \int \frac {\paren {p - q \cos a x} \rd x} {\paren {p^2 - q^2} + q^2 \sin^2 a x} | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} Let $p^2 > q^2$. Thus, let $p^2 - q^2 = d^2$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \int \frac {\paren {p - q \cos a x} \rd x} {d^2 + q^2 \sin^2 a x} | c = }} {{eqn | r = \int \frac {p \rd x} {d^2 + q^2 \sin^2 a x} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac p {a d \sqrt{d^2 + q^2} } \map \arctan {\frac {\sqrt {d^2 + q^2} \tan a x} d} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Primitive of Reciprocal of p squared plus square of q by Sine of a x|Primitive of $\dfrac 1 {p^2 + q^2 \sin^2 a x}$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \int \frac {q \cos a x \rd x} {d^2 + q^2 \sin^2 a x} | c = }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 a \int \frac {q \paren {\sin a x}' \rd x} {d^2 + q^2 \sin^2 a x} | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 a \int \frac {q \rd u} {d^2 + q^2 u^2} | c = letting $u = \sin a x$ }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a q} \int \frac {\d u} {\frac {d^2} {q^2} + u^2} | c = }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a q} \frac q d \map \arctan {\frac {q u} d} + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | r = \frac 1 {a d} \map \arctan {\frac {p \tan a x} d} - \frac 1 {a d} \map \arctan {\frac {q \sin a x} d} + C | c = substituting $u = \sin a x$ }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \paren {\map \arctan {\frac {p \tan a x} {\sqrt {p^2-q^2} } } - \map \arctan {\frac {q \sin a x} {\sqrt {p^2 - q^2} } } } + C | c = substituting $d = \sqrt{p^2 - q^2}$ }} {{eqn-intertext|While this would usually be considered as an acceptable form to leave such an expression, there is some way to go to obtain the result requested.}} {{eqn | r = \frac 1 {a \sqrt{p^2 - q^2} } \map \arctan {\frac {\frac {p \tan a x} {\sqrt {p^2 - q^2} } - \frac {q \sin a x} {\sqrt {p^2 - q^2} } } {1 + \frac {p \tan a x} {\sqrt {p^2 - q^2} } \frac {q \sin a x} {\sqrt {p^2 - q^2} } } } + C | c = [[Difference of Arctangents]] }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \tan a x - q \sin a x} } {p^2 - q^2 + p q \tan a x \sin a x} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \paren {\frac {2 \tan \frac {a x} 2} {1 - \paren {\tan \frac {a x} 2}^2 } } - q \paren {\frac {2 \tan \frac {a x} 2} {1 + \paren {\tan \frac {a x} 2}^2 } } } } {p^2 - q^2 + p q \paren {\frac {2 \tan \frac {a x} 2} {1 - \paren {\tan \frac {a x} 2}^2 } } \paren {\frac {2 \tan \frac {a x} 2} {1 + \paren {\tan \frac {a x} 2}^2 } } } } + C | c = [[Tangent Half-Angle Substitution for Sine]] and [[Double Angle Formula for Tangent]] }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {p \paren {\frac {2 u} {1 - u^2 } } - q \paren {\frac {2 u} {1 + u^2 } } } } {p^2 - q^2 + p q \paren {\frac {2 u} {1 - u^2 } } \paren {\frac {2 u} {1 + u^2} } } } + C | c = letting $u = \tan \frac {a x} 2$ }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {2 p u \paren {1 + u^2} - 2 q u \paren {1 - u^2} } } {\paren {p^2 - q^2} \paren {1 - u^2} \paren {1 + u^2} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {\sqrt {p^2 - q^2} \paren {2 p u + 2 p u^3 - 2 q u + 2 q u^3} } {\paren {p^2 - q^2} \paren {1 - u^4} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {p^2 - q^2} } \map \arctan {\frac {2 \sqrt {p^2 - q^2} \paren {\paren {p - q} u + \paren {p + q} u^3 } } {\paren {p^2 - q^2} \paren {1 - u^4} + 4 p q u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} \paren {m u + n u^3} } {m n \paren {1 - u^4} + \paren {n^2 - m^2} u^2} } + C | c = letting $m = p - q$ and $n = p + q$ }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m n - m n u^4 + n^2 u^2 - m^2 u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m n - m^2 u^2 + n^2 u^2 - m n u^4} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {m \paren {n - m u^2} + n u^2 \paren {n - m u^2} } } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u \paren {m + n u^2} } {\paren {m + n u^2} \paren {n - m u^2} } } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {m n} u} {n - m u^2} } + C | c = }} {{eqn | r = \frac 1 {a \sqrt {m n} } \map \arctan {\frac {2 \sqrt {\frac m n} u} {1 - \frac m n u^2} } + C | c = }} {{eqn | r = \frac 2 {a \sqrt {m n} } \map \arctan {\sqrt {\frac m n} u} + C | c = [[Sum of Arctangents]] }} {{eqn | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } u} + C | c = substituting $m = p - q$ and $n = p + q$ }} {{eqn | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } \tan \frac {a x} 2 } + C | c = substituting $u = \tan \frac {a x} 2$ }} {{end-eqn}} {{qed|lemma}} {{questionable|The derivation up till here has been done on the assumption that $p^2 > q^2$. Hence it is a failure of logic to take the expression above and extend it to $p^2 < q^2$ without having first established in the above that it is valid so to do.}} Now let $p^2 < q^2$. {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p + q \cos a x} | r = \frac 2 {a \sqrt {p^2 - q^2} } \map \arctan {\sqrt {\frac {p - q} {p + q} } \tan \frac {a x} 2 } | c = }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \map \arctan {i \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2 } | c = where $i$ is the [[Definition:Imaginary Unit|imaginary unit]] }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {1 + \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2} {1 - \sqrt {\frac {q - p} {q + p} } \tan \frac {a x} 2} } | c = [[Arctangent of Imaginary Number]] }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {\sqrt {\frac {q + p} {q - p} } + \tan \frac {a x} 2} {\sqrt {\frac {q + p} {q - p} } - \tan \frac {a x} 2} } | c = }} {{eqn | r = \frac 2 {a i \sqrt {q^2 - p^2} } \frac i 2 \ln \size {\frac {\tan \frac {a x} 2 + \sqrt {\frac {q + p} {q - p} } } {\tan \frac {a x} 2 - \sqrt {\frac {q + p} {q - p} } } } | c = }} {{eqn | r = \frac 1 {a \sqrt {q^2 - p^2} } \ln \size {\frac {\tan \frac {a x} 2 + \sqrt {\frac {q + p} {q - p} } } {\tan \frac {a x} 2 - \sqrt {\frac {q + p} {q - p} } } } | c = }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {\sin a x \left({1 - \cos a x}\right)} = \frac {-1} {2 a \left({1 - \cos a x}\right)} + \frac 1 {2 a} \ln \left\vert{\tan \frac {a x} 2}\right\vert + C$	0
{{begin-eqn}} {{eqn | l = \cot 15^\circ | r = \frac {\cos 15^\circ} {\sin 15^\circ} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac{\frac {\sqrt 6 + \sqrt 2} 4} {\frac {\sqrt 6 - \sqrt 2} 4} | c = [[Cosine of 15 Degrees]] and [[Sine of 15 Degrees]] }} {{eqn | r = \frac {\sqrt 6 + \sqrt 2} {\sqrt 6 - \sqrt 2} | c = simplifying }} {{eqn | r = \frac {\left({\sqrt 6 + \sqrt 2}\right)^2} {\left({\sqrt 6 - \sqrt 2}\right) \left({\sqrt 6 + \sqrt 2}\right)} | c = multiplying top and bottom by $\sqrt 6 + \sqrt 2$ }} {{eqn | r = \frac {6 + 2 \sqrt 6 \sqrt 2 + 2 } {6 - 2} | c = multiplying out, and [[Difference of Two Squares]] }} {{eqn | r = \frac {8 + 4 \sqrt 3} 4 | c = simplifying }} {{eqn | r = 2 + \sqrt 3 | c = dividing top and bottom by $4$ }} {{end-eqn}} {{qed}}	0
:$\sin 210 \degrees = \sin \dfrac {7 \pi} 6 = -\dfrac 1 2$	0
Let $q \in \N_{>0}$, $n \in \N$. Let $\map {c_q} n$ be the [[Definition:Ramanujan Sum|Ramanujan sum]]. Then $\map {c_q} n$ is [[Definition:Multiplicative Arithmetic Function|multiplicative]] in $q$.	0
:$\displaystyle \frac {\map \sin {2 n + 1} \theta} {\sin \theta} = \paren {2 n + 1} \prod_{k \mathop = 1}^n \paren {1 - \frac {\sin^2 \theta} {\map {\sin^2} {\frac {k \pi} {2 n + 1} } } }$ for $\sin \theta \ne 0$.	0
{{begin-eqn}} {{eqn | l = \sin \paren {\pi - \theta} | r = \sin \pi \cos \theta - \cos \pi \sin \theta | c = [[Sine of Difference]] }} {{eqn | r = 0 \times \cos \theta - \paren {-1} \times \sin \theta | c = [[Sine of Straight Angle]] and [[Cosine of Straight Angle]] }} {{eqn | r = \sin \theta }} {{end-eqn}} {{qed}}	0
The following [[Definition:Positive Integer|positive integers]] cannot be expressed as the [[Definition:Integer Addition|sum]] of [[Definition:Distinct|distinct]] [[Definition:Non-Pythagorean Prime|non-pythagorean primes]]: :$1, 2, 4, 5, 6, 8, 9, 12, 13, 15, 16, 17, 20, 24, 25, 27, 28, 32, 35, 36, 39, 48, 51, 55$ {{OEIS|A048262}} All [[Definition:Positive Integer|positive integers]] greater than $55$ can be so expressed.	0
Let $n \in \N_{>0}$ be a non-zero [[Definition:Natural Number|natural number]]. Let $f: \hointr 0 \infty \to \R$ be the [[Definition:Real Function|real function]] defined by $\map f x = x^{1/n}$. Then $f$ is [[Definition:Continuous Real Function at Point|continuous]] at each $\xi > 0$ and [[Definition:Right-Continuous at Point|continuous on the right]] at $\xi = 0$.	0
Let $G \left({z}\right)$ be the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{a_n}\right\rangle$. Consider the [[Definition:Subsequence|subsequence]] $\left\langle{b_n}\right\rangle := \left({a_0, a_2, a_4, \ldots}\right)$ Then the [[Definition:Generating Function|generating function]] for $\left\langle{b_n}\right\rangle$ is: :$\dfrac 1 2 \left({G \left({z}\right) + G \left({-z}\right)}\right)$	0
Let: {{begin-eqn}} {{eqn | l = z | r = \paren {2 a x + b}^2 | c = }} {{eqn | ll= \leadsto | l = \frac {\d z} {\d x} | r = 4 a \paren {2 a x + b} | c = [[Derivative of Power]] and [[Chain Rule for Derivatives]] }} {{eqn | ll= \leadsto | l = \frac {\d z} {\d x} | r = 4 a \sqrt z | c = [[Derivative of Power]] and [[Chain Rule for Derivatives]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | o = | r = \int \sqrt {a x^2 + b x + c} \rd x | c = }} {{eqn | r = \int \sqrt {\frac {\paren {2 a x + b}^2 + \paren {4 a c - b^2} } {4 a} } \rd x | c = [[Completing the Square]] }} {{eqn | r = \int \frac {\sqrt {z + \paren {4 a c - b^2} } \rd z} {\paren {2 \sqrt a} \paren {4 a \sqrt z} } | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {8 a \sqrt a} \int \frac {\sqrt {z + \paren {4 a c - b^2} } \rd z} {\sqrt z} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 {8 a \sqrt a} \paren {\sqrt z \sqrt {z + \paren {4 a c - b^2} } + \frac {4 a c - b^2} 2 \int \frac {\d z} {\sqrt z \sqrt {z + \paren {4 a c - b^2} } } } | c = [[Primitive of Root of p x + q over Root of a x + b|Primitive of $\dfrac {\sqrt{p x + q} } {\sqrt{a x + b} }$]] }} {{eqn | r = \frac {\sqrt z \sqrt {z + \paren {4 a c - b^2} } } {8 a \sqrt a} + \frac {4 a c - b^2} {16 a \sqrt a} \int \frac {\d z} {\sqrt z \sqrt {z + \paren {4 a c - b^2} } } | c = multiplying out }} {{eqn | r = \frac {\paren {2 a x + b} \sqrt {\paren {2 a x + b}^2 + \paren {4 a c - b^2} } } {8 a \sqrt a} + \frac {4 a c - b^2} {16 a \sqrt a} \int \frac {4 a \rd x} {\sqrt {\paren {2 a x + b}^2 + \paren {4 a c - b^2} } } | c = substituting back for $z$ and $\d z$ }} {{eqn | r = \frac {\paren {2 a x + b} \paren {2 \sqrt a \sqrt {a x^2 + b x + c} } } {8 a \sqrt a} + \frac {4 a c - b^2} {16 a \sqrt a} \int \frac {4 a \rd x} {2 \sqrt a \sqrt {a x^2 + b x + c} } | c = substituting back for $\sqrt {a x^2 + b x + c}$ }} {{eqn | r = \frac {\paren {2 a x + b} \sqrt {a x^2 + b x + c} } {4 a} + \frac {4 a c - b^2} {8 a} \int \frac {\d x} {\sqrt {a x^2 + b x + c} } | c = simplifying }} {{end-eqn}} {{qed}}	0
Let us define $\eqclass {\tuple {a, b} } \boxtimes$ as in the [[Definition:Integer/Formal Definition|formal definition of integers]]. That is, $\eqclass {\tuple {a, b} } \boxtimes$ is an [[Definition:Equivalence Class|equivalence class]] of [[Definition:Ordered Pair|ordered pairs]] of [[Definition:Natural Numbers|natural numbers]] under the [[Definition:Congruence Relation|congruence relation]] $\boxtimes$. $\boxtimes$ is the [[Definition:Congruence Relation|congruence relation]] defined on $\N \times \N$ by: :$\tuple {x_1, y_1} \boxtimes \tuple {x_2, y_2} \iff x_1 + y_2 = x_2 + y_1$ In order to streamline the notation, we will use $\eqclass {a, b} {}$ to mean $\eqclass {\tuple {a, b} } \boxtimes$, [[Definition:Integer/Formal Definition/Notation|as suggested]]. From [[Construction of Inverse Completion|the method of construction]], $\eqclass {c, c} {}$, where $c$ is any element of the [[Definition:Natural Numbers|natural numbers]] $\N$, is [[Definition:Isomorphic Copy|the isomorphic copy]] of $0 \in \N$. So, we need to show that: :$\forall a, b, c \in \N: \eqclass {a, b} {} + \eqclass {c, c} {} = \eqclass {a, b} {} = \eqclass {c, c} {} + \eqclass {a, b} {}$ Thus: {{begin-eqn}} {{eqn | l = \eqclass {a, b} {} + \eqclass {c, c} {} | r = \eqclass {a + c, b + c} {} | c = }} {{eqn | r = \eqclass {a, b} {} | c = [[Construction of Inverse Completion/Equivalence Relation/Members of Equivalence Classes|Construction of Inverse Completion: Members of Equivalence Classes]] }} {{end-eqn}} So: :$\eqclass {a, b} {} + \eqclass {c, c} {} = \eqclass {a, b} {}$ The identity $\eqclass {a, b} {} = \eqclass {c, c} {} + \eqclass {a, b} {}$ is demonstrated similarly. {{qed}}	0
This proof assumes that $\mathbb K = \C$. === Necessary Condition === Assume that $f$ is [[Definition:Complex-Differentiable Function|differentiable]] in $z$. By [[Definition:Open Set (Complex Analysis)|definition of open set]], there exists $r \in \R_{>0}$ such that the [[Definition:Open Ball|open ball]] $B_r \left({z}\right) \subseteq D$. Define $\epsilon: B_r \left({0}\right) \setminus \left\{ {0}\right\} \to \C$ by: :$\epsilon \left({h}\right) = \dfrac{f \left({z + h}\right) - f \left({z}\right)} h - f' \left({z}\right)$ If $h \in B_r \left({0}\right) \setminus \left\{ {0}\right\}$, then $z+h \in B_r \left({z}\right) \setminus \left\{ {z}\right\} \subseteq D$, so $\epsilon$ is well-defined. As $f$ is [[Definition:Complex-Differentiable Function|differentiable]] in $z$, it follows that: :$\displaystyle \lim_{h \to 0} \ \epsilon \left({h}\right) = \lim_{h \to 0} \dfrac{ f \left({z + h}\right) - f \left({z}\right) } h - f' \left({z}\right) = f' \left({z}\right) - f' \left({z}\right) = 0$ If we put $\alpha = f' \left({z}\right)$, it follows that for all $h \in B_r \left({0}\right) \setminus \left\{ {0}\right\}$: :$f\left({z + h}\right) = f \left({z}\right) + h \left({\alpha + \epsilon \left({h}\right) }\right)$ {{qed|lemma}} === Sufficient condition === Rewrite the equation of the assumption as: :$\dfrac{f \left({z + h}\right) - f \left({z}\right)} h = \alpha + \epsilon \left({h}\right)$ From [[Sum Rule for Limits of Functions]]: :$\displaystyle \lim_{h \to 0} \dfrac{f \left({z + h}\right) - f \left({z}\right)} h = \lim_{h \to 0} \left({ \alpha + \epsilon \left({h}\right) }\right) = \alpha$ By [[Definition:Complex-Differentiable Function|definition of differentiability]], $f$ is differentiable at $z$ with $f' \left({z}\right) = \alpha$. {{qed}}	0
:$\displaystyle \int_0^\infty \frac {x^{p - 1} \ln x} {1 + x} \rd x = -\pi^2 \csc p \pi \cot p \pi$	0
{{begin-eqn}} {{eqn | l = a | o = \equiv | r = b | rr= \pmod {r s} | c = }} {{eqn | ll= \implies | l = a - b | r = q r s | c = {{Defof|Congruence Modulo Integer}} }} {{eqn | ll= \implies | l = a - b | r = \paren {q r} s | c = }} {{eqn | lo= \land | l = a - b | r = \paren {q s} r | c = }} {{eqn | l = a | o = \equiv | r = b | rr= \pmod r | c = {{Defof|Congruence Modulo Integer}}: $q s$ is an [[Definition:Integer|integer]] }} {{eqn | lo= \land | l = a | o = \equiv | r = b | rr= \pmod s | c = {{Defof|Congruence Modulo Integer}}: $q r$ is an [[Definition:Integer|integer]] }} {{end-eqn}} {{qed}}	0
Using [[De Moivre's Formula]]: :$\sin x = \dfrac {\left({\cos \dfrac x n + i \sin \dfrac x n}\right)^n - \left({\cos \dfrac x n - i \sin \dfrac x n}\right)^n} {2i}$ The difference between two [[Definition:Power (Algebra)|$n$th powers]] can be extracted into linear factors using [[Definition:Complex Roots of Unity|$n$th roots of unity]]. For large $n$, we can replace: : $\cos \dfrac x n$ by $1$ : $\sin \dfrac x n$ by $\dfrac x n$ {{proof wanted}}	0
Let $f \left({x}\right) = \tan x - x$. By [[Derivative of Tangent Function]], $f' \left({x}\right) = \sec^2 x - 1$. By [[Shape of Secant Function]], $\sec^2 x > 1$ for $x \in \left({0 \,.\,.\, \dfrac {\pi} 2}\right)$. Hence $f' \left({x}\right) > 0$. From [[Derivative of Monotone Function]], $f \left({x}\right)$ is [[Definition:Strictly Increasing|strictly increasing]] in this [[Definition:Open Real Interval|interval]]. Since $f \left({0}\right) = 0$, it follows that $f \left({x}\right) > 0$ for all $x$ in $x \in \left({0 \,.\,.\, \dfrac {\pi} 2}\right)$. {{qed}} [[Category:Tangent Function]] [[Category:Inequalities]] jyj6srbdnfe2spqq4ob5afa9mk69130	0
Let $\family {a_i}{i \mathop \in I}$ be a [[Definition:Indexed Family|family of elements]] of the [[Definition:Positive Real Number|non-negative real numbers]] $\R_{\ge 0}$ [[Definition:Indexing Set|indexed]] by $I$. Let $\map R i$ be a [[Definition:Propositional Function|propositional functions]] of $i \in I$. Let $\displaystyle \sup_{\map R i} a_i$ be the [[Definition:Indexed Supremum by Propositional Function|indexed supremum]] on $\family {a_i}$. Then: :$\displaystyle \sup_{\map R i} a_i = \sup_{\map R j} a_j$	0
:$\log_{10} x = \dfrac {\ln x} {\ln 10} = \dfrac {\ln x} {2 \cdotp 30258 \, 50929 \, 94 \ldots}$	0
{{begin-eqn}} {{eqn | l = \frac 1 {1 + x} | r = \sum_{k \mathop = 0}^\infty \left({-1}\right)^k x^k | c = [[Power Series Expansion of Reciprocal of 1 + x|Power Series Expansion of $\dfrac 1 {1 + x}$]] }} {{eqn | ll= \leadsto | l = \frac \d {\d x} \frac 1 {1 + x} | r = \frac \d {\d x} \sum_{k \mathop = 0}^\infty \left({-1}\right)^k x^k | c = }} {{eqn | ll= \leadsto | l = -\frac 1 {\left({1 + x}\right)^2} | r = \sum_{k \mathop = 0}^\infty \left({-1}\right)^k k x^{k - 1} | c = [[Definition:Differentiation|differentiating]] {{WRT|Differentiation}} $x$ }} {{eqn | ll= \leadsto | l = \frac 1 {\left({1 + x}\right)^2} | r = \sum_{k \mathop = 0}^\infty \left({-1}\right)^{k - 1} k x^{k - 1} | c = taking one of the $-1$s out }} {{eqn | r = \sum_{k \mathop = 1}^\infty \left({-1}\right)^{k - 1} k x^{k - 1} | c = the term in $k = 0$ vanishes }} {{eqn | r = \sum_{k \mathop = 0}^\infty \left({-1}\right)^k \left({k + 1}\right) k x^k | c = [[Translation of Index Variable of Product]] }} {{end-eqn}} {{qed}}	0
The [[Definition:Quadratic Residue|quadratic residues]] of $p$ are the [[Definition:Integer|integers]] which result from the evaluation of the [[Definition:Square Number|squares]]: : $1^2, 2^2, \ldots, \left({p - 1}\right)^2$ modulo $p$ But: :$r^2 = \left({-r}\right)^2$ and so these $p - 1$ [[Definition:Integer|integers]] fall into [[Definition:Congruence Modulo Integer|congruent]] pairs modulo $p$, namely: {{begin-eqn}} {{eqn | l = 1^2 | o = \equiv | r = \left({p - 1}\right)^2 | rr= \pmod p }} {{eqn | l = 2^2 | o = \equiv | r = \left({p - 2}\right)^2 | rr= \pmod p }} {{eqn | o = \ldots }} {{eqn | l = \left({\frac {p - 1} 2}\right)^2 | o = \equiv | r = \left({\frac {p + 1} 2}\right)^2 | rr= \pmod p | c = Note: we require $p$ to be [[Definition:Odd Integer|odd]] here. }} {{end-eqn}} Therefore each [[Definition:Quadratic Residue|quadratic residue]] of $p$ is [[Definition:Congruence Modulo Integer|congruent modulo $p$]] to one of the $\dfrac {p-1} 2$ [[Definition:Integer|integers]] $1^2, 2^2, \ldots, \left({\dfrac {p-1} 2}\right)^2$. Note that as $r^2 \not \equiv 0 \pmod p$ for $1 \le r < p$, the [[Definition:Integer|integer]] $0$ is not among these. All we need to do now is show that no two of these [[Definition:Integer|integers]] are [[Definition:Congruence Modulo Integer|congruent modulo $p$]]. So, suppose that $r^2 \equiv s^2 \pmod p$ for some $1 \le r \le s \le \dfrac {p-1} 2$. What we are going to do is prove that $r = s$. Now $r^2 \equiv s^2 \pmod p$ means that $p$ is a [[Definition:Divisor of Integer|divisor]] of $r^2 - s^2 = \left({r + s}\right) \left({r - s}\right)$. From [[Euclid's Lemma]] either: :$p \mathrel \backslash \left({r + s}\right)$ or: :$p \mathrel \backslash \left({r - s}\right)$ $p \mathrel \backslash \left({r + s}\right)$ is impossible as $2 \le r + s \le p - 1$. Take $p \mathrel \backslash \left({r - s}\right)$. As $0 \le r - s < \dfrac {p-1} 2$, that can happen only when: :$r - s = 0$ or: :$r = s$ So there must be exactly $\dfrac {p-1} 2$ [[Definition:Quadratic Residue|quadratic residues]]. That means there must also be exactly $\dfrac {p-1} 2$ [[Definition:Quadratic Non-Residue|quadratic non-residues]]. {{qed}} [[Category:Prime Numbers]] [[Category:Quadratic Residues]] gu8df47d8cj4ooq8wlpmxutqwv31zsi	0
The [[Definition:Divisor of Integer|divisibility]] relation is a [[Definition:Transitive Relation|transitive relation]] on $\Z$, the set of [[Definition:Integer|integers]]. That is: :$\forall x, y, z \in \Z: x \divides y \land y \divides z \implies x \divides z$	0
:$\ln x \to -\infty$ as $x \to 0^+$	0
Let $S \subseteq \R$. Let $x \in S$. Let $\sequence {f_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Real Function|real functions]]. Let $f_n$ be [[Definition:Continuous Function|continuous]] at $x$ for all $n \in \N$. Let the infinite series: :$\displaystyle \sum_{n \mathop = 1}^\infty f_n$ be [[Definition:Uniform Convergence/Infinite Series|uniformly convergent]] to a real function $f : S \to \R$. Then $f$ is [[Definition:Continuous Function|continuous]] at $x$.	0
With a view to expressing the problem in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = x^2 | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = 2 x | c = [[Power Rule for Derivatives]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac x {\sqrt {x^2 + a^2} } | c = }} {{eqn | ll= \leadsto | l = v | r = \sqrt {x^2 + a^2} | c = [[Primitive of x over Root of x squared plus a squared|Primitive of $\dfrac x {\sqrt {x^2 + a^2} }$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x^3 \rd x} {\sqrt {x^2 + a^2} } | r = \int x^2 \frac {x \rd x} {\sqrt {x^2 + a^2} } | c = }} {{eqn | r = x^2 \sqrt {x^2 + a^2} - \int 2 x \sqrt {x^2 + a^2} \rd x | c = [[Integration by Parts]] }} {{eqn | r = x^2 \sqrt {x^2 + a^2} - 2 \paren {\frac {\paren {\sqrt {x^2 + a^2} }^3} 3 } + C | c = [[Primitive of x by Root of x squared plus a squared|Primitive of $x \sqrt {x^2 + a^2}$]] }} {{eqn | r = \paren {x^2 + a^2 - a^2} \sqrt {x^2 + a^2} - 2 \paren {\frac {\paren {\sqrt {x^2 + a^2} }^3} 3 } + C | c = }} {{eqn | r = \paren {\sqrt {x^2 + a^2} }^3 - a^2 \sqrt {x^2 + a^2} - 2 \paren {\frac {\paren {\sqrt {x^2 + a^2} }^3} 3 } + C | c = }} {{eqn | r = \frac {\paren {\sqrt {x^2 + a^2} }^3} 3 - a^2 \sqrt {x^2 + a^2} + C | c = }} {{end-eqn}} {{qed}}	0
First note that: {{begin-eqn}} {{eqn | n = 1 | l = \map {\frac {\d} {\d x} } {\sin a x - \cos a x} | r = a \paren {\cos a x + \sin a x} | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] and [[Derivative of Cosine of a x|Derivative of $\cos a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\sin a x \rd x} {\sin a x - \cos a x} | r = \int \frac {\paren {\sin a x - \cos a x + \cos a x} \rd x} {\sin a x - \cos a x} | c = }} {{eqn | r = \int \frac {\paren {\sin a x - \cos a x} \rd x} {\sin a x - \cos a x} + \int \frac {\cos a x \rd x} {\sin a x - \cos a x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \int \rd x + \int \frac {\cos a x \rd x} {\sin a x - \cos a x} | c = simplification }} {{eqn | r = \int \rd x + \int \frac {\paren {\cos a x + \sin a x - \sin a x} \rd x} {\sin a x - \cos a x} | c = }} {{eqn | r = \int \rd x + \int \frac {\paren {\cos a x + \sin a x} \rd x} {\sin a x - \cos a x} - \int \frac {\sin a x \rd x} {\sin a x - \cos a x} | c = [[Linear Combination of Integrals]] }} {{eqn | ll= \leadsto | l = 2 \int \frac {\sin a x \rd x} {\sin a x - \cos a x} | r = \int \rd x + \int \frac {\paren {\cos a x + \sin a x} \rd x} {\sin a x - \cos a x} | c = rearranging }} {{eqn | ll= \leadsto | l = \int \frac {\sin a x \rd x} {\sin a x + \cos a x} | r = \frac 1 2 \int \rd x + \frac 1 {2 a} \int \frac {a \paren {\cos a x + \sin a x} \rd x} {\sin a x - \cos a x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac x 2 + \frac 1 {2 a} \int \frac {a \paren {\cos a x + \sin a x} \rd x} {\sin a x - \cos a x} + C | c = [[Primitive of Constant]] }} {{Eqn-intertext|Then from $(1)$:}} {{eqn | r = \frac x 2 + \frac 1 {2 a} \ln \size {\sin a x - \cos a x} + C | c = [[Primitive of Function under its Derivative]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn |l = \lim_{x \mathop \to 0} \frac {\tan x} x |r = \lim_{x \mathop \to 0} \frac 1 {\cos x} \frac {\sin x} x |c = {{Defof|Tangent Function}} }} {{eqn |r = \lim_{x \mathop \to 0} \frac 1 {\cos x} \lim_{x \mathop \to 0} \frac {\sin x} x |c = [[Product Rule for Limits of Functions]] }} {{eqn |r = \lim_{x \mathop \to 0} \frac {\sin x} x |c = [[Cosine of Zero is One]] }} {{eqn |r = 1 |c = [[Limit of Sine of X over X]] }} {{end-eqn}} {{qed}}	0
:$\cot 270^\circ = \cot \dfrac {3 \pi} 2 = 0$	0
The [[Definition:Decimal Expansion|decimal expansion]] of the [[Definition:Reciprocal|reciprocal]] of $49$ contains the [[Definition:Integer Power|powers]] of $2$: :$\dfrac 1 {49} = 0 \cdotp \dot 02040 \, 81632 \, 65306 \, 12244 \, 89795 \, 91836 \, 73469 \, 38775 \, 5 \dot 1$ {{OEIS|A007450}}	0
From [[Index Laws for Semigroup/Product of Indices|Index Laws for Semigroup: Product of Indices]] we have: :$+^{z \times y} x = +^z \left({+^y x}\right)$ By definition of [[Definition:Natural Number Multiplication|multiplication]], this amounts to: :$x \times \left({z \times y}\right) = \left({x \times y}\right) \times z$ From [[Natural Number Multiplication is Commutative]], we have: :$x \times \left({z \times y}\right) = x \times \left({y \times z}\right)$ {{qed}}	0
The [[Definition:Real Number|set of real numbers]] $\R$ forms an [[Definition:Integral Domain|integral domain]] under [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]]: $\struct {\R, +, \times}$.	0
Let the [[Definition:Root of Polynomial|roots]] of $P$ be $z_1, z_2, \ldots, z_n$. Then $P$ can be written in factored form as: :$\displaystyle a_n \prod_{k \mathop = 1}^n \paren {z - z_k} = a_0 \paren {z - z_1} \paren {z - z_2} \dotsm \paren {z - z_n}$ Multiplying this out, $P$ can be expressed as: :$a_n \paren {z^n - \paren {z_1 + z_2 + \dotsb + z_n} z^{n - 1} + \dotsb + \paren {-1}^n z_1 z_2 \dotsm z_n} = 0$ where the coefficients of $z^{n - 2}, z^{n - 3}, \ldots$ are more complicated and irrelevant. Equating powers of $z$, it follows that: :$a_n \paren {-1}^n z_1 z_2 \dotsm z_n = a_0$ from which: :$z_1 z_2 \dotsm z_n = \dfrac {\paren {-1}^n a_0} {a_n}$ {{qed}}	0
Let $y \in \R: y \ne 0$. Then: {{begin-eqn}} {{eqn | l = a | o = \equiv | r = b | rr= \pmod z | c = }} {{eqn | ll= \iff | l = a \bmod z | r = b \bmod z | c = {{Defof|Congruence (Number Theory)|Congruence}} }} {{eqn | ll= \iff | l = y \left({a \bmod z}\right) | r = y \left({b \bmod z}\right) | c = Left hand implication valid only when $y \ne 0$ }} {{eqn | ll= \iff | l = \left({y a}\right) \bmod \left({y z}\right) | r = \left({y b}\right) \bmod \left({y z}\right) | c = [[Product Distributes over Modulo Operation]] }} {{eqn | l = y a | o = \equiv | r = y b | rr= \pmod {y z} | c = {{Defof|Congruence (Number Theory)|Congruence}} }} {{end-eqn}} Hence the result. Note the invalidity of the third step when $y = 0$. {{qed}}	0
The largest [[Definition:Prime Number|prime number]] with $9$ [[Definition:Digit|digits]] is $999 \, 999 \, 937$.	0
{{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\left({a x + b}\right)^2} | r = \int \frac {a x \ \mathrm d x} {a \left({a x + b}\right)^2} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $a$ }} {{eqn | r = \int \frac {\left({a x + b - b}\right) \ \mathrm d x} {a \left({a x + b}\right)^2} | c = adding and subtracting $b$ }} {{eqn | r = \frac 1 a \int \frac {\left({a x + b}\right) \ \mathrm d x} {\left({a x + b}\right)^2} - \frac b a \int \frac {\mathrm d x} {\left({a x + b}\right)^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 a \int \frac {\mathrm d x} {a x + b} - \frac b a \int \frac {\mathrm d x} {\left({a x + b}\right)^2} | c = simplification }} {{eqn | r = \frac 1 a \left({\frac 1 a \ln \left\vert{a x + b}\right\vert}\right) - \frac b a \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + C | c = [[Primitive of Reciprocal of a x + b|Primitive of Reciprocal of $\dfrac 1 {\left({a x + b}\right)}$]] }} {{eqn | r = \frac 1 a \left({\frac 1 a \ln \left\vert{a x + b}\right\vert}\right) - \frac b a \left({-\frac 1 {a \left({a x + b}\right)} }\right) + C | c = [[Primitive of Reciprocal of a x + b squared|Primitive of Reciprocal of $\dfrac 1 {\left({a x + b}\right)^2}$]] }} {{eqn | r = \frac b {a^2 \left({a x + b}\right)} + \frac 1 {a^2} \ln \left\vert{a x + b}\right\vert + C | c = simplification }} {{end-eqn}} {{qed}}	0
We have that [[Integers are Euclidean Domain]], where the [[Definition:Euclidean Valuation|Euclidean valuation]] $\nu$ is defined as: :$\map \nu x = \size x$ The result follows from [[Bézout's Lemma on Euclidean Domain]]. {{qed}}	0
:$\map \exp {2 m i \arccot p} \paren {\dfrac {p i + 1} {p i - 1} }^m = 1$	0
From [[Real Number Line is Complete Metric Space]], $\R$ under the [[Definition:Usual Metric|usual metric]] is a [[Definition:Metric Space|metric space]]. The result then follows as a special case of [[Convergent Sequence in Metric Space is Cauchy Sequence]]. {{qed}}	0
:$\cos \alpha \cos \beta = \dfrac {\map \cos {\alpha - \beta} + \map \cos {\alpha + \beta} } 2$	0
:$\csc 360 \degrees = \csc 2 \pi$ is undefined	0
[[Definition:Spence's Function|Spence's function]] has a [[Definition:Power Series|power series expansion]]: :$\displaystyle \operatorname {Li}_2 \left({z}\right) = \sum_{n \mathop = 1}^\infty \frac {z^n} {n^2}$ This [[Definition:Convergent Series|converges]] for $\left|{z}\right| \le 1$. {{explain|The domain needs to be clarified. As it stands, the notation and construction of the proof suggests $\R$.}}	0
Let $\map {B_n} x$ denote the $n$th [[Definition:Bernoulli Polynomial|Bernoulli polynomial]]. Then the [[Definition:Generating Function|generating function]] for $B_n$ is: :$\displaystyle \frac {t e^{t x} } {e^t - 1} = \sum_{k \mathop = 0}^\infty \frac {\map {B_k} x} {k!} t^k$	0
The [[Definition:Gamma Function|Gamma function]] is never equal to $0$.	0
Let us write $p_n = p \left({n}\right)$. From [[Bertrand's Conjecture]], for each $n \ge 2$ there exists a [[Definition:Prime Number|prime]] $p$ such that $n < p < 2 n$. For all $n \in \N_{>0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: : $p_n < 2^n$ $P(1)$ is the statement: :$p_1 = 2 = 2^1$ As this does not fulfil the criterion: :$p \left({n}\right) < 2^n$ it is not included in the result. === Basis for the Induction === $P(2)$ is true, as this just says: :$p_2 = 3 < 2^2 = 4$ This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$p_k < 2^k$ Then we need to show: :$p_{k+1} < 2^{k+1}$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: Suppose that $p_k < 2^k$ for some $k \ge 2$. By [[Bertrand's Conjecture]], there exists a prime $q$ such that $2^k < q < 2^{k+1}$. So: : $p < 2^k < q < 2^{k+1}$ But as $q$ is a prime exceeding $p_k$, it means: : $p_{k+1} \le q < 2^{k+1}$ So $P \left({k}\right) \implies P \left({k+1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: : $\forall n \in \N_{>1}: p \left({n}\right) < 2^n$ {{qed}} [[Category:Upper Bounds for Prime Numbers]] gshqsisc4nz55ru10a6lfuqs81lt1vg	0
From [[Sum of Geometric Sequence]]: :$\displaystyle \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$ The result follows by setting $x = 2$. {{Qed}}	0
:$\tan z = \dfrac {e^{i z} - e^{-i z} } {i \paren {e^{i z} + e^{-i z} } }$	0
Let $a \divides b$. From [[Integer Divides Zero]]: :$a \divides 0$ Thus $a$ is a [[Definition:Common Divisor of Integers|common divisor]] of $b$ and $0$. From [[Common Divisor Divides Integer Combination]]: :$\forall p, q \in \Z: a \divides \paren {p \cdot b + q \cdot 0}$ Putting $p = c$ and $q = 1$ (for example): :$a \divides \paren {c b + 0}$ Hence the result. {{qed}}	0
=== Necessary Condition === Let $n^{\size m} \notin \Z$. We have that: :$n^m = n^{u/v}$ for some $u \in\Z$ and $v \in \Z_{\ne 0}$ Then it follows from the definition of a [[Definition:Rational Power|rational power]] and the [[Existence of Positive Root of Positive Real Number|existence]] of a [[Definition:Real Number|real]] [[Definition:Root (Analysis)|$v$th root of $n^u$]] that: :$n^m = n^{u/v} = \paren {n^u}^{1/v} \in \R$ {{AimForCont}} $n^m \in \Q$. We have that: :$\size m = 0 \implies n^{\size m} \in \Z$ It follows that: :$\size m > 0$ Then: :$\size m > 0$ :$m \in \Q$ :$n^{\size m} > 0$ :$n^m \in \Q$ imply that :$n^{\size m} = n^{p/q} = \dfrac r s$ for some $\tuple {p, q, r, s} \in \Z_{>0}^4$ where $r$ and $s$ have no [[Definition:Common Divisor of Integers|common]] [[Definition:Prime Factor|prime factors]]. Raising both sides of $n^{p/q} = \dfrac r s$ to the power of $q$ yields: :$n^p = \dfrac {r^q} {s^q}$ We have that: :$\dfrac {r^q} {s^q} = n^p \in \Z$ Hence $r^q$ is [[Definition:Divisor of Integer|divisible]] by $s^q$. By the [[Fundamental Theorem of Arithmetic]], $r^q$ and $s^q$ have the same [[Definition:Prime Factor|prime factors]] as $r$ and $s$ respectively. But we have that: :$s^q \divides r^q$ and: :$s \ne 1$ imply that $r$ and $s$ have a [[Definition:Common Divisor of Integers|common]] [[Definition:Prime Factor|prime factor]]. But this would contradict the fact that $r$ and $s$ have no [[Definition:Common Divisor of Integers|common]] [[Definition:Prime Factor|prime factors]]. Therefore $s = 1$. Then: :$n^{\size m} = \dfrac r s$ and: :$s = 1$ imply that: :$r = n^{\size m} \notin \Z$ From this [[Definition:Contradiction|contradiction]] it follows that: :$n^m \notin \Q$ and since $n^m \in \R$: :$n^m \in \R \setminus \Q$ That is, $n^m$ is an [[Definition:Irrational Number|irrational number]]. {{qed|lemma}} === Sufficient Condition === Now let $n^m$ be an [[Definition:Irrational Number|irrational number]]. That is: :$n^m \in \R \setminus \Q$ Then because $n^{-m}$ is the [[Definition:Reciprocal|reciprocal]] of $n^m$: :$n^{-m} \in \R \setminus \Q$ So: :$n^{\size m} \in \R \setminus \Q$ and so since $\Z \subseteq \Q$: :$n^{\size m} \notin \Z$ {{qed}} [[Category:Number Theory]] [[Category:Irrational Numbers]] na3dwtd25a4dmxxztwcd54gbp631ts3	0
Consider the [[Definition:Real Number|set of real numbers]] $\R$ as a [[Real Number Line is Complete Metric Space|(complete) metric space]] with the [[Definition:Euclidean Metric on Real Number Line|usual (Euclidean) metric]]. Then $\R$ forms a [[Definition:Perfect Set|perfect set]].	0
Let $x, y \in \R$. Then the following are equivalent: :$(1): \quad x < y$ :$(2): \quad y - x > 0$	0
Recall the analytic definitions of [[Definition:Complex Sine Function|sine]] and [[Definition:Complex Cosine Function|cosine]]: :$\displaystyle \sin x = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {x^{2 n + 1}} {\left({2 n + 1}\right)!}$ :$\displaystyle \cos x = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {x^{2 n}} {\left({2 n}\right)!}$ Let: {{begin-eqn}} {{eqn | l = g \left({a}\right) | r = \sin \left({a + b}\right) - \sin a \cos b - \cos a \sin b }} {{eqn | l = h \left({a}\right) | r = \cos \left({a + b}\right) - \cos a \cos b + \sin a \sin b }} {{end-eqn}} Let us [[Definition:Differentiation With Respect To|differentiate these with respect to]] $a$, keeping $b$ constant. Then from [[Derivative of Sine Function]] and [[Derivative of Cosine Function]], we have: {{begin-eqn}} {{eqn | l = g' \left({a}\right) | r = \cos \left({a + b}\right) - \cos a \cos b + \sin a \sin b = h \left({a}\right) }} {{eqn | l = h' \left({a}\right) | r = - \sin \left({a + b}\right) + \sin a \cos b + \cos a \sin b = - g \left({a}\right) }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = D_a \left({\left({g \left({a}\right)}\right)^2 + \left({h \left({a}\right)}\right)^2}\right) | r = 2 g \left({a}\right) g' \left({a}\right) + 2 h \left({a}\right) h' \left({a}\right) }} {{eqn | r = 0 }} {{end-eqn}} Thus from [[Derivative of Constant]]: :$\forall a \in \R: g \left({a}\right)^2 + h \left({a}\right)^2 = c$ In particular, it is true for $a = 0$, and so: :$g \left({0}\right)^2 + h \left({0}\right)^2 = 0$ So: :$g \left({a}\right)^2 + h \left({a}\right)^2 = 0$ But from [[Square of Real Number is Non-Negative]]: :$g \left({a}\right)^2 \ge 0$ and $h \left({a}\right)^2 \ge 0$ So it follows that: :$g \left({a}\right) = 0$ and: :$h \left({a}\right) = 0$ Hence the result. {{qed}}	0
Let $x \in \R$ such that $-1 < x < 1$. Then: {{begin-eqn}} {{eqn | l = \dfrac 1 {\paren {1 + x}^3} | r = \sum_{k \mathop = 0}^\infty \paren {-1}^k \frac {\paren {k + 2} \paren {k + 1} } 2 x^k | c = }} {{eqn | r = 1 - 3 x + 6 x^2 - 10 x^3 + 15 x^4 - \cdots | c = }} {{end-eqn}}	0
:$\forall n \in \Z: \map \sin {n + \dfrac 1 2} \pi = \paren {-1}^n$	0
From [[Reciprocal times Derivative of Gamma Function]]: :$\displaystyle \dfrac {\map {\Gamma'} z} {\map \Gamma z} = -\gamma + \sum_{n \mathop = 1}^\infty \paren {\frac 1 n - \frac 1 {z + n - 1} }$ Setting $n = 1$: {{begin-eqn}} {{eqn | l = \frac {\map {\Gamma'} 1} {\map \Gamma 1} | r = -\gamma + \sum_{n \mathop = 1}^\infty \paren {\frac 1 n - \frac 1 {1 + n - 1} } | c = }} {{eqn | r = -\gamma + \sum_{n \mathop = 1}^\infty \paren {\frac 1 n - \frac 1 n} | c = }} {{eqn | r = -\gamma + 0 | c = }} {{eqn | r = -\gamma | c = }} {{end-eqn}} Using [[Gamma Function Extends Factorial]]: :$\map \Gamma 1 = \paren {1 - 1}! = 1$ Hence: :$\map {\Gamma'} 1 = -\gamma \map \Gamma 1 = -\gamma$ {{qed}}	0
{{begin-eqn}} {{eqn | o = | r = \map \bigcup {n^+} | c = }} {{eqn | r = \map \bigcup {\set n \cup n} | c = {{Defof|Von Neumann Construction of Natural Numbers}} }} {{eqn | r = \bigcup \set n \cup \bigcup n | c = [[Union Distributes over Union]] }} {{eqn | r = n \cup \bigcup n | c = [[Union of Singleton]] }} {{end-eqn}} From [[Natural Number is Superset of its Union]] we have: :$\bigcup n \subseteq n$ Then from [[Union with Superset is Superset]]: :$\bigcup n \subseteq n \iff \paren {n \cup \bigcup n} = n$ and the result follows. {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = z | r = x^2 - a^2 }} {{eqn | ll= \leadsto | l = \frac {\d z} {\d x} | r = 2 x | c = [[Power Rule for Derivatives]] }} {{eqn | ll= \leadsto | l = \int \frac {x \rd x} {\paren {\sqrt {x^2 - a^2} }^3} | r = \int \frac {x \rd z} {2 x z^{3/2} } | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 2 \int z^{-3/2} \rd z | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 2 \paren {\frac {-z^{1/2} } {\frac 1 2} } + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {\sqrt z} + C | c = simplifying }} {{eqn | r = \frac {-1} {\sqrt {x^2 - a^2} } + C | c = substituting for $z$ }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\log_b} {\frac x y} + \log_b y | r = \map {\log_b} {\frac x y \times y} | c = [[Sum of Logarithms]] }} {{eqn | r = \log_b x | c = }} {{eqn | ll= \leadsto | l = \map {\log_b} {\frac x y} | r = \log_b x - \log_b y | c = subtracting $\log_b y$ from both sides }} {{end-eqn}} {{qed}}	0
Let $\lambda \in \R \setminus \Z$ be a [[Definition:Real Number|real number]] which is not an [[Definition:Integer|integer]]. Then: :$\displaystyle \pi \csc \pi \lambda = \sum_{n \mathop = 1}^\infty \paren {-1}^n \paren {\frac 1 {n + \lambda} + \frac 1 {n - 1 - \lambda} }$	0
Let $f: \C \to \C$ be a [[Definition:Complex Function|function]] [[Definition:Meromorphic Function|meromorphic]] on some [[Definition:Region (Complex Analysis)|region]], $D$, containing $a$. Let $f$ have a single [[Definition:Pole|pole]] in $D$, of order $N$, at $a$. Then the [[Definition:Residue (Complex Analysis)|residue]] of $f$ at $a$ is given by: :$\displaystyle \Res f a = \frac 1 {\paren {N - 1}!} \lim_{z \mathop \to a} \frac { \d^{N - 1} } { \d z^{N - 1} } \paren {\paren {z - a}^N \map f z}$	0
Using the [[Axiom:Axiomatization of 1-Based Natural Numbers|following axioms]]: {{:Axiom:Axiomatization of 1-Based Natural Numbers}} === [[Left Distributive Law for Natural Numbers]] === First we show that: :$n \times \paren {x + y} = \paren {n \times x} + \paren {n \times y}$ {{:Left Distributive Law for Natural Numbers}}{{qed|lemma}} === [[Right Distributive Law for Natural Numbers]] === Then we show that: :$\paren {x + y} \times n = \paren {x \times n} + \paren {y \times n}$ {{:Right Distributive Law for Natural Numbers}}{{qed|lemma}} The result follows. {{qed}}	0
Let $z = x + i y \in \C$ be a [[Definition:Complex Number|complex number]], where $x, y \in \R$. Let $\sin z$ denote the [[Definition:Complex Sine Function|complex sine function]]. Then: :$\map \Re {\sin z} = \sin x \cosh y$ where: :$\Re z$ denotes the [[Definition:Real Part|real part]] of a [[Definition:Complex Number|complex number]] $z$ :$\sin$ denotes the [[Definition:Sine Function|sine function]] ([[Definition:Real Sine Function|real]] and [[Definition:Complex Sine Function|complex]]) :$\cosh$ denotes the [[Definition:Hyperbolic Cosine|hyperbolic cosine function]].	0
:$\displaystyle \int \frac {\d x} {p \sin a x + q \cos a x - \sqrt {p^2 + q^2} } = \frac {-1} {a \sqrt {p^2 + q^2} } \map \tan {\frac \pi 4 + \frac {a x + \arctan \frac q p} 2} + C$	0
{{proof wanted}} [[Category:Analytic Number Theory]] dcfnpn6q6rk91zwth2b331fnv9zb9ge	0
Mark off the [[Definition:Integer|integer]] $N$ being tested into groups of $3$ [[Definition:Digit|digits]]. Because of the standard way of presenting [[Definition:Integer|integers]], this may already be done, for example: :$N = 22 \, 846 \, 293 \, 462 \, 733 \, 356$ Number the groups of $3$ from the right: :$N = \underbrace{22}_6 \, \underbrace{846}_5 \, \underbrace{293}_4 \, \underbrace{462}_3 \, \underbrace{733}_2 \, \underbrace{356}_1$ Considering each group a [[Definition:Digit|$3$-digit]] [[Definition:Integer|integer]], [[Definition:Integer Addition|add]] the [[Definition:Even Integer|even]] numbered groups together, and [[Definition:Integer Subtraction|subtract]] the [[Definition:Odd Integer|odd]] numbered groups: :$22 - 846 + 293 - 462 + 733 - 356 = -616$ where the [[Definition:Sign of Number|sign]] is irrelevant. If the result is [[Definition:Divisor of Integer|divisible]] by $7$, $11$ or $13$, then so is $N$. In this case: :$616 = 2^3 \times 7 \times 11$ and so $N$ is divisible by $7$ and $11$ but not $13$.	0
Let $a, b$ be any [[Definition:Integer|integers]]. Let $\Bbb S = \set {a x + b y: x, y \in \Z}$. Then the [[Definition:Algebraic Structure|algebraic structure]]: :$\struct {\Bbb S, +, \times}$ is an [[Definition:Ideal of Ring|ideal]] of $\Z$.	0
By definition of [[Definition:Modulo Operation|modulo operation]]: :$x \bmod y := x - y \floor {\dfrac x y}$ for $y \ne 0$. We have: {{begin-eqn}} {{eqn | l = \dfrac {0 \cdotp 11} {-0 \cdotp 1} | r = \dfrac {1 \cdotp 1} {-1} | c = }} {{eqn | r = -1 \cdotp 1 | c = }} {{end-eqn}} and so: :$\floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } = -2$ Thus: {{begin-eqn}} {{eqn | l = 0 \cdotp 11 \bmod -0 \cdotp 1 | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } | c = }} {{eqn | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \paren {-2} | c = }} {{eqn | r = 0 \cdotp 11 - 0 \cdotp 2 | c = }} {{eqn | r = -0 \cdotp 09 | c = }} {{end-eqn}} {{qed}}	0
From the definition of the [[Definition:Dirichlet Convolution|Dirichlet convolution]]: :$\displaystyle \left({f * g}\right) \left({n}\right) = \sum_{a b \mathop = n} f \left({a}\right) g \left({b}\right)$ By definition, [[Definition:Arithmetic Function|arithmetic functions]] are mappings from the [[Definition:Natural Numbers|natural numbers]] $\N$ to the [[Definition:Complex Number|complex numbers]] $\C$. Thus $f \left({a}\right), g \left({b}\right) \in \C$ and commutativity follows from [[Complex Multiplication is Commutative|commutativity of multiplication of complex numbers]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = p_n | r = \dfrac {p_{n - 1} + p_{n + 1} } 2 | c = [[Definition:Balanced Prime/Definition 1|Definition 1]] }} {{eqn | ll= \iff | l = 2 p_n | r = p_{n - 1} + p_{n + 1} | c = }} {{eqn | ll= \iff | l = p_n - p_{n - 1} | r = p_{n + 1} - p_n | c = [[Definition:Balanced Prime/Definition 3|Definition 3]]: equal [[Definition:Prime Gap|prime gaps]] }} {{eqn | r = d | c = for some $d \in \Z$ }} {{eqn | ll= \iff | l = p_{n - 1} + d | r = p_n | c = [[Definition:Balanced Prime/Definition 2|Definition 2]] }} {{eqn | l = p_{n - 1} + 2 d | r = p_{n + 1} | c = }} {{end-eqn}} {{qed}} [[Category:Balanced Primes]] a8nmmveebhimoee5r36swim77fm2unp	0
Let $\struct {\Q, \tau_d}$ be the [[Definition:Rational Number Space|rational number space]] under the [[Definition:Euclidean Topology on Real Number Line|usual (Euclidean) topology]] $\tau_d$. Let $B_\alpha$ be the [[Definition:Singleton|singleton]] containing the [[Definition:Rational Number|rational number]] $\alpha$. Then: :$\displaystyle \bigcap_{\alpha \mathop \in \Q} B_\alpha^e = \R \setminus \Q$ where $B_\alpha^e$ denotes the [[Definition:Exterior (Topology)|exterior]] of $B_\alpha$ in $\R$.	0
{{begin-eqn}} {{eqn | l = \tan 225 \degrees | r = \map \tan {360 \degrees - 135 \degrees} | c = }} {{eqn | r = -\tan 135 \degrees | c = [[Tangent of Conjugate Angle]] }} {{eqn | r = 1 | c = [[Tangent of 135 Degrees|Tangent of $135 \degrees$]] }} {{end-eqn}} {{qed}}	0
From the [[Definition:Directed Smooth Curve|definition of directed smooth curve]], it follows that $\sigma_i = \gamma_i \circ \phi_i$ for all $i \in \set {1, \ldots, n}$. Here, $\phi_i: \closedint {c_i} {d_i} \to \closedint {a_i} {b_i}$ is a [[Definition:Bijection|bijective]] [[Definition:Differentiable on Interval|differentiable]] [[Definition:Strictly Increasing Real Function|strictly increasing function]]. For all $i \in set {1, \ldots, n}$, $\gamma_i$ and $\sigma_i$ are [[Definition:Continuous Complex Function|continuous]]. From [[Complex Modulus Function is Continuous]] and [[Continuity of Composite Mapping/Corollary|Continuity of Composite Mapping: Corollary]], it follows that $\size {\gamma_i'}$ and $\size {\sigma_i'}$ are [[Definition:Continuous Real Function on Subset|continuous]]. From [[Continuous Real Function is Darboux Integrable]], we find that $\displaystyle \sum_{i \mathop = 1}^n \int_{a_i}^{b_i} \size {\map {\gamma_i'} t} \rd t$ and $\displaystyle \sum_{i \mathop = 1}^n \int_{c_i}^{d_i} \size {\map {\sigma_i'} t} \rd t$ are defined. Hence, all [[Definition:Definite Integral|real integrals]] in the theorem are defined. Then: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \int_{a_i }^{b_i} \size {\map {\gamma_i'} t} \rd t | r = \sum_{i \mathop = 1}^n \int_{\map {\phi_i^{-1} } {a_i} }^{\map {\phi_i^{-1} } {b_i} } \size {\map {\gamma_i'} {\map \phi u} } \map {\phi'} u \rd u | c = [[Integration by Substitution|substitution]] with $t = \map \phi u$ }} {{eqn | r = \sum_{i \mathop = 1}^n \int_{\map {\phi_i^{-1} } {a_i} }^{\map {\phi_i^{-1} } {b_i} } \size {\map {\gamma_i'} {\map \phi u} \map {\phi'} u}\rd u | c = as $\map {\phi'} u > 0$ by [[Derivative of Monotone Function]] }} {{eqn | r = \sum_{i \mathop = 1}^n \int_{\map {\phi_i^{-1} } {a_i} }^{\map {\phi_i^{-1} } {b_i} } \size {\map {\sigma'} u} \rd u | c = [[Derivative of Complex Composite Function]] }} {{eqn | r = \sum_{i \mathop = 1}^n \int_{c_i}^{d_i} \size {\map {\sigma'} u} \rd u | c = [[Reparameterization of Directed Smooth Curve Maps Endpoints To Endpoints]] }} {{end-eqn}} {{qed}} [[Category:Contour Integration]] ijjqjztb5xqpzf8c0vs04d5aqltuys9	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\exp z$ denote the [[Definition:Complex Exponential Function|complex exponential]] of $z$. Let $\arg z$ denote the [[Definition:Argument of Complex Number|argument]] of $z$. Then: :$\map \arg {\exp z} = \set {\Im z + 2 k \pi: k \in \Z}$ where $\Im z$ denotes the [[Definition:Imaginary Part|imaginary part]] of $z$.	0
Let $m \in \Z$ be an [[Definition:Integer|integer]]. Then [[Definition:Modulo Addition|addition modulo $m$]] has an [[Definition:Identity Element|identity]]: :$\forall \eqclass x m \in \Z_m: \eqclass x m +_m \eqclass 0 m = \eqclass x m = \eqclass 0 m +_m \eqclass x m$ That is: :$\forall a \in \Z: a + 0 \equiv a \equiv 0 + a \pmod m$	0
=== [[Definition:Euler's Number/Limit of Sequence|As the Limit of a Sequence]] === {{:Definition:Euler's Number/Limit of Sequence}} === [[Definition:Euler's Number/Limit of Series|As the Limit of a Series]] === {{:Definition:Euler's Number/Limit of Series}} === [[Definition:Euler's Number/Base of Logarithm|As the Base of the Natural Logarithm]] === As the [[Definition:Base of Logarithm|base]] of the [[Definition:Natural Logarithm|Natural Logarithm]]: {{:Definition:Euler's Number/Base of Logarithm}} === [[Definition:Euler's Number/Exponential Function|In Terms of the Exponential Function]] === In terms of the [[Definition:Real Exponential Function|exponential function]]: {{:Definition:Euler's Number/Exponential Function}} === [[Definition:Euler's Number/Base of Exponential|As the Base of the Exponential with Derivative One at Zero]] === {{:Definition:Euler's Number/Base of Exponential}}	0
Note that if one of the [[Definition:Prime Number|primes]] $p_i$ does not appear in the [[Definition:Prime Decomposition|prime decompositions]] of either one of $a$ or $b$, then its corresponding index $k_i$ or $l_i$ will be [[Definition:Zero (Number)|zero]]. Let $a \divides m$. Then: :$m$ is of the form $p_1^{h_1} p_2^{h_2} \ldots p_r^{h_r}, \forall i: 1 \le i \le r, 0 \le k_i \le h_i$ :$a \divides l \iff \forall i: 1 \le i \le r, 0 \le l_i \le h_i$ So: :$a \divides m \land b \divides m \iff \forall i: 1 \le i \le r, 0 \le \max \set {k_i, l_i} \le h_i$ For $m$ to be at its smallest, we want the smallest possible exponent for each of these [[Definition:Prime Number|primes]]. So for each $i \in \closedint 1 r$, $h_i$ needs to ''equal'' $\max \set {k_i, l_i}$. Hence the result: :$\lcm \set {a, b} = p_1^{\max \set {k_1, l_1} } p_2^{\max \set {k_2, l_2} } \ldots p_r^{\max \set {k_r, l_r} }$ {{Qed}}	0
{{ProofWanted|Need to establish exactly what is to be proved}}	0
Let $m$ be a [[Definition:Repunit|repunit]] with $r$ [[Definition:Digit|digits]] such that $r > 1$. By definition, $m$ is [[Definition:Odd Integer|odd]]. Thus from [[Square Modulo 4]], if $m$ were [[Definition:Square Number|square]] it would be of the form: :$m \equiv 1 \pmod 4$. $m$ is of the form $\displaystyle \sum_{k \mathop = 0}^{r - 1} 10^k$ where $r$ is the number of [[Definition:Digit|digits]]. Thus for $r \ge 2$: {{begin-eqn}} {{eqn | l = m | r = 11 + 100 s | c = for some $s \in \Z$ }} {{eqn | r = \paren {2 \times 4} + 3 + 4 \times \paren {25 s} | c = }} {{eqn | r = 3 + 4 t | c = for some $t \in \Z$ }} {{end-eqn}} Hence: :$m \equiv 3 \pmod 4$ and so cannot be [[Definition:Square Number|square]]. {{qed}}	0
Let $m \in \Z_{> 0}$. Let $x, y, c \in \Z$. Let $x \equiv y \pmod m$. Then: :$c x \equiv c y \pmod m$	0
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{j \mathop = 0}^n 2^j \binom n j = 3^n$ $\map P 0$ is the case: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^0 2^j \binom n j | r = \dbinom 0 0 | c = }} {{eqn | r = 1 | c = }} {{eqn | r = 3^0 | c = }} {{end-eqn}} Thus $\map P 0$ is seen to hold. === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^1 2^j \binom n j | r = \dbinom 1 0 + 2 \dbinom 1 1 | c = }} {{eqn | r = 1 + 2 | c = }} {{eqn | r = 3^1 | c = }} {{end-eqn}} Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 0}^k 2^j \binom k j = 3^k$ from which it is to be shown that: :$\displaystyle \sum_{j \mathop = 0}^{k + 1} 2^j \binom {k + 1} j = 3^{k + 1}$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^{k + 1} 2^j \binom {k + 1} j | r = \binom {k + 1} 0 + \sum_{j \mathop = 1}^k 2^j \binom {k + 1} j + 2^{k + 1} \dbinom {k + 1} {k + 1} | c = separating out top and bottom indices }} {{eqn | r = 1 + \sum_{j \mathop = 1}^k 2^j \binom {k + 1} j + 2^{k + 1} | c = [[Binomial Coefficient with Zero]], [[Binomial Coefficient with Self]] }} {{eqn | r = 1 + \sum_{j \mathop = 1}^k 2^j \paren {\binom k j + \binom k {j - 1} } + 2^{k + 1} | c = [[Pascal's Rule]] }} {{eqn | r = 1 + \sum_{j \mathop = 1}^k 2^j \binom k j + 2 \sum_{j \mathop = 1}^k 2^{j - 1} \binom k {j - 1} + 2^{k + 1} | c = }} {{eqn | r = 1 + \sum_{j \mathop = 1}^k 2^j \binom k j + 2 \sum_{j \mathop = 0}^{k - 1} 2^j \binom k j + 2^{k + 1} | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = 1 + \paren {\sum_{j \mathop = 0}^k 2^j \binom k j - 2^0 \binom k 0} + 2 \paren {\sum_{j \mathop = 0}^k 2^j \binom k j - 2^k \binom k k} + 2^{k + 1} | c = rectifying the end points }} {{eqn | r = \sum_{j \mathop = 0}^k 2^j \binom k j + 2 \sum_{j \mathop = 0}^k 2^j \binom k j - 2 \times 2^k + 2^{k + 1} | c = simplifying }} {{eqn | r = 3^k + 2 \times 3^k - 2 \times 2^k + 2^{k + 1} | c = [[Sum of Sequence of Binomial Coefficients by Powers of 2#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = 3^{k + 1} | c = tidying up }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{\ge 0}: \displaystyle \sum_{j \mathop = 0}^n 2^j \binom n j = 3^n$ {{qed}}	0
If $n$ is even, then at least the last digit of $n^2$ is even. So for $n^2$ to have its digits all odd, $n$ itself must be odd. We can see immediately that $1 = 1^2$ and $9 = 3^2$ fit the criterion. Of the other 1-digit odd integers, we have $5^2 = 25, 7^2 = 49, 9^2 = 81$, all of which have an even digit. Now, let $n > 10$ be an odd integer. There are five cases to consider: * $n = 10 p + 1$: we have $\left({10p + 1}\right)^2 = 100 p^2 + 20 p + 1 = 10 \left({10 p^2 + 2 p}\right) + 1$. * $n = 10 p + 3$: we have $\left({10p + 3}\right)^2 = 100 p^2 + 60 p + 9 = 10 \left({10 p^2 + 6 p}\right) + 9$. * $n = 10 p + 5$: we have $\left({10p + 5}\right)^2 = 100 p^2 + 100 p + 25 = 10 \left({10 p^2 + 10 p + 2}\right)+ 5$. * $n = 10 p + 7$: we have $\left({10p + 7}\right)^2 = 100 p^2 + 140 p + 49 = 10 \left({10 p^2 + 14 p + 4}\right)+ 9$. * $n = 10 p + 9$: we have $\left({10p + 9}\right)^2 = 100 p^2 + 180 p + 81 = 10 \left({10 p^2 + 18 p + 8}\right)+ 1$. It is clear that in all cases the 10's digit is even. So the square of every odd integer greater than $3$ always has at least one [[Definition:Even Integer|even]] digit. Hence the result. {{qed}}	0
=== Necessary Condition === Let: :$\forall x \in \R_{\ge 1}: \left\lfloor{\log_b x}\right\rfloor = \left\lfloor{\log_b \left\lfloor{x}\right\rfloor}\right\rfloor$ Let $x = b$. Then: :$\left\lfloor{\log_b b}\right\rfloor = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor$ {{begin-eqn}} {{eqn | l = \left\lfloor{\log_b b}\right\rfloor | r = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor | c = }} {{eqn | ll= \leadsto | l = \left\lfloor{1}\right\rfloor | r = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor | c = [[Logarithm of Base]] }} {{eqn | ll= \leadsto | l = 1 | r = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor | c = [[Real Number is Integer iff equals Floor]] }} {{end-eqn}} {{AimForCont}} $b \notin \Z$. {{begin-eqn}} {{eqn | l = \left\lfloor{b}\right\rfloor | o = < | r = b | c = [[Floor of Non-Integer]] }} {{eqn | ll= \leadsto | l = \log_b \left\lfloor{b}\right\rfloor | o = < | r = 1 | c = }} {{eqn | ll= \leadsto | l = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor | o = < | r = \left\lfloor{\log_b b}\right\rfloor | c = }} {{eqn | ll= \leadsto | l = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor | o = \ne | r = \left\lfloor{\log_b b}\right\rfloor | c = which [[Definition:Contradiction|contradicts]] $\left\lfloor{\log_b b}\right\rfloor = \left\lfloor{\log_b \left\lfloor{b}\right\rfloor}\right\rfloor$ }} {{end-eqn}} Thus by [[Proof by Contradiction]]: :$b \in \Z$ But for $\log_b$ to be defined, $b > 0$ and $b \ne 1$. Hence: :$b \in \Z_{> 1}$ {{qed|lemma}} === Sufficient Condition === Let $b \in \Z_{> 1}$. Let $\left\lfloor{\log_b x}\right\rfloor = n$. Then: {{begin-eqn}} {{eqn | l = n | m = \left\lfloor{\log_b x}\right\rfloor | c = }} {{eqn | ll= \iff | l = n | o = \le | m = \log_b x | mo= < | r = n + 1 | c = [[Integer equals Floor iff Number between Integer and One More]] }} {{eqn | ll= \iff | l = b^n | o = \le | m = x | mo= < | r = b^{n + 1} | c = [[Power Function is Strictly Increasing over Positive Reals/Natural Exponent|Power Function is Strictly Increasing over Positive Reals]] }} {{eqn | ll= \iff | l = b^n | o = \le | m = \left \lfloor {x}\right \rfloor | mo= < | r = b^{n + 1} | c = [[Number not less than Integer iff Floor not less than Integer]] }} {{eqn | ll= \iff | l = n | o = \le | m = \log_b \left\lfloor{x}\right\rfloor | mo= < | r = n + 1 | c = [[Power Function is Strictly Increasing over Positive Reals/Natural Exponent|Power Function is Strictly Increasing over Positive Reals]] }} {{eqn | ll= \iff | l = n | m = \left \lfloor {\log_b \left\lfloor{x}\right\rfloor}\right \rfloor | c = [[Integer equals Floor iff Number between Integer and One More]] }} {{eqn | ll= \iff | l = \left\lfloor{\log_b x}\right\rfloor | m = \left\lfloor{\log_b \left\lfloor{x}\right\rfloor}\right\rfloor | c = Definition of $n$ }} {{end-eqn}} {{qed}}	0
'''Stirling numbers''' come in various forms. In the below: : $\delta_{n k}$ is the [[Definition:Kronecker Delta|Kronecker delta]] : $n$ and $k$ are [[Definition:Non-Negative Integer|non-negative integers]]. === [[Definition:Stirling Numbers of the First Kind/Unsigned|Unsigned Stirling Numbers of the First Kind]] === {{:Definition:Stirling Numbers of the First Kind/Unsigned}} === [[Definition:Stirling Numbers of the First Kind/Signed|Signed Stirling Numbers of the First Kind]] === {{:Definition:Stirling Numbers of the First Kind/Signed}} === [[Definition:Stirling Numbers of the Second Kind|Stirling Numbers of the Second Kind]] === {{:Definition:Stirling Numbers of the Second Kind|Stirling Numbers of the Second Kind}}	0
If $n \in E$ then $n$ is of the form $n = 2 k$ where $k \in \N$. We have that: * if the [[Definition:Characteristic Function of Set|characteristic function]] $\chi_E \left({n}\right) = 1$ then $\chi_E \left({n + 1}\right) = 0$. * if the [[Definition:Characteristic Function of Set|characteristic function]] $\chi_E \left({n}\right) = 0$ then $\chi_E \left({n + 1}\right) = 1$. So $\chi_E$ can be defined by: :$\chi_E \left({n}\right) = \begin{cases} 1 & : n = 0 \\ \overline{\operatorname{sgn}} \left({\chi_E \left({n-1}\right)}\right) & : n > 0 \end{cases}$ So $\chi_E$ is obtained by [[Definition:Primitive Recursion|primitive recursion]] from the [[Constant Function is Primitive Recursive|constant $1$]] and the [[Signum Function is Primitive Recursive|primitive recursive function $\overline{\operatorname{sgn}}$]]. Hence the result. {{qed}} [[Category:Primitive Recursive Functions]] nx2plwwvqk0tjkft9114n6rqixp076f	0
By definition, the [[Definition:Order of Group|order]] of a [[Definition:Group|group]] is the [[Definition:Cardinality|cardinality]] of its [[Definition:Underlying Set|underlying set]]. By definition, the [[Definition:Underlying Set|underlying set]] of $\struct {\Z_m, +_m}$ is the [[Definition:Set of Residue Classes|set of residue classes]] $\Z_m$: :$\Z_m = \set {\eqclass 0 m, \eqclass 1 m, \ldots, \eqclass {m - 1} m}$ From [[Cardinality of Set of Residue Classes]], $\Z_m$ has $m$ [[Definition:Element|elements]]. Hence the result. {{qed}}	0
We have that: :$\forall j: 1 \le j \le k: n_{\sigma \left({j}\right)} = \operatorname{pr}^k_{\sigma \left({j}\right)}$. Thus $h$ is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from $f$ and the [[Definition:Basic Primitive Recursive Function/Projection Function|projection functions]] $\operatorname{pr}^k_{\sigma \left({j}\right)}$. The result follows. {{qed}} It follows that if a [[Definition:Function|function]] $h$ can be obtained from known [[Definition:Primitive Recursive Function|primitive recursive functions]] by [[Definition:Primitive Recursion|primitive recursion]] where a variable other than the last one is taken as the recursion variable, then $h$ is [[Definition:Primitive Recursive Function|primitive recursive]]. [[Category:Primitive Recursive Functions]] pxm8bkuciiztzv4ivz1shvcb1041jz0	0
Let $n$ be [[Definition:Triangular Number|triangular]]. Then: :$\exists k \in \Z: n = \dfrac {k \paren {k + 1} } 2$ So: {{begin-eqn}} {{eqn | l = 49 n + 6 | r = 49 \frac {k \paren {k + 1} } 2 + 6 | c = }} {{eqn | r = \frac {49 k^2 + 49 k + 12} 2 | c = }} {{eqn | r = \frac {\paren {7 k + 3} \paren {7 k + 4} } 2 | c = }} {{end-eqn}} which is [[Definition:Triangular Number|triangular]]. {{qed}}	0
:$\displaystyle \map \beta s = \frac 1 {\map \Gamma s} \int_0^\infty \frac {x^{s - 1} e^{-x} } {1 + e^{-2 x} } \rd x$	0
:$\forall z \in \C \setminus \set {0, -1, -2, \ldots}: \map \Gamma {\overline z} = \overline {\map \Gamma z}$	0
Let $a, b, c, d \in \Z$ be [[Definition:Integer|integers]] such that: :$\dfrac a b = \dfrac {c^3} {d^3}$ Let $a$ be a [[Definition:Cube Number|cube number]]. Then $b$ is also a [[Definition:Cube Number|cube number]]. {{:Euclid:Proposition/VIII/25}}	0
From [[Bell Number as Summation over Lower Index of Stirling Numbers of the Second Kind]], we have that: :$B_n = \displaystyle \sum_{k \mathop = 0}^n {n \brace k}$ But when $n > 0$: :$\displaystyle {n \brace 0} = 0$ Hence the result. {{qed}} [[Category:Bell Numbers]] [[Category:Stirling Numbers]] ldgte9oswzxmfqn4olwei3ptsrjkkxs	0
[[Definition:Modulo Addition|Addition modulo $m$]] is [[Definition:Associative|associative]]: :$\forall \eqclass x m, \eqclass y m, \eqclass z m \in \Z_m: \paren {\eqclass x m +_m \eqclass y m} +_m \eqclass z m = \eqclass x m +_m \paren {\eqclass y m +_m \eqclass z m}$ where $\Z_m$ is the [[Definition:Integers Modulo m|set of integers modulo $m$]]. That is: :$\forall x, y, z \in \Z: \paren {x + y} + z \equiv x + \paren {y + z} \pmod m$	0
Can be performed by brute-force investigation.	0
:$5 \bmod -3 = 5$	0
Let $m \in \Z_{> 0}$. Let $x_1, x_2, y_1, y_2, c_1, c_2 \in \Z$. Let: :$x_1 \equiv y_1 \pmod m$ :$x_2 \equiv y_2 \pmod m$ Then: :$c_1 x_1 + c_2 x_2 \equiv c_1 y_1 + c_2 y_2 \pmod m$	0
The sums of the following [[Definition:Finite Sequence|sequences]] of successive [[Definition:Square Number|squares]] are themselves [[Definition:Square Number|square]]: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 7}^{29} k^2 | r = 7^2 + 8^2 + \cdots + 29^2 | c = }} {{eqn | l = \sum_{i \mathop = 7}^{39} k^2 | r = 7^2 + 8^2 + \cdots + 39^2 | c = }} {{eqn | l = \sum_{i \mathop = 7}^{56} k^2 | r = 7^2 + 8^2 + \cdots + 56^2 | c = }} {{eqn | l = \sum_{i \mathop = 7}^{190} k^2 | r = 7^2 + 8^2 + \cdots + 190^2 | c = }} {{end-eqn}}	0
Let $a, b \in \Z$. Then: :$a^3 \divides b^3 \iff a \divides b$ where $\divides$ denotes [[Definition:Divisor of Integer|integer divisibility]]. {{:Euclid:Proposition/VIII/15}}	0
{{begin-eqn}} {{eqn | l = \map G z | r = \sum_{n \mathop = 0}^\infty r z^n | c = {{Defof|Generating Function}} }} {{eqn | r = r \sum_{n \mathop = 0}^\infty z^n | c = }} {{eqn | r = \frac r {1 - z} | c = [[Sum of Infinite Geometric Sequence]] }} {{end-eqn}} for $\size z < 1$. {{qed}}	0
The equivalence class $\eqclass a m$ is defined as: :$\eqclass a m = \set {x \in \Z: x = a + k m: k \in \Z}$ That is, the set of all [[Definition:Integer|integers]] which differ from $a$ by an [[Definition:Integer Multiple|integer multiple]] of $m$. Thus the notation for addition of two [[Definition:Integers Modulo m|set of integers modulo $m$]] is not usually $\eqclass a m +_m \eqclass b m$. What is more normally seen is $a + b \pmod m$. Using this notation: {{begin-eqn}} {{eqn | l = a | o = \equiv | r = b | rr= \pmod m | c = }} {{eqn | lo= \land | l = c | o = \equiv | r = d | rr= \pmod m | c = }} {{eqn | ll= \leadsto | l = a \bmod m | r = b \bmod m | c = {{Defof|Congruence Modulo Integer}} }} {{eqn | lo= \land | l = c \bmod m | r = d \bmod m | c = }} {{eqn | ll= \leadsto | l = a | r = b + k_1 m | c = for some $k_1 \in \Z$ }} {{eqn | lo= \land | l = c | r = d + k_2 m | c = for some $k_2 \in \Z$ }} {{eqn | ll= \leadsto | l = a + c | r = b + d + \paren {k_1 + k_2} m | c = {{Defof|Integer Addition}} }} {{eqn | ll= \leadsto | l = a + c | o = \equiv | r = b + d | rr= \pmod m | c = {{Defof|Modulo Addition}} }} {{end-eqn}} {{qed}}	0
By definition of [[Definition:Modulo Operation|modulo operation]]: :$x \bmod y := x - y \left \lfloor {\dfrac x y}\right \rfloor$ for $y \ne 0$. We have: :$\dfrac 5 3 = 1 + \dfrac 2 3$ and so: :$\left\lfloor{\dfrac 5 3}\right\rfloor = 1$ Thus: :$5 \bmod 3 = 5 - 3 \times \left\lfloor{\dfrac 5 3}\right\rfloor = 5 - 3 \times 1 = 2$ {{qed}}	0
:$\dbinom 1 m_q = \delta_{0 m} + \delta_{1 m}$ That is: :$\dbinom 1 m_q = \begin{cases} 1 & : m = 0 \text { or } m = 1 \\ 0 & : \text{otherwise} \end{cases}$ where $\dbinom 1 m_q$ denotes a [[Definition:Gaussian Binomial Coefficient|Gaussian binomial coefficient]].	0
It is sufficient to consider the case $a_n = 1$: :$\displaystyle \map P x = \prod_{k \mathop = 1}^n \paren {x - z_k}$ The proof proceeds by [[Principle of Mathematical Induction|induction]]. Let $\map {\Bbb P} n$ be the statement that the identity below holds for all sets $\set {z_1, \ldots, z_n}$. {{begin-eqn}} {{eqn | l = \prod_{j \mathop = 1}^n \paren {x - z_j} | r = x^n + \sum_{j \mathop = 1}^n \paren {-1}^{n - j + 1} e_{n - j + 1} \paren {\set {z_1, \ldots, z_n} } \, x^{j - 1} }} {{eqn | r = x^n + \paren {-1} \, e_1 \paren {\set {z_1, \ldots, z_n} } \, x^{n - 1} + \paren {-1}^2 \, e_2 \paren {\set {z_1, \ldots, z_n} } \, x^{n - 2} + \cdots + \paren {-1}^n e_n \paren {\set {z_1, \ldots, z_n} } }} {{end-eqn}} [[Definition:Basis for the Induction|Basis for the Induction]]: $\map {\Bbb P} 1$ holds because $\map {e_1} {\set {z_1} } = z_1$. [[Definition:Induction Step|Induction Step]] $\map {\Bbb P} n$ implies $\map {\Bbb P} {n + 1}$: Assume $\map {\Bbb P} n$ holds and $n \ge 1$. Let for given values $\set {z_1, \ldots, z_n, z_{n + 1} }$: :$\displaystyle \map Q x = \paren {x - z_{n + 1} } \prod_{k \mathop = 1}^n \paren {x - z_k}$ Expand the right side product above using induction hypothesis $\map {\Bbb P} n$. Then $\map Q x$ equals $x^{n + 1}$ plus terms for $x^{j - 1}$, $1 \le j \le n + 1$. If $j = 1$, then one term occurs for $x^{j - 1}$: :$\displaystyle \paren {-x_{n + 1} } \, \paren {\paren {-1}^{n - 1 + 1} \map {e_{n - 1 + 1} } {\set {z_1, \ldots, z_n} } x^{1 - 1} } = \paren {-1}^{n + 1} \map {e_n} {\set {z_1, \ldots, z_n, z_{n + 1} } }$ If $2 \le j \le n + 1$, then two terms $T_1$ and $T_2$ occur for $x^{j - 1}$: {{begin-eqn}} {{eqn | l = T_1 | r = \paren x \paren {\paren {-1}^{n - j + 2} \, \map {e_{n - j + 2} } {\set {z_1, \ldots, z_n} } x^{j - 2} } }} {{eqn | l = T_2 | r = \paren {-z_{n + 1 } } \, \paren {\paren {-1}^{n - j + 1} \, \map {e_{n - j + 1} } {\set {z_1, \ldots, z_n} } x^{j - 1} } }} {{end-eqn}} The coefficient $c$ of $x^{j - 1}$ for $2 \le j \le n + 1$ is: {{begin-eqn}} {{eqn | l = c | r = \dfrac {T_1 + T_2} {x^{j - 1} } }} {{eqn | r = \paren {-1}^m \, \map {e_m} {\set {z_1, \ldots, z_n} } + \paren {-1}^m \, \map {e_{m - 1} } {\set {z_1, \ldots, z_n} } z_{n + 1} | c = where $m = n - j + 2$. }} {{end-eqn}} Use [[Elementary Symmetric Function/Examples/Recursion|recursion identity]] to simplify the expression for $c$: {{begin-eqn}} {{eqn | l = \map {e_m} {\set {z_1, \ldots, z_n, z_{n + 1} } } | r = z_{n + 1} \, \map {e_{m - 1} } {\set {z_1, \ldots, z_n} } + \map {e_m} {\set {z_1, \ldots, z_n} } }} {{eqn | ll= \leadsto | l = c | r = \paren {-1}^{n - j + 2} \, \map {e_{n - j + 2} } {\set {z_1, \ldots, z_n, z_{n + 1} } } }} {{end-eqn}} Thus $\map {\Bbb P} {n + 1}$ holds and the induction is complete. Set equal the two identities for $\map P x$: :$\displaystyle x^n + \sum_{k \mathop = 0}^{n - 1} a_k x^k = x^n + \paren {-1} \, \map {e_1} {\set {z_1, \ldots, z_n} } x^{n - 1} + \paren {-1}^2 \, \map {e_2} {\set {z_1, \ldots, z_n} } x^{n - 2} + \cdots + \paren {-1}^n \map {e_n} {\set {z_1, \ldots, z_n} }$ Linear independence of the powers $1, x, x^2, \ldots$ implies polynomial coefficients match left and right. Hence the coefficient $a_k$ of $x^k$ on the {{LHS}} matches $\paren {-1}^{n - k} \, \map {e_{n - k} } {\set {z_1, \ldots, z_n} }$ on the {{RHS}}. {{qed}} {{proofread}}	0
The smallest [[Definition:Sequence|sequence]] of [[Definition:Ordered Quadruple|quadruplets]] of consecutive [[Definition:Integer|integers]] each of which is [[Definition:Divisor of Integer|divisible]] by a [[Definition:Cube Number|cube]] greater than $1$ is: :$\tuple {22 \, 624, 22 \, 625, 22 \, 626, 22 \, 627}$	0
Let $n \in \Z_{\ge 0}$ be a [[Definition:Positive Integer|positive integer]]. Let $A_n \left({x, t}\right)$ be the [[Definition:Polynomial over Real Numbers|polynomial]] of [[Definition:Degree of Polynomial|degree $n$]] defined as: :$A_n \left({x, t}\right) := \dbinom {x - n t} n \dfrac x {x - n t}$ for $x \ne n t$. Let $z = x^{t + 1} - x^t$. Then: :$\displaystyle \sum_k A_k \left({r, t}\right) z^k = x^r$ for [[Definition:Sufficiently Small|sufficiently small]] $z$.	0
[[Proof by Counterexample]]: Let $a = 30, b = 40, r = 2, s = 10$. We have that: {{begin-eqn}} {{eqn | l = 30 | o = \equiv | r = 40 | rr= \pmod 2 | c = }} {{eqn | l = 30 | o = \equiv | r = 40 | rr= \pmod {10} | c = }} {{eqn-intertext|But note that:}} {{eqn | l = 30 | o = \not \equiv | r = 40 | rr= \pmod {20} | c = }} {{end-eqn}} {{qed}}	0
Taking the [[Definition:Group Axioms|group axioms]] in turn: === G0: Closure === By [[Composite of Permutations is Permutation]], $S$ is itself a [[Definition:Permutation|permutation]] on $S$. Thus $\struct {\map \Gamma S, \circ}$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed|lemma}} === G1: Associativity === From [[Set of all Self-Maps is Monoid]], we have that $\struct {\map \Gamma S, \circ}$ is [[Definition:Associative|associative]]. {{qed|lemma}} === G2: Identity === From [[Set of all Self-Maps is Monoid]], we have that $\struct {\map \Gamma S, \circ}$ has an [[Definition:Identity Element|identity]], that is, the [[Identity Mapping is Bijection|identity mapping]]. {{qed|lemma}} === G3: Inverses === By [[Inverse of Permutation is Permutation]], if $f$ is a [[Definition:Permutation|permutation]] of $S$, then so is its [[Definition:Inverse of Mapping|inverse]] $f^{-1}$. {{qed|lemma}} Thus all the [[Definition:Group Axioms|group axioms]] have been fulfilled, and the result follows. {{qed}}	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $S = \set {a_1, a_2, \ldots, a_p}$ be a [[Definition:Complete Residue System|complete residue system modulo $p$]]. Then for all [[Definition:Integer|integers]] $n \in \Z$ and [[Definition:Non-Negative Integer|non-negative integer]] $s \in \Z_{\ge 0}$, there exists a [[Definition:Congruence Modulo Integer|congruence]] of the form: :$n \equiv \displaystyle \sum_{j \mathop = 0}^s b_j p^j \pmod {p^{s + 1} }$ where $b_j \in S$.	0
{{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^n \left({-1}\right)^{n + 1} j \binom n j | r = \sum_{j \mathop = 1}^n \left({-1}\right)^{n + 1} j \binom n j | c = as $0 \dbinom n 0 = 0$ }} {{eqn | r = \sum_{j \mathop = 1}^n \left({-1}\right)^{n + 1} n \binom {n - 1} {j - 1} | c = [[Factors of Binomial Coefficient]] }} {{eqn | r = n \sum_{j \mathop = 0}^{n - 1} \left({-1}\right)^{n - 1} \binom {n - 1} j | c = [[Translation of Index Variable of Summation]], and $\left({-1}\right)^{n + 1} = \left({-1}\right)^{n - 1}$ }} {{eqn | r = 0 | c = [[Alternating Sum and Difference of Binomial Coefficients for Given n]] }} {{end-eqn}} {{qed}}	0
Let $s \in \R, r, t, m \in \Z_{\ge 0}$. Then: :$\displaystyle \sum_{k \mathop = 0}^r \binom {r - k} m \binom s {k - t} \paren {-1}^{k - t} = \binom {r - t - s} {r - t - m}$	0
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \left[{n + 1 \atop 1}\right] = n!$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \left[{1 \atop 1}\right] | r = \delta_{1 1} | c = [[Unsigned Stirling Number of the First Kind of 1]] }} {{eqn | r = 1 | c = {{Defof|Kronecker Delta}} }} {{eqn | r = 0! | c = {{Defof|Factorial}} }} {{end-eqn}} This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({k}\right)$ is true, where $k \ge 2$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\displaystyle \left[{k + 1 \atop 1}\right] = k!$ from which it is to be shown that: :$\displaystyle \left[{k + 2 \atop 1}\right] = \left({k + 1}\right)!$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \left[{k + 2 \atop 1}\right] | r = \left({k + 1}\right) \left[{k + 1 \atop 1}\right] + \left[{k + 1 \atop 0}\right] | c = {{Defof|Unsigned Stirling Numbers of the First Kind}} }} {{eqn | r = \left({k + 1}\right) \left[{k + 1 \atop 1}\right] + 0 | c = [[Unsigned Stirling Number of the First Kind of n+1 with 0]] }} {{eqn | r = \left({k + 1}\right) \times k! | c = [[Unsigned Stirling Number of the First Kind of n+1 with 1#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \left({k + 1}\right)! | c = {{Defof|Factorial}} }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \Z_{\ge 0}: \left[{n + 1 \atop 1}\right] = n!$ {{qed}}	0
We need to show that $\paren {\dfrac 2 3 + \sqrt {8 \paren {\dfrac n m} - 8} } - \paren {\dfrac 1 2 + \sqrt {6 \paren {\dfrac n m} - 3}} > 4$ when $\dfrac n m \ge 116$. Let $x = \dfrac n m - 1$. Then: {{begin-eqn}} {{eqn | l = \paren {\frac 2 3 + \sqrt {8 \paren {\frac n m} - 8} } - \paren {\frac 1 2 + \sqrt {6 \paren {\frac n m} - 3} } | o = > | r = 4 }} {{eqn | ll= \leadstoandfrom | l = \sqrt {8 x} - \sqrt {6 x + 3} | o = > | r = \frac {23} 6 }} {{end-eqn}} To simplify calculations, we consider: {{begin-eqn}} {{eqn | l = \sqrt {8 x} - \sqrt {6 x + 3} | o = > | r = 4 | c = which is greater than $\dfrac {23} 6$ }} {{eqn | ll= \leadstoandfrom | l = 8 x + 6 x + 3 - 2 \sqrt {48 x^2 + 24 x} | o = > | r = 16 | c = squaring Both Sides }} {{eqn | ll= \leadstoandfrom | l = \sqrt {48 x^2 + 24 x} | o = < | r = 7 x - \frac {13} 2 }} {{eqn | ll= \leadstoandfrom | l = 48 x^2 + 24 x | o = < | r = 49 x^2 - 91 x + \frac {169} 4 | c = squaring Both Sides }} {{eqn | ll= \leadstoandfrom | l = x^2 - 115 x + \frac {169} 4 | o = > | r = 0 }} {{eqn | ll= \leadstoandfrom | l = x \paren {x - 115} + \frac {169} 4 | o = > | r = 0 }} {{end-eqn}} which is true when $x \ge 115$. Thus this condition is satisfied when $\dfrac n m \ge 116$. {{qed}}	0
=== Necessary Condition === We have that $3! = 6$ and that $4$ does not [[Definition:Divisor of Integer|divide]] $6$. So in order for $n$ to [[Definition:Divisor of Integer|divide]] $n - 1$ it is [[Definition:Necessary Condition|necessary]] that $n \ne 4$. {{qed|lemma}} === Sufficient Condition === Let $n \ne 4$ be [[Definition:Composite Number|composite]]. Let $n = r s$ where: :$r, s \in \Z_{> 1}$ :$r \ne s$ :$r, s < n$ This is always possible unless $n = p^2$ for some [[Definition:Prime Number|prime number]] $p$. {{WLOG}}, let $r < s$. Then by definition of [[Definition:Factorial|factorial]]: :$\paren {n - 1}! = 1 \times 2 \times \ldots \times r \times \ldots \times s \times \ldots \times \paren {n - 2} \times \paren {n - 1}$ and so: :$n = r s \divides \paren {n - 1}!$ Let $n = p^2$. As $n \ne 4$, we have that $p \ne 2$. Hence $p > 2$. Hence: :$2 p < p^2$ and so: :$2 p \divides \paren {n - 1}!$ By definition of [[Definition:Factorial|factorial]]: :$\paren {n - 1}! = 1 \times 2 \times \ldots \times p \times \ldots \times 2 p \times \ldots \times \paren {n - 2} \times \paren {n - 1}$ and so: :$n = p^2 \divides \paren {n - 1}!$ Hence the result. {{qed}}	0
Let $x \in \Z$ be an [[Definition:Integer|integer]]. Then one of the following holds: {{begin-eqn}} {{eqn | l = x^2 | o = \equiv | r = 0 \pmod 3 | c = }} {{eqn | l = x^2 | o = \equiv | r = 1 \pmod 3 | c = }} {{end-eqn}}	0
It can easily be seen that $S_k$ is a [[Definition:Total Function|total function]]. Suppose $e = \gamma \left({P}\right)$ for some [[Definition:URM Program|URM program]] $P$. At stage $0$, we are about to carry out [[Definition:Unlimited Register Machine#Basic Instructions|instruction]] $1$ with the [[Definition:Unlimited Register Machine#Input|input]] $\left({n_1, n_2, \ldots, n_k}\right)$. So we have: :$S_k \left({e, n_1, n_2, \ldots, n_k, 0}\right) = \begin{cases} 2^1 3^{n_1} 5^{n_2} \cdots p_{k+1}^{n_k} & : e \in \operatorname{Prog} \\ 0 & : \text{otherwise} \end{cases}$ where $\operatorname{Prog}$ is the set of code numbers of all [[Definition:URM Program|URM programs]]. We see that $S_k \left({e, n_1, n_2, \ldots, n_k, 0}\right)$ does not actually depend upon the actual program being run, beyond the fact that it matters whether it actually ''is'' a program or not. Now [[Set of Codes for URM Programs is Primitive Recursive|$\operatorname{Prog}$ is a primitive recursive set]]. So from [[:Category:Primitive Recursive Functions|results about primitive recursive functions]], the relations defining the cases are primitive recursive. We can also deduce from various [[:Category:Primitive Recursive Functions|results about primitive recursive functions]] that the functions given by the formulas $2^1 3^{n_1} 5^{n_2} \cdots p_{k+1}^{n_k}$ and $0$ are [[Definition:Primitive Recursive Function|primitive recursive]]. In particular, we use the results: * [[Multiplication is Primitive Recursive]]; * [[Exponentiation is Primitive Recursive]]; * [[Prime Enumeration Function is Primitive Recursive]]. So from [[Definition by Cases is Primitive Recursive]], $S_k \left({e, n_1, n_2, \ldots, n_k, 0}\right)$ is [[Definition:Primitive Recursive Function|primitive recursive]]. Now we want to be able to express $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right)$ in terms of $e, \left({n_1, n_2, \ldots, n_k}\right), t$ and $S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ using known [[Definition:Primitive Recursive Function|primitive recursive functions]]. We need to consider a number of cases: #$e$ does not code a URM program; #$e = \gamma \left({P}\right)$ and the computation halts at stage $t$ or earlier; #$e = \gamma \left({P}\right)$ and the instruction to be carried out at stage $t$ is a <tt>Zero</tt> instruction; #$e = \gamma \left({P}\right)$ and the instruction to be carried out at stage $t$ is a <tt>Successor</tt> instruction; #$e = \gamma \left({P}\right)$ and the instruction to be carried out at stage $t$ is a <tt>Copy</tt> instruction; #$e = \gamma \left({P}\right)$ and the instruction to be carried out at stage $t$ is a <tt>Jump</tt> instruction. These cases are clearly mutually exclusive and exhaustive. First we need to check that each case corresponds to a [[Definition:Primitive Recursive Relation|primitive recursive relation]]. * The set $\operatorname{Prog}$ is primitive recursive so its [[Complement of Primitive Recursive Set|complement is also primitive recursive]]. So 1. is a [[Definition:Primitive Recursive Relation|primitive recursive relation]]. * So we have that $e$ codes a URM program. Call that program $P$. From the definition of [[Unique Code for State of URM Program|state code]], we see that if a computation halts at stage $t$ or earlier, then the number of the instruction to be carried out at stage $t$ is greater than the number of instructions in $P$. From the definition of the [[Unique Code for URM Program|code number]] of $P$, the number of instructions in $P$ is $\operatorname{len} \left({e}\right)$ where $\operatorname{len} \left({e}\right)$ is the [[Definition:Length of an Integer|length of $e$]], which is [[Length Function is Primitive Recursive|primitive recursive]]. Now let $r = S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$. Let $\left({r}\right)_j$ be defined as the [[Definition:Prime Exponent Function|prime exponent function]]. By the definition of the [[Unique Code for State of URM Program|state code]], the number of the instruction to be carried out at stage $t$ is $\left({r}\right)_1$, which is [[Prime Exponent Function is Primitive Recursive|primitive recursive]]. So 2. can be expressed as: :$e \in \operatorname{Prog} \text { and } \left({S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)}\right)_1 > \operatorname{len} \left({e}\right)$ Both $\operatorname{Prog}$ and $\left({r}\right)_1$ are primitive recursive, so from [[Set Operations on Primitive Recursive Relations]], 2. is a [[Definition:Primitive Recursive Relation|primitive recursive relation]]. * So, let the number of the instruction to be carried out at stage $t$ be $a = \left({S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)}\right)_1$. From the definition of the [[Unique Code for URM Program|code number]] of $P$, the [[Unique Code for URM Instruction|code number of this instruction]] is $\left({e}\right)_a$. Now from [[Set of Codes for URM Instructions is Primitive Recursive]], each of the sets $\operatorname{Zinstr}$, $\operatorname{Sinstr}$, $\operatorname{Cinstr}$ and $\operatorname{Jinstr}$ are [[Definition:Primitive Recursive Set|primitive recursive]]. So each of 3. to 6. above can be expressed as: :$e \in \operatorname{Prog} \text { and } a \le \operatorname{len} \left({e}\right) \text { and } \left({e}\right)_a \in \operatorname{Instr}$ and is a [[Definition:Primitive Recursive Relation|primitive recursive relation]]. So relations 1. to 6. are all [[Definition:Primitive Recursive Relation|primitive recursive]]. Now we need to show how, in each case, $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right)$ can be obtained from $e, \left({n_1, n_2, \ldots, n_k}\right), t$ and $S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ using known [[Definition:Primitive Recursive Function|primitive recursive functions]]. First, if $e$ does not code a URM program then $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right) = 0$, which is primitive recursive. Second, we have adopted the convention that if $P$ has halted, then $S_k$ does not change. So if $P$ halts at or before stage $t$, we have that $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right) = S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ Next, we look at the individual commands. As an example we will investigate the <tt>Successor</tt> command. The others are treated similarly. Suppose the instruction to be carried out at stage $t$ is a <tt>Successor</tt> command. We know that the [[Unique Code for URM Instruction|code number]] $c$ is given by $c = \left({e}\right)_a$ where $a = \left({S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)}\right)_1$. Suppose the instruction is $S \left({n}\right)$. Then $c = 6 n$. So $n = \operatorname{quot} \left({6, n}\right)$ which is recursive from [[Quotient is Primitive Recursive]]. This instruction adds $1$ to the number in $R_n$. This increases the exponent $p_{n+1}$ in the state code by $1$. This is achieved by multiplying $S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ by $p \left({n+1}\right)$, where $p \left({n+1}\right)$ is the [[Definition:Prime Enumeration Function|prime enumeration function]] which is [[Prime Enumeration Function is Primitive Recursive|primitive recursive]]. Since the instruction to be carried out at stage $t$ is a <tt>Successor</tt> the instruction number at stage $t+1$ is $a+1$ so the factor $2^a$ in $S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ is replaced by $2^{a+1}$. So: :$S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right) = 2 \times p_{n+1} \times S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)$ where $n = \operatorname{quot} \left({6, n}\right)$, $c = \left({e}\right)_a$ and $a = \left({S_k \left({e, n_1, n_2, \ldots, n_k, t}\right)}\right)_1$. This is the required expression for $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right)$ obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from [[Definition:Primitive Recursive Function|primitive recursive functions]]. The proofs for $\operatorname{Zinstr}$, $\operatorname{Cinstr}$ and $\operatorname{Jinstr}$ are along the same lines. In each case, the value of $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right)$ can be obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from [[Definition:Primitive Recursive Function|primitive recursive functions]] (but I'd hate to have to do the calculations on my fingers). Thus by [[Definition by Cases is Primitive Recursive]], $S_k \left({e, n_1, n_2, \ldots, n_k, t+1}\right)$ is [[Definition:Primitive Recursive Function|primitive recursive]]. Hence $S_k$ is defined by [[Definition:Primitive Recursion|primitive recursion]] from functions known to be primitive recursive. Hence the result. {{qed}}	0
It is taken for granted that the [[Definition:Gamma Function|Gamma function]] [[Definition:Strictly Increasing Mapping|increases monotonically]] on $\R_{\ge 1}$. {{handwaving|Replace the above with a link to a proof}} We begin with an inequality that can easily be verified using cross multiplication. {{handwaving|... so verify it.}} Let $x$ be a [[Definition:Real Number|real number]] between $0$ and $1$. Let $n$ is a [[Definition:Positive Integer|positive integer]]. Then: :$\displaystyle \frac {\log \Gamma \left({n - 1}\right) - \log \Gamma \left({n}\right)} {\left({n - 1}\right) - n} \le \frac {\log \Gamma \left({x + n}\right) - \log \Gamma \left({n}\right)} {\left({x + n}\right) - n} \le \frac {\log \Gamma \left({n + 1}\right) - \log \Gamma \left({n}\right)}{\left({n + 1}\right) - n}$ Since n is a [[Definition:Positive Integer|positive integer]], we can make use of the identity: :$\Gamma \left({n}\right) = \left({n - 1}\right)!$ Simplifying, we get: :$\log \left({n - 1}\right) \le \dfrac {\log \Gamma \left({x + n}\right) - \log \left({\left({n - 1}\right)!}\right)} x \le \log \left({n}\right)$ We now make use of the identity: :$\displaystyle \Gamma \left({x + n}\right) = \prod_{k \mathop = 1}^n \left({x + n - k}\right) \Gamma \left({x}\right)$ along with the fact that the [[Gamma Function is Log-Convex]], to simplify the inequality: :$\displaystyle \left({n - 1}\right)^x \left({n - 1}\right)! \prod_{k \mathop = 1}^n \left({x + n - k}\right)^{-1} \le \Gamma \left({x}\right) \le n^x \left({n - 1}\right)!\prod_{k \mathop = 1}^n \left({x + n - k}\right)^{-1}$ Taking the limit as $n$ goes to infinity and using the [[Squeeze Theorem]]: :$\displaystyle \Gamma \left({x}\right) = \lim_{n \mathop \to \infty} n^x n! \prod_{k \mathop = 0}^n \left({x + n - k}\right)^{-1}$ which is another representation of [[Definition:Gamma Function/Euler Form|Euler's form]]. This proves equivalence for $x$ between $0$ and $1$. The result follows from the [[Gamma Difference Equation]]. {{qed}}	0
:$\displaystyle \forall n \in \N: \sum_{i \mathop = 1}^n i^2 = \frac {n \paren {n + 1} \paren {2 n + 1} } 6$	0
By definition of [[Definition:Modulo 0|modulo $0$]]: :$\forall x \in \R: x \bmod 0 = x$ Hence: :$-100 \bmod 0 = -100$ {{qed}}	0
Let $a_1, a_2, a_3, \ldots$ be the sequence: :$\left \langle{a_n}\right \rangle = 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, \ldots$ Then: :$a_n = \left \lceil{\dfrac {\sqrt {1 + 8 n} - 1} 2}\right \rceil$	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $p$ be a [[Definition:Prime Number|prime number]]. Let $n$ be expressed in [[Definition:Number Base|base $p$ representation]]. Let $r$ be the [[Definition:Digit Sum|digit sum]] of the representation of $n$ in [[Definition:Number Base|base $p$]]. Then $n!$ is [[Definition:Divisor of Integer|divisible]] by $p^\mu$ but not by $p^{\mu + 1}$, where: :$\mu = \dfrac {n - r} {p - 1}$	0
The proof proceeds by [[Second Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \forall m \in \Z_{\ge 0}: \sum_k \left\{ {n \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{n - k} = \delta_{m n}$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \sum_k \left\{ {0 \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{0 - k} | r = \sum_k \delta_{k 0} \left[{k \atop m}\right] \left({-1}\right)^{-k} | c = Definition of [[Definition:Stirling Numbers of the Second Kind|Stirling Numbers of the Second Kind]] }} {{eqn | r = \left[{0 \atop m}\right] | c = as all other terms vanish by $\delta_{k 0}$ }} {{eqn | r = \delta_{m 0} | c = Definition of [[Definition:Unsigned Stirling Numbers of the First Kind|Unsigned Stirling Numbers of the First Kind]] }} {{end-eqn}} Thus $P \left({0}\right)$ has been shown to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({r}\right)$ is true for all $r \ge 0$, then it logically follows that $P \left({r + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\displaystyle \forall m \in \Z_{\ge 0}: \sum_k \left\{ {r \atop k}\right\}\left[{k \atop m}\right] \left({-1}\right)^{r - k} = \delta_{m r}$ from which it is to be shown that: :$\displaystyle \forall m \in \Z_{\ge 0}: \sum_k \left\{ { {r + 1} \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} = \delta_{m \left({r + 1}\right)}$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | r = \sum_k \left\{ { {r + 1} \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} | o = | c = }} {{eqn | r = \sum_k \left({k \left\{ {r \atop k}\right\} + \left\{ {r \atop k - 1}\right\} }\right) \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} | c = Definition of [[Definition:Stirling Numbers of the Second Kind|Stirling Numbers of the Second Kind]] }} {{eqn | r = \sum_k k \left\{ {r \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} + \sum_k \left\{ {r \atop k - 1}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | r = \sum_k k \left\{ {r \atop k}\right\} \frac 1 k \left({\left[{k + 1 \atop m}\right] - \left[{k \atop m - 1}\right]}\right) \left({-1}\right)^{r + 1 - k} | c = Definition of [[Definition:Unsigned Stirling Numbers of the First Kind|Unsigned Stirling Numbers of the First Kind]] }} {{eqn | o = | r = + \sum_k \left\{ {r \atop k - 1}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | r = \sum_k \left\{ {r \atop k}\right\} \left[{k + 1 \atop m}\right] \left({-1}\right)^{r + 1 - k} - \sum_k \left\{ {r \atop k}\right\} \left[{k \atop m - 1}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | o = | r = + \sum_k \left\{ {r \atop k - 1}\right\} \left[{k \atop m}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | r = \sum_k \left\{ {r \atop k}\right\} \left[{k + 1 \atop m}\right] \left({-1}\right)^{r + 1 - k} - \sum_k \left\{ {r \atop k}\right\} \left[{k \atop m - 1}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | o = | r = + \sum_k \left\{ {r \atop k}\right\} \left[{k + 1 \atop m}\right] \left({-1}\right)^{r + 1 - k + 1} | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \sum_k \left\{ {r \atop k}\right\} \left[{k + 1 \atop m}\right] \left({-1}\right)^{r + 1 - k} - \sum_k \left\{ {r \atop k}\right\} \left[{k \atop m - 1}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | o = | r = - \sum_k \left\{ {r \atop k}\right\} \left[{k + 1 \atop m}\right] \left({-1}\right)^{r + 1 - k} | c = }} {{eqn | r = \sum_k \left\{ {r \atop k}\right\} \left[{k \atop m - 1}\right] \left({-1}\right)^{r - k} | c = Simplification }} {{eqn | r = \delta_{\left({m - 1}\right) r} | c = [[Second Inversion Formula for Stirling Numbers#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \delta_{m \left({r + 1}\right)} | c = Definition of [[Definition:Kronecker Delta|Kronecker Delta]] }} {{end-eqn}} So $P \left({r}\right) \implies P \left({r + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall m, n \in \Z_{\ge 0}: \sum_k \left\{ {n \atop k}\right\} \left[{k \atop m}\right] \left({-1}\right)^{n - k} = \delta_{m n}$ {{qed}}	0
From [[Binomial Coefficient expressed using Beta Function]]: :$(1): \quad \dbinom r k \dbinom {r - \frac 1 2} k = \dfrac 1 {\left({r + 1}\right) \Beta \left({k + 1, r - k + 1}\right) \left({r + \frac 1 2}\right) \Beta \left({k + 1, r - k + \frac 1 2}\right)}$ Then: {{begin-eqn}} {{eqn | l = \dbinom r {k + 1} \dbinom {r - \frac 1 2} {k + 1} | r = \dfrac 1 {\left({r + 1}\right) \Beta \left({k + 2, r - k}\right) \left({r + \frac 1 2}\right) \Beta \left({k + 2, r - k - \frac 1 2}\right)} | c = }} {{eqn | r = \dfrac 1 {\left({r + 1}\right) \frac {k + 1} {r + 1} \Beta \left({k + 1, r - k}\right) \left({r + \frac 1 2}\right) \frac {k + 1} {r + \frac 1 2} \Beta \left({k + 1, r - k - \frac 1 2}\right)} | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac 1 {\left({r + 1}\right) \frac {k + 1} {r + 1} \frac {r + 1} {r - k} \Beta \left({k + 1, r - k + 1}\right) \left({r + \frac 1 2}\right) \frac {k + 1} {r + \frac 1 2} \frac {r + \frac 1 2} {r - k - \frac 1 2} \Beta \left({k + 1, r - k + \frac 1 2}\right)} | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac {\left({r - k}\right) \left({r - k - \frac 1 2}\right)} {\left({k + 1}\right)^2} \times \frac 1 {\left({r + 1}\right) \Beta \left({k + 1, r - k + 1}\right) \left({r + \frac 1 2}\right) \Beta \left({k + 1, r - k + \frac 1 2}\right)} | c = simplifying }} {{eqn | n = 2 | r = \dfrac {\left({r - k}\right) \left({r - k - \frac 1 2}\right)} {\left({k + 1}\right)^2} \dbinom r k \dbinom {r - \frac 1 2} k | c = from $(1)$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \dbinom {2 r} {2 k + 2} \dbinom {2 k + 2} {2 k + 2} | r = \dfrac 1 {\left({2 r + 1}\right) \Beta \left({2 k + 3, 2 r - 2 k - 1}\right) \left({2 k + 3}\right) \Beta \left({k + 2, k + 2}\right)} | c = }} {{eqn | r = \dfrac 1 {\left({2 r + 1}\right) \frac {2 k + 2} {2 r + 1} \Beta \left({2 k + 2, 2 r - 2 k - 1}\right) \left({2 k + 3}\right) \frac {k + 1} {2 k + 3} \Beta \left({k + 1, k + 2}\right)} | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac 1 {\left({2 k + 2}\right) \Beta \left({2 k + 2, 2 r - 2 k - 1}\right) \left({k + 1}\right) \Beta \left({k + 1, k + 2}\right)} | c = simplification }} {{eqn | r = \dfrac 1 {\left({2 k + 2}\right) \frac {2 k + 1} {2 r} \Beta \left({2 k + 1, 2 r - 2 k - 1}\right) \left({k + 1}\right) \frac {k + 1} {2 k + 2} \Beta \left({k + 1, k + 1}\right)} | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac {2 r} {\left({k + 1}\right)^2 \Beta \left({2 k + 1, 2 r - 2 k - 1}\right) \left({2 k + 1}\right) \Beta \left({k + 1, k + 1}\right)} | c = simplifying }} {{eqn | r = \dfrac {2 r} {\left({k + 1}\right)^2 \Beta \left({2 k + 1, 2 r - 2 k - 1}\right)} \binom {2 k} k | c = [[Binomial Coefficient expressed using Beta Function]] }} {{eqn | r = \dfrac {2 r} {\left({k + 1}\right)^2 \frac {2 r} {2 r - 2 k - 1} \Beta \left({2 k + 1, 2 r - 2 k}\right)} \binom {2 k} k | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac {2 r - 2 k - 1} {\left({k + 1}\right)^2 \Beta \left({2 k + 1, 2 r - 2 k}\right)} \binom {2 k} k | c = simplifying }} {{eqn | r = \dfrac {2 r - 2 k - 1} {\left({k + 1}\right)^2 \frac {2 r + 1} {2 r - 2 k} \Beta \left({2 k + 1, 2 r - 2 k + 1}\right)} \binom {2 k} k | c = [[Beta Function of x with y+1 by x+y over y|Beta Function of $x$ with $y+1$ by $\dfrac {x+y} y$]] }} {{eqn | r = \dfrac {\left({2 r - 2 k}\right) \left({2 r - 2 k - 1}\right)} {\left({k + 1}\right)^2 \left({2 r + 1}\right) \Beta \left({2 k + 1, 2 r - 2 k + 1}\right)} \binom {2 k} k | c = simplifying }} {{eqn | r = \dfrac {\left({2 r - 2 k}\right) \left({2 r - 2 k - 1}\right)} {\left({k + 1}\right)^2} \dbinom {2 r} {2 k} \binom {2 k} k | c = [[Binomial Coefficient expressed using Beta Function]] }} {{eqn | r = \dfrac {4 \left({r - k}\right) \left({r - k - \frac 1 2}\right)} {\left({k + 1}\right)^2} \dbinom {2 r} {2 k} \binom {2 k} k | c = }} {{end-eqn}} {{finish}}	0
Let $\EE$ be an [[Definition:Experiment|experiment]] with a [[Definition:Probability Space|probability space]] $\struct {\Omega, \Sigma, \Pr}$. Let $\Omega$ be a [[Definition:Finite Set|finite set]]. Then the [[Definition:Event Space|event space]] $\Sigma$ consists of an [[Definition:Even Integer|even number]] of [[Definition:Subset|subsets]] of $\Omega$.	0
The largest [[Definition:Positive Integer|positive integer]] which cannot be expressed as the [[Definition:Integer Addition|sum]] of the [[Definition:Square (Algebra)|squares]] of [[Definition:Distinct|distinct]] [[Definition:Prime Number|prime numbers]] is $17 \, 163$.	0
Let $x = \floor x$. As $\floor x \in \Z$, then so must $x$ be. Now let $x \in \Z$. We have: :$\floor x = \sup \set {m \in \Z: m \le x}$ As $x \in \sup \set {m \in \Z: m \le x}$, and there can be no greater $n \in \Z$ such that $n \in \sup \set {m \in \Z: m \le x}$, it follows that: :$x = \floor x$ {{qed}}	0
Let $n \ge 5$ be an [[Definition:Integer|integer]]. Then the $n$th [[Definition:Alternating Group|alternating group]] $A_n$ is [[Definition:Simple Group|simple]].	0
Let $p \left({j}\right)$ be the [[Definition:Prime Enumeration Function|prime enumeration function]]. For $n \ne 0$ and $j \ne 0$, we see that $\left({n}\right)_j$ is the largest value of $k$ for which $p \left({j}\right)^k$ is a [[Definition:Divisor of Integer|divisor]] of $n$. Thus $\left({n}\right)_j$ is the ''smallest'' value of $k$ for which $p \left({j}\right)^{k+1}$ is ''not'' a [[Definition:Divisor of Integer|divisor]] of $n$. We note that if $r \ge n$ and $j \ne 0$, we have $p \left({j}\right)^r \ge 2^r \ge 2^n> n$. Thus $n$ is a (generous) [[Definition:Upper Bound of Mapping|upper bound]] of $\left({n}\right)_j$. The condition that $p \left({j}\right)^{k+1}$ is not a [[Definition:Divisor of Integer|divisor]] of $n$ can be expressed as: :$\operatorname{div} \left({n, p \left({j}\right)^{k+1}}\right) = 0$ where: : [[Divisor Relation is Primitive Recursive|$\operatorname{div}$ is primitive recursive]] : The [[Equality Relation is Primitive Recursive]] : [[Prime Enumeration Function is Primitive Recursive|$p \left({j}\right)$ is primitive recursive]] : [[Exponentiation is Primitive Recursive]] : [[Addition is Primitive Recursive]]. So we see that the relation: :$\mathcal R \left({n, j, k}\right) \iff \operatorname{div} \left({n, p \left({j}\right)^{k+1}}\right) = 0$ is [[Definition:Primitive Recursive Relation|primitive recursive]]. From [[Bounded Minimization is Primitive Recursive]], we also see that: :$\left({n}\right)_j = \begin{cases} \mu k \le n \mathcal R \left({n, j, k}\right) & : n \ne 0 \land j \ne 0 \\ 0 & : \text{otherwise} \end{cases}$ is [[Definition:Primitive Recursive Function|primitive recursive]]. The result follows. {{qed}} [[Category:Primitive Recursive Functions]] r4rtulcl7hz4vjh4k8lh3amsarwife2	0
{{begin-eqn}} {{eqn | l = \ln \left({G \left({z}\right)}\right) | r = \sum_{k \mathop \ge 1} \left({-1}\right)^{k + 1} \dfrac {S_k z^k} k | c = }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = a | o = \equiv | r = b | rr= \pmod z | c = }} {{eqn | l = c | o = \equiv | r = d | rr= \pmod z | c = }} {{eqn | ll= \leadsto | l = a \bmod z | r = b \bmod z | c = {{Defof|Congruence (Number Theory)|Congruence}} }} {{eqn | l = x \bmod z | r = y \bmod z | c = }} {{eqn | ll= \leadsto | lo= \exists k_1 \in \Z: | l = a - b | r = k_1 z | c = }} {{eqn | lo= \exists k_2 \in \Z: | l = x - y | r = k_2 z | c = }} {{eqn | ll= \leadsto | l = \paren {a + x} - \paren {b + y} | r = \paren {k_1 + k_2} z | c = {{Defof|Integer Addition}} }} {{eqn | ll= \leadsto | l = a + x | o = \equiv | r = b + y | rr= \pmod z | c = {{Defof|Congruence (Number Theory)|Congruence}} }} {{end-eqn}} {{qed}}	0
:$18 \bmod 3 = 0$	0
The number $1091$ has the property that: :$x^6 + 1091$ is [[Definition:Composite Number|composite]] for all [[Definition:Integer|integer]] values of $x$ from $1$ to $3905$.	0
Let $a$ be an [[Definition:Even Integer|even]] [[Definition:Perfect Number|perfect number]]. From the [[Theorem of Even Perfect Numbers]], $a$ is in the form $2^{p - 1} \paren {2^p - 1}$ where $2^p - 1$ is [[Definition:Prime Number|prime]]. Thus: {{begin-eqn}} {{eqn | l = a | r = \paren {2^p - 1} 2^{p - 1} | c = }} {{eqn | r = 2^{p - 1} \paren {2 \times 2^{p - 1} - 1} | c = }} {{eqn | r = n \paren {2 n - 1} | c = where $n = 2^{p - 1}$ }} {{end-eqn}} The result follows from [[Closed Form for Hexagonal Numbers]]. {{qed}}	0
Let $n \in \Z$ be an [[Definition:Integer|integer]]. Then $n$ can be expressed as the [[Definition:Integer Addition|sum]] of $5$ [[Definition:Cube Number|cubes]] (either [[Definition:Positive Integer|positive]] or [[Definition:Negative Integer|negative]]) in an [[Definition:Infinite|infinite]] number of ways.	0
Let $H$ be a [[Definition:Subgroup|subgroup]] of $\struct {\Z_m, +_m}$ Suppose: : $(1): \quad h + \ideal m \in H$, where $\ideal m$ is a [[Definition:Principal Ideal of Ring|principal ideal]] of $\struct {\Z_m, +_m, \times_m}$ and : $(2): \quad n \in \N_{>0}$. Then by definition of [[Definition:Integer Multiplication|multiplication on integers]] and [[Homomorphism of Powers/Integers|Homomorphism of Powers as applied to integers]]: {{begin-eqn}} {{eqn | l = \paren {n + \ideal m} \times \paren {h + \ideal m} | r = \map {q_m} n \times \map {q_m} h | c = where $q_m$ is the [[Definition:Quotient Mapping|quotient mapping]] }} {{eqn | r = \map {q_m} {n \times h} | c = }} {{eqn | r = \map {q_m} {n \cdot h} | c = }} {{eqn | r = n \cdot \map {q_m} h | c = }} {{end-eqn}} But: :$n \cdot \map {q_m} h \in \gen {\map {q_m} h}$ where $\gen {\map {q_m} h}$ is the [[Definition:Generator of Cyclic Group|group generated by $\map {q_m} h$]]. Hence by [[Epimorphism from Integers to Cyclic Group]], $n \cdot \map {q_m} h \in H$. The result follows. {{qed}}	0
We will show that: {{begin-eqn}} {{eqn | l = 1375 | r = 11 \times 5^3 | c = }} {{eqn | l = 1376 | r = 172 \times 2^3 | c = }} {{eqn | l = 1377 | r = 51 \times 3^3 | c = }} {{end-eqn}} is the smallest such [[Definition:Ordered Triple|triplet]]. Each number in such [[Definition:Ordered Triple|triplets]] of consecutive [[Definition:Integer|integers]] is [[Definition:Divisor of Integer|divisible]] by a [[Definition:Cube Number|cube]] of some [[Definition:Prime Number|prime number]]. Only $2, 3, 5, 7, 11$ are less than $\sqrt [3] {1377}$. Since the numbers involved are small, we can check the result by brute force. For general results one is encouraged to use the [[Chinese Remainder Theorem]]. === Case $1$: a number is divisible by $11^3$ === The only multiple of $11^3$ less than $1377$ is $1331$, and: {{begin-eqn}} {{eqn | l = 1330 | r = 2 \times 5 \times 7 \times 19 }} {{eqn | l = 1332 | r = 2^2 \times 3^2 \times 37 }} {{end-eqn}} Since neither $1330$ nor $1332$ are [[Definition:Divisor of Integer|divisible]] by a [[Definition:Cube Number|cube]] of some [[Definition:Prime Number|prime number]], $1331$ is not in a [[Definition:Ordered Triple|triplet]]. {{qed|lemma}} === Case $2$: a number is divisible by $7^3$ === The only multiples of $7^3$ less than $1377$ are $343, 686, 1029, 1372$, and: {{begin-eqn}} {{eqn | l = 342 | r = 2 \times 3^2 \times 19 }} {{eqn | l = 344 | r = 2^3 \times 43 }} {{eqn | l = 345 | r = 3 \times 5 \times 23 }} {{eqn | l = 685 | r = 5 \times 137 }} {{eqn | l = 687 | r = 3 \times 229 }} {{eqn | l = 1028 | r = 2^2 \times 257 }} {{eqn | l = 1030 | r = 2 \times 5 \times 103 }} {{eqn | l = 1371 | r = 3 \times 457 }} {{eqn | l = 1373 | o = \text {is} | r = \text {prime} }} {{end-eqn}} Hence none of these numbers is in a [[Definition:Ordered Triple|triplet]]. {{qed|lemma}} === Case $3$: the numbers are divisible by $2^3, 3^3, 5^3$ respectively === Let $n = k \times 5^3$. We show that $k$ cannot be [[Definition:Divisor of Integer|divisible]] by $3$ or $4$. Suppose $3 \divides k$. Then none of $n \pm 1, n \pm 2$ are [[Definition:Divisor of Integer|divisible]] by $3$, and consequently $3^3$. Suppose $4 \divides k$. Then none of $n \pm 1, n \pm 2$ are [[Definition:Divisor of Integer|divisible]] by $4$, and consequently $2^3$. The only multiples of $5^3$ less than $1377$ are $125, 250, 375, 500, 625, 750, 875, 1000, 1125, 1250, 1375$, and we eliminate $375, 500, 750, 1000, 1125$ due to the reasons above. Now: {{begin-eqn}} {{eqn | l = 124 | r = 2^2 \times 31 }} {{eqn | l = 126 | r = 2 \times 3^2 \times 7 }} {{eqn | l = 249 | r = 3 \times 83 }} {{eqn | l = 251 | o = \text {is} | r = \text {prime} }} {{eqn | l = 623 | r = 7 \times 89 }} {{eqn | l = 624 | r = 2^4 \times 3 \times 13 }} {{eqn | l = 626 | r = 2 \times 313 }} {{eqn | l = 874 | r = 2 \times 19 \times 23 }} {{eqn | l = 876 | r = 2^2 \times 3 \times 73 }} {{eqn | l = 1030 | r = 2 \times 5 \times 103 }} {{eqn | l = 1249 | o = \text {is} | r = \text {prime} }} {{eqn | l = 1251 | r = 3^2 \times 139 }} {{end-eqn}} Hence none of these numbers is in a [[Definition:Ordered Triple|triplet]]. {{qed}}	0
Let $\EE$ be an [[Definition:Experiment|experiment]] whose [[Definition:Sample Space|sample space]] is $\Omega$. Let $\powerset \Omega$ be the [[Definition:Power Set|power set]] of $\Omega$. Then $\powerset \Omega$ is an [[Definition:Event Space|event space]] of $\EE$.	0
Every [[Definition:Unlimited Register Machine#State|state]] of a [[Definition:URM Program|URM program]] can be assigned a unique '''code number'''. This code number is called the '''state code''' (or '''situation code''').	0
By definition of [[Definition:Binomial Coefficient|binomial coefficient]]: :$\dbinom m n = \begin{cases} \dfrac {m!} {n! \paren {m - n}!} & : 0 \le n \le m \\ & \\ 0 & : \text { otherwise } \end{cases}$ Thus when $n > 0$: :$\dbinom 0 n = 0$ and when $n = 0$: :$\dbinom 0 0 = \dfrac {0!} {0! \paren {0 - 0}!} = 1$ by definition of [[Definition:Factorial|factorial]]. Hence the result by definition of the [[Definition:Kronecker Delta|Kronecker delta]]. {{qed}}	0
From [[Prime Power of Sum Modulo Prime]] we have: :$(1): \quad \paren {a + b}^{p^n} \equiv \paren {a^{p^n} + b^{p^n} } \pmod p$ We can write this: :$\paren {a + b}^{p^n k} = \paren {\paren {a + b}^{p^n} }^k$ By $(1)$ and [[Congruence of Powers]], we therefore have: :$\paren {a + b}^{p^n k} \equiv \paren {a^{p^n} + b^{p^n} }^k \pmod p$ The coefficient $\dbinom {p^n k} {p^n}$ is the [[Definition:Binomial Coefficient|binomial coefficient]] of $b^{p^n}$ in $\paren {a + b}^{p^n k} = \paren {\paren {a + b}^{p^n} }^k$. Expanding $\paren {a^{p^n} + b^{p^n} }^k$ using the [[Binomial Theorem]], we find that the coefficient of $b^{p^n}$, the second term, is $\dbinom k 1 = k$. So: :$\dbinom {p^n k} {p^n} \equiv k \pmod p$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \exists k \in \Z: a | r = k z | c = }} {{eqn | ll= \leadstoandfrom | l = \exists k \in \Z: a | r = 0 + k z | c = }} {{end-eqn}} Thus by definition of [[Definition:Congruence (Number Theory)|congruence]], $a \equiv 0 \pmod z$ and the result is proved. If $z$ is an [[Definition:Integer|integer]], then by definition of [[Definition:Divisor of Integer|divisor]]: :$z \divides a \iff \exists k \in \Z: a = k z$ Hence the result for [[Definition:Integer|integer]] $z$. {{qed}} [[Category:Modulo Arithmetic]] 87apiiualuplsor2u54hkqv8t4i4x69	0
:$\floor x = n \iff n \le x < n + 1$	0
Let $\powerset \Omega := \Sigma$. ;[[Definition:Event Space|Event Space Axiom $(\text {ES} 1)$]]: From [[Empty Set is Subset of All Sets]] we have that $\O \subseteq \Omega$. By the definition of [[Definition:Power Set|power set]]: :$\O \in \Sigma$ thus fulfilling [[Definition:Event Space|axiom $(\text {ES} 1)$]]. {{qed|lemma}} ;[[Definition:Event Space|Event Space Axiom $(\text {ES} 2)$]]: Let $A \in \Sigma$. Then by the definition of [[Definition:Power Set|power set]]: :$A \subseteq \Omega$ From [[Set with Relative Complement forms Partition]]: :$\Omega \setminus A \subseteq \Omega$ and so by the definition of [[Definition:Power Set|power set]]: :$\Omega \setminus A \in \Sigma$ thus fulfilling [[Definition:Event Space|axiom $(\text {ES} 2)$]]. {{qed|lemma}} ;[[Definition:Event Space|Event Space Axiom $(\text {ES} 3)$]]: Let $\sequence {A_i}$ be a [[Definition:Countable Set|countably infinite]] [[Definition:Sequence|sequence]] of [[Definition:Set|sets]] in $\Sigma$. Then from [[Power Set is Closed under Countable Unions]]: :$\ds \bigcup_{i \mathop \in \N} A_i \in \Sigma$ thus fulfilling [[Definition:Event Space|axiom $(\text {ES} 3)$]]. {{qed|lemma}} All the [[Definition:Event Space|event space axioms]] are seen to be fulfilled by $\powerset \Omega$. Hence the result. {{qed}}	0
Let $x$ be a [[Definition:Real Number|real number]]. Informally, the '''ceiling function of $x$''' is the smallest [[Definition:Integer|integer]] greater than or equal to $x$. === [[Definition:Ceiling Function/Definition 1|Definition 1]] === {{Definition:Ceiling Function/Definition 1}} === [[Definition:Ceiling Function/Definition 2|Definition 2]] === {{Definition:Ceiling Function/Definition 2}} === [[Definition:Ceiling Function/Definition 3|Definition 3]] === {{Definition:Ceiling Function/Definition 3}}	0
Let $x^{\overline n}$ denote the [[Definition:Rising Factorial|$n$th rising factorial power]] of $x$. Then: :$x x^{\overline n} = x^{\overline {n + 1} } - n x^{\overline n}$	0
The [[Definition:Closed-Form Expression|closed-form expression]] for the $n$th [[Definition:Hexagonal Pyramidal Number|hexagonal pyramidal number]] is: :$S_n = \dfrac {n \paren {n + 1} \paren {4 n - 1} } 6$	0
{{begin-eqn}} {{eqn | l = s \left({1, n}\right) | r = s \left({0, {n - 1} }\right) - 0 \times s \left({0, n}\right) | c = Definition of [[Definition:Signed Stirling Numbers of the First Kind|Signed Stirling Number of the First Kind]] }} {{eqn | r = s \left({0, {n - 1} }\right) | c = }} {{eqn | r = \delta_{0 \left({n - 1}\right)} | c = Definition of [[Definition:Signed Stirling Numbers of the First Kind|Signed Stirling Number of the First Kind]] }} {{eqn | r = \delta_{1 n} | c = $0 = n - 1 \iff 1 = n$ }} {{end-eqn}} {{qed}}	0
From [[Sum of Sequence of Squares]]: {{:Sum of Sequence of Squares}} Thus: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 7}^{29} i^2 | r = \sum_{i \mathop = 1}^{29} i^2 - \sum_{i \mathop = 1}^6 i^2 | c = }} {{eqn | r = \frac {29 \left({29 + 1}\right) \left({2 \times 29 + 1}\right)} 6 - \frac {6 \left({6 + 1}\right) \left({2 \times 6 + 1}\right)} 6 | c = }} {{eqn | r = \frac {29 \times 30 \times 59} 6 - \frac {6 \times 7 \times 13} 6 | c = }} {{eqn | r = 8 \, 555 - 91 | c = }} {{eqn | r = 8 \, 464 | c = }} {{eqn | r = 92^2 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 7}^{39} i^2 | r = \sum_{i \mathop = 1}^{39} i^2 - \sum_{i \mathop = 1}^6 i^2 | c = }} {{eqn | r = \frac {39 \left({39 + 1}\right) \left({2 \times 39 + 1}\right)} 6 - \frac {6 \left({6 + 1}\right) \left({2 \times 6 + 1}\right)} 6 | c = }} {{eqn | r = \frac {39 \times 40 \times 79} 6 - \frac {6 \times 7 \times 13} 6 | c = }} {{eqn | r = 20 \, 540 - 91 | c = }} {{eqn | r = 20 \, 449 | c = }} {{eqn | r = 143^2 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 7}^{56} i^2 | r = \sum_{i \mathop = 1}^{56} i^2 - \sum_{i \mathop = 1}^6 i^2 | c = }} {{eqn | r = \frac {56 \left({56 + 1}\right) \left({2 \times 56 + 1}\right)} 6 - \frac {6 \left({6 + 1}\right) \left({2 \times 6 + 1}\right)} 6 | c = }} {{eqn | r = \frac {56 \times 57 \times 113} 6 - \frac {6 \times 7 \times 13} 6 | c = }} {{eqn | r = 60 \, 116 - 91 | c = }} {{eqn | r = 60 \, 025 | c = }} {{eqn | r = 245^2 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 7}^{190} i^2 | r = \sum_{i \mathop = 1}^{190} i^2 - \sum_{i \mathop = 1}^6 i^2 | c = }} {{eqn | r = \frac {190 \left({190 + 1}\right) \left({2 \times 190 + 1}\right)} 6 - \frac {6 \left({6 + 1}\right) \left({2 \times 6 + 1}\right)} 6 | c = }} {{eqn | r = \frac {190 \times 191 \times 381} 6 - \frac {6 \times 7 \times 13} 6 | c = }} {{eqn | r = 2 \, 304 \, 415 - 91 | c = }} {{eqn | r = 2 \, 304 \, 324 | c = }} {{eqn | r = 1 \, 518^2 | c = }} {{end-eqn}} {{qed}}	0
Proof by [[Principle of Mathematical Induction|induction]] on $s$: === Basis for the Induction === For $s = 0$, we apply the definition of a [[Definition:Complete Residue System|complete residue system modulo $p$]]: :$\forall n \in \Z: \exists a_i \in S: n \equiv a_i \pmod p$ This is our [[Definition:Basis for the Induction|base case]]. === Induction Hypothesis === This is our [[Definition:Induction Hypothesis|induction hypothesis]]: :For some $k \in \Z_{\ge 0}$, for all [[Definition:Integer|integers]] $n \in \Z$, there exists a [[Definition:Congruence Modulo Integer|congruence]] of the form: ::$n \equiv \displaystyle \sum_{j \mathop = 0}^k b_j p^j \pmod {p^{k + 1} }$ It is to be demonstrated that: :For all [[Definition:Integer|integers]] $n \in \Z$, there exists a [[Definition:Congruence Modulo Integer|congruence]] of the form: ::$n \equiv \displaystyle \sum_{j \mathop = 0}^{k + 1} b_j p^j \pmod {p^{k + 2} }$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: From $n \equiv \displaystyle \sum_{j \mathop = 0}^k b_j p^j \pmod {p^{k + 1} }$ we have: :$\exists q \in \Z: n = \displaystyle \sum_{j \mathop = 0}^k b_j p^j + q p^{k + 1}$ From the definition of a [[Definition:Complete Residue System|complete residue system modulo $p$]]: :$\exists a_i \in S: q \equiv a_i \pmod p$ This gives: :$\exists r \in \Z: q = a_i + r p$ Substituting this to the original equation we have: {{begin-eqn}} {{eqn | l = n | r = \sum_{j \mathop = 0}^k b_j p^j + \paren {a_i + r p} p^{k + 1} }} {{eqn | r = a_i p^{k + 1} + \sum_{j \mathop = 0}^k b_j p^j + r p^{k + 2} }} {{eqn | o = \equiv | r = a_i p^{k + 1} + \sum_{j \mathop = 0}^k b_j p^j | rr = \pmod {p^{k + 2} } }} {{end-eqn}} This shows that $n$ can be expressed as the form: :$n \equiv \displaystyle \sum_{j \mathop = 0}^{k + 1} b_j p^j \pmod {p^{k + 2} }$ By the [[Principle of Mathematical Induction]], the theorem is true for any $s$. Note that in the proof above, we did not use the fact that $p$ is a [[Definition:Prime Number|prime number]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \sum_k \binom r {m + k} \binom s {n + k} | r = \sum_k \binom r {r - m - k} \binom s {s - n - k} | c = [[Symmetry Rule for Binomial Coefficients]] }} {{eqn | r = \sum_k \binom r k \binom s {s - n - \left({r - m - k}\right)} | c = [[Change of Index Variable of Summation]] }} {{eqn | r = \sum_k \binom r k \binom s {r - m - k + n} | c = [[Symmetry Rule for Binomial Coefficients]] }} {{eqn | r = \sum_k \binom r k \binom s {\left({r - m + n}\right) - k} | c = }} {{eqn | r = \binom {r + s} {r - m + n} | c = [[Chu-Vandermonde Identity]] }} {{end-eqn}} {{qed}}	0
:$\dbinom n k_\mathcal F = F_{k - 1} \dbinom {n - 1} k_\mathcal F + F_{n - k + 1} \dbinom {n - 1} {k - 1}_\mathcal F$ where: :$\dbinom n k_\mathcal F$ denotes a [[Definition:Fibonomial Coefficient|Fibonomial coefficient]] :$F_{k - 1}$ etc. denote [[Definition:Fibonacci Number|Fibonacci numbers]].	0
We have that: :$x \equiv y \pmod m$ From [[Congruence of Powers]]: :$x^k \equiv y^k \pmod m$ From [[Modulo Multiplication is Well-Defined]]: :$\forall k \in \set {0, 2, \ldots, r}: a_k x^k \equiv a_k y^k \pmod m$ The result follows from [[Modulo Addition is Well-Defined]]. {{qed}}	0
{{begin-eqn}} {{eqn | lo= \forall n \in \Z_{>0}: | l = F_n | r = \sum_{k \mathop = 0}^{\floor {\frac {n - 1} 2} } \dbinom {n - k - 1} k | c = }} {{eqn | r = \binom {n - 1} 0 + \binom {n - 2} 1 + \binom {n - 3} 2 + \dotsb + \binom {n - j} {j - 1} + \binom {n - j - 1} j | c = where $j = \floor {\frac {n - 1} 2}$ }} {{end-eqn}}	0
From [[Sum over k of r-kt choose k by r over r-kt by z^k|Sum over $k$ of $\dbinom {r - k t} k$ by $\dfrac r {r - k t}$ by $z^k$]]: :$\displaystyle \sum_k A_k \left({r, t}\right) z^k = x^r$ and: :$\displaystyle \sum_k A_k \left({s, t}\right) z^k = x^s$ Hence: {{begin-eqn}} {{eqn | l = x^{r + s} | r = \sum_k A_k \left({r, t}\right) z^k \sum_k A_k \left({s, t}\right) z^k | c = }} {{eqn | r = \sum_k A_k \left({r + s, t}\right) z^k | c = }} {{end-eqn}} Taking the $z^n$ coefficient: {{begin-eqn}} {{eqn | l = \sum_k A_k \left({r, t}\right) z^k \sum_k A_{n - k} \left({s, t}\right) z^{n - k} | r = A_n \left({r + s, t}\right) z^n | c = }} {{eqn | ll= \leadsto | l = \sum_k A_k \left({r, t}\right) \sum_k A_{n - k} \left({s, t}\right) | r = A_n \left({r + s, t}\right) }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | r = \sum_k \binom {n + k} {2 k} \binom {2 k} k \frac {\left({-1}\right)^k} {k + 1} | o = | c = }} {{eqn | r = \sum_k \binom {n + k} k \binom n k \frac {\left({-1}\right)} {k + 1} | c = [[Product of r Choose m with m Choose k|Product of $\dbinom r m$ with $\dbinom m k$]] }} {{eqn | r = \sum_k \binom {n + k} k \binom {n + 1} {k + 1} \frac {\left({-1}\right)^k} {n + 1} | c = [[Factors of Binomial Coefficient]] }} {{eqn | r = \frac 1 {n + 1} \sum_{k \mathop \ge 0} \binom {n + k} n \binom {n + 1} {k + 1} \left({-1}\right)^k | c = [[Symmetry Rule for Binomial Coefficients]] and rearranging }} {{eqn | r = -\frac 1 {n + 1} \sum_{k \mathop \ge 1} \binom {n - 1 + k} n \binom {n + 1} k \left({-1}\right)^k | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = -\frac 1 {n + 1} \sum_{k \mathop \ge 0} \binom {n - 1 + k} n \binom {n + 1} k \left({-1}\right)^k + \frac 1 {n + 1} \binom {n - 1} n | c = separating out the case where $k = 0$ }} {{eqn | r = -\frac 1 {n + 1} \left({-1}\right)^{n + 1} \binom {n - 1} {-1} + \frac 1 {n + 1} \binom {n - 1} n | c = [[Sum over k of r Choose k by s+k Choose n by -1^r-k|Sum over $k$ of $\dbinom r k \dbinom {s + k} n (-1)^{r - k}$]] }} {{eqn | r = \frac 1 {n + 1} \binom {n - 1} n | c = as $\dbinom {n - 1} {-1} = 0$ }} {{eqn | r = \left[{n = 0}\right] | c = as $\dbinom {n - 1} n = 1 \iff n = 0$ }} {{end-eqn}} {{qed}}	0
Let $q \in \R_{\ne 1}, n \in \Z_{>0}, k \in \Z$. Then: :$\dbinom n k_q = \dbinom n {n - k}_q$ where $\dbinom n k_q$ is a [[Definition:Gaussian Binomial Coefficient|Gaussian binomial coefficient]].	0
The proof proceeds by [[Principle of Mathematical Induction|induction]] on $n$. For all $n \in \Z_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\dbinom n k < \dbinom n {k + 1} \iff 0 \le k < \dfrac {n - 1} 2$ First we investigate the edge case. Let $n = 1$. Then we have: {{begin-eqn}} {{eqn | l = \dbinom 1 0 | r = 1 | c = [[Binomial Coefficient with Zero]] }} {{eqn | l = \dbinom 1 1 | r = 1 | c = [[Binomial Coefficient with Self]] }} {{end-eqn}} Thus we see: :there are no $k$ such that $0 \le k < \dfrac {1 - 1} 2 = 0$ and: :there are no $k$ such that $\dbinom 1 k < \dbinom 1 {k + 1}$ Thus $\map P 1$ is seen to hold. === Basis for the Induction === Let $n = 2$. Then we have: {{begin-eqn}} {{eqn | l = \dbinom 2 0 | r = 1 | c = [[Binomial Coefficient with Zero]] }} {{eqn | l = \dbinom 2 1 | r = 2 | c = [[Binomial Coefficient with One]] }} {{eqn | l = \dbinom 2 2 | r = 1 | c = [[Binomial Coefficient with Self]] }} {{end-eqn}} Thus we see: :there is one $k$ such that $0 \le k < \dfrac {2 - 1} 2 = \dfrac 1 2$, and that is $k = 0$ and: :$\dbinom 2 k < \dbinom 2 {k + 1}$ holds for exactly $k = 0$. Thus $\map P 2$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P r$ is true, where $r \ge 1$, then it logically follows that $\map P {r + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\dbinom r k < \dbinom r {k + 1} \iff 0 \le k < \dfrac {r - 1} 2$ from which it is to be shown that: :$\dbinom {r + 1} k < \dbinom {r + 1} {k + 1} \iff 0 \le k < \dfrac r 2$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \dbinom {r + 1} k | r = \dbinom r k + \dbinom r {k - 1} | c = [[Pascal's Rule]] }} {{eqn | o = < | r = \dbinom r k + \dbinom r k | rr= \iff 0 \le {k - 1} < \dfrac {r - 1} 2 | c = [[Condition for Increasing Binomial Coefficients#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | o = < | r = \dbinom r {k + 1} + \dbinom r k | rr= \iff 0 \le {k - 1} < \dfrac {r - 1} 2 \text { and } 0 \le k < \dfrac {r - 1} 2 | c = [[Condition for Increasing Binomial Coefficients#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | o = < | r = \dbinom r {k + 1} + \dbinom r k | rr= \iff 0 \le k < \dfrac {r - 1} 2 | c = }} {{eqn | r = \dbinom {r + 1} {k + 1} | rr= \iff 0 \le k < \dfrac {r - 1} 2 | c = [[Pascal's Rule]] }} {{end-eqn}} {{finish|Can't get my head round the inequalities on $k$, needs more thought}} So $\map P r \implies \map P {r + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{> 0}: \dbinom n k < \dbinom n {k + 1} \iff 0 \le k < \dfrac {n - 1} 2$	0
From [[Integer equals Floor iff between Number and One Less]] we have: :$x - 1 < \floor x \le x$ and so, by multiplying both sides by -1: :$-x + 1 > -\floor x \ge -x$ From [[Integer equals Ceiling iff between Number and One More]] we have: :$\ceiling x = n \iff x \le n < x + 1$ Hence: :$-x \le -\floor x < -x + 1 \implies \ceiling {-x} = -\floor x$ {{Qed}}	0
{{begin-eqn}} {{eqn | l = 0 | o = \le | m = \frac {x \bmod y} y | mo= < | r = 1 | c = [[Quotient of Modulo Operation with Modulus]] }} {{eqn | ll= \leadsto | l = 0 | o = \le | m = \frac {x \bmod y} y \times y | mo= < | r = 1 \times y | c = [[Real Number Ordering is Compatible with Multiplication]] }} {{eqn | ll= \leadsto | l = 0 | o = \le | m = x \bmod y | mo= < | r = y | c = }} {{end-eqn}} Hence the result. {{qed}}	0
When calculating $\mu$, the easiest way to calculate the next term is simply to divide the previous term by $p$ and discard the remainder: :$\floor {\dfrac n {p^{k + 1} } } = \floor {\floor {\dfrac n {p^k} } / p}$	0
By inspection: {{begin-eqn}} {{eqn | l = 1 | r = T_1 | c = $1$ way }} {{eqn | l = 2 | r = T_1 + T_1 | c = $1$ way }} {{eqn | l = 3 | r = T_1 + T_1 + T_1 | c = }} {{eqn | r = T_2 | c = $2$ ways }} {{eqn | l = 4 | r = T_2 + T_1 | c = $1$ way }} {{eqn | l = 5 | r = T_2 + T_1 + T_1 | c = $1$ way }} {{eqn | l = 6 | r = T_2 + T_2 }} {{eqn | r = T_3 | c = $2$ ways }} {{eqn | l = 7 | r = T_2 + T_2 + T_1 }} {{eqn | r = T_3 + T_1 | c = $2$ ways }} {{eqn | l = 8 | r = T_3 + T_1 + T_1 | c = $1$ way }} {{eqn | l = 9 | r = T_3 + T_2 }} {{eqn | r = T_2 + T_2 + T_2 | c = $2$ ways }} {{eqn | l = 10 | r = T_4 }} {{eqn | r = T_3 + T_2 + T_1 | c = $2$ ways }} {{eqn | l = 11 | r = T_4 + T_1 | c = $1$ way }} {{eqn | l = 12 | r = T_4 + T_1 + T_1 }} {{eqn | r = T_3 + T_3 | c = }} {{eqn | r = T_3 + T_2 + T_2 | c = $3$ ways }} {{eqn | l = 13 | r = T_4 + T_2 | c = }} {{eqn | r = T_3 + T_3 + T_1 | c = $2$ ways }} {{eqn | l = 14 | r = T_4 + T_2 + T_1 | c = $1$ way }} {{eqn | l = 15 | r = T_5 }} {{eqn | r = T_3 + T_3 + T_2 | c = $2$ ways }} {{eqn | l = 16 | r = T_5 + T_1 }} {{eqn | r = T_4 + T_3 | c = }} {{eqn | r = T_4 + T_2 + T_2 | c = $3$ ways }} {{eqn | l = 17 | r = T_5 + T_1 + T_1 }} {{eqn | r = T_4 + T_3 + T_1 | c = $2$ ways }} {{eqn | l = 18 | r = T_5 + T_2 }} {{eqn | r = T_3 + T_3 + T_3 | c = $2$ ways }} {{eqn | l = 19 | r = T_5 + T_2 + T_1 }} {{eqn | r = T_4 + T_3 + T_2 | c = $2$ ways }} {{eqn | l = 20 | r = T_4 + T_4 | c = $1$ way }} {{eqn | l = 21 | r = T_6 }} {{eqn | r = T_5 + T_3 | c = }} {{eqn | r = T_5 + T_2 + T_2 | c = }} {{eqn | r = T_4 + T_4 + T_1 | c = $4$ ways }} {{end-eqn}} {{OEIS|A002636}}{{qed}}	0
Obviously the product cannot be a [[Definition:Square Number|square]] if $n$ is a [[Definition:Prime Number|prime]]. For $n$ [[Definition:Composite Number|composite]], we can write: :$n = a b$ where $a, b \in \Z_{>1}$. Then: {{begin-eqn}} {{eqn | o = | r = n! \paren {n - 1}! \paren {a!} \paren {a - 1}! \paren {b!} \paren {b - 1}! }} {{eqn | r = n a b \paren {\paren {n - 1}! \paren {a - 1}! \paren {b - 1}!}^2 }} {{eqn | r = \paren {n! \paren {a - 1}! \paren {b - 1}!}^2 }} {{end-eqn}} which is a [[Definition:Square Number|square]]. Hence no more than $6$ [[Definition:Factorial|factorials]] is required. To show that $527$ is the smallest that actually requires $6$, observe that: {{tidy}} {{explain|It might be worth extracting some of the below statements into lemmata, for example: "If $n$ is itself [[Definition:Square Number|square]], then so is $n! \paren {n - 1}!$" and "... Then $n! \paren {n - 1}! b! \paren {b - 1}!$ is [[Definition:Square Number|square]]" -- they're really easy to prove, even I can do them :-) but it takes more than a glance to recognise that they are true.}} If $n$ is itself [[Definition:Square Number|square]], then so is $n! \paren {n - 1}!$. If $n$ is not [[Definition:Square-Free Integer|square-free]], write $n = a^2 b$, where $b$ is [[Definition:Square-Free Integer|square-free]]. Then $n! \paren {n - 1}! b! \paren {b - 1}!$ is [[Definition:Square Number|square]]. If $n$ is [[Definition:Divisor of Integer|divisible]] by $2$, write $n = 2 m$. Then $\paren {2 m}! \paren {2 m - 1}! \paren {m!} \paren {m - 1}! \paren {2!}$ is [[Definition:Square Number|square]]. If $n$ is [[Definition:Divisor of Integer|divisible]] by $3$, write $n = 3 m$. Then $\paren {3 m}! \paren {3 m - 1}! \paren {2 m}! \paren {2 m - 1}! \paren {3!}$ is [[Definition:Square Number|square]]. If $n$ is [[Definition:Divisor of Integer|divisible]] by $5$, write $n = 5 m$. Then $\paren {5 m}! \paren {5 m - 1}! \paren {m!} \paren {m - 1}! \paren {6!}$ is [[Definition:Square Number|square]]. If $n$ is [[Definition:Divisor of Integer|divisible]] by $7$, write $n = 7 m$. Then $\paren {7 m}! \paren {7 m - 1}! \paren {5 m}! \paren {5 m - 1}! \paren {7!}$ is [[Definition:Square Number|square]]. If $n$ is [[Definition:Divisor of Integer|divisible]] by $11$, write $n = 11 m$. Then $\paren {11 m}! \paren {11 m - 1}! \paren {7 m}! \paren {7 m - 1}! \paren {11!}$ is [[Definition:Square Number|square]]. The remaining numbers less than $527$ that are not of the above forms are: :$221, 247, 299, 323, 377, 391, 403, 437, 481, 493$ Each of the following is a product of $5$ factorials which is square: :$221! \, 220! \, 18! \, 11! \, 7!$ :$247! \, 246! \, 187! \, 186! \, 20!$ :$299! \, 298! \, 27! \, 22!$ :$323! \, 322! \, 20! \, 14! \, 6!$ :$377! \, 376! \, 29! \, 23! \, 10!$ :$391! \, 389! \, 24! \, 21! \, 17!$ :$403! \, 402! \, 33! \, 30! \, 14!$ :$437! \, 436! \, 51! \, 49! \, 28!$ :$481! \, 479! \, 38! \, 33! \, 22!$ :$493! \, 491! \, 205! \, 202! \, 7!$ {{finish|The fact that $527$ has no such representation can be verified by a direct (but lengthy) computation.}}	0
Because $a_n = \map \OO {\sequence {b_n} }$, there exists $K \ge 0$ and $n_0 \in \N$ such that $\size {a_n} \le K \cdot \size {b_n}$ for $n \ge n_0$. Because $b_n = \map \OO {\sequence {c_n} }$, there exists $L \ge 0$ and $n_1 \in \N$ such that $\size {b_n} \le L \cdot \size {c_n}$ for $n \ge n_1$. Then $\size {a_n} \le K L \cdot \size {c_n}$ for $n \ge \max \set {n_0, n_1}$. Thus $a_n = \map \OO {\sequence {c_n} }$. {{qed}} [[Category:Asymptotic Notation]] irejv5z2pk0odh7n08v6of2ee5kohph	0
First we establish the result for when $\left({R, \odot}\right)$ has an [[Definition:Identity Element|identity element]] $e$. For $n = 0$ we have: :$\displaystyle \odot^0 \left({x + y}\right) = e = {0 \choose 0} \left({\odot^{0 - 0} x}\right) \odot \left({\odot^0 y}\right) = \sum_{k \mathop = 0}^0 {0 \choose k} x^{0 - k} \odot y^k$ For $n = 1$ we have: :$\displaystyle \odot^1 \left({x + y}\right) = \left({x + y}\right) = {0 \choose 1} \left({\odot^{1 - 0} x}\right) \odot \left({\odot^0 y}\right) + {1 \choose 1} \left({\odot^{1 - 1} x}\right) \odot \left({\odot^1 y}\right) = \sum_{k \mathop = 0}^1 {1 \choose k} x^{1 - k} \odot y^k$ === Basis for the Induction === For $n = 2$ we have: {{begin-eqn}} {{eqn | l = \odot^2 \left({x + y}\right) | r = \left({x + y}\right) \odot \left({x + y}\right) | c = }} {{eqn | r = \left({x \odot x}\right) + \left({x \odot y}\right) + \left({y \odot x}\right) + \left({y \odot y}\right) | c = }} {{eqn | r = \left({x \odot x}\right) + 2 \left({x \odot y}\right) + \left({y \odot y}\right) | c = $+$ is [[Definition:Commutative Operation|commutative]] in $R$ }} {{eqn | r = \odot^2 x + 2 \left({\odot^1 x}\right) \odot \left({\odot^1 y}\right) + \odot^2 y | c = }} {{eqn | r = \odot^2 x + {2 \choose 1} \left({\odot^{2-1} x}\right) \odot \left({\odot^1 y}\right) + \odot^2 y | c = }} {{end-eqn}} This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === This is our [[Principle of Mathematical Induction#Induction Hypothesis|inductive hypothesis]]: :$\displaystyle \forall n \ge 2: \odot^n \left({x + y}\right) = \odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \odot^{n + 1} \left({x + y}\right) | r = \left({x + y}\right) \odot \left({\odot^n \left({x + y}\right)}\right) | c = }} {{eqn | r = x \odot \left({\odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y}\right) | c = }} {{eqn | o = | ro= + | r = ~ y \odot \left({\odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y}\right) | c = [[Binomial Theorem/Ring Theory#Inductive Hypothesis|Inductive Hypothesis]] }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + x \odot \left({\odot^n y}\right) | c = }} {{eqn | o = | ro= + | r = ~ y \odot \left({\odot^n x}\right) + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \sum_{k \mathop = 0}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \sum_{k \mathop = 1}^n {n \choose k - 1} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n + 1 \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \odot^{n + 1} y | c = [[Pascal's Rule]] }} {{end-eqn}} The result follows by the [[Principle of Mathematical Induction]]. {{qed}}	0
Let $G \left({z}\right)$ be the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{a_n}\right\rangle$. Let $m \in \Z_{\ge 0}$ be a [[Definition:Non-Negative Integer|non-negative integer]]. Then $z^m G \left({z}\right)$ is the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{a_{n - m} }\right\rangle$.	0
:$\left({a^2 + b^2}\right) \left({c^2 + d^2}\right) = \left({a c - b d}\right)^2 + \left({a d + b c}\right)^2$	0
By definition, the entries in the $n$th [[Definition:Lesser Diagonal of Pascal's Triangle|lesser diagonal]] of [[Definition:Pascal's Triangle|Pascal's triangle]] are: :$\dbinom n 0, \dbinom {n - 1} 1, \dbinom {n - 2} 2, \dbinom {n - 3} 3, \ldots$ and so the statement can be written: :$F_{n + 1} = \displaystyle \sum_{k \mathop \ge 0} \dbinom {n - k} k$ The proof proceeds by [[Second Principle of Mathematical Induction|strong induction]]. For all $n \in \Z_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$F_{n + 1} = \displaystyle \sum_{k \mathop \ge 0} \dbinom {n - k} k$ $\map P 0$ is the case: {{begin-eqn}} {{eqn | l = \sum_{k \mathop \ge 0} \dbinom {0 - k} k | r = \dbinom 0 0 | c = }} {{eqn | r = 1 | c = }} {{eqn | r = F_1 | c = }} {{end-eqn}} Thus $\map P 0$ is seen to hold. === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \sum_{k \mathop \ge 0} \dbinom {1 - k} k | r = \dbinom 1 0 + \dbinom 0 1 | c = }} {{eqn | r = 1 + 0 | c = }} {{eqn | r = F_2 | c = }} {{end-eqn}} Thus $\map P 1$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P j$ is true, for all $j$ such that $0 \le j \le r$, then it logically follows that $\map P {r + 1}$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: {{begin-eqn}} {{eqn | l = F_r | r = \sum_{k \mathop \ge 0} \dbinom {r - 1 - k} k }} {{eqn | l = F_{r + 1} | r = \sum_{k \mathop \ge 0} \dbinom {r - k} k }} {{end-eqn}} from which it is to be shown that: :$F_{r + 2} = \displaystyle \sum_{k \mathop \ge 0} \dbinom {r + 1 - k} k$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = F_{r + 2} | r = F_r + F_{r + 1} | c = {{Defof|Fibonacci Number}} }} {{eqn | r = \sum_{k \mathop \ge 0} \dbinom {r - 1 - k} k + \sum_{k \mathop \ge 0} \dbinom {r - k} k | c = [[Sum of Entries in Lesser Diagonal of Pascal's Triangle equal Fibonacci Number#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \dbinom {r - 1} 0 + \dbinom {r - 2} 1 + \dbinom {r - 3} 2 + \dbinom {r - 4} 3 + \ldots | c = }} {{eqn | ro= + | o = | r = \dbinom r 0 + \dbinom {r - 1} 1 + \dbinom {r - 2} 2 + \dbinom {r - 3} 3 + \dbinom {r - 4} 4 + \ldots | c = }} {{eqn | r = \dbinom r 0 + \dbinom r 1 + \dbinom {r - 1} 2 + \dbinom {r - 2} 3 + \dbinom {r - 3} 4 + \ldots | c = [[Pascal's Rule]] }} {{eqn | r = \dbinom {r + 1} 0 + \dbinom r 1 + \dbinom {r - 1} 2 + \dbinom {r - 2} 3 + \dbinom {r - 3} 4 + \ldots | c = [[Binomial Coefficient with Zero]] }} {{eqn | r = \sum_{k \mathop \ge 0} \dbinom {r + 1 - k} k | c = }} {{end-eqn}} So $\map P r \implies \map P {r + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{>0}: F_{n + 1} = \displaystyle \sum_{k \mathop \ge 0} \dbinom {n - k} k$ {{qed}}	0
From [[Gamma Difference Equation]]: :$\map \Gamma {z + 1} = z \, \map \Gamma z$ which is valid for all $z \notin \Z_{\le 0}$. The result follows by dividing by $z$.	0
{{begin-eqn}} {{eqn | l = \sum_{j \mathop = 1}^n \paren {2 j - 1} | r = n^2 | c = [[Odd Number Theorem]] }} {{eqn | ll= \leadsto | l = \sum_{j \mathop = 1}^n \paren {2 j - 1} + \sum_{j \mathop = 1}^n 1 | r = n^2 + n | c = }} {{eqn | ll= \leadsto | l = \sum_{j \mathop = 1}^n \paren {2 j} | r = n \paren {n + 1} | c = }} {{eqn | ll= \leadsto | l = \sum_{j \mathop = 1}^n j | r = \frac {n \paren {n + 1} } 2 | c = }} {{end-eqn}} {{qed}}	0
A [[Definition:Recurrence Relation|recurrence relation]] for the [[Definition:Square Number|square numbers]] is: :$S_n = S_{n - 1} + 2 n - 1$	0
=== Necessary Condition === Suppose $c \in \Z_{>0}$ is the [[Definition:Multiplicative Order of Integer|multiplicative order of $a$ modulo $n$]]. Then by definition: :$a^c \equiv 1 \pmod n$ Hence, by definition, $a^c = k n + 1$ for some $k \in \Z$. Thus: :$a r + n s = 1$ where $r = a^{c-1}$ and $s = -k$. It follows from [[Integer Combination of Coprime Integers]] that $a$ and $n$ are [[Definition:Coprime Integers|coprime]]. === Sufficient Condition === Suppose $a \perp n$. Then by [[Euler's Theorem]]: : $a^{\phi \left({n}\right)} \equiv 1 \pmod n$ where $\phi \left({n}\right)$ is the [[Definition:Euler Phi Function|Euler Phi Function]] of $n$. Hence the [[Definition:Multiplicative Order of Integer|multiplicative order of $a$ modulo $n$]] exists, by taking $c = \phi \left({n}\right)$. {{qed}} [[Category:Coprime Integers]] [[Category:Modulo Arithmetic]] ciiwxi45ftzbw816mx9aob2x1wfd3x4	0
We have that: {{begin-eqn}} {{eqn | l = 160 \, 426 \, 514 | r = 729 + 47 \, 045 \, 881 + 113 \, 379 \, 904 | c = }} {{eqn | r = 3^6 + 19^6 + 22^6 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 160 \, 426 \, 514 | r = 1 \, 000 \, 000 + 11 \, 390 \, 625 + 148 \, 035 \, 889 | c = }} {{eqn | r = 10^6 + 15^6 + 23^6 | c = }} {{end-eqn}} {{ProofWanted|It remains to be shown there are no smaller}}	0
Let $\sequence {a_n}$, $\sequence {b_n}$ and $\sequence {c_n}$ be [[Definition:Sequence|sequences]] of [[Definition:Real Number|real]] or [[Definition:Complex Number|complex numbers]]. Let $a_n = \map \OO {\sequence {b_n} }$ and $b_n = \map \OO {\sequence {c_n} }$, where $O$ denotes [[Definition:Big-O Notation for Sequences|big-O notation]]. Then $a_n = \map \OO {\sequence {c_n} }$.	0
{{begin-eqn}} {{eqn | l = T_{9 n + 4} - T_{3 n + 1} | r = \dfrac {\paren {9 n + 4} \paren {9 n + 5} } 2 - \dfrac {\paren {3 n + 1} \paren {3 n + 2} } 2 | c = [[Closed Form for Triangular Numbers]] }} {{eqn | r = \dfrac {\paren {81 n^2 + 81 n + 20} - \paren {9 n^2 + 9 n + 2} } 2 | c = }} {{eqn | r = \dfrac {72 n^2 + 72 n + 18} 2 | c = }} {{eqn | r = 36 n^2 + 36 n + 9 | c = }} {{eqn | r = 9 \paren {4 n^2 + 4 n + 1} | c = }} {{eqn | r = 9 \paren {2 n + 1}^2 | c = }} {{eqn | r = 9 m^2 | c = }} {{eqn | r = \paren {3 m}^2 | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | ll= \forall n \in \N: | l = \frac {n \paren {n + 1} } 2 | r = \sum_{j \mathop = 0}^{n - 1} \paren {-1}^j \paren {n - j}^2 | c = }} {{eqn | r = n^2 - \paren {n - 1}^2 + \paren {n - 2}^2 - \cdots + \paren {-1}^{n - 1} | c = }} {{end-eqn}} Thus the $n$th [[Definition:Triangular Number|triangular number]] can be expressed as the alternating sum and difference of [[Definition:Square Number|squares]]: So: {{begin-eqn}} {{eqn | l = 1 | r = 1^2 | c = }} {{eqn | l = 3 | r = 2^2 - 1^2 | c = }} {{eqn | l = 6 | r = 3^2 - 2^2 + 1^2 | c = }} {{eqn | l = 10 | r = 4^2 - 3^2 + 2^2 - 1^2 | c = }} {{end-eqn}} and so on.	0
Let $m$ be a [[Definition:Repunit|repunit base $9$]]. Then $m$ is a [[Definition:Triangular Number|triangular number]].	0
Let $n \in \Z_{\ge 0}$ be a [[Definition:Positive Integer|positive integer]]. Let $n!$ denote the [[Definition:Factorial|factorial]] of $n$. Let $n!$ be expressed in [[Definition:Decimal Notation|decimal notation]]. Then $n!$ cannot end in the following numbers of [[Definition:Zero|zeroes]]: :$5, 11, 17, 23, 29, 30, 36, 42, \ldots$ {{OEIS|A000966}}	0
When $k \ge 0$, we have: {{begin-eqn}} {{eqn | l = \binom n {k + 1} | r = \frac {n!} {\left({k + 1}\right)! \left({n - k - 1}\right)!} | c = Definition of [[Definition:Binomial Coefficient|Binomial Coefficient]] }} {{eqn | r = \frac {n - k} {n - k} \frac {n!} {\left({k + 1}\right)! \left({n - k - 1}\right)!} | c = }} {{eqn | r = \frac {n - k} {\left({k + 1}\right) \left({n - k}\right)} \frac {n!} {k! \left({n - k - 1}\right)!} | c = extracting $k + 1$ from its [[Definition:Factorial|factorial]] }} {{eqn | r = \frac {n - k} {k + 1} \frac {n!} {k! \left({n - k}\right)!} | c = inserting $n - k$ into its [[Definition:Factorial|factorial]] }} {{eqn | r = \frac {n - k} {k + 1} \binom n k | c = Definition of [[Definition:Binomial Coefficient|Binomial Coefficient]] }} {{end-eqn}} In order for $S_n$ to be [[Definition:Strictly Increasing Sequence|strictly increasing]], it is necessary for $\dfrac {n - k} {k + 1} > 1$. So: {{begin-eqn}} {{eqn | l = \dfrac {n - k} {k + 1} | o = > | r = 1 | c = }} {{eqn | ll= \iff | l = n - k | o = > | r = k + 1 | c = }} {{eqn | ll= \iff | l = n | o = > | r = 2 k + 1 | c = }} {{eqn | ll= \iff | l = n | o = > | r = 2 \left({k + 1}\right) - 1 | c = }} {{end-eqn}} Thus $\dbinom n {k + 1} > \dbinom n k$ {{iff}} $k + 1$ is less than half of $n$. Hence the result. {{Qed}} [[Category:Binomial Coefficients]] h1tjmwqkc19n3s0v7w6d6nvdds38hk9	0
From [[Surjection iff Right Inverse]], $f$ has a [[Definition:Right Inverse Mapping|right inverse]] $g: S \to S$. From [[Right Inverse Mapping is Injection]], $g$ is an [[Definition:Injection|injection]]. From [[Injection from Finite Set to Itself is Permutation]], $g$ is a [[Definition:Permutation|permutation]] and so a [[Definition:Bijection|bijection]]. From [[Inverse of Bijection is Bijection]], $f$ is also a [[Definition:Bijection|bijection]]. Thus as $f$ is a [[Definition:Bijection|bijection]] to itself, it is by definition a [[Definition:Permutation|permutation]]. {{qed}}	0
Consider the number of paths in the integer lattice from $\left({0, 0}\right)$ to $\left({n, n}\right)$ using only single steps of the form: :$\left({i, j}\right) \to \left({i + 1, j}\right)$ :$\left({i, j}\right) \to \left({i, j + 1}\right)$ that is, either to the right or up. This process takes $2 n$ steps, of which $n$ are steps to the right. Thus the total number of paths through the graph is equal to $\dbinom {2 n} n$. Now let us count the paths through the grid by first counting the paths: :$(1): \quad$ from $\left({0, 0}\right)$ to $\left({k, n - k}\right)$ and then the paths: :$(2): \quad$ from $\left({k, n - k}\right)$ to $\left({n, n}\right)$. Note that each of these paths is of length $n$. Since each path is $n$ steps long, every endpoint will be of the form $\left({k, n - k}\right)$ for some $k \in \left\{{1, 2, \ldots, n}\right\}$, representing $k$ steps right and $n-k$ steps up. Note that the number of paths through $\left({k, n - k}\right)$ is equal to $\dbinom n k$, since we are free to choose the $k$ steps right in any order. We can also count the number of $n$-step paths from the point $\left({k, n - k}\right)$ to $\left({n, n}\right)$. These paths will be composed of $n-k$ steps to the right and $k$ steps up. Therefore the number of these paths is equal to $\dbinom n {n - k} = \dbinom n k$. Thus the total number of paths from $\left({0, 0}\right)$ to $\left({n, n}\right)$ that pass through $\left({k, n - k}\right)$ is equal to the product of: :the number of possible paths from $\left({0, 0}\right)$ to $\left({k, n - k}\right)$, which equals $\dbinom n k$ and: :the number of possible paths from $\left({k, n - k}\right)$ to $\left({n, n}\right)$, which equals $\dbinom n k$. So the total number of paths through $\left({k, n - k}\right)$ is equal to $\dbinom n k^2$. Summing over all possible values of $k \in 0, \ldots, n$ gives the total number of paths. Thus we get: :$\displaystyle \sum_{k \mathop = 0}^n \binom n k^2 = \binom {2 n} n$ {{qed}}	0
Let $G \left({z}\right) H \left({z}\right)$ be the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\left\langle{c'_n}\right\rangle$. By definition of [[Definition:Generating Function|generating function]]: {{begin-eqn}} {{eqn | l = G \left({z}\right) H \left({z}\right) | r = \sum_{k \mathop \ge 0} \dfrac {a_k} {k!} z^k \sum_{k \mathop \ge 0} \dfrac {b_k} {k!} z^k | c = }} {{eqn | r = \left({\dfrac {a_0} {0!} + \dfrac {a_1} {1!} z + \dfrac {a_2} {2!} z^2 + \cdots}\right) \left({\dfrac {b_0} {0!} + \dfrac {b_1} {1!} z + \dfrac {b_2} {2!} z^2 + \cdots}\right) | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = c'_n | r = \sum_{k \mathop = 0}^n \dfrac {a_k} {k!} \dfrac {b_{n - k} } {\left({n - k}\right)!} | c = [[Product of Generating Functions]] }} {{eqn | r = \dfrac 1 {n!} \left({\sum_{k \mathop = 0}^n \dfrac {n!} {k! \left({n - k}\right)!} a_k b_{n - k} }\right) | c = }} {{eqn | r = \dfrac 1 {n!} \left({\sum_{k \mathop = 0}^n \dbinom n k a_k b_{n - k} }\right) | c = {{Defof|Binomial Coefficient/Integers|index = 1|disp = Binomial Coefficient}} }} {{eqn | r = \dfrac {c_n} {n!} | c = where $c_n = \displaystyle \sum_{k \mathop = 0}^n \dbinom n k a_k b_{n - k}$ }} {{end-eqn}} Hence the result. {{qed}}	0
:$-100 \bmod 0 = -100$	0
The only [[Definition:Factorial|factorials]] which are the product of consecutive [[Definition:Factorial|factorials]] are: {{begin-eqn}} {{eqn | l = 0! | r = 0! \times 1! | c = }} {{eqn | l = 1! | r = 0! \times 1! | c = }} {{eqn | l = 2! | r = 1! \times 2! | c = }} {{eqn | r = 0! \times 1! \times 2! | c = }} {{eqn | l = 10! | r = 6! \times 7! | c = }} {{end-eqn}}	0
:$\displaystyle \sum_{k \mathop \ge 1} \dfrac 1 {T_k} = 2$ where $T_k$ denotes the $k$th [[Definition:Triangular Number|triangular number]].	0
Taking the [[Definition:Finite Group Axioms|finite group axioms]] in turn: === $\text {FG} 0$: Closure === From [[Modulo Multiplication on Reduced Residue System is Closed]]: :$\struct {\Z'_m, \times}$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed|lemma}} === $\text {FG} 1$: Associativity === We have that [[Modulo Multiplication is Associative]]. {{qed|lemma}} === $\text {FG} 2$: Finiteness === The order of $\struct {\Z'_m, \times}$ is $\map \phi n$ by definition, where $\map \phi n$ denotes the [[Definition:Euler Phi Function|Euler $\phi$ function]]. As $\map \phi n < n$ it follows that $\struct {\Z'_m, \times}$ is of [[Definition:Finite Order (Structure)|finite order]]. {{qed|lemma}} === $\text {FG} 3$: Cancellability === We have that [[Modulo Multiplication on Reduced Residue System is Cancellable]]. {{qed|lemma}} Thus all the [[Definition:Finite Group Axioms|finite group axioms]] are fulfilled, and $\struct {\Z'_m, \times}$ is a [[Definition:Group|group]]. It remains to note that [[Modulo Multiplication is Commutative]] to confirm that $\struct {\Z'_m, \times}$ is [[Definition:Abelian Group|abelian]]. {{qed}}	0
For all $n \in \Z$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {s - t \paren {n - k} } {n - k} \frac r {r - t k} = \binom {r + s - t n} n$ Let the {{LHS}} of this equation be denoted $\tuple {r, s, t, n}$. Let $n = 0$. Then: {{begin-eqn}} {{eqn | o = | r = \tuple {r, s, t, 0} }} {{eqn | r = \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {s + t k} {- k} \frac r {r - t k} | c = }} {{eqn | r = \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {s + t k} {- k} \frac r {r - t k} \delta_{k 0} | c = [[N Choose Negative Number is Zero]]: $\dbinom {s + t k} {- k} = 0$ for all $k > 0$ }} {{eqn | r = \binom r 0 \binom s 0 | c = {{Defof|Kronecker Delta}} }} {{eqn | r = 1 | c = [[Binomial Coefficient with Zero]] }} {{eqn | r = \binom {r + s} 0 | c = [[Binomial Coefficient with Zero]] }} {{eqn | r = \binom {r + s - t n} n | c = putting $n = 0$ }} {{end-eqn}} Thus $\map P 0$ holds. Let $n < 0$. Let $n = -m$ where $m > 0$. Then: {{begin-eqn}} {{eqn | o = | r = \tuple {r, s, t, n} }} {{eqn | r = \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {s - t \paren {-m - k} } {-m - k} \frac r {r - t k} | c = }} {{eqn | r = 0 | c = [[N Choose Negative Number is Zero]]: $\dbinom {s - t \paren {-m - k} } {-m - k} = 0$ }} {{eqn | r = \binom {r + s + t m} {-m} | c = [[N Choose Negative Number is Zero]]: $\dbinom {r + s + t m} {-m} = 0$ for all $m$ }} {{end-eqn}} Thus $\map P n$ holds for all $n < 0$. It remains to demonstrate that $\map P n$ holds for all $n > 0$. The proof continues by [[Second Principle of Mathematical Induction|strong induction]] on $n$. The procedure is to substitute $n - r + n t + m$ for the variable $s$ and establish that the identity holds for all $m \ge 0$. For all $m \in \Z_{\ge 0}$, let $\map P m$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {\paren {n - r + n t + m} - t \paren {n - k} } {n - k} \frac r {r - t k} = \binom {r + \paren {n - r + n t + m} - t n} n$ That is: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {n - r + m + t k} {n - k} \frac r {r - t k} = \binom {n + m} n$ === [[Sum over k of r-tk Choose k by s-t(n-k) Choose n-k by r over r-tk/Proof 1/Basis for the Induction|Basis for the Induction]] === Consider the special case where $s = n - 1 - r + n t$. {{:Sum over k of r-tk Choose k by s-t(n-k) Choose n-k by r over r-tk/Proof 1/Basis for the Induction}}{{qed|lemma}} This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P j$ is true, for all $j$ such that $0 \le j \le m$, then it logically follows that $\map P {m + 1}$ is true. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {n - r + m + t k} {n - k} \frac r {r - t k} = \binom {n + m} n$ from which it is to be shown that: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {n - r + m + 1 + t k} {n - k} \frac r {r - t k} = \binom {n + m + 1} n$ === [[Sum over k of r-tk Choose k by s-t(n-k) Choose n-k by r over r-tk/Proof 1/Lemma|Lemma]] === First a [[Definition:Lemma|lemma]]: {{:Sum over k of r-tk Choose k by s-t(n-k) Choose n-k by r over r-tk/Proof 1/Lemma}} === Induction Step === This is the [[Definition:Induction Step|induction step]]: We have that $\tuple {r, n - 1 - r + n t, t, n}$ holds. We have also determined that if: :$\tuple {r, s, t, n}$ holds and: :$\tuple {r, s - t, t, n - 1}$ holds then: :$\tuple {r, s + 1, t, n}$ holds. So $\map P m \implies \map P {m + 1}$. Thus $\tuple {r, s, t, n}$ is shown to hold for [[Definition:Infinite Set|infinitely many]] $s$. As both the {{LHS}} and {{RHS}} are [[Definition:Polynomial over Real Numbers|polynomials]] in $s$ it follows that $\tuple {r, s, t, n}$ holds for all $s$. Therefore: :$\displaystyle \sum_{k \mathop \ge 0} \binom {r - t k} k \binom {s - t \left({n - k}\right)} {n - k} \frac r {r - t k} = \binom {r + s - t n} n$ for all $r, s, t \in \R, n \in \Z$. {{qed}}	0
If $\gcd \set {a, n} = 1$, then $a x \equiv b \pmod n$ has a [[Definition:Unique|unique]] solution.	0
{{Proofread}} From the [[Definition:Weierstrass Form of Gamma Function|Weierstrass form of the Gamma function]]: :$\displaystyle \frac 1 {\Gamma \left({z}\right)} = z e^{\gamma z} \prod_{n \mathop = 1}^\infty \left({\left({1 + \frac z n}\right) e^{-z / n} }\right)$ Taking the [[Definition:Reciprocal|reciprocal]] of both sides: :$\displaystyle \Gamma \left({z}\right) = \frac {e^{-\gamma z}} z \prod_{n \mathop = 1}^\infty \frac {e^{z/n}} {1 + \frac z n}$ Taking the [[Definition:Derivative|derivative]] of both sides: {{begin-eqn}} {{eqn | l = \Gamma\,' \left({z}\right) | r = -\frac {e^{-\gamma z} \left({1 + \gamma z}\right)} {z^2} \prod_{n \mathop = 1}^\infty \left({\frac {e^{z / n} } {\left({1 + \frac z n}\right)} }\right) + \frac {e^{-\gamma z} } z \sum_{n \mathop = 1}^\infty \left({\frac z {n \left({z + n}\right)} \prod_{i \mathop = 1}^\infty \frac {e^{z / i} } {1 + \frac z i} }\right) | c = [[Product Rule for Derivatives/General Result|General Product Rule for Derivatives]] }} {{eqn | r = -\frac {e^{-\gamma z} \left({1 + \gamma z}\right)} {z^2} \frac z {e^{-\gamma z} } \Gamma \left({z}\right) + \frac {e^{-\gamma z} } z \sum_{n \mathop = 1}^\infty \left({\frac z {n \left({z + n}\right)} \frac z {e^{-\gamma z} } \Gamma \left({z} \right) }\right) | c = simplifying the [[Definition:Product Notation (Algebra)|product notation]] }} {{eqn | r = -\frac{1 + \gamma z} z \Gamma \left({z}\right) + \sum_{n \mathop = 1}^\infty \left({\frac {z \Gamma \left({z} \right)} {n \left({z + n}\right)} }\right) | c = simplifying }} {{end-eqn}} Dividing both sides by $\Gamma \left({z}\right)$: {{begin-eqn}} {{eqn | l = \frac {\Gamma\,' \left({z}\right)} {\Gamma \left({z} \right)} | r = -\frac {1 + \gamma z} z + \sum_{n \mathop = 1}^\infty \left({ \frac z {n \left({z + n}\right)} }\right) }} {{eqn | r = -\gamma - \frac 1 z + \sum_{n \mathop = 1}^\infty \left({ \frac 1 n - \frac 1 {z + n} }\right) }} {{eqn | r = -\gamma + \sum_{n \mathop = 1}^\infty \left({ \frac 1 n - \frac 1 {z + n - 1} }\right) | c = rearranging the series }} {{end-eqn}} {{qed}}	0
{{ProofWanted|Resembles [[Binet's Formula for Logarithm of Gamma Function]]}}	0
Let $n \in \Z$ such that $n \ge 4$. Let $\dbinom n k$ denote a [[Definition:Binomial Coefficient|binomial coefficient]] for $k \in \Z$. Then: :$\dbinom n k = \dbinom {n - 2} {k - 2} + 2 \dbinom {n - 2} {k - 1} + \dbinom {n - 2} k$ for $2 \le k \le n - 2$.	0
Let $\eqclass x {16}, \eqclass y {16} \in \struct {\Z_{16}, +}$. Then: {{begin-eqn}} {{eqn | l = \map \phi {\eqclass x {16} } \times \map \phi {\eqclass y {16} } | r = \map \phi {x + 16 m_1} \times \map \phi {y + 16 m_2} | c = {{Defof|Residue Class}}: for some representative $m_1, m_2 \in \Z$ }} {{eqn | r = 3 \uparrow \paren {x + 16 m_1} \times 3 \uparrow \paren {y + 16 m_2} | c = using [[Definition:Knuth Uparrow Notation|Knuth uparrow notation]] $3 \uparrow k := 3^k$ }} {{eqn | r = 3 \uparrow \paren {x + 16 m_1 + y + 16 m_2} | c = [[Product of Powers]] }} {{eqn | r = 3 \uparrow \paren {\paren {x + y} + 16 \paren {m_1 + m_2} } | c = }} {{eqn | r = 3 \uparrow \paren {\eqclass {x + y} {16} } | c = {{Defof|Residue Class}} and {{Defof|Modulo Addition}} }} {{eqn | r = \map \phi {\eqclass x {16} + \eqclass y {16} } | c = Definition of $\phi$ }} {{end-eqn}} Thus it is seen that $\phi$ is a [[Definition:Group Homomorphism|group homomorphism]]. {{qed|lemma}} It remains to be seen that $\phi$ is a [[Definition:Bijection|bijection]]. Because $17$ is [[Definition:Prime Number|prime]]: $\forall x \in \Z, 1 \le x < 17: x \perp 17$ where $\perp$ denotes [[Definition:Coprime Integers|coprimality]]. Thus by definition of [[Definition:Multiplicative Group of Reduced Residues|multiplicative group of reduced residues modulo $17$]]: :$\order {\struct {\Z'_{17}, \times} } = 16$ where $\order {\, \cdot \,}$ denotes the [[Definition:Order of Group|order]] of a [[Definition:Group|group]]. Similarly, by definition of [[Definition:Additive Group of Integers Modulo m|additive group of integers modulo $16$]]: :$\order {\struct {\Z_{16}, +} } = 16$ So: :$\order {\struct {\Z'_{17}, \times} } = \order {\struct {\Z_{16}, +} }$ which is a [[Definition:Necessary Condition|necessary condition]] for [[Definition:Group Isomorphism|group isomorphism]]. {{qed|lemma}} Now we have: {{begin-eqn}} {{eqn | l = 16 | o = \equiv | r = 0 | rr= \pmod {16} | c = }} {{eqn | ll= \leadsto | l = \map \phi {\eqclass {16} {16} } | r = \map \phi {\eqclass 0 {16} } | c = }} {{eqn | ll= \leadsto | l = \map \phi {\eqclass {16} {16} } | r = \eqclass 1 {17} | c = [[Group Homomorphism Preserves Identity]] }} {{eqn | n = 1 | ll= \leadsto | l = 3^{16} | o = \equiv | r = 1 | rr= \pmod {17} | c = Definition of $\phi$ }} {{end-eqn}} Now let $\eqclass x {16}, \eqclass y {16} \in \Z_{16}$ such that $\map \phi {\eqclass x {16} } = \map \phi {\eqclass y {16} }$. We have: {{begin-eqn}} {{eqn | l = \map \phi {\eqclass x {16} } | r = \map \phi {\eqclass y {16} } | c = }} {{eqn | ll= \leadsto | lo= \forall m_1, m_2 \in \Z | l = \map \phi {x + 16 m_1} | r = \map \phi {y + 16 m_2} | c = {{Defof|Residue Class}} }} {{eqn | ll= \leadsto | l = 3 \uparrow \paren {x + 16 m_1} | r = 3 \uparrow \paren {y + 16 m_2} | c = Definition of $\phi$ }} {{eqn | ll= \leadsto | l = 3^x \paren {3^{16} }^{m_1} | r = 3^y \paren {3^{16} }^{m_2} | c = [[Product of Powers]], [[Power of Power]] }} {{eqn | ll= \leadsto | l = 3^x \times 1^{m_1} | r = 3^y \times 1^{m_2} | c = as $3^{16} = 1 \pmod {17}$ from $(1)$ }} {{eqn | ll= \leadsto | l = 3^x | r = 3^y | c = }} {{eqn | ll= \leadsto | l = x | r = y | c = }} {{end-eqn}} Thus $\phi$ is an [[Definition:Injection|injection]]. From [[Equivalence of Mappings between Sets of Same Cardinality]] it follows that $\phi$ is a [[Definition:Bijection|bijection]]. {{qed|lemma}} Thus $\phi$ is a [[Definition:Bijection|bijective]] [[Definition:Group Homomorphism|group homomorphism]]. Hence the result by definition of [[Definition:Group Isomorphism|group isomorphism]]. {{Qed}}	0
Let $x \in \Z$ be a [[Definition:Positive Integer|positive integer]]. Then: :$\ds \lim_{n \mathop \to \infty} \dfrac {\paren {n + x}!} {n! n^x} = 1$	0
{{ProofWanted|Proof given later in book. Chapter III gets technical.}}	0
Let $x, y \in \R$ be [[Definition:Real Number|real numbers]]. Then $x$ is an '''integral multiple''' of $y$ {{iff}} $x$ is [[Definition:Congruence Modulo Integer|congruent to $0$ modulo $y$]]: :$x \equiv 0 \pmod y$ That is: :$\exists k \in \Z: x = 0 + k y$	0
Let $S$ be a [[Definition:Set|set]] with $n$ [[Definition:Element|elements]]. From the definition of [[Definition:Combination|$r$-combination]], $\displaystyle \sum_{i \mathop = 0}^n \binom n i$ is the total number of [[Definition:Subset|subsets]] of $S$. Hence $\displaystyle \sum_{i \mathop = 0}^n \binom n i$ is equal to the [[Cardinality of Power Set of Finite Set|cardinality of the power set]] of $S$. Hence the result. {{qed}}	0
The statement: :$\dbinom n k$ is not [[Definition:Divisor of Integer|divisible]] by $p$ is equivalent to: :$\dbinom n k \not \equiv 0 \pmod p$ The [[Lucas' Theorem/Corollary|corollary to Lucas' Theorem]] gives: :$\displaystyle \dbinom n k \equiv \prod_{j \mathop = 0}^r \dbinom {a_j} {b_j} \pmod p$ where: : $n, k \in \Z_{\ge 0}$ and $p$ is [[Definition:Prime Number|prime]] : the [[Definition:Number Base|representations of $n$ and $k$ to the base $p$]] are given by: ::$n = a_r p^r + \cdots + a_1 p + a_0$ ::$k = b_r p^r + \cdots + b_1 p + b_0$ Consider the form of $n = a p^m - 1$ when [[Definition:Number Base|represented to the base $p$]]: {{begin-eqn}} {{eqn | l = n | r = a p^m - 1 | c = }} {{eqn | r = a \paren {p^m - 1} + \paren {a - 1} | c = }} {{eqn | r = a \paren {p - 1} \sum_{j \mathop = 0}^{m - 1} p^i + \paren {a - 1} | c = [[Sum of Geometric Sequence/Corollary 1|Sum of Geometric Sequence: Corollary 1]] }} {{eqn | r = \paren {a - 1} p^m + \sum_{j \mathop = 0}^{m - 1} \paren {p - 1} p^i | c = after algebra }} {{end-eqn}} That is, all of the [[Definition:Digit|digits]] of $n$ are $p - 1$ except perhaps the first one. For $k$ such that $0 \le k \le n$, the [[Definition:Digit|digits]] of $k$ will all range over $0$ to $p - 1$. === Necessary Condition === Let $n = a p^m - 1$. Then all the [[Definition:Binomial Coefficient|binomial coefficients]] $\dbinom {a_j} {b_j}$ (except perhaps the first) are of the form $\dbinom {p - 1} k$ for $0 \le k < p$. By [[Binomial Coefficient of Prime Minus One Modulo Prime]]: :$\dbinom {p - 1} k \equiv \paren {-1}^k \pmod p$ and so: :$\dbinom {a_j} {b_j} \equiv \paren {-1}^k \pmod p$ for $0 \le j < r$ For the first digit, we have $0 \le b_r \le a_r < p$. We have $\dbinom {a_r} {b_r} > 0$. Since $a_r!$ is not [[Definition:Divisor of Integer|divisible]] by $p$, neither does $\dbinom {a_r} {b_r}$. Thus: :$\dbinom {a_r} {b_r} \not \equiv 0 \pmod p$ Hence: :$\displaystyle \prod_{j \mathop = 0}^r \dbinom {a_j} {b_j} \equiv \pm \dbinom {a_r} {b_r} \not \equiv 0 \pmod p$ which means $\dbinom n k$ is not [[Definition:Divisor of Integer|divisible]] by $p$ for any $k \in \Z_{\ge 0}$ where $0 \le k \le n$. {{qed|lemma}} === Sufficient Condition === We show the [[Definition:Contrapositive Statement|contrapositive]]. Suppose $n \ne a p^m - 1$. Then one of the [[Definition:Digit|digits]] $a_j$ (that is not the first one) will be less than $p - 1$. But the [[Definition:Digit|digits]] of all possible $k$ will range over $0$ to $p - 1$. Therefore there must be at least one $\dbinom {a_j} {b_j}$ such that $b_j > a_j$. Such a condition leads to $\dbinom {a_j} {b_j} = 0$. That is: :$\displaystyle \prod_{j \mathop = 0}^r \dbinom {a_j} {b_j} \equiv 0 \pmod p$ which means $\dbinom n k$ is [[Definition:Divisor of Integer|divisible]] by $p$ for some $k \le n$. {{qed}}	0
{{ProofWanted}} [[Category:Asymptotic Notation]] gv2fsl4w3ps2083791hiislfjhiw1zw	0
Let $\struct {\Z'_7, \times_7}$ denote the [[Multiplicative Group of Reduced Residues Modulo 7|multiplicative group of reduced residues modulo $7$]]. Then $\struct {\Z'_7, \times_7}$ is [[Definition:Cyclic Group|cyclic]].	0
Take the definition of [[Definition:Euler Numbers/Generating Function|Euler numbers]]: {{begin-eqn}} {{eqn | l = \sum_{n \mathop = 0}^\infty \frac {E_n x^n} {n!} | r = \frac {2e^x} {e^{2x} + 1} | c = }} {{eqn | r = \paren {\frac {2e^x } {e^{2x} + 1 } } \paren {\frac {e^{-x } } {e^{-x } } } | c = Multiply by [[1]] }} {{eqn | r = \paren {\frac 2 {e^{x} + e^{-x} } } | c = }} {{end-eqn}} From the definition of the [[Definition:Exponential Function/Real/Sum of Series|exponential function]]: {{begin-eqn}} {{eqn | l = e^x | r = \sum_{n \mathop = 0}^\infty \frac {x^n} {n!} | c = }} {{eqn | r = 1 + x + \frac {x^2} {2!} + \frac {x^3} {3!} + \frac {x^4} {4!} + \cdots | c = }} {{eqn | l = e^{-x} | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-x }^n } {n!} | c = }} {{eqn | r = 1 - x + \frac {x^2} {2!} - \frac {x^3} {3!} + \frac {x^4} {4!} - \cdots | c = }} {{eqn | l = \paren {\frac {e^x + e^{-x } } 2 } | r = \paren {\sum_{n \mathop = 0}^\infty \frac {x^{2n } } {\paren {2n }!} } | c = }} {{eqn | r = 1 + \frac {x^2} {2!} + \frac {x^4} {4!} + \cdots | c = [[Definition:Odd Integer|odd]] terms cancel in the [[Definition:Sum|sum]]. }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = 1 | r = \paren {\frac 2 {e^x + e^{-x } } } \paren {\frac {e^x + e^{-x } } 2 } | c = }} {{eqn | r = \paren {\sum_{n \mathop = 0}^\infty \frac {E_n x^n} {n!} } \paren {\sum_{n \mathop = 0}^\infty \frac {x^{2n } } {\paren {2n }!} } | c = }} {{end-eqn}} By [[Product of Absolutely Convergent Series]], we will let: {{begin-eqn}} {{eqn | l = a_n | r = \frac {E_n x^n} {n!} | c = }} {{eqn | l = b_n | r = \frac {x^{2n} } {\paren {2n }!} | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \sum_{n \mathop = 0}^\infty c_n | r = \paren { \displaystyle \sum_{n \mathop = 0}^\infty a_n } \paren {\displaystyle \sum_{n \mathop = 0}^\infty b_n } | rrr = =1 | c = }} {{eqn | l = c_n | r = \sum_{k \mathop = 0}^n a_k b_{n - k} | c = }} {{eqn | l = c_0 | r = \frac {E_0 x^0} {0!} \frac {x^{0} } {0!} | rrr = = 1 | c = $c_0 = \paren {a_0 } \paren {b_{0 - 0 } } = \paren {a_0 } \paren {b_0 }$ }} {{eqn | lll = \leadsto | l = \sum_{n \mathop = 1}^\infty c_n | r = \paren { \displaystyle \sum_{n \mathop = 0}^\infty a_n } \paren {\displaystyle \sum_{n \mathop = 0}^\infty b_n } - a_0 b_0 | rrr = =0 | c = [[Definition:Subtraction|Subtract]] [[1]] from both sides of the [[Definition:Equation|equation]]. }} {{end-eqn}} $\forall n \in \Z_{\gt 0}$, term by term $c_n$ is equal to: {{begin-eqn}} {{eqn | l = c_1 | r = \frac {E_0 x^0} {0!} \frac {x^2 } {2!} + \frac {E_1 x^1} {1!} \frac {x^0 } {0!} | rrr = = \frac {x^2 } {2! } E_0 | c = $= a_0 b_1 + a_1 b_0$ }} {{eqn | l = c_2 | r = \frac {E_0 x^0} {0!} \frac {x^4 } {4!} + \frac {E_1 x^1} {1!} \frac {x^2 } {2!} + \frac {E_2 x^2} {2!} \frac {x^0 } {0!} | rrr = = \frac {x^4 } {4! } E_0 + \frac {x^2 } {2! } E_2 | c = $= a_0 b_2 + a_1 b_1 + a_2 b_0$ }} {{eqn | l = c_3 | r = \frac {E_0 x^0} {0!} \frac {x^6 } {6!} + \frac {E_1 x^1} {1!} \frac {x^4 } {4!} + \frac {E_2 x^2} {2!} \frac {x^2 } {2!} + \frac {E_3 x^3} {3!} \frac {x^0 } {0!} | rrr = = \frac {x^6 } {6! } E_0 + \frac {x^4 } {2! 2! } E_2 | c = $= a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0$ }} {{eqn | l = c_4 | r = \frac {E_0 x^0} {0!} \frac {x^8 } {8!} + \frac {E_1 x^1} {1!} \frac {x^6 } {6!} + \frac {E_2 x^2} {2!} \frac {x^4 } {4!} + \frac {E_3 x^3} {3!} \frac {x^2 } {2!} + \frac {E_4 x^4} {4!} \frac {x^0 } {0!} | rrr = = \frac {x^8 } {0! 8! } E_0 + \frac {x^6 } {2! 4! } E_2 + \frac {5 x^4 } {4! 0! } E_4 | c = $= a_0 b_4 + a_1 b_3 + a_2 b_2 + a_3 b_1 + a_4 b_0$ }} {{eqn | l = \cdots | r = \cdots }} {{eqn | l = c_n | r = \frac {E_0 x^0} {0!} \frac {x^{2n} } {\paren {2n }!} + \frac {E_1 x^1} {1!} \frac {x^{2n - 2} } {\paren {2n - 2 }!} + \frac {E_2 x^2} {2!} \frac {x^{2n - 4} } {\paren {2n - 4 }!} + \cdots + \frac {E_n x^n} {n!} \frac {x^0 } {0!} | c = }} {{end-eqn}} Grouping like [[Definition:Even Integer|even]] terms produces: {{begin-eqn}} {{eqn | l = \paren {\frac 1 {0! 2!} } E_0 + \paren {\frac 1 {2! 0!} } E_2 | r = 0 | c = $x^2$ term }} {{eqn | l = \paren {\frac 1 {0! 4!} } E_0 + \paren {\frac 1 {2! 2!} } E_2 + \paren {\frac 1 {4! 0!} } E_4 | r = 0 | c = $x^4$ term }} {{end-eqn}} Multiplying $c_n$ through by $n!$ gives: {{begin-eqn}} {{eqn | l = n! c_n | r = \frac {E_0 x^0} {0!} \frac {n! x^n } {n!} + \frac {E_1 x^1} {1!} \frac {n! x^{n-1} } {\paren {n - 1 }!} + \cdots + \frac {E_n x^n} {n!} \frac {n! x^{0} } {0!} | rrr = = 0 | c = }} {{eqn | r = x^n \paren {\frac {n! } {0! n!} E_0 + \frac {n! } {1! \paren {n - 1 }!} E_1 + \cdots + \frac {n! } {n! 0!} E_n } | rrr = = 0 | c = factoring out $x^n$ }} {{end-eqn}} But those coefficients are the [[Definition:Binomial Coefficient|binomial coefficients]]: {{begin-eqn}} {{eqn | l = n! c_n | r = \dbinom n 0 E_0 + \dbinom n 1 E_1 + \dbinom n 2 E_2 + \cdots + \dbinom n n E_n | rrr = = 0 | c = }} {{end-eqn}} Hence the result. {{qed}} {{begin-eqn}} {{eqn | l = \sum_{k \mathop = 0}^{n} \dbinom {2 n} {2 k} E_{2 n - 2 k} | r = \binom {2 n} 0 E_{2 n} + \binom {2 n} 2 E_{2 n - 2} + \binom {2 n} 4 E_{2 n - 4} + \binom {2 n} 6 E_{2 n - 6} + \cdots + 1 | rrr = = 0 | c = }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = {}^{n - 1} P_n | r = n^{\underline {n - 1} } | c = [[Number of Permutations]]: $n^{\underline {n - 1} }$ denotes [[Definition:Falling Factorial|Falling Factorial]] }} {{eqn | r = n! | c = [[Integer to Power of Itself Less One Falling is Factorial]] }} {{eqn | r = {}^n P_n | c = [[Number of Permutations]] }} {{end-eqn}} {{qed}}	0
The [[Definition:Subset|subset]] $\left\{{0}\right\} \subset \N$ is [[Definition:Primitive Recursive Set|primitive recursive]].	0
Let $\left \langle {a_n}\right \rangle$ be the [[Definition:Sequence|sequence]] defined as: : $\displaystyle \forall n \in \N: a_n = \begin{cases} \binom m n & : n = 0, 1, 2, \ldots, m \\ 0 & : \text{otherwise}\end{cases}$ where $\displaystyle \binom m n$ denotes a [[Definition:Binomial Coefficient|binomial coefficient]]. Then the [[Definition:Generating Function|generating function]] for $\left \langle {a_n}\right \rangle$ is given as: : $\displaystyle G \left({z}\right) = \sum_{n \mathop = 0}^m \binom m n z^n = \left({1 + z}\right)^m$	0
'''Nonnumerical analysis''' is a branch of [[Definition:Discrete Mathematics|discrete mathematics]] that studies subjects emerging from the evolution of [[Definition:Computer Science|computer science]], and includes such topics as: :information processing :analysis of algorithms and so on.	0
A direct application of [[Set of Invertible Mappings forms Symmetric Group]]. {{qed}}	0
:$\displaystyle \sum_{j \mathop = 0}^n \left({-1}\right)^{n + 1} j \binom n j = 0$	0
{{begin-eqn}} {{eqn | l = \dbinom n k | r = \dbinom {n - 1} k + \dbinom {n - 1} {k - 1} | c = [[Pascal's Rule]] }} {{eqn | r = \paren {\dbinom {n - 2} {k - 1} + \dbinom {n - 2} k} + \paren {\dbinom {n - 2} {k - 2} + \dbinom {n - 2} {k - 1} } | c = [[Pascal's Rule]] (twice) }} {{eqn | r = \dbinom {n - 2} {k - 2} + 2 \dbinom {n - 2} {k - 1} + \dbinom {n - 2} k | c = simplifying }} {{end-eqn}} In the expression $\dbinom {n - 2} {k - 2} + 2 \dbinom {n - 2} {k - 1} + \dbinom {n - 2} k$ we note that: :if $k < 2$ then $\dbinom {n - 2} {k - 2}$ has a [[Definition:Negative Integer|negative]] coefficient on the bottom :if $k > n - 2$ then $\dbinom {n - 2} k$ has a coefficient on the bottom that is greater than $n$. Hence the usual comfortable range of $k$ is exceeded and so it cannot be guaranteed that the conditions are satisfied for the equation to be true. If $n \le 3$ then $2 \le k \le n - 2$ cannot be fulfilled. Hence the bounds on both $k$ and $n$. {{qed}}	0
This [[Definition:Set|set]] of $5$ [[Definition:Integer|integers]] has the property that the [[Definition:Integer Addition|sum]] of any $3$ of them is [[Definition:Square Number|square]]: {{begin-eqn}} {{eqn | l = 26 \, 072 \, 323 \, 311 \, 568 \, 661 \, 931 | o = }} {{eqn | l = 43 \, 744 \, 839 \, 742 \, 282 \, 591 \, 947 | o = }} {{eqn | l = 118 \, 132 \, 654 \, 413 \, 675 \, 138 \, 222 | o = }} {{eqn | l = 186 \, 378 \, 732 \, 807 \, 587 \, 076 \, 747 | o = }} {{eqn | l = 519 \, 650 \, 114 \, 814 \, 905 \, 002 \, 347 | o = }} {{end-eqn}}	0
It is to be demonstrated that $d$ satisfies all the [[Definition:Metric Space Axioms|metric space axioms]]. Let $u, v, w \in \map V {n, p}$ be arbitrary. === Proof of $(\text M 1)$ === By definition of [[Definition:Distance between Linear Codewords|distance]]: :$\map d {u, u} = 0$ So [[Definition:Metric Space Axioms|axiom $(\text M 1)$]] holds for $d$. {{qed|lemma}} === Proof of $(\text M 2)$ === Consider $\map d {u, v} + \map d {v, w}$. Let $\map d {u, w} \ne 0$. Then at each [[Definition:Term of Sequence|term]] at which $u$ and $w$ are different, those corresponding terms in either $u$ and $v$ or $v$ and $w$ must be different. So every contribution to the value of $\map d {u, w}$ is present in either $\map d {u, v}$ or $\map d {v, w}$. It follows that $\map d {u, v} + \map d {v, w} \ge \map d {u, w}$. So [[Definition:Metric Space Axioms|axiom $(\text M 2)$]] holds for $d$. {{qed|lemma}} === Proof of $(\text M 3)$ === $\map d {u, v} = \map d {v, u}$ by definition of [[Definition:Distance between Linear Codewords|distance]]. So [[Definition:Metric Space Axioms|axiom $(\text M 3)$]] holds for $d$. {{qed|lemma}} === Proof of $(\text M 4)$ === {{begin-eqn}} {{eqn | l = u | o = \ne | r = v | c = }} {{eqn | ll= \leadsto | l = \map d {u, v} | o = > | r = 0 | c = {{Defof|Distance between Linear Codewords}} }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $(\text M 4)$]] holds for $d$. {{qed|lemma}} Thus $d$ satisfies all the [[Definition:Metric Space Axioms|metric space axioms]] and so is a [[Definition:Metric|metric]]. {{qed}}	0
That $\beta$ is an [[Definition:Equivalence Relation|equivalence relation]] is proved in [[Equivalence Relation on Integers Modulo 5 induced by Squaring]]. The [[Definition:Set of Residue Classes|set of residue classes modulo $5$]] is: :$\set {\eqclass 0 5, \eqclass 1 5, \eqclass 2 5, \eqclass 3 5, \eqclass 4 5}$ Then: {{begin-eqn}} {{eqn | l = 0 \times 0 | r = 0 | c = }} {{eqn | ll= \leadsto | l = \eqclass 0 5 \times_5 \eqclass 0 5 | r = \eqclass 0 5 | c = }} {{eqn | l = 1 \times 1 | r = 1 | c = }} {{eqn | ll= \leadsto | l = \eqclass 1 5 \times_5 \eqclass 1 5 | r = \eqclass 1 5 | c = }} {{eqn | l = 2 \times 2 | r = 4 | c = }} {{eqn | ll= \leadsto | l = \eqclass 2 5 \times_5 \eqclass 2 5 | r = \eqclass 4 5 | c = }} {{eqn | l = 3 \times 3 | r = 9 | c = }} {{eqn | o = \equiv | r = 4 | rr= \pmod 5 | c = }} {{eqn | ll= \leadsto | l = \eqclass 3 5 \times_5 \eqclass 3 5 | r = \eqclass 4 5 | c = }} {{eqn | l = 4 \times 4 | r = 16 | c = }} {{eqn | o = \equiv | r = 1 | rr= \pmod 5 | c = }} {{eqn | ll= \leadsto | l = \eqclass 4 5 \times_5 \eqclass 4 5 | r = \eqclass 1 5 | c = }} {{end-eqn}} Thus we have that: :$\eqclass 1 5^2 = \eqclass 4 5^2 = \eqclass 1 \beta$ :$\eqclass 2 5^2 = \eqclass 3 5^2 = \eqclass 2 \beta$ Hence the result. {{qed}}	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $\Z_p$ be the [[Definition:Set of Residue Classes|set of residue classes modulo $p$]]. Let $C := \tuple {n, k}$ be a [[Definition:Linear Code|linear code]] of a [[Definition:Master Code|master code]] $\map V {n, p}$. Then $C$ satisfies the following conditions: :$(C \, 1): \quad \forall \mathbf x, \mathbf y \in C: \mathbf x + \paren {-\mathbf y} \in C$ :$(C \, 2): \quad \forall \mathbf x \in C, m \in \Z_p: m \times \mathbf x \in C$ where $+$ and $\times$ are the operations of [[Definition:Addition of Codewords in Linear Code|codeword addition]] and [[Definition:Multiple of Codeword in Linear Code|codeword multiplication]] respectively. {{expand|Add a page defining the difference between codewords.}}	0
If $k = 0$ then $\dbinom r k = r \dbinom {r - 1} {k - 1} = 0$ by [[Definition:Binomial Coefficient|definition]]. Otherwise: {{begin-eqn}} {{eqn | l = k \binom r k | r = k \frac {r^{\underline k} } {k!} | c = }} {{eqn | r = k \frac {r \paren {r - 1} \paren {r - 2} \dotsm \paren {r - k + 1} } {k \paren {k - 1} \paren {k - 2} \dotsm 1} | c = }} {{eqn | r = \frac {r \paren {r - 1} \paren {r - 2} \dotsm \paren {r - k + 1} } {\paren {k - 1} \paren {k - 2} \dotsm 1} | c = }} {{eqn | r = r \frac {\paren {r - 1} \paren {r - 2} \dotsm \paren {\paren {r - 1} - \paren {k - 1} + 1} } {\paren {k - 1} \paren {k - 2} \dotsm 1} | c = }} {{eqn | r = r \binom {r - 1} {k - 1} | c = }} {{end-eqn}} {{qed|lemma}} If $k \ne 0$, we can divide both sides of: :$k \dbinom r k = r \dbinom {r - 1} {k - 1}$ by $k$ to obtain: :$\dbinom r k = \dfrac r k \dbinom {r - 1} {k - 1}$ {{qed|lemma}} If $k \ne 0$ and $r \ne 0$, we can divide both sides of: :$\dbinom r k = \dfrac r k \dbinom {r - 1} {k - 1}$ by $r$ to obtain: :$\dfrac 1 r \dbinom r k = \dfrac 1 k \dbinom {r - 1} {k - 1}$ {{qed}}	0
Let $m, n \in \Z_{>0}$ be [[Definition:Strictly Positive Integer|(strictly) positive integers]]. Let $\struct {\Z_m, +}$ denote the [[Definition:Additive Group of Integers Modulo m|additive group of integers modulo $m$]]. The number of distinct [[Definition:Group Homomorphism|homomorphisms]] $\phi: \struct {\Z_m, +} \to \struct {\Z_n, +}$ is $\gcd \set {m, n}$.	0
The $5$th [[Definition:Hardy-Ramanujan Number|Hardy-Ramanujan number]] $\map {\operatorname {Ta} } 5$ is $48 \, 988 \, 659 \, 276 \, 962 \, 496$: {{begin-eqn}} {{eqn | l = 48 \, 988 \, 659 \, 276 \, 962 \, 496 | r = 38 \, 787^3 + 365 \, 757^3 | c = }} {{eqn | r = 107 \, 839^3 + 362 \, 753^3 | c = }} {{eqn | r = 205 \, 292^3 + 342 \, 952^3 | c = }} {{eqn | r = 221 \, 424^3 + 336 \, 588^3 | c = }} {{eqn | r = 231 \, 518^3 + 331 \, 954^3 | c = }} {{end-eqn}}	0
Apart from the general pattern, following directly from the definition of the [[Definition:Factorial|factorial]]: :$\paren {n!}! = n! \paren {n! - 1}!$ the only known [[Definition:Factorial|factorial]] which is the product of two [[Definition:Factorial|factorials]] is: :$10! = 6! \, 7!$	0
:$\displaystyle \sum_{k \mathop = 1}^n H_k = \left({n + 1}\right) H_n - n$ where $H_k$ denotes the $k$th [[Definition:Harmonic Number|harmonic number]].	0
Let $m \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Then: :$\displaystyle \sum_{k \mathop \in \Z} \dbinom m k_\mathcal F \left({-1}\right)^{\left\lceil{\left({m - k}\right) / 2}\right\rceil} {F_{n + k} }^{m - 1} = 0$ where: :$\dbinom m k_\mathcal F$ denotes a [[Definition:Fibonomial Coefficient|Fibonomial coefficient]] :$F_{n + k}$ denotes the $n + k$th [[Definition:Fibonacci Number|Fibonacci number]] :$\left\lceil{\, \cdot \,}\right\rceil$ denotes the [[Definition:Ceiling Function|ceiling function]]	0
We have: {{begin-eqn}} {{eqn | l = n | r = \sqbrk {a_r a_{r - 1} \dotso a_1 a_0}_H | c = }} {{eqn | r = \sum_{j \mathop = 0}^r a_j 16^j | c = {{Defof|Hexadecimal Notation}} }} {{end-eqn}} We have that: :$0 \le a_j < 16$ and so: {{begin-eqn}} {{eqn | l = a_j | r = \sqbrk {b_{j 3} b_{j 2} b_{j 1} b_{j 0} }_2 | c = }} {{eqn | r = \sum_{k \mathop = 0}^3 b_{j k} 2^k | c = {{Defof|Binary Notation}} }} {{end-eqn}} and so: {{begin-eqn}} {{eqn | l = n | r = \sqbrk {a_r a_{r - 1} \dotso a_1 a_0}_H | c = }} {{eqn | r = \sum_{j \mathop = 0}^r a_j 16^j | c = {{Defof|Hexadecimal Notation}} }} {{eqn | r = \sum_{j \mathop = 0}^r \paren {\sum_{k \mathop = 0}^3 b_{j k} 2^k} 16^j | c = {{Defof|Binary Notation}} }} {{eqn | r = \sum_{j \mathop = 0}^r \paren {\sum_{k \mathop = 0}^3 b_{j k} 2^k} 2^{4 j} | c = }} {{eqn | r = \sum_{j \mathop = 0}^r \paren {\sqbrk {b_{j 3} b_{j 2} b_{j 1} b_{j 0} }_2} 2^{4 j} | c = {{Defof|Binary Notation}} }} {{eqn | r = \sqbrk {b_{r 3} b_{r 2} b_{r 1} b_{r 0} }_2 2^{4 r} + \sqbrk {b_{\paren {r - 1} 3} b_{\paren {r - 1} 2} b_{\paren {r - 1} 1} b_{\paren {r - 1} 0} }_2 2^{4 {r - 1} } + \cdots + \sqbrk {b_{1 3} b_{1 2} b_{1 1} b_{1 0} }_2 2^4 + \sqbrk {b_{0 3} b_{0 2} b_{0 1} b_{0 0} }_2 | c = }} {{eqn | r = \sqbrk {b_{r 3} b_{r 2} b_{r 1} b_{r 0} b_{\paren {r - 1} 3} b_{\paren {r - 1} 2} b_{\paren {r - 1} 1} b_{\paren {r - 1} 0} \dotso b_{1 3} b_{1 2} b_{1 1} b_{1 0} b_{0 3} b_{0 2} b_{0 1} b_{0 0} }_2 | c = }} {{end-eqn}} Hence the result. {{qed}}	0
Let $f: S \to S$ is a [[Definition:Permutation|permutation]] of $S$. By definition, a [[Definition:Permutation|permutation]] is a [[Definition:Bijection|bijection]] such that the [[Definition:Domain of Mapping|domain]] and [[Definition:Codomain of Mapping|codomain]] are the same [[Definition:Set|set]]. From [[Bijection iff Inverse is Bijection]], it follows $f^{-1}$ is a [[Definition:Bijection|bijection]]. From the definition of [[Definition:Inverse Relation|inverse relation]], the [[Definition:Domain of Mapping|domain]] of a [[Definition:Relation|relation]] is the [[Definition:Codomain of Mapping|codomain]] of its [[Definition:Inverse Relation|inverse]] and vice versa. Thus the [[Definition:Domain of Mapping|domain]] and [[Definition:Codomain of Mapping|codomain]] of $f^{-1}$ are both $S$ and it follows that $f^{-1}$ is a [[Definition:Permutation|permutation]]. {{Qed}}	0
Consider [[Abel's Generalisation of Binomial Theorem]]: {{:Abel's Generalisation of Binomial Theorem}} This holds in the special case where $x + y = 0$.	0
From [[Solution of Linear Congruence/Existence|Solution of Linear Congruence: Existence]]: :the problem of finding all integers satisfying the [[Definition:Linear Congruence|linear congruence]] $a x \equiv b \pmod n$ is the same problem as: :the problem of finding all the $x$ values in the [[Definition:Linear Diophantine Equation|linear Diophantine equation]] $a x - n y = b$. Let: :$\gcd \set {a, n} = 1$ Let $x = x_0, y = y_0$ be one solution to the [[Definition:Linear Diophantine Equation|linear Diophantine equation]]: :$a x - n y = b$ From [[Solution of Linear Diophantine Equation]], the general solution is: :$\forall k \in \Z: x = x_0 + n k, y = y_0 + a k$ But: :$\forall k \in \Z: x_0 + n k \equiv x_0 \pmod n$ Hence $x \equiv x_0 \pmod n$ is the only solution of $a x \equiv b \pmod n$. {{qed}}	0
Let $S$ be a [[Definition:Set|set]] of $n$ [[Definition:Element|elements]]. Let $r \in \N: r \le n$. An '''$r$-permutation of $S$''' is an ordered selection of $r$ [[Definition:Element|elements]] of $S$.	0
Let $a, b \in \Z$. Then: :$a^2 \divides b^2 \iff a \divides b$ where $\divides$ denotes [[Definition:Divisor of Integer|integer divisibility]]. {{:Euclid:Proposition/VIII/14}}	0
{{begin-eqn}} {{eqn | l = \paren {\eqclass x m +_m \eqclass y m} +_m \eqclass z m | r = \eqclass {x + y} m +_m \eqclass z m | c = {{Defof|Modulo Addition}} }} {{eqn | r = \eqclass {\paren {x + y} + z} m | c = {{Defof|Modulo Addition}} }} {{eqn | r = \eqclass {x + \paren {y + z} } m | c = [[Associative Law of Addition]] }} {{eqn | r = \eqclass x m +_m \eqclass {y + z} m | c = {{Defof|Modulo Addition}} }} {{eqn | r = \eqclass x m +_m \paren {\eqclass y m +_m \eqclass z m} | c = {{Defof|Modulo Addition}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sum_k \binom r k \binom s k k | r = \sum_k \binom r k \binom {s - 1} {k - 1} s | c = [[Factors of Binomial Coefficient]] }} {{eqn | r = s \sum_k \binom r k \binom {s - 1} {k - 1} | c = as $s$ is [[Definition:Constant|constant]] }} {{eqn | r = s \binom {r + s - 1} {r - 1} | c = [[Sum over k of r Choose m+k by s Choose n+k|Sum over $k$ of $\dbinom r {m + k} \dbinom s {n + k}$]]: $s \gets s - 1$, $m \gets 0$, $n \gets -1$ }} {{end-eqn}} {{qed}}	0
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$H_n = \dfrac {\left[{ {n + 1} \atop 2}\right]} {n!}$ $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = H_0 | r = 0 | c = }} {{eqn | r = \dfrac {\left[{1 \atop 2}\right]} {0!} | c = [[Unsigned Stirling Number of the First Kind of Number with Greater]] }} {{end-eqn}} Thus $P \left({0}\right)$ is seen to hold. === Basis for the Induction === $P \left({1}\right)$ is the case: {{begin-eqn}} {{eqn | l = H_1 | r = 1 | c = }} {{eqn | r = \dfrac {\left[{2 \atop 2}\right]} {1!} | c = [[Unsigned Stirling Number of the First Kind of Number with Self]] }} {{end-eqn}} Thus $P \left({1}\right)$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$H_k = \dfrac {\left[{ {k + 1} \atop 2}\right]} {k!}$ from which it is to be shown that: :$H_{k + 1} = \dfrac {\left[{ {k + 2} \atop 2}\right]} {\left({k + 1}\right)!}$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \dfrac {\left[{ {k + 2} \atop 2}\right]} {\left({k + 1}\right)!} | r = \dfrac {\left({k + 1}\right) \left[{ {k + 1} \atop 2}\right] + \left[{ {k + 1} \atop 1}\right]} {\left({k + 1}\right)!} | c = {{Defof|Unsigned Stirling Numbers of the First Kind}} }} {{eqn | r = \dfrac {\left({k + 1}\right) \left[{ {k + 1} \atop 2}\right] + k!} {\left({k + 1}\right)!} | c = [[Unsigned Stirling Number of the First Kind of n+1 with 1]] }} {{eqn | r = \dfrac {\left[{ {k + 1} \atop 2}\right]} {k!} + \dfrac 1 {k + 1} | c = simplifying }} {{eqn | r = H_k + \dfrac 1 {k + 1} | c = [[Harmonic Number as Unsigned Stirling Number of First Kind over Factorial#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = H_{k + 1} | c = {{Defof|Harmonic Number}} }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{\ge 0}: H_n = \dfrac {\left[{ {n + 1} \atop 2}\right]} {n!}$ {{qed}}	0
Let $ABC$ be the given [[Definition:Segment of Circle|segment of a circle]] whose [[Definition:Base of Segment|base]] is $AC$. Bisect $AC$ at $D$, draw $DB$ [[Definition:Perpendicular|perpendicular]] to $AC$, and join $AB$. First suppose that $ABC$ is such that $\angle ABD > \angle BAD$: :[[File:Euclid-III-25a.png|250px]] On $BA$, construct $\angle BAE$ equal to $\angle ABD$. Join $BD$ through to $E$ and join $EC$. Then $E$ is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. Second, suppose that $ABC$ is such that $\angle ABD = \angle BAD$: :[[File:Euclid-III-25b.png|250px]] Then $D$ is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. Finally, suppose that $ABC$ is such that $\angle ABD < \angle BAD$: :[[File:Euclid-III-25c.png|250px]] On $BA$, construct $\angle BAE$ equal to $\angle ABD$. The point $E$, which falls on $BD$, is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. === Proof of Construction === First suppose that $ABC$ is such that $\angle ABD > \angle BAD$: :[[File:Euclid-III-25a.png|250px]] Since $\angle ABE = \angle BAE$, from [[Triangle with Two Equal Angles is Isosceles]] we have that $EB = EA$. Since $AD = DC$ and $DE$ is common, and $\angle ADE$ is a [[Definition:Right Angle|right angle]], by [[Triangle Side-Angle-Side Equality]] we have that $\triangle ADE = \triangle CDE$. Hence $AE = CE$ both of which are equal to $BE$ from above. So from [[Condition for Point to be Center of Circle]] $E$ is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. Second, suppose that $ABC$ is such that $\angle ABD = \angle BAD$: :[[File:Euclid-III-25b.png|250px]] From [[Triangle with Two Equal Angles is Isosceles]] we have that $AD = DB$ and so also equal to $DC$. So from [[Condition for Point to be Center of Circle]] $D$ is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. Incidentally, note that in this case [[Definition:Segment of Circle|segment]] $ABC$ is actually a [[Definition:Semicircle|semicircle]]. Finally, suppose that $ABC$ is such that $\angle ABD < \angle BAD$: :[[File:Euclid-III-25c.png|250px]] The same proof applies: Since $\angle ABE = \angle BAE$, from [[Triangle with Two Equal Angles is Isosceles]] we have that $EB = EA$. Since $AD = DC$ and $DE$ is common, and $\angle ADE$ is a [[Definition:Right Angle|right angle]], by [[Triangle Side-Angle-Side Equality]] we have that $\triangle ADE = \triangle CDE$. Hence $AE = CE$ both of which are equal to $BE$ from above. So from [[Condition for Point to be Center of Circle]] $E$ is the [[Definition:Center of Circle|center]] of the required [[Definition:Circle|circle]]. {{qed}}	0
:[[File:CenterOfCurvature.png|400px]] Let $P = \tuple {x, y}$ be a general [[Definition:Point|point]] on $C$. Let $Q = \tuple {X, Y}$ be the [[Definition:Center of Curvature|center of curvature]] of $C$ at $P$. From the above diagram: :$x - X = \pm \rho \sin \psi$ :$Y - y = \pm \rho \cos \psi$ where: :$\rho$ is the [[Definition:Radius of Curvature|radius of curvature]] of $C$ at $P$ :$\psi$ is the [[Definition:Angle|angle]] between the [[Definition:Tangent to Curve|tangent]] to $C$ at $P$ and the [[Definition:X-Axis|$x$-axis]]. Whether the sign is plus or minus depends on whether the curve is [[Definition:Convex Real Function|convex]] or [[Definition:Concave Real Function|concave]]. By definition of [[Definition:Radius of Curvature|radius of curvature]]: :$(1): \quad \begin {cases} x - X = \dfrac 1 k \sin \psi \\ Y - y = \dfrac 1 k \cos \psi \end {cases}$ where $k$ is the [[Definition:Curvature|curvature]] of $C$ at $P$, given by: :$k = \dfrac {x' y'' - y' x''} {\paren {x'^2 + y'^2}^{3/2} }$ We have that: :$\sin \psi = \dfrac {\d y} {\d s} = \dfrac {y'} {\sqrt {x'^2 + y'^2} }$ :$\cos \psi = \dfrac {\d x} {\d s} = \dfrac {x'} {\sqrt {x'^2 + y'^2} }$ Substituting for $k$ and $\psi$ in $(1)$ gives: {{begin-eqn}} {{eqn | l = x - X | r = \dfrac {\paren {x'^2 + y'^2}^{3/2} } {x' y'' - y' x''} \dfrac {y'} {\sqrt {x'^2 + y'^2} } | c = }} {{eqn | r = \dfrac {y' \paren {x'^2 + y'^2} } {x' y'' - y' x''} | c = }} {{eqn | ll= \leadsto | l = X | r = x - \dfrac {y' \paren {x'^2 + y'^2} } {x' y'' - y' x''} | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = Y - y | r = \dfrac {\paren {x'^2 + y'^2}^{3/2} } {x' y'' - y' x''} \dfrac {x'} {\sqrt {x'^2 + y'^2} } | c = }} {{eqn | r = \dfrac {x' \paren {x'^2 + y'^2} } {x' y'' - y' x''} | c = }} {{eqn | ll= \leadsto | l = Y | r = y + \dfrac {x' \paren {x'^2 + y'^2} } {x' y'' - y' x''} | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sec i | r = \sech 1 | c = [[Hyperbolic Secant in terms of Secant]] }} {{eqn | r = \frac 2 {e^1 + e^{-1} } | c = {{Defof|Hyperbolic Secant}} }} {{eqn | r = \frac {2 e} {e^2 + 1} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $e$ }} {{end-eqn}} {{qed}}	0
{{:Euclid:Proposition/XI/33/Porism}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arcsin \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac 1 {\sqrt {a^2 - x^2} } | c = [[Derivative of Arcsine of x over a|Derivative of $\arcsin \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = 1 | c = }} {{eqn | ll= \leadsto | l = v | r = x | c = [[Primitive of Constant]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \arcsin \frac x a \rd x | r = x \arcsin \frac x a - \int x \paren {\frac 1 {\sqrt {a^2 - x^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = x \arcsin \frac x a - \int \frac {x \rd x} {\sqrt {a^2 - x^2} } + C | c = simplifying }} {{eqn | r = x \arcsin \frac x a - \paren {-\sqrt {a^2 - x^2} } + C | c = [[Primitive of x over Root of a squared minus x squared|Primitive of $\dfrac x {\sqrt {a^2 - x^2} }$]] }} {{eqn | r = x \arcsin \frac x a + \sqrt {a^2 - x^2} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Consider the [[Definition:Folium of Descartes|folium of Descartes]] defined in [[Definition:Folium of Descartes/Parametric Form|parametric form]] as: :$\begin {cases} x = \dfrac {3 a t} {1 + t^3} \\ y = \dfrac {3 a t^2} {1 + t^3} \end {cases}$ :[[File:FoliumOfDescartes.png|500px]] The [[Definition:Point|point]] on the loop at which the $x$ value is at a maximum occurs when $t = \sqrt [3] {\dfrac 1 2}$, corresponding to the point $P$ defined as: :$P = \tuple {2^{2/3} a, 2^{1/3} a}$	0
Let $x$ be a [[Definition:Real Number|real number]]. Then: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \cos x | o = < | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{end-eqn}} where $\cos$ is the [[Definition:Real Cosine Function|real cosine function]].	0
It follows from [[Argument of Product equals Sum of Arguments]] that the [[Definition:Argument of Complex Number|$\arg \left({z}\right)$ function]] for all $z \in \C$ satisfies the relationship: :$\arg \left({z_1 z_2}\right) = \arg \left({z_1}\right) + \arg \left({z_2}\right)$ which means that $\arg \left({z}\right)$ is a kind of [[Definition:General Logarithm|logarithm]], in the sense that it satisfies the fundamental property of logarithms: :$\log x y = \log x + \log y$ Notice that $\arg \left({z}\right)$ can not be considered a generalization to complex values of the ordinary $\log$ function for real values, since for $x \in \R$, we have: :$0 = \arg \left({x}\right) \ne \log x$ If we do wish to generalize the $\log$ [[Definition:Real Function|function]] to complex values, we can use $\arg \left({z}\right)$ to define a set of functions: :$\operatorname{alog} \left({z}\right) = a \arg \left({z}\right) + \log \left\vert{z}\right\vert$ for any $a \in \C$, where $\left\vert{z}\right\vert$ is the [[Definition:Modulus of Complex Number|modulus]] of $z$. All [[Definition:Complex Function|functions]] satisfy the fundamental property of logarithms and also coincide with the $\log$ function for all real values. This is established in the following lemma. === Lemma 1 === For all $a,z \in \C$, define the (complex valued) function $\operatorname{alog}$ as: : $\operatorname{alog} \left({z}\right) = a \arg \left({z}\right) + \log \left\vert{z}\right\vert$ then, for any $z_1, z_2 \in \C$ and $x \in \R$: : $\operatorname{alog} \left({z_1 z_2}\right) = \operatorname{alog} \left({z_1}\right) + \operatorname{alog} \left({z_2}\right)$ and: : $\operatorname{alog} \left({x}\right) = \log x$ This means that our (complex valued) $\operatorname{alog}$ functions can genuinely be considered generalizations of the (real valued) $\log$ function. === Proof of Lemma 1 === Let $z_1, z_2$ be any two [[Definition:Complex Number|complex numbers]], straightforward substitution on the definition of $\operatorname{alog}$ yields: {{begin-eqn}} {{eqn | l = \operatorname{alog} \left({z_1 z_2}\right) | r = a \arg \left({z_1 z_2}\right) + \log \left\vert {z_1 z_2} \right\vert }} {{eqn | r = a \left({\arg \left({z_1}\right) + \arg \left({z_1}\right)}\right) + \log \left({\left\vert {z_1} \right\vert \left\vert {z_2} \right\vert}\right) }} {{eqn | r = a \left({\arg \left({z_1}\right) + \arg \left({z_1}\right)}\right) + \log \left\vert {z_1} \right\vert + \log \left\vert {z_2} \right\vert }} {{eqn | r = a \arg \left({z_1}\right) + \log \left\vert {z_1} \right\vert + a \arg \left({z_2}\right) + \log \left\vert {z_2} \right\vert }} {{eqn | r = \operatorname{alog} \left({z_1}\right) + \operatorname{alog} \left({z_1}\right) }} {{end-eqn}} Second part of our lemma is even more straightforward since for $x \in \R$, we have: :$\arg \left({x}\right) = 0$ Then: {{begin-eqn}} {{eqn | l = \operatorname{alog} \left({x}\right) | r = a \arg \left({x}\right) + \log \left\vert{x}\right\vert }} {{eqn | r = \log x }} {{end-eqn}} which concludes the proof of Lemma 1. {{qed|lemma}} We're left with an infinitude of possible generalizations of the $\log$ function, namely one for each choice of $a$ in our definition of $\operatorname{alog}$. The following lemma proves that there's a value for $a$ that guarantees our definition of $\operatorname{alog}$ satisfies the much desirable property of $\log$: :$\dfrac{\mathrm d \log x} {\mathrm d x} = \dfrac 1 x$ === Lemma 2 === Let $\operatorname{alog} \left({z}\right) = a \arg \left({z}\right) + \log \left|{z}\right|$. Then if: : $\dfrac {\mathrm d \left({\operatorname{alog} z}\right)} {\mathrm d z} = \dfrac 1 z$ we must have: : $a = i$ === Proof of Lemma 2 === Let $z \in \C$ be such that: : $\left\vert{z}\right\vert = 1$ and: : $\arg \left({z}\right) = \theta$ Then: : $z = \cos \theta + i \sin \theta$ Plugging those values in our definition of $\operatorname{alog}$: {{begin-eqn}} {{eqn | l = \operatorname{alog} \left({z}\right) | r = a \arg \left({\cos \theta + i \sin \theta}\right) + \log \left\vert{z}\right\vert }} {{eqn | r = a \theta + \log 1 = a \theta }} {{end-eqn}} We now have: :$a \theta = \operatorname{alog} \left({\cos \theta + i \sin \theta}\right)$ Taking the derivative with respect to $\theta$ on both sides, we have {{begin-eqn}} {{eqn | l = \frac{\mathrm d}{\mathrm d \theta} (a \theta) | r = \frac{\mathrm d}{\mathrm d \theta} \left({\operatorname{alog} \left({\cos \theta + i \sin \theta}\right)}\right) }} {{eqn | l = a | r = \dfrac {\mathrm d \left({\cos \theta + i \sin \theta}\right)} {\mathrm d \theta} \dfrac {\mathrm d \left({\operatorname{alog} \left({\cos \theta + i \sin \theta}\right)}\right)} {\mathrm d \left({\cos \theta + i \sin \theta}\right)} | c = [[Chain Rule for Derivatives]] }} {{eqn | l = a | r = \left({-\sin \theta + i \cos \theta}\right) \frac 1 {\cos \theta + i \sin \theta} | c = from our assumption that $\dfrac {\mathrm d \left({\operatorname{alog} z}\right)} {\mathrm d z} = \dfrac 1 z$ }} {{end-eqn}} This last equation is true regardless of the value of $\theta$. In particular, for $\theta = 0$, we must have: : $a = i$ which proves the lemma. {{qed|lemma}} We now have established there is one function which truly deserves to be called the logarithm of complex numbers, defined as: :$\log \left({z}\right) = i \arg \left({z}\right) + \log \left\vert{z}\right\vert$ Since for any $z, z_1, z_2 \in \C, x \in \R$ it satisfies: : $\log \left({z_1 z_2}\right) = \log \left({z_1}\right) + \log \left({z_2}\right)$ : $\log \left({x}\right) = \log x$ : $\dfrac {\mathrm d \left({\log \left({z}\right)}\right)} {\mathrm d z} = \dfrac 1 z$ Let its [[Definition:Inverse Mapping|inverse function]] be referred to as the '''exponential''' of complex numbers, denoted as $e^z$. If we write $z$ in [[Definition:Polar Form of Complex Number|its polar form]]: : $z = \left\vert{z}\right\vert \left({\cos \theta + i \sin \theta}\right)$ we have that: : $e^{i \theta + \log \left\vert{z}\right\vert} = \left\vert{z}\right\vert \left({\cos \theta + i \sin \theta}\right)$ Consider this equation for any number $z$ such that $\left\vert{z}\right\vert = 1$. Then: : $e^{i \theta} = \cos \theta + i \sin \theta$ {{qed}}	0
:$\displaystyle \int \cos a x \cos p x \rd x = \frac {\map \sin {\paren {a - p} x} } {2 \paren {a - p} } + \frac {\map \sin {\paren {a + p} x} } {2 \paren {a + p} } + C$	0
Let a [[Definition:Set|set]] of identical [[Definition:Sphere (Geometry)|spheres]] in a [[Definition:Dimension (Geometry)|$24$-dimensional space]] be arranged in a [[Definition:Leech Lattice|Leech lattice]]. Then each [[Definition:Sphere (Geometry)|sphere]] will touch $196 \, 560$ other [[Definition:Sphere (Geometry)|spheres]]. This is believed to be the densest possible [[Definition:Sphere (Geometry)|sphere]] packing in [[Definition:Dimension (Geometry)|$24$ dimensions]].	0
In equal [[Definition:Circle|circles]], [[Definition:Angle|angles]] standing on equal [[Definition:Arc of Circle|arcs]] are equal to one another, whether at the [[Definition:Center of Circle|center]] or at the [[Definition:Circumference of Circle|circumference]] of those [[Definition:Circle|circles]]. {{:Euclid:Proposition/III/27}}	0
Since the [[Definition:Open Real Interval|open interval]] $\openint 0 \infty$ is [[Definition:Connected (Topology)|connected]], then so is $G$ by [[Continuous Image of Connected Space is Connected]]. It is enough, from [[Set between Connected Set and Closure is Connected]], to show that $J \subseteq \map \cl G$. Let $p \in J$, say, $\tuple {0, y}$ where $-1 \le y \le 1$. We need to show that: :$\forall \epsilon > 0: \map {N_\epsilon} p \cap G \ne \O$ where $\map {N_\epsilon} p$ is the [[Definition:Epsilon-Neighborhood (Real Number Line)|$\epsilon$-neighborhood]] of $p$. Let us choose $n \in \N: \dfrac 1 {2 n \pi} < \epsilon$. From [[Sine of Half-Integer Multiple of Pi]]: :$\map \sin {\dfrac {\paren {4 n + 1} \pi} 2} = 1$ and: :$\map \sin {\dfrac {\paren {4 n + 3} \pi} 2} = -1$ So by the [[Intermediate Value Theorem]], $\map \sin {\dfrac 1 x}$ takes every value between $-1$ and $1$ in the [[Definition:Closed Real Interval|closed interval]] $\closedint {\dfrac 2 {\paren {4 n + 3} \pi} } {\dfrac 2 {\paren {4 n + 1} \pi} }$. In particular, $\map \sin {\dfrac 1 {x_0} } = y$ for some $x_0$ in this interval. The distance between the points $\tuple {0, y}$ and $\tuple {x_0, \map \sin {\dfrac 1 {x_0} } } = \tuple {x_0, y}$ is $x_0 < \epsilon$. So: :$\tuple {x_0, \map \sin {\dfrac 1 {x_0} } } \in \map {N_\epsilon} p \cap G$ as required. {{qed}}	0
The [[Definition:Arc Length|length]] of the [[Definition:Arc of Circle|arc]] of $C_2$ between two adjacent [[Definition:Cusp of Hypocycloid|cusps]] of $H$ is $2 \pi b$. The total length of the [[Definition:Circumference of Circle|circumference]] of $C_1$ is $2 \pi a$. Thus the total number of [[Definition:Cusp of Hypocycloid|cusps]] of $H$ is: :$\dfrac {2 \pi a} {2 \pi b} = \dfrac {2 \pi n b} {2 \pi b} = n$ {{qed}}	0
:$\displaystyle \int \sec^n a x \tan a x \ \mathrm d x = \frac {\sec^n a x} {n a} + C$	0
{{begin-eqn}} {{eqn | l = \sec 315 \degrees | r = \map \sec {360 \degrees - 45 \degrees} | c = }} {{eqn | r = \sec 45 \degrees | c = [[Secant of Conjugate Angle]] }} {{eqn | r = \sqrt 2 | c = [[Secant of 45 Degrees|Secant of $45 \degrees$]] }} {{end-eqn}} {{qed}}	0
For the first part, if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$: {{begin-eqn}} {{eqn | l = \cos x | r = +\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Cosine in terms of Tangent]] }} {{eqn | ll= \leadsto | l = \frac 1 {\paren {\frac 1 {\cos x} } } | r = +\frac 1 {\sqrt {1 + \tan^2 x} } }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\paren {\frac {\sin x} {\cos x} } } | r = +\frac 1 {\sqrt {1 + \tan^2 x} } | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\sin x$ }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\tan x} | r = + \frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | ll= \leadsto | l = \sin x | r = + \frac {\tan x} {\sqrt {1 + \tan^2 x} } }} {{end-eqn}} For the second part, if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$: {{begin-eqn}} {{eqn | l = \cos x | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Cosine in terms of Tangent]] }} {{eqn | ll= \leadsto | l = \frac 1 {\paren {\frac 1 {\cos x} } } | r = -\frac 1 {\sqrt {1 + \tan^2 x} } }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\paren {\frac {\sin x} {\cos x} } } | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\sin x$ }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\tan x} | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | ll= \leadsto | l = \sin x | r = -\frac {\tan x} {\sqrt {1 + \tan^2 x} } }} {{end-eqn}} When $\cos x = 0$, $\tan x$ is undefined. {{qed}}	0
Let $a, b, c$ be the [[Definition:Length of Line|lengths]] of the [[Definition:Side of Polygon|sides]] of a [[Definition:Pythagorean Triangle|Pythagorean triangle]] $T$. Thus $a, b, c$ form a [[Definition:Pythagorean Triple|Pythagorean triple]]. By definition of [[Definition:Pythagorean Triple|Pythagorean triple]], $a, b, c$ are in the form: :$2 m n, m^2 - n^2, m^2 + n^2$ We have that $m^2 + n^2$ is always the [[Definition:Hypotenuse|hypotenuse]]. Thus the [[Definition:Area|area]] of $T$ is given by: :$\AA = m n \paren {m^2 - n^2}$ The [[Definition:Perimeter|perimeter]] of $T$ is given by: :$\PP = m^2 - n^2 + 2 m n + m^2 + n^2 = 2 m^2 + 2 m n$ We need to find all $m$ and $n$ such that $\PP = 2 \AA$. Thus: {{begin-eqn}} {{eqn | l = 2 m^2 + 2 m n | r = 2 m n \paren {m^2 - n^2} | c = }} {{eqn | ll= \leadsto | l = m + n | r = n \paren {m + n} \paren {m - n} | c = }} {{eqn | ll= \leadsto | l = n \paren {m - n} | r = 1 | c = }} {{end-eqn}} As $m$ and $n$ are both [[Definition:Strictly Positive Integer|(strictly) positive integers]], it follows immediately that: :$n = 1$ :$m - n = 1$ and so: :$m = 2, n = 1$ and the result follows. {{qed}}	0
The [[Definition:Integer Multiplication|product]] of the number of [[Definition:Edge of Polyhedron|edges]], [[Definition:Edge of Polyhedron|edges]] per [[Definition:Face of Polyhedron|face]] and [[Definition:Face of Polyhedron|faces]] of a [[Definition:Regular Icosahedron|regular icosahedron]] is $1800$.	0
Let $\C$ be the [[Definition:Complex Plane|complex plane]]. Let $E$ be an [[Definition:Ellipse|ellipse]] in $\C$ whose [[Definition:Major Axis of Ellipse|major axis]] is $d \in \R_{>0}$ and whose [[Definition:Focus of Ellipse|foci]] are at $\alpha, \beta \in \C$. Then $C$ may be written as: :$\cmod {z - \alpha} + \cmod {z - \beta} = d$ where $\cmod {\, \cdot \,}$ denotes [[Definition:Complex Modulus|complex modulus]].	0
:$\map {\dfrac \d {\d x} } {\csc x} = -\csc x \cot x$ where $\sin x \ne 0$.	0
{{begin-eqn}} {{eqn | l = \sin 3 x | r = 3 \sin x - 4 \sin^3 x | c = [[Triple Angle Formula for Sine]] }} {{eqn | ll = \implies | l = 4 \sin^3 x | r = 3 \sin x - \sin 3 x | c = rearranging }} {{eqn | ll = \implies | l = \sin^3 x | r = \frac {3 \sin x - \sin 3 x} 4 | c = dividing both sides by $4$ }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\rd v} {\rd x} \rd x = u v - \int v \frac {\rd u} {\rd x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \sin a x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = a \cos a x | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^n} | c = }} {{eqn | r = x^{-n} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^{-n + 1} } {- n + 1} | c = [[Primitive of Power]] }} {{eqn | r = \frac {-1} {\paren {n - 1} x^{n - 1} } | c = simplifying }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\sin a x} {x^n} \rd x | r = \sin a x \paren {\frac {-1} {\paren {n - 1} x^{n - 1} } } - \int \paren {\frac {-1} {\paren {n - 1} x^{n - 1} } } \paren {a \cos a x} \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\sin a x} {\paren {n - 1} x^{n - 1} } + \frac a {n - 1} \int \frac {\cos a x} {x^{n - 1} } \rd x | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sin^4 x - \cos^4 x | r = \sin^2 x \left({1 - \cos^2 x}\right) - \cos^2 x \left({1 - \sin^2 x}\right) | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \sin^2 x - \sin^2 x \ \cos^2 x - \cos^2 x + \sin^2 x \ \cos^2 x }} {{eqn | r = \sin^2 x - \cos^2 x }} {{end-eqn}} {{qed}}	0
'''Clockwise''' is this direction: :[[File:Clockwise.png|400px]]	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sin x} | r = \int \csc x \rd x | c = {{Defof|Real Cosecant Function}} }} {{eqn | r = \ln \size {\csc x - \cot x} + C | c = [[Primitive of Cosecant Function/Cosecant minus Cotangent Form|Primitive of $\csc x$: Cosecant minus Cotangent Form]] }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {\sin a x} | r = \frac 1 a \ln \size {\csc a x - \cot a x} + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \sec x \rd x | r = \ln \size {\sec x + \tan x} | c = [[Primitive of Secant Function/Secant plus Tangent Form|Primitive of $\sec x$: Secant plus Tangent Form]] }} {{eqn | ll= \leadsto | l = \int \sec a x \rd x | r = \frac 1 a \ln \size {\sec a x + \tan a x} + C | c = [[Primitive of Function of Constant Multiple]] }} {{end-eqn}} {{qed}}	0
[[File:Midline and Median of Triangle Bisect Each Other.png|400px]] Construct the [[Definition:Midline of Triangle|midlines]] $DF$ and $EF$. Then by the [[Midline Theorem]] $DF \parallel AE$ and $EF \parallel AD$. Thus by [[Quadrilateral is Parallelogram iff Both Pairs of Opposite Sides are Equal or Parallel]], $\Box ADFE$ is a [[Definition:Parallelogram|parallelogram]]. By construction, $AF$ and $DE$ are the [[Definition:Diagonal of Quadrilateral|diagonals]] of $\Box ADFE$. The result follows from [[Diameters of Parallelogram Bisect each other]]. {{qed}}	0
The proof strategy is to how that for all $z \in S$: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} = \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ Note that when $z = -1 + 0 i$: {{begin-eqn}} {{eqn | l = 1 + z | r = 0 + 0 i | c = }} {{eqn | ll= \implies | l = \frac {1 + z} {1 - z} | r = 0 | c = }} {{eqn | ll= \implies | l = \ln \left({\dfrac {1 + z} {1 - z} }\right) | o = | r = \text {is undefined} | c = }} {{end-eqn}} Similarly, when $z = 1 + 0 i$: {{begin-eqn}} {{eqn | l = 1 - z | r = 0 + 0 i | c = }} {{eqn | ll= \implies | l = \frac {1 + z} {1 - z} | o = | r = \text {is undefined} | c = }} {{end-eqn}} Thus let $z \in \C \setminus \left\{{-1 + 0 i, 1 + 0 i}\right\}$. === Definition 1 implies Definition 2 === It is demonstrated that: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} \subseteq \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ Let $w \in \left\{{w \in \C: z = \tanh \left({w}\right)}\right\}$. Then: {{begin-eqn}} {{eqn | l = z | r = \frac {e^w - e^{- w} } {e^w + e^{- w} } | c = Definition of [[Definition:Hyperbolic Tangent|Hyperbolic Tangent]] }} {{eqn | l = z | r = \frac {e^{2 w} - 1} {e^{2 w} + 1} | c = multiplying top and bottom by $e^w$ }} {{eqn | ll= \implies | l = e^{2 w} | r = \frac {1 + z} {1 - z} | c = solving for $e^{2 w}$ }} {{eqn | ll= \implies | l = \ln \left({e^{2 w} }\right) | r = \ln \frac {1 + z} {1 - z} }} {{eqn | ll= \implies | l = 2 w + 2 k' \pi i: k' \in \Z | r = \ln \frac {1 + z} {1 - z} | c = Definition of [[Definition:Complex Natural Logarithm|Complex Natural Logarithm]] }} {{eqn | ll= \implies | l = w | r = \frac 1 2 \ln \frac {1 + z} {1 - z} + k \pi i: k \in \Z | c = putting $k = -k'$ }} {{end-eqn}} Thus by definition of [[Definition:Subset|subset]]: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} \subseteq \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ {{qed|lemma}} === Definition 2 implies Definition 1 === It is demonstrated that: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} \supseteq \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ Let $w \in \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$. Then: {{begin-eqn}} {{eqn | ll= \exists k \in \Z: | l = w | r = \dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i | c = }} {{eqn | lll= \implies | ll= \exists k \in \Z: | l = 2 w + 2 \left({-k}\right) \pi i | r = \ln \left({\dfrac {1 + z} {1 - z} }\right) | c = }} {{eqn | lll= \implies | l = e^{2 w + 2 \left({-k}\right) \pi i} | r = \dfrac {1 + z} {1 - z} | c = Definition of [[Definition:Complex Natural Logarithm|Complex Natural Logarithm]] }} {{eqn | lll= \implies | l = e^{2 w} | r = \dfrac {1 + z} {1 - z} | c = [[Complex Exponential Function has Imaginary Period]] }} {{eqn | lll= \implies | l = z | r = \dfrac {e^w - e^{- w} } {e^w + e^{- w} } | c = }} {{eqn | lll= \implies | l = z | r = \tanh w | c = Definition of [[Definition:Hyperbolic Tangent|Hyperbolic Tangent]] }} {{eqn | lll= \implies | l = w | o = \in | r = \left\{ {w \in \C: \tanh \left({w}\right) = z}\right\} | c = }} {{end-eqn}} Thus by definition of [[Definition:Superset|superset]]: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} \supseteq \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ {{qed|lemma}} Thus by definition of [[Definition:Set Equality|set equality]]: :$\left\{{w \in \C: z = \tanh \left({w}\right)}\right\} = \left\{{\dfrac 1 2 \ln \left({\dfrac {1 + z} {1 - z} }\right) + k \pi i: k \in \Z}\right\}$ {{qed}} [[Category:Inverse Hyperbolic Tangent]] mqr7r0y3zyb1dfvmowcj3eu6d0s6e0n	0
We demonstrate that $f$ is [[Definition:Injection|injective]] {{iff}} $b c - a d \ne 0$. {{begin-eqn}} {{eqn | l = \map f {z_1} | r = \map f {z_2} | c = }} {{eqn | ll= \leadstoandfrom | l = \dfrac {a z_1 + b} {c z_1 + d} | r = \dfrac {a z_2 + b} {c z_2 + d} | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {a z_1 + b} \paren {c z_2 + d} | r = \paren {a z_2 + b} \paren {c z_1 + d} | c = }} {{eqn | ll= \leadstoandfrom | l = a c z_1 z_2 + b c z_2 + a d z_1 + b c | r = a c z_2 z_1 + b c z_1 + a d z_2 + b c | c = }} {{eqn | ll= \leadstoandfrom | l = b c z_2 + a d z_1 | r = b c z_1 + a d z_2 | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {b c - a d} z_2 | r = \paren {b c - a d} z_1 | c = }} {{end-eqn}} demonstrating that when $z \ne -\dfrac d c$ and $z \ne \infty$: :$\map f {z_1} = \map f {z_2} \implies z_1 = z_2$ {{iff}} $b c - a d \ne 0$ It remains to investigate the edge cases. First we look at the case where $c \ne 0$. {{begin-eqn}} {{eqn | l = \dfrac {a z + b} {c z + d} | r = \dfrac a c | c = }} {{eqn | ll= \leadsto | l = c \paren {a z + b} | r = a \paren {c z + d} | c = }} {{eqn | ll= \leadsto | l = a c z + b c | r = a c z + a d | c = }} {{eqn | ll= \leadsto | l = b c - a d | r = 0 | c = }} {{end-eqn}} That is, for $z \in \R$: :$\dfrac {a z + b} {c z + d} = \dfrac c a$ only if $b c - a d = 0$ and so if $\map f {z_1} = \map f {z_2} = \dfrac a c$ it follows that $z_1 = z_2 = \infty$. The case where $\map f {z_1} = \map f {z_2} = \infty$ follows by definition either that: :$z_1 = z_2 = \dfrac a c$ when $c \ne 0$ or: :$z_1 = z_2 = \infty$ when $c = 0$. Thus we have that $f$ is an [[Definition:Injection|injection]]. Now we investigate the [[Definition:Inverse of Mapping|inverse]] of $f$. From [[Inverse Element of Injection]] we have that: :$\map f z = w \implies \map {f^{-1} } w = z$ So, let $w = \map f z$. First we recall that if $z = -\dfrac d c$, then $c z + d = 0$ and so $\dfrac {a z + b} {c z + d}$ is undefined. Hence the need to investigate that case separately. Take the general case, where $z \ne -\dfrac d c$ and $z \ne \infty$: {{begin-eqn}} {{eqn | l = w | r = \dfrac {a z + b} {c z + d} | c = }} {{eqn | ll= \leadstoandfrom | l = w \paren {c z + d} | r = a z + b | c = }} {{eqn | ll= \leadstoandfrom | l = w c z + w d | r = a z + b | c = }} {{eqn | ll= \leadstoandfrom | l = w c z - a z | r = b - w d | c = }} {{eqn | ll= \leadstoandfrom | l = z \paren {c w - a} | r = - d w + b | c = }} {{eqn | ll= \leadstoandfrom | l = z | r = \dfrac {- d w + b} {c w - a} | c = }} {{end-eqn}} Thus we have that: :$\map {f^{-1} } w = \dfrac {- d w + b} {c w - a}$ which is again a [[Definition:Möbius Transformation|Möbius transformation]], defined over all $w \in \C$ except where $w = \dfrac a c$. We define: :$\map {f^{-1} } {\dfrac a c} = \infty$ and: :$\map {f^{-1} } \infty = -\dfrac d c$ except when $c = \infty$, where we define: :$\map {f^{-1} } \infty = \infty$ Hence we have that the [[Definition:Inverse of Mapping|inverse]] of $f$ is another [[Definition:Möbius Transformation|Möbius transformation]]. So as $f^{-1}$ is also a [[Definition:Möbius Transformation|Möbius transformation]], it follows that: :$\map {f^{-1} } {w_1} = \map {f^{-1} } {w_2} \implies w_1 = w_2$ {{iff}} $\paren {-d} c - b \paren {-a} = 0$ which is the same thing as $b c - a d \ne 0$. Again, we have that $\dfrac {- d w + b} {c w - a} = -\dfrac d c$ only if $\paren {-d} c - b \paren {-a} = 0$. As seen above, this is the same thing as $b c - a d \ne 0$. Finally, we note that: :$\map {f^{-1} } {w_1} = \map {f^{-1} } {w_2} = \infty \implies w_1 = w_2 = \dfrac {-d} c$ Thus we have that $f^{-1}$ is [[Definition:Injection|injective]] {{iff}} $b c - a d \ne 0$. It follows from [[Injection is Bijection iff Inverse is Injection]] that $f$ is a [[Definition:Bijection|bijection]]. {{qed}} [[Category:Möbius Transformations]] ksdlnkksdje2sfvkw7hllf1ifjl61xs	0
[[File:Arccosech.png|600px|right]] Let $x > 1$. Then we have: {{begin-eqn}} {{eqn | l = y | r = \csch^{-1} x | c = }} {{eqn | ll= \leadsto | l = x | r = \csch y | c = where $y \ne 0$ }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d y} | r = -\csch y \coth y | c = [[Derivative of Hyperbolic Cosecant]] }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \dfrac {-1} {\csch y \coth y} | c = [[Derivative of Inverse Function]] }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \frac {-1} {\csch y \sqrt {1 + \csch^2 y} } | c = [[Difference of Squares of Hyperbolic Cotangent and Cosecant]] }} {{eqn | ll= \leadsto | l = \map {\frac \d {\d x} } {\csch^{-1} x} | r = \frac {-1} {x \sqrt {1 + x^2} } | c = Definition of $x$ and $y$ }} {{eqn | r = \frac {-1} {\size x \sqrt {1 + x^2} } | c = as $x > 0$ }} {{end-eqn}} We have that [[Inverse Hyperbolic Cosecant is Odd Function]]. Hence from [[Derivative of Odd Function is Even]], $\map {\dfrac \d {\d x} } {\csch^{-1} x}$ is [[Definition:Even Function|even]]. Hence for $x < -1$ we have that: :$\map {\dfrac \d {\d x} } {\csch^{-1} x} = \dfrac {-1} {\paren {-x} \sqrt {1 + x^2} }$ and so for $x < -1$: :$\map {\dfrac \d {\d x} } {\csch^{-1} x} = \dfrac {-1} {\size x \sqrt {1 + x^2} }$ and the result follows. {{qed}} [[Category:Derivatives of Inverse Hyperbolic Functions]] [[Category:Inverse Hyperbolic Cosecant]] q0u3omwylgs3hn10jhsv7daovgjgqwd	0
Let $a, b \in \C$ be [[Definition:Complex Number|complex numbers]] expressed as [[Definition:Complex Number as Vector|vectors]] $\mathbf a$ and $\mathbf b$ respectively. Let $OA$ and $OB$ be two [[Definition:Adjacent Sides|adjacent sides]] of the [[Definition:Parallelogram|parallelogram]] $OACB$ such that $OA$ corresponds to $\mathbf a$ and $OB$ corresponds to $\mathbf b$. Then the [[Definition:Diagonal of Parallelogram|diagonal]] $OC$ of $OACB$ corresponds to $\mathbf a + \mathbf b$, the [[Definition:Vector Sum|sum]] of $a$ and $b$ expressed as a [[Definition:Complex Number as Vector|vector]].	0
The [[Definition:Evolute|evolute]] of $E$ is given by the [[Definition:Cartesian Equation|Cartesian equation]]: :$\paren {a x}^{2 / 3} + \paren {b y}^{2 / 3} = \paren {a^2 - b^2}^{2 / 3}$	0
:$\cot 180^\circ = \cot \pi$ is undefined	0
:$\sin^4 x - \cos^4 x = \sin^2 x - \cos^2 x$	0
Let $\map f x$ be the [[Definition:Real Function|function]] defined as: :$\forall x \in \openint 0 4: \begin{cases} x & : 0 < x \le 2 \\ x - 2 & : 2 < x < 4 \end{cases}$ Let $f$ be expressed by a [[Definition:Half-Range Fourier Cosine Series|half-range Fourier cosine series]]: :$\displaystyle \map f x \sim \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty a_n \cos \frac {n \pi x} 4$ where for all $n \in \Z_{> 0}$: :$a_n = \displaystyle \frac 2 l \int_0^l \map f x \cos \frac {n \pi x} l \rd x $ In this context, $l = 4$ and so this can be expressed as: {{begin-eqn}} {{eqn | l = a_n | r = \frac 2 4 \paren {\int_0^2 x \cos \frac {n \pi x} 4 \rd x + \int_2^4 \paren {x - 2} \cos \frac {n \pi x} 4 \rd x} | c = }} {{eqn | r = \frac 1 2 \paren {\int_0^2 x \cos \frac {n \pi x} 4 \rd x + \int_2^4 \paren {x - 2} \cos \frac {n \pi x} 4 \rd x} | c = }} {{end-eqn}} First the case when $n = 0$: {{begin-eqn}} {{eqn | l = a_0 | r = \frac 1 2 \paren {\int_0^2 x \rd x + \int_2^4 \paren {x - 2} \rd x} | c = }} {{eqn | r = \frac 1 2 \paren {\intlimits {\frac {x^2} 2} 0 2 + \intlimits {\frac {x^2} 2 - 2 x} 2 4} | c = [[Primitive of Power]] }} {{eqn | r = \frac 1 2 \paren {\paren {\frac {2^2} 2 - \frac {0^2} 2} + \paren {\paren {\frac {4^2} 2 - 2 \times 4} - \paren {\frac {2^2} 2 - 2 \times 2} } } | c = }} {{eqn | r = \frac 1 2 \paren {\paren {\frac 4 2 - 0} + \paren {\paren {\frac {16} 2 - 8} - \paren {\frac 4 2 - 4} } } | c = }} {{eqn | r = \frac 1 2 \paren {2 + 0 + 2} | c = }} {{eqn | r = 2 | c = }} {{end-eqn}} When $n \ne 0$: {{begin-eqn}} {{eqn | l = a_n | r = \frac 1 2 \paren {\int_0^2 x \cos \frac {n \pi x} 4 \rd x + \int_2^4 \paren {x - 2} \cos \frac {n \pi x} 4 \rd x} | c = }} {{eqn | r = \frac 1 2 \paren {\int_0^2 x \cos \frac {n \pi x} 4 \rd x + \int_2^4 x \cos \frac {n \pi x} 4 \rd x - 2 \int_2^4 \cos \frac {n \pi x} 4 \rd x} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 2 \int_0^4 x \cos \frac {n \pi x} 4 \rd x - \int_2^4 \cos \frac {n \pi x} 4 \rd x | c = [[Sum of Integrals on Adjacent Intervals for Integrable Functions]] }} {{end-eqn}} Splitting it up into two: {{begin-eqn}} {{eqn | o = | r = \frac 1 2 \int_0^4 x \cos \frac {n \pi x} 4 \rd x | c = }} {{eqn | r = \frac 1 2 \intlimits {\frac {16} {n^2 \pi^2} \cos \frac {n \pi x} 4 + \frac 4 {n \pi} x \sin \frac {n \pi x} 4 } 0 4 | c = [[Primitive of x by Cosine of a x|Primitive of $x \cos a x$]] }} {{eqn | r = \frac 1 2 \paren {\paren {\frac {16} {n^2 \pi^2} \cos n \pi + \frac {16} {n \pi} \sin n \pi} - \paren {\frac {16} {n^2 \pi^2} \cos 0 + \frac 4 {n \pi} \times 0 \sin 0} } | c = }} {{eqn | r = \frac 8 {n^2 \pi^2} \paren {\cos n \pi - \cos 0} | c = [[Sine of Multiple of Pi]] and simplification }} {{eqn | r = \frac {8 \paren {\paren {-1}^n - 1} } {n^2 \pi^2} | c = [[Cosine of Multiple of Pi]] }} {{eqn | r = \begin {cases} 0 & : \text {$n$ even} \\ \dfrac {-16} {n^2 \pi^2} & : \text {$n$ odd} \end {cases} | c = }} {{eqn | r = \dfrac {-16} {\paren {2 r - 1}^2 \pi^2} | c = substituting $n = 2 r - 1$ }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | o = | r = \int_2^4 \cos \frac {n \pi x} 4 \rd x | c = }} {{eqn | r = \intlimits {\frac 4 {n \pi} \sin \frac {n \pi x} 4} 2 4 | c = [[Primitive of Cosine of a x|Primitive of $\cos a x$]] }} {{eqn | r = \frac 4 {n \pi} \paren {\sin \frac {4 n \pi} 4 - \sin \frac {2 n \pi} 4 } | c = }} {{eqn | r = \frac 4 {n \pi} \paren {\sin n \pi - \sin \frac {n \pi} 2} | c = }} {{eqn | r = -\frac 4 {n \pi} \sin \frac {n \pi} 2 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} The above expression is $0$ when $n$ is even, by [[Sine of Multiple of Pi]]. Thus we may substitute $n = 2 r - 1$. Then: {{begin-eqn}} {{eqn | o = | r = \int_2^4 \cos \frac {n \pi x} 4 \rd x | c = }} {{eqn | r = -\frac 4 {n \pi} \sin \frac {n \pi} 2 | c = }} {{eqn | r = -\frac 4 {\paren {2 r - 1} \pi} \sin \frac {\paren {2 r - 1} \pi} 2 | c = }} {{eqn | r = -\frac 4 {\paren {2 r - 1} \pi} \sin \paren {\paren {r - 1} + \frac 1 2} \pi | c = }} {{eqn | r = -\frac 4 {\paren {2 r - 1} \pi} \paren {-1}^{r - 1} | c = [[Sine of Half-Integer Multiple of Pi]] }} {{end-eqn}} Combining the results, {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty a_n \cos \frac {n \pi x} 4 }} {{eqn | r = \frac {a_0} 2 + \sum_{n \text{ odd} } a_n \cos \frac {n \pi x} 4 | c = as both integrals vanish for even $n$ }} {{eqn | r = \frac {a_0} 2 + \sum_{r \mathop = 1}^\infty a_{2 r - 1} \cos \frac {\paren{2 r - 1} \pi x} 4 | c = using our substitution $n = 2 r - 1$ }} {{eqn | r = \frac 2 2 + \sum_{r \mathop = 1}^\infty \paren{\dfrac {-16} {\paren {2 r - 1}^2 \pi^2} + \frac 4 {\paren {2 r - 1} \pi} \paren {-1}^{r - 1} } \cos \frac {\paren{2 r - 1} \pi x} 4 | c = from above }} {{eqn | r = 1 + \frac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{r - 1} } {2 r - 1} \paren {\frac {- 4 \paren {-1}^{r - 1} } {\paren {2 r - 1} \pi} + 1} \cos \frac {\paren {2 r - 1} \pi x} 4 | c = factorisation }} {{eqn | r = 1 + \frac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{r - 1} } {2 r - 1} \paren {1 + \frac {4 \paren {-1}^r} {\paren {2 r - 1} \pi} } \cos \frac {\paren {2 r - 1} \pi x} 4 | c = }} {{end-eqn}} {{qed}}	0
The proof strategy is to how that for all $z \in \C$: :$\left\{{w \in \C: z = \cosh \left({w}\right)}\right\} = \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ Thus let $z \in \C$. === Definition 1 implies Definition 2 === It is demonstrated that: :$\left\{{w \in \C: z = \cosh \left({w}\right)}\right\} \subseteq \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ Let $w \in \left\{{w \in \C: z = \cosh \left({w}\right)}\right\}$. By definition of [[Definition:Hyperbolic Cosine|hyperbolic cosine]]: :$(1): \quad z = \dfrac {e^w + e^{-w} } 2$ Let $v = e^w$. Then: {{begin-eqn}} {{eqn | l = 2 z | r = v + \frac 1 v | c = multiplying $(1)$ by $2$ }} {{eqn | ll= \implies | l = v^2 - 2 z v + 1 | r = 0 | c = multiplying by $v$ and rearranging }} {{eqn | ll= \implies | l = v | r = z + \left({z^2 - 1}\right)^{1/2} | c = [[Quadratic Formula]] }} {{end-eqn}} Let $s = z^2 - 1$. Then: {{begin-eqn}} {{eqn | l = v | r = z + s^{1/2} | c = }} {{eqn | r = z + \sqrt{\left\vert{s}\right\vert} \left({\cos \left({\frac {\arg \left({s}\right)} 2}\right) + i \sin \left({\frac {\arg \left({s}\right)} 2}\right)}\right) | c = Definition of [[Definition:Complex Square Root|Complex Square Root]] }} {{eqn | n = 2 | ll= \implies | l = \ln v | r = \ln \left({z + \sqrt{\left\vert{s}\right\vert} \left({\cos \left({\frac {\arg \left({s}\right)} 2}\right) + i \sin \left({\frac {\arg \left({s}\right)} 2}\right)}\right)}\right) | c = where $\ln$ denotes the [[Definition:Complex Natural Logarithm|Complex Natural Logarithm]] }} {{end-eqn}} We have that: {{begin-eqn}} {{eqn | l = v | r = e^w | c = }} {{eqn | ll= \implies | l = \ln v | r = \ln \left({e^w}\right) | c = }} {{eqn | n = 3 | r = w + 2 k' \pi i: k' \in \Z | c = Definition of [[Definition:Complex Natural Logarithm|Complex Natural Logarithm]] }} {{end-eqn}} Thus from $(2)$ and $(3)$: {{begin-eqn}} {{eqn | l = w + 2 k' \pi i | r = \ln \left({z + \sqrt{\left\vert{s}\right\vert} \left({\cos \left({\frac {\arg \left({s}\right)} 2}\right) + i \sin \left({\frac {\arg \left({s}\right)} 2}\right)}\right)}\right) | c = }} {{eqn | ll= \implies | l = w | r = \ln \left({z + \sqrt{\left\vert{s}\right\vert} \left({\cos \left({\frac {\arg \left({s}\right)} 2}\right) + i \sin \left({\frac {\arg \left({s}\right)} 2}\right)}\right)}\right) + 2 k \pi i | c = putting $k = -k'$ }} {{eqn | ll= \implies | l = w | r = \ln \left({z + \sqrt{\left\vert{z^2 - 1}\right\vert} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i | c = Definition of [[Definition:Exponential Form of Complex Number|Exponential Form of Complex Number]] }} {{end-eqn}} Thus by definition of [[Definition:Subset|subset]]: :$\left\{{w \in \C: z = \cos \left({w}\right)}\right\} \subseteq \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ {{qed|lemma}} === Definition 2 implies Definition 1 === It is demonstrated that: :$\left\{{w \in \C: z = \cos \left({w}\right)}\right\} \supseteq \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ Let $w \in \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$. Then: {{begin-eqn}} {{eqn | ll= \exists k \in \Z: | l = w + 2 \left({-k}\right) \pi i | r = \ln \left({z + \sqrt{\left\vert{z^2 - 1}\right\vert} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) | c = }} {{eqn | lll= \implies | l = e^{w + 2 \left({-k}\right) \pi i} | r = z + \sqrt{\left\vert{z^2 - 1}\right\vert} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} | c = Definition of [[Definition:Complex Natural Logarithm|Complex Natural Logarithm]] }} {{eqn | lll= \implies | l = e^w | r = z + \sqrt{\left\vert{z^2 - 1}\right\vert} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} | c = [[Complex Exponential Function has Imaginary Period]] }} {{eqn | lll= \implies | l = e^w - z | r = \sqrt{\left\vert{z^2 - 1}\right\vert} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} | c = }} {{eqn | lll= \implies | l = \left({e^w - z}\right)^2 | r = \left\vert{z^2 - 1}\right\vert e^{i \arg \left({z^2 - 1}\right)} | c = [[Roots of Complex Number]] }} {{eqn | lll= \implies | l = \left({e^w - z}\right)^2 | r = z^2 - 1 | c = Definition of [[Definition:Exponential Form of Complex Number|Exponential Form of Complex Number]] }} {{eqn | lll= \implies | l = e^{2 w} - 2 z e^w + z^2 | r = z^2 - 1 | c = [[Square of Difference]] }} {{eqn | lll= \implies | l = e^{2 w} + 1 | r = 2 z e^w | c = }} {{eqn | lll= \implies | l = e^w + \frac 1 {e^w} | r = 2 z | c = }} {{eqn | lll= \implies | l = z | r = \frac {e^w + e^{-w} } 2 | c = }} {{eqn | lll= \implies | l = z | r = \cosh w | c = Definition of [[Definition:Hyperbolic Cosine|Hyperbolic Cosine]] }} {{eqn | lll= \implies | l = w | o = \in | r = \left\{ {w \in \C: z = \cosh \left({w}\right)}\right\} | c = }} {{end-eqn}} Thus by definition of [[Definition:Superset|superset]]: :$\left\{{w \in \C: z = \cosh \left({w}\right)}\right\} \supseteq \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ {{qed|lemma}} Thus by definition of [[Definition:Set Equality|set equality]]: :$\left\{{w \in \C: z = \cosh \left({w}\right)}\right\} = \left\{{\ln \left({z + \sqrt{\left|{z^2 - 1}\right|} e^{\left({i / 2}\right) \arg \left({z^2 - 1}\right)} }\right) + 2 k \pi i: k \in \Z}\right\}$ {{qed}} [[Category:Inverse Hyperbolic Cosine]] ahoj4zmh89p4icjj7hsx901ompx3v4w	0
Let $\triangle ABC$ be a [[Definition:Triangle (Geometry)|triangle]]. Let $\triangle DEF$ be the [[Definition:Medial Triangle|medial triangle]] of $\triangle ABC$. Let $K$ be the [[Definition:Circumcenter of Triangle|circumcenter]] of $\triangle ABC$. Then $K$ is the [[Definition:Orthocenter|orthocenter]] of $\triangle DEF$.	0
For any given [[Definition:Circle|circle]], it is possible to find its [[Definition:Center of Circle|center]]. {{:Euclid:Proposition/III/1}}	0
Let the [[Definition:Circle|circle]] of [[Definition:Radius of Circle|radius]] $r$ be divided into many [[Definition:Sector|sectors]]: :[[File:AreaOfCircleProof5.png|400px]] If they are made small enough, they can be approximated to [[Definition:Triangle (Geometry)|triangles]] whose [[Definition:Height of Triangle|heights]] are all $r$. Let the [[Definition:Base of Triangle|bases]] of these [[Definition:Triangle (Geometry)|triangles]] be denoted: :$b_1, b_2, b_3, \ldots$ From [[Area of Triangle in Terms of Side and Altitude]], their [[Definition:Area|areas]] are: :$\dfrac {r b_1} 2, \dfrac {r b_2} 2, \dfrac {r b_3} 2, \ldots$ The [[Definition:Area|area]] $\mathcal A$ of the [[Definition:Circle|circle]] is given by the sum of the [[Definition:Area|areas]] of each of these [[Definition:Triangle (Geometry)|triangles]]: {{begin-eqn}} {{eqn | l = \mathcal A | r = \dfrac {r b_1} 2 + \dfrac {r b_2} 2 + \dfrac {r b_3} 2 + \cdots | c = }} {{eqn | r = \dfrac r 2 \left({b_1 + b_2 + b_3 + \cdots}\right) | c = }} {{end-eqn}} But $b_1 + b_2 + b_3 + \cdots$ is the [[Definition:Length (Linear Measure)|length]] of the [[Definition:Circumference of Circle|circumference]] of the [[Definition:Circle|circle]]. From [[Perimeter of Circle]]: :$b_1 + b_2 + b_3 + \cdots = 2 \pi r$ Hence: {{begin-eqn}} {{eqn | l = \mathcal A | r = \dfrac r 2 \left({b_1 + b_2 + b_3 + \cdots}\right) | c = }} {{eqn | r = \dfrac r 2 \left({2 \pi r}\right) | c = }} {{eqn | r = \pi r^2 | c = }} {{end-eqn}} It needs to be noted that this proof is intuitive and non-rigorous. {{qed}}	0
:$\ds \int \sin x \rd x = -\cos x + C$	0
{{begin-eqn}} {{eqn | l = \map \csc {2 \pi - \theta} | r = \frac 1 {\map \sin {2 \pi - \theta} } | c = [[Cosecant is Reciprocal of Sine]] }} {{eqn | r = \frac 1 {-\sin \theta} | c = [[Cosine of Conjugate Angle]] and [[Sine of Conjugate Angle]] }} {{eqn | r = -\csc \theta | c = [[Cosecant is Reciprocal of Sine]] }} {{end-eqn}} {{qed}}	0
Given a [[Definition:Triangle (Geometry)|triangle]] and a [[Definition:Point|point]] inside it, the sum of the lengths of the [[Definition:Line Segment|line segments]] from the [[Definition:Endpoint of Line|endpoints]] of one [[Definition:Side of Polygon|side]] of the triangle to the point is less than the sum of the other two sides of the triangle. {{:Euclid:Proposition/I/21}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^2 - a^2} | r = \int \frac {\d x} {\paren {x - a} \paren {x + a} } | c = [[Difference of Two Squares]] }} {{eqn | r = \int \frac {\d x} {2 a \paren {x - a} } - \int \frac {\d x} {2 a \paren {x + a} } | c = [[Primitive of Reciprocal of x squared minus a squared/Logarithm Form/Proof 2/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac 1 {2 a} \int \frac {\d x} {x - a} - \frac 1 {2 a} \int \frac {\d x} {x + a} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 {2 a} \ln \size {x - a} - \frac 1 {2 a} \ln \size {x + a} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \dfrac 1 {2 a} \ln \size {\dfrac {x - a} {x + a} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
In [[Definition:Isosceles Triangle|isosceles triangles]], the [[Definition:Angle|angles]] at the [[Definition:Base of Isosceles Triangle|base]] are equal to each other. Also, if the equal [[Definition:Line Segment|straight lines]] are extended, the [[Definition:Angle|angles]] under the [[Definition:Base of Isosceles Triangle|base]] will also be equal to each other. {{:Euclid:Proposition/I/5}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arccsc \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \begin{cases} \dfrac {-a} {x \sqrt {x^2 - a^2} } & : 0 < \arccsc \dfrac x a < \dfrac \pi 2 \\ \dfrac a {x \sqrt {x^2 - a^2} } & : -\dfrac \pi 2 < \arccsc \dfrac x a < 0 \\ \end{cases} | c = [[Derivative of Arccosecant of x over a|Derivative of $\arccsc \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {x^2} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {-1} x | c = [[Primitive of Power]] }} {{end-eqn}} First let $\arccsc \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\openint 0 {\dfrac \pi 2}$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\arccsc \frac x a} {x^2} | r = \frac {-1} x \arccsc \frac x a - \int \frac {-1} x \paren {\frac {-a} {x \sqrt {x^2 - a^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\arccsc \frac x a} x - a \int \frac {\d x} {x \sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {-\arccsc \frac x a} x - a \paren {\frac {\sqrt {x^2 - a^2} } {a^2 x} } + C | c = [[Primitive of Reciprocal of x squared by Root of x squared minus a squared|Primitive of $\dfrac 1 {x^2 \sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {-\arccsc \frac x a} x - \frac {\sqrt{x^2 - a^2} } {a x} + C | c = simplifying }} {{end-eqn}} Similarly, let $\arccsc \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\openint {-\dfrac \pi 2} 0$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\arccsc \frac x a} {x^2} | r = \frac {-1} x \arccsc \frac x a - \int \frac {-1} x \paren {\frac a {x \sqrt {x^2 - a^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\arccsc \frac x a} x + a \int \frac {\d x} {x \sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {-\arccsc \frac x a} x + a \paren {\frac {\sqrt {x^2 - a^2} } {a^2 x} } + C | c = [[Primitive of Reciprocal of x squared by Root of x squared minus a squared|Primitive of $\dfrac 1 {x^2 \sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {-\arccsc \frac x a} x + \frac {\sqrt{x^2 - a^2} } {a x} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:[[File:Euclid-XI-30.png|400px]] Let $CM$ and $CN$ be [[Definition:Parallelepiped|parallelepipeds]] on the same [[Definition:Base of Parallelepiped|base]] $AB$ and of the same [[Definition:Height of Parallelepiped|height]]. Let the [[Definition:Endpoint of Line|endpoints]] of their vertical sides: :$AF, AG, LM, LN, CD, CE, BH, BK$ be not on the same [[Definition:Straight Line|straight lines]]. It is to be demonstrated that the [[Definition:Parallelepiped|parallelepiped]] $CM$ is equal to the [[Definition:Parallelepiped|parallelepiped]] $CN$. Let $NK$ and $DH$ be produced, and meet each other at $R$. Let $FM$ and $GE$ be produced to $P$ and $Q$. Let $AO, LP, CQ, BR$ be joined. We have that $CM$ and $CP$ are on the same [[Definition:Parallelogram|parallelogram]] $ABCL$ and of the same [[Definition:Height of Parallelepiped|height]], and the [[Definition:Endpoint of Line|endpoints]] of their vertical sides: :$AF, AO, LM, LP, CD, CQ, BH, BR$ are on the same [[Definition:Straight Line|straight lines]] $FP$ and $FR$. So from {{EuclidPropLink|book = XI|prop = 29|title = Parallelepipeds on Same Base and Same Height whose Extremities are on Same Lines are Equal in Volume}}: :the [[Definition:Parallelepiped|parallelepiped]] $CM$, of which the [[Definition:Parallelogram|parallelogram]] $ABCL$ is the [[Definition:Base of Parallelepiped|base]] and $FDHM$ the [[Definition:Opposite Face of Parallelepiped|opposite]] is equal to :the [[Definition:Parallelepiped|parallelepiped]] $CP$, of which the [[Definition:Parallelogram|parallelogram]] $ABCL$ is the [[Definition:Base of Parallelepiped|base]] and $OQRP$ the [[Definition:Opposite Face of Parallelepiped|opposite]]. But we also have that $CP$ and $CN$ are on the same [[Definition:Parallelogram|parallelogram]] $ABCL$ and of the same [[Definition:Height of Parallelepiped|height]], and the [[Definition:Endpoint of Line|endpoints]] of their vertical sides: :$AG, AO, LN, LP, CE, CQ, BK, BR$ are on the same [[Definition:Straight Line|straight lines]] $GQ$ and $NR$. So from {{EuclidPropLink|book = XI|prop = 29|title = Parallelepipeds on Same Base and Same Height whose Extremities are on Same Lines are Equal in Volume}}: :the [[Definition:Parallelepiped|parallelepiped]] $CN$, of which the [[Definition:Parallelogram|parallelogram]] $ABCL$ is the [[Definition:Base of Parallelepiped|base]] and $ACBL$ the [[Definition:Opposite Face of Parallelepiped|opposite]] is equal to :the [[Definition:Parallelepiped|parallelepiped]] $CP$, of which the [[Definition:Parallelogram|parallelogram]] $ABCL$ is the [[Definition:Base of Parallelepiped|base]] and $OQRP$ the [[Definition:Opposite Face of Parallelepiped|opposite]]. Hence the [[Definition:Parallelepiped|parallelepiped]] $CM$ equals the [[Definition:Parallelepiped|parallelepiped]] $CN$. {{qed}} {{Euclid Note|30|XI}}	0
Recall the definition of the [[Definition:Real Cosine Function|cosine function]]: :$\displaystyle \cos z = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {z^{2 n} } {\left({2 n}\right)!} = 1 - \frac {z^2} {2!} + \frac {z^4} {4!} - \cdots$ From [[Even Power is Non-Negative]]: : $\forall n \in \N: z^{2 n} = \paren {-z}^{2 n}$ The result follows. {{qed}}	0
Let $\map f t = \sin \sqrt t$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = \dfrac {\cos \sqrt t} {2 \sqrt t} | c = }} {{eqn | l = \map f 0 | r = 0 | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \laptrans {\map {f'} t} | r = \dfrac 1 2 \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | c = }} {{eqn | r = s \, \map F s - \map f 0 | c = [[Laplace Transform of Derivative]] }} {{eqn | r = \dfrac {\sqrt \pi} {2 s^{1/2} } \map \exp {-\dfrac 1 {4 s} } | c = [[Laplace Transform of Sine of Root]] }} {{eqn | ll= \leadsto | l = \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | r = \sqrt {\dfrac \pi s} \, \map \exp {-\dfrac 1 {4 s} } | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \map \cos {\theta + k \alpha} | r = \map \cos {\theta + \alpha} + \map \cos {\theta + 2 \alpha} + \map \cos {\theta + 3 \alpha} + \dotsb }} {{eqn | r = \map \cos {\theta + \frac {n + 1} 2 \alpha} \frac {\map \sin {n \alpha / 2} } {\map \sin {\alpha / 2} } }} {{end-eqn}}	0
We use the technique of [[Definition:Formation of Ordinary Differential Equation by Elimination|formation of ordinary differential equation by elimination]]. [[Definition:Differentiation|Differentiating]] $(1)$ {{WRT|Differentiation}} $x$ gives: :$\dfrac {\d y} {\d x} = 1 - C e^{-x}$ Eliminating $C$: {{begin-eqn}} {{eqn | l = C | o = = | r = \frac {y - x} {e^{-x} } | c = }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = 1 - \frac {y - x} {e^{-x} } e^{-x} | c = }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} + y | r = x + 1 | c = }} {{end-eqn}} Thus from [[Orthogonal Trajectories of One-Parameter Family of Curves]], the [[Definition:Orthogonal Trajectories|family of orthogonal trajectories]] is given by: :$-\dfrac {\d y} {\d x} = - x = 1 - y$ The [[Definition:Integrating Factor|integrating factor]] is $e^y$, giving: :$\displaystyle e^y x = \int y e^y - e^y \rd y$ Using [[Primitive of x by Exponential of a x|Primitive of $x e^{a x}$]]: :$\displaystyle \int y e^y \rd y = y e^y - e^y$ Thus we get: :$e^y x = y e^y - e^y - e^y + C$ which gives us: :$x = y - 2 + C e^{-y}$ {{qed}}	0
From [[Rate of Change of Cartesian Coordinates of Cycloid]], the [[Definition:Rate of Change|rate of change]] of $y$ is given by: :$\dfrac {\d y} {\d t} = \mathbf v_0 \sin \theta$. This is a maximum when $\sin \theta$ is a maximum. That happens when $\sin \theta = 1$. That happens when $\theta = \dfrac \pi 2 + 2 n \pi$ where $n \in \Z$. When $\sin \theta = 1$ we have: :$\dfrac {\d y} {\d t} = \mathbf v_0$ Hence the result. {{qed}}	0
{{:Euclid:Proposition/II/5}} :[[File:Euclid-II-5.png|400px]] Let $AB$ be cut into equal segments at $C$ and unequal segments at $D$. Then the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$ together with the square on $CD$ equals the square on $BC$. (That is, let $x = AC, y = CD$. Then $\paren {x + y} \paren {x - y} + y^2 = x^2$.) This is proved as follows. [[Construction of Square on Given Straight Line|Construct the square $CBFE$]] on $CB$, and join $BE$. [[Construction of Parallel Line|Construct $DG$ parallel]] to $CE$ through $G$, and let $DG$ cross $BE$ at $H$. [[Construction of Parallel Line|Construct $KM$ parallel]] to $AB$ through $H$. [[Construction of Parallel Line|Construct $AK$ parallel]] to $BF$ through $A$. From [[Complements of Parallelograms are Equal]]: :$\Box CDHL = \Box FGHM$. Add the square $DBMH$ to each. Then $\Box CBML = \Box DBFG$. But as $AC = CB$, from [[Parallelograms with Equal Base and Same Height have Equal Area]] we have that: :$\Box ACLK = \Box CBML$ Add $\Box CDHL$ to each. Then $\Box ADHK$ is equal in area to the [[Definition:Gnomon|gnomon]] $CBFGHL$. But $\Box ADHK$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$, because $DB = DH$. So the [[Definition:Gnomon|gnomon]] $CBFGHL$ is equal in area to the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$. Now $\Box LHGE$ is equal to the square on $CD$. Add $\Box LHGE$ to each of the [[Definition:Gnomon|gnomon]] $CBFGHL$ and $\Box ADHK$. Then the [[Definition:Gnomon|gnomon]] $CBFGHL$ together with $\Box LHGE$ equals the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$ and the square on $CD$. But the [[Definition:Gnomon|gnomon]] $CBFGHL$ together with $\Box LHGE$ is the square $CBFE$. Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot z | r = \frac {\cos z} {\sin z} | c = {{Defof|Complex Cotangent Function}} }} {{eqn | r = \frac {e^{i z} + e^{-i z} } 2 / \frac {e^{i z} - e^{-i z} } {2 i} | c = [[Sine Exponential Formulation]] and [[Cosine Exponential Formulation]] }} {{eqn | r = i \frac {e^{i z} + e^{-i z} } {e^{i z} - e^{-i z} } | c = multiplying [[Definition:Numerator|numerator]] and [[Definition:Denominator|denominator]] by $2 i$ }} {{end-eqn}} {{qed}}	0
:[[File:Euclid-XI-2.png|400px]] Let the two [[Definition:Straight Line|straight lines]] $AB$ and $CD$ [[Definition:Intersection (Geometry)|intersect]] at the [[Definition:Point|point]] $E$. Let $F$ and $G$ be arbitrary [[Definition:Point|points]] on $EC$ and $EB$. Let $CB$ and $FG$ be connected. Let $FH$ and $GK$ be drawn across. Suppose part of $\triangle ECB$ is in the [[Definition:Plane of Reference|plane of reference]] and the rest of it in another [[Definition:Plane|plane]]. Then a part of one of the [[Definition:Straight Line|straight lines]] $EC$ and $EB$ will be in the [[Definition:Plane of Reference|plane of reference]] and the result in another [[Definition:Plane|plane]]. But from {{EuclidPropLink|book = XI|prop = 1|title = Straight Line cannot be in Two Planes}}, this cannot happen. Therefore $\triangle ECB$ is all in one [[Definition:Plane|plane]]. But whatever [[Definition:Plane|plane]] $\triangle ECB$ is in, each of the [[Definition:Straight Line|straight lines]] $EC$ and $EB$ are also in that [[Definition:Plane|plane]]. And in whatever [[Definition:Plane|plane]] $EC$ and $EB$ are in, the [[Definition:Straight Line|straight lines]] $AB$ and $CD$ are also in that [[Definition:Plane|plane]]. Therefore $AB$ and $CD$ are in one [[Definition:Plane|plane]], and every [[Definition:Triangle (Geometry)|triangle]] is in one [[Definition:Plane|plane]]. {{qed}} {{Euclid Note|2|XI}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arccsc \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \begin{cases} \dfrac {-a} {x \sqrt {x^2 - a^2} } & : 0 < \arccsc \dfrac x a < \dfrac \pi 2 \\ \dfrac a {x \sqrt {x^2 - a^2} } & : -\dfrac \pi 2 < \arccsc \dfrac x a < 0 \\ \end{cases} | c = [[Derivative of Arccosecant of x over a|Derivative of $\arccsc \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^2} 2 | c = [[Primitive of Power]] }} {{end-eqn}} First let $\arccsc \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\openint 0 {\dfrac \pi 2}$. Then: {{begin-eqn}} {{eqn | l = \int x \arccsc \frac x a \rd x | r = \frac {x^2} 2 \arccsc \frac x a - \int \frac {x^2} 2 \paren {\frac {-a} {x \sqrt {x^2 - a^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a + \frac a 2 \int \frac {x \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a + \frac a 2 \sqrt {x^2 - a^2} + C | c = [[Primitive of x over Root of x squared minus a squared|Primitive of $\dfrac x {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a + \frac {a \sqrt{x^2 - a^2} } 2 + C | c = simplifying }} {{end-eqn}} Similarly, let $\arccsc \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\openint {-\dfrac \pi 2} 0$. Then: {{begin-eqn}} {{eqn | l = \int x \arccsc \frac x a \rd x | r = \frac {x^2} 2 \arccsc \frac x a - \int \frac {x^2} 2 \paren {\frac {-a} {x \sqrt {x^2 - a^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a - \frac a 2 \int \frac {x \rd x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a - \frac a 2 \sqrt {x^2 - a^2} + C | c = [[Primitive of x over Root of x squared minus a squared|Primitive of $\dfrac x {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^2} 2 \arccsc \frac x a - \frac {a \sqrt{x^2 - a^2} } 2 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cos 60 \degrees | r = \map \cos {90 \degrees - 30 \degrees} | c = }} {{eqn | r = \sin 30 \degrees | c = [[Cosine of Complement equals Sine]] }} {{eqn | r = \dfrac 1 2 | c = [[Sine of 30 Degrees|Sine of $30 \degrees$]] }} {{end-eqn}} {{qed}}	0
Let $\map f x$ be an [[Definition:Even Function|even]] [[Definition:Real Function|real function]] defined on the [[Definition:Real Interval|interval]] $\openint {-\lambda} \lambda$. Then the [[Definition:Fourier Series|Fourier series]] of $\map f x$ can be expressed as: :$\map f x \sim \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty a_n \cos \frac {n \pi x} \lambda$ where for all $n \in \Z_{\ge 0}$: :$a_n = \dfrac 2 \lambda \displaystyle \int_0^\lambda \map f x \cos \frac {n \pi x} \lambda \rd x$	0
Let $T = \struct {S, \tau_{\bar p} }$ be an [[Definition:Excluded Point Topology|excluded point space]] with at least three points. Then $T^*_{\bar p}$ is not [[Definition:Irreducible Space|irreducible]].	0
The '''magnitude''' (or '''size''') of a quantity (either [[Definition:Vector Quantity|vector]] or [[Definition:Scalar Quantity|scalar]]) is a measure of how big it is. It is usually encountered explicitly in the context of [[Definition:Vector Quantity|vectors]]: If $\mathbf v$ is the [[Definition:Vector Quantity|vector quantity]] in question, then its '''magnitude''' is denoted: :$\size {\mathbf v}$ or :$v$	0
{{begin-eqn}} {{eqn | l = \cot 195 \degrees | r = \map \cot {360 \degrees - 165 \degrees} | c = }} {{eqn | r = -\cot 165 \degrees | c = [[Cotangent of Conjugate Angle]] }} {{eqn | r = 2 + \sqrt 3 | c = [[Cotangent of 165 Degrees|Cotangent of $165 \degrees$]] }} {{end-eqn}} {{qed}}	0
:[[File:RegularPolygonAreaInscribed.png|400px]] From [[Regular Polygon composed of Isosceles Triangles]], let $\triangle OAB$ be one of the $n$ [[Definition:Isosceles Triangle|isosceles triangles]] that compose $P$. Then $\mathcal P$ is equal to $n$ times the [[Definition:Base of Isosceles Triangle|base]] of $\triangle OAB$. Let $d$ be the [[Definition:Length of Line|length]] of one [[Definition:Side of Polygon|side]] of $P$. Then $d$ is the [[Definition:Length of Line|length]] of the [[Definition:Base of Isosceles Triangle|base]] of $\triangle OAB$. The [[Definition:Angle|angle]] $\angle AOB$ is equal to $\dfrac {2 \pi} n$. Then: :$d = 2 r \sin \dfrac \pi n$ So: {{begin-eqn}} {{eqn | l = \mathcal P | r = n d | c = }} {{eqn | r = n \paren {2 r \sin \dfrac \pi n} | c = substituting from above }} {{eqn | r = 2 n r \sin \dfrac \pi n | c = simplifying }} {{end-eqn}} {{qed}}	0
&nbsp; :[[File:LogarithmicSpiralLength.png|400px]] {{ProofWanted}}	0
From [[Volume of Solid of Revolution]]: {{begin-eqn}} {{eqn | l = V | r = \pi \int_1^a \frac 1 {x^2} \, \mathrm d x | c = }} {{eqn | r = \pi \left[{-\dfrac 1 x}\right]_1^a | c = [[Primitive of Power]] }} {{eqn | r = \pi \left[{\dfrac 1 x}\right]_a^1 | c = }} {{eqn | r = \pi \left({1 - \dfrac 1 a}\right) | c = }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = u | r = \arccos \frac x a | c = }} {{eqn | n = 1 | ll= \implies | l = \cos u | r = \frac x a | c = Definition of [[Definition:Arccosine|Arccosine]] }} {{eqn | n = 2 | ll= \implies | l = \sin u | r = \sqrt {1 - \frac {x^2} {a^2} } | c = [[Sum of Squares of Sine and Cosine]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \arccos \frac x a \ \mathrm d x | r = -a \int u \left({a \cos u}\right) \sin u \ \mathrm d u | c = [[Primitive of Function of Arcsine]] }} {{eqn | r = -a \int a u \frac {\sin 2 u} 2 \ \mathrm d u | c = [[Double Angle Formula for Sine]] }} {{eqn | r = -\frac {a^2} 2 \int u \sin 2 u \ \mathrm d u | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = -\frac {a^2} 2 \left({\frac {\sin 2 u} 4 - \frac {u \cos 2 u} 2}\right) + C | c = [[Primitive of x by Sine of a x|Primitive of $x \sin a x$]] where $a = 2$ }} {{eqn | r = -\frac {a^2 \sin 2 u} 8 + \frac {a^2 u \cos 2 u} 4 + C | c = simplifying }} {{eqn | r = -\frac {a^2 \sin u \cos u} 4 + \frac {a^2 u \cos 2 u} 4 + C | c = [[Double Angle Formula for Sine]] }} {{eqn | r = -\frac {a^2 \sin u \cos u} 4 + \frac {a^2 u \left({\cos^2 u - \sin^2 u}\right)} 4 + C | c = [[Double Angle Formula for Cosine]] }} {{eqn | r = -\frac {a^2 \sin u \cos u} 4 + \frac {a^2 \arccos \frac x a \left({\cos^2 u - \sin^2 u}\right)} 4 + C | c = substituting for $u$ }} {{eqn | r = -\frac {a^2 \sin u \frac x a} 4 + \frac {a^2 \arccos \frac x a \left({\frac {x^2} {a^2} - \sin^2 u}\right)} 4 + C | c = substituting for $\cos u$ from $(1)$ }} {{eqn | r = -\frac {a^2 \sqrt {1 - \frac {x^2} {a^2} } \frac x a} 4 + \frac {a^2 \arccos \frac x a \left({\frac {x^2} {a^2} - \left({1 - \frac {x^2} {a^2} }\right)}\right)} 4 + C | c = substituting for $\sin u$ from $(2)$ }} {{eqn | r = \left({\frac {x^2} 2 - \frac {a^2} 4}\right) \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\map \cot {a + b i} = \dfrac {i \cot a \coth b - 1} {\cot a - i \coth b}$	0
:$\cot 285 \degrees = \cot \dfrac {19 \pi} {12} = -\paren {2 - \sqrt 3}$	0
{{begin-eqn}} {{eqn | l = \cos 240 \degrees | r = \map \cos {360 \degrees - 120 \degrees} | c = }} {{eqn | r = \cos 120 \degrees | c = [[Cosine of Conjugate Angle]] }} {{eqn | r = -\frac 1 2 | c = [[Cosine of 120 Degrees|Cosine of $120 \degrees$]] }} {{end-eqn}} {{qed}}	0
:$\ds \int \frac {\d x} {\cos a x} = \frac 1 a \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C$	0
{{begin-eqn}} {{eqn |l = \lim_{x \mathop \to 0} \frac {\tan x} x |r = \lim_{x \mathop \to 0} \frac 1 {\cos x} \frac {\sin x} x |c = {{Defof|Tangent Function}} }} {{eqn |r = \lim_{x \mathop \to 0} \frac 1 {\cos x} \lim_{x \mathop \to 0} \frac {\sin x} x |c = [[Product Rule for Limits of Functions]] }} {{eqn |r = \lim_{x \mathop \to 0} \frac {\sin x} x |c = [[Cosine of Zero is One]] }} {{eqn |r = 1 |c = [[Limit of Sine of X over X]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\sin a x \rd x} {p + q \cos a x} = \frac {-1} {a q} \ln \size {p + q \cos a x} + C$	0
Suppose it is possible. Then from a [[Definition:Cube (Geometry)|cube]] of [[Definition:Edge of Polyhedron|edge]] [[Definition:Length of Line|length]] $L$ we can construct a new [[Definition:Cube (Geometry)|cube]] with [[Definition:Edge of Polyhedron|edge]] [[Definition:Length of Line|length]] $\sqrt [3] 2 L$. $\sqrt [3] 2$ is [[Definition:Algebraic Number|algebraic]] of [[Definition:Degree of Algebraic Number|degree]] $3$. This contradicts [[Constructible Length with Compass and Straightedge]]. {{qed}}	0
:$\displaystyle \int \cot^n a x \csc^2 a x \ \mathrm d x = \frac {-\cot^{n + 1} a x} {\left({n + 1}\right) a} + C$	0
From the [[General Binomial Theorem]]: {{begin-eqn}} {{eqn | l = \paren {1 - x^2}^{-1/2} | r = 1 + \frac 1 2 x^2 + \frac {1 \times 3} {2 \times 4} x^4 + \frac {1 \times 3 \times 5} {2 \times 4 \times 6} x^6 + \cdots | c = }} {{eqn | n = 1 | r = \sum_{n \mathop = 0}^\infty \frac {\paren {2 n}!} {2^{2 n} \paren {n!}^2} x^{2 n} | c = }} {{end-eqn}} for $-1 < x < 1$. From [[Power Series is Termwise Integrable within Radius of Convergence]], $(1)$ can be [[Definition:Integration|integrated]] term by term: {{begin-eqn}} {{eqn | l = \int_0^x \frac 1 {\sqrt{1 - t^2} } \rd t | r = \sum_{n \mathop = 0}^\infty \int_0^x \frac {\paren {2 n}!} {2^{2 n} \paren {n!}^2} t^{2 n} \rd t | c = }} {{eqn | ll= \leadsto | l = \arcsin x | r = \sum_{n \mathop = 0}^\infty \frac {\paren {2 n}!} {2^{2 n} \paren {n!}^2} \frac {x^{2 n + 1} } {2 n + 1} | c = [[Derivative of Arcsine Function]] }} {{end-eqn}} We will now prove that the series [[Definition:Convergent Series|converges]] for $-1 \le x \le 1$. By [[Stirling's Formula]]: {{begin-eqn}} {{eqn | l = \frac {\paren {2 n}!} {2^{2 n} \paren {n!}^2} \frac {x^{2 n + 1} } {2 n + 1} | o = \sim | r = \frac {\paren {2 n}^{2 n} e^{-2 n} \sqrt {4 \pi n} } {2^{2 n} n^{2 n} e^{-2 n} 2 \pi n} \frac {x^{2 n + 1} } {2 n + 1} | c = }} {{eqn | r = \frac 1 {\sqrt {\pi n} } \frac {x^{2 n + 1} } {2 n + 1} | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \size {\frac 1 {\sqrt {\pi n} } \frac {x^{2 n + 1} } {2 n + 1} } | o = < | r = \size {\frac {x^{2 n + 1} } {n^{3/2} } } | c = }} {{eqn | o = \le | r = \frac 1 {n^{3/2} } | c = }} {{end-eqn}} Hence by [[Convergence of P-Series]]: :$\displaystyle \sum_{n \mathop = 1}^\infty \frac 1 {n^{3/2} }$ is [[Definition:Convergent Series|convergent]]. So by the [[Comparison Test]], the [[Definition:Taylor Series|Taylor series]] is [[Definition:Convergent Series|convergent]] for $-1 \le x \le 1$. {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arcsec} \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \begin{cases} \dfrac a {x \sqrt {x^2 - a^2} } & : 0 < \operatorname{arcsec} \dfrac x a < \dfrac \pi 2 \\ \dfrac {-a} {x \sqrt {x^2 - a^2} } & : \dfrac \pi 2 < \operatorname{arcsec} \dfrac x a < \pi \\ \end{cases} | c = [[Derivative of Arcsecant of x over a|Derivative of $\operatorname{arcsec} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x^2 | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^3} 3 | c = [[Primitive of Power]] }} {{end-eqn}} First let $\operatorname{arcsec} \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\left({0 \,.\,.\,\dfrac \pi 2}\right)$. Then: {{begin-eqn}} {{eqn | l = \int x^2 \operatorname{arcsec} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \int \frac {x^3} 3 \left({\frac a {x \sqrt {x^2 - a^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac a 3 \int \frac {x^2 \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac a 3 \left({\frac {x \sqrt {x^2 - a^2} } 2 + \frac {a^2} 2 \ln \left({x + \sqrt {x^2 - a^2} }\right)}\right) + C | c = [[Primitive of x squared over Root of x squared minus a squared|Primitive of $\dfrac {x^2} {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac {a x \sqrt{x^2 - a^2} } 6 - \frac {a^3} 6 \ln \left({x + \sqrt {x^2 - a^2} }\right) + C | c = simplifying }} {{end-eqn}} Similarly, let $\operatorname{arcsec} \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\left({\dfrac \pi 2 \,.\,.\, \pi}\right)$. Then: {{begin-eqn}} {{eqn | l = \int x^2 \operatorname{arcsec} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \int \frac {x^3} 3 \left({\frac {-a} {x \sqrt {x^2 - a^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac a 3 \int \frac {x^2 \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac a 3 \left({\frac {x \sqrt {x^2 - a^2} } 2 + \frac {a^2} 2 \ln \left({x + \sqrt {x^2 - a^2} }\right)}\right) + C | c = [[Primitive of x squared over Root of x squared minus a squared|Primitive of $\dfrac {x^2} {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac {a x \sqrt{x^2 - a^2} } 6 + \frac {a^3} 6 \ln \left({x + \sqrt {x^2 - a^2} }\right) + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:[[File:EllipseFromSeventhSolution.png|400px]] Let the points be labelled to simplify: :$A := \tuple {1, 4}$ :$B := \tuple {2, 8}$ :$C := \tuple {4, 2}$ :$D := \tuple {8, 5}$ :$E := \tuple {7, 1}$ :$F := \tuple {5, 7}$ Let $ABCDEF$ be considered as a [[Definition:Hexagon|hexagon]]. We join the opposite points of $ABCDEF$: :$AF: \tuple {1, 4} \to \tuple {5, 7}$ :$BC: \tuple {2, 8} \to \tuple {4, 2}$ :$BE: \tuple {2, 8} \to \tuple {7, 1}$ :$AD: \tuple {1, 4} \to \tuple {8, 5}$ :$CD: \tuple {4, 2} \to \tuple {8, 5}$ :$EF: \tuple {7, 1} \to \tuple {5, 7}$ It is to be shown that the [[Definition:Intersection (Geometry)|intersections]] of: :$AF$ and $BC$ :$BE$ and $AD$ :$CD$ and $EF$ all lie on the same [[Definition:Straight Line|straight line]]. The result then follows from [[Pascal's Mystic Hexagram]]. From [[Equation of Straight Line in Plane through Two Points]]: :$\dfrac {y - y_1} {x - x_1} = \dfrac {y_2 - y_1} {x_2 - x_1}$ Thus: {{begin-eqn}} {{eqn | n = AF | l = \frac {y - 4} {x - 1} | r = \frac {7 - 4} {5 - 1} | c = }} {{eqn | r = \frac 3 4 | c = }} {{eqn | ll= \leadsto | l = 4 \paren {y - 4} | r = 3 \paren {x - 1} | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac 3 4 x + \dfrac {13} 4 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = BC | l = \frac {y - 8} {x - 2} | r = \frac {2 - 8} {4 - 2} | c = }} {{eqn | r = -3 | c = }} {{eqn | ll= \leadsto | l = y - 8 | r = -3 \paren {x - 2} | c = }} {{eqn | ll= \leadsto | l = y | r = -3 x + 14 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = BE | l = \frac {y - 8} {x - 2} | r = \frac {1 - 8} {7 - 2} | c = }} {{eqn | r = -\frac 7 5 | c = }} {{eqn | ll= \leadsto | l = 5 \paren {y - 8} | r = -7 \paren {x - 2} | c = }} {{eqn | ll= \leadsto | l = 5 y - 40 | r = -7 x + 14 | c = }} {{eqn | ll= \leadsto | l = y | r = -\frac 7 5 x + \frac {54} 5 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = AD | l = \frac {y - 4} {x - 1} | r = \frac {5 - 4} {8 - 1} | c = }} {{eqn | r = \frac 1 7 | c = }} {{eqn | ll= \leadsto | l = 7 \paren {y - 4} | r = x - 1 | c = }} {{eqn | ll= \leadsto | l = 7 y - 28 | r = x - 1 | c = }} {{eqn | ll= \leadsto | l = y | r = \frac x 7 + \frac {27} 7 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = CD | l = \frac {y - 2} {x - 4} | r = \frac {5 - 2} {8 - 4} | c = }} {{eqn | r = \frac 3 4 | c = }} {{eqn | ll= \leadsto | l = 4 \paren {y - 2} | r = 3 \paren {x - 4} | c = }} {{eqn | ll= \leadsto | l = 4 y - 8 | r = 3 x - 12 | c = }} {{eqn | ll= \leadsto | l = y | r = \frac 3 4 x - 1 | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | n = EF | l = \frac {y - 1} {x - 7} | r = \frac {7 - 1} {5 - 7} | c = }} {{eqn | r = -3 | c = }} {{eqn | ll= \leadsto | l = y - 1 | r = -3 x + 21 | c = }} {{eqn | ll= \leadsto | l = y | r = -3 x + 22 | c = }} {{end-eqn}} Evaluate the [[Definition:Intersection (Geometry)|intersection]] of $AF$ and $BC$: {{begin-eqn}} {{eqn | l = y | r = \dfrac 3 4 x + \dfrac {13} 4 | c = }} {{eqn | l = y | r = -3 x + 14 | c = }} {{eqn | ll= \leadsto | l = \dfrac 3 4 x + \dfrac {13} 4 | r = -3 x + 14 | c = }} {{eqn | ll= \leadsto | l = 3 x + 13 | r = -12 x + 56 | c = }} {{eqn | ll= \leadsto | l = 15 x | r = 43 | c = }} {{eqn | ll= \leadsto | l = x | r = \dfrac {43} {15} | c = }} {{eqn | ll= \leadsto | l = y | r = -3 \paren {\dfrac {43} {15} } + 14 | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {-129 + 210} {15} | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {81} {15} | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {27} 5 | c = }} {{end-eqn}} So $AF$ and $BC$ [[Definition:Intersection (Geometry)|intersect]] at $\paren {\dfrac {43} {15}, \dfrac {27} 5}$. Evaluate the [[Definition:Intersection (Geometry)|intersection]] of $BE$ and $AD$: {{begin-eqn}} {{eqn | l = y | r = -\frac 7 5 x + \frac {54} 5 | c = }} {{eqn | l = y | r = \frac x 7 + \frac {27} 7 | c = }} {{eqn | ll= \leadsto | l = -\frac 7 5 x + \frac {54} 5 | r = \frac x 7 + \frac {27} 7 | c = }} {{eqn | ll= \leadsto | l = -49 x + 378 | r = 5 x + 135 | c = }} {{eqn | ll= \leadsto | l = 54 x | r = 243 | c = }} {{eqn | ll= \leadsto | l = x | r = \dfrac 9 2 | c = }} {{eqn | ll= \leadsto | l = y | r = \frac 1 7 \paren {\frac 9 2} + \frac {27} 7 | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {9 + 54} {14} | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {63} {14} | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac 9 2 | c = }} {{end-eqn}} So $BE$ and $AD$ [[Definition:Intersection (Geometry)|intersect]] at $\paren {\dfrac 9 2, \dfrac 9 2}$. Evaluate the [[Definition:Intersection (Geometry)|intersection]] of $CD$ and $EF$: {{begin-eqn}} {{eqn | l = y | r = \frac 3 4 x - 1 | c = }} {{eqn | l = y | r = -3 x + 22 | c = }} {{eqn | ll= \leadsto | l = \frac 3 4 x - 1 | r = -3 x + 22 | c = }} {{eqn | ll= \leadsto | l = \frac 3 x - 4 | r = -12 x + 88 | c = }} {{eqn | ll= \leadsto | l = 15 x | r = 92 | c = }} {{eqn | ll= \leadsto | l = x | r = \dfrac {92} {15} | c = }} {{eqn | ll= \leadsto | l = y | r = -3 \paren {\dfrac {92} {15} } + 22 | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {-92 + 110} 5 | c = }} {{eqn | ll= \leadsto | l = y | r = \dfrac {18} 5 | c = }} {{end-eqn}} So $CD$ and $EF$ [[Definition:Intersection (Geometry)|intersect]] at $\paren {\dfrac {92} {15}, \dfrac {18} 5}$. It remains to be shown that those points of [[Definition:Intersection (Geometry)|intersection]]: :$\paren {\dfrac {43} {15}, \dfrac {27} 5}$, $\paren {\dfrac 9 2, \dfrac 9 2}$, $\paren {\dfrac {92} {15}, \dfrac {18} 5}$ all lie on the same [[Definition:Straight Line|straight line]]. From [[Equation of Straight Line in Plane through Two Points]]: :$\dfrac {y - y_1} {x - x_1} = \dfrac {y_2 - y_1} {x_2 - x_1}$ Thus: {{begin-eqn}} {{eqn | l = \frac {y - \frac {27} 5} {x - \frac {43} {15} } | r = \frac {\frac {18} 5 - \frac {27} 5} {\frac {92} {15} - \frac {43} {15} } | c = }} {{eqn | ll= \leadsto | l = \frac {5 y - 27} {15 x - 43} | r = \frac {18 - 27} {92 - 43} | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {5 y - 27} {15 x - 43} | r = -\frac 9 {49} | c = }} {{eqn | ll= \leadsto | l = 245 y - 1323 | r = - 135 x + 387 | c = }} {{eqn | ll= \leadsto | l = 245 y | r = - 135 x + 1710 | c = }} {{eqn | ll= \leadsto | l = 49 y + 27 x | r = 342 | c = }} {{end-eqn}} It remains to demonstrate that $\paren {\dfrac 9 2, \dfrac 9 2}$ lies on this line: {{begin-eqn}} {{eqn | l = 49 \dfrac 9 2 + 27 \dfrac 9 2 | r = \dfrac {441 + 243} 2 | c = }} {{eqn | r = \dfrac {684} 2 | c = }} {{eqn | r = 342 | c = }} {{end-eqn}} Bingo. {{qed}}	0
It is apparent by inspection that: :$(1): \quad g$ is an [[Definition:Extension of Mapping|extension]] of $f$ :$(2): \quad g$ is an [[Definition:Even Function|even function]]. Let $\map T x$ be the [[Definition:Fourier Series|Fourier series]] representing $g$: :$\map g x \sim \map T x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos \frac {n \pi x} \lambda + b_n \sin \frac {n \pi x} \lambda}$ where for all $n \in \Z_{> 0}$: :$a_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \cos \frac {n \pi x} \lambda \rd x$ :$b_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \sin \frac {n \pi x} \lambda \rd x$ From [[Fourier Sine Coefficients for Even Function over Symmetric Range]]: :$\forall n \in \Z_{> 0}: b_n = 0$ and from [[Fourier Cosine Coefficients for Even Function over Symmetric Range]]: :$\forall n \in \Z_{>0}: a_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map g x \cos \frac {n \pi x} \lambda \rd x$ But on $\openint 0 \lambda$, by definition: :$\forall x \in \openint 0 \lambda: \map g x = \map f x$ Hence: :$\map T x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren a_n \cos \frac {n \pi x} \lambda$ where: :$a_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map f x \cos \frac {n \pi x} \lambda \rd x$ That is, $\map T x$ is the same as $\map S x$. Hence the result. {{qed}} [[Category:Half-Range Fourier Series]] 4jkesnhoecrlce0p4zm4l919iuqymah	0
{{begin-eqn}} {{eqn | l = \map \sec {x + \frac {3 \pi} 2} | r = \frac 1 {\map \cos {x + \frac {3 \pi} 2} } | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 1 {\sin x} | c = [[Cosine of Angle plus Three Right Angles]] }} {{eqn | r = \csc x | c = [[Cosecant is Reciprocal of Sine]] }} {{end-eqn}} {{qed}}	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $X$ be an [[Definition:Irreducible Space|irreducible]] [[Definition:Subset|subset]] of $S$ such that :$\complement_S\left({X}\right) \in \tau$ Let $L = \left({\tau, \preceq}\right)$ be an [[Definition:Inclusion Ordered Set|inclusion ordered set]] of topology $\tau$. Then $\complement_S\left({X}\right)$ is [[Definition:Prime Element (Order Theory)|prime element]] in $L$.	0
Let $n$ be an [[Definition:Odd Integer|odd]] [[Definition:Positive Integer|positive integer]]. Let $R$ be a '''[[Definition:Rhodonea Curve|rhodonea curve]]''' defined by one of the [[Definition:Polar Equation|polar equations]]: {{begin-eqn}} {{eqn | l = r | r = a \cos n \theta }} {{eqn | l = r | r = a \sin n \theta }} {{end-eqn}} Then $R$ has $n$ [[Definition:Petal of Rhodonea Curve|petals]].	0
From [[Picard's Existence Theorem]], every point in a given rectangle is passed through by some curve which is the solution of a given integral curve of a differential equation. The equation of this family can be written as: :$y = \map y {x, c}$ where different values of $c$ give different curves. The integral curve which passes through $\tuple {x_0, y_0}$ corresponds to the value of $c$ such that: :$y_0 = \map y {x_0, c}$ Conversely, consider the [[Definition:One-Parameter Family of Curves|one-parameter family of curves]] described by: :$\map f {x, y, c} = 0$ Differentiate $f$ {{WRT|Differentiation}} $x$ to get a relation in the form: :$\map g {x, y, \dfrac {\d y} {\d x}, c} = 0$ Then eliminate $c$ between these expressions for $f$ and $g$ and get: :$\map F {x, y, \dfrac {\d y} {\d x} } = 0$ which is a [[Definition:First Order Ordinary Differential Equation|first order ordinary differential equation]]. {{qed}}	0
Since $a > b > 0$, we have $a^2 > b^2$. So: {{begin-eqn}} {{eqn | l = \int_0^{\pi/2} \frac 1 {a + b \cos x} \rd x | r = \intlimits {\frac 2 {\sqrt {a^2 - b^2} } \map \arctan {\sqrt {\frac {a - b} {a + b} } \tan \frac x 2} } 0 1 | c = [[Primitive of Reciprocal of p plus q by Cosine of a x|Primitive of $\dfrac 1 {p + q \cos x}$]] }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \paren {2 \map \arctan {\sqrt {\frac {a - b} {a + b} } } } }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \paren {2 \map \arctan {\sqrt {\frac {1 - \frac b a} {1 + \frac b a} } } } }} {{eqn | r = \frac 1 {\sqrt {a^2 - b^2} } \map \arccos {\frac b a} | c = [[Arccosine in terms of Arctangent]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sin 120 \degrees | r = \map \sin {90 \degrees + 30 \degrees} | c = }} {{eqn | r = \cos 30 \degrees | c = [[Sine of Angle plus Right Angle]] }} {{eqn | r = \frac {\sqrt 3} 2 | c = [[Cosine of 30 Degrees|Cosine of $30 \degrees$]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {t \sin a t} | r = -\map {\dfrac \d {\d s} } {\laptrans {\sin a t} } | c = [[Derivative of Laplace Transform]] }} {{eqn | r = -\map {\dfrac \d {\d s} } {\dfrac a {s^2 + a^2} } | c = [[Laplace Transform of Sine]] }} {{eqn | r = \dfrac {2 a s} {\paren {s^2 + a^2}^2} | c = [[Quotient Rule for Derivatives]] }} {{end-eqn}} {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \sum_{n \mathop = -m}^m e^{i \paren {n + \alpha} \theta} | r = \sum_{n \mathop = -m}^m e^{i n \theta} e^{i \alpha \theta} | c = }} {{eqn | r = e^{i \alpha \theta} e^{-i m \theta} \sum_{n \mathop = 0}^{2 m} e^{i n \theta} | c = }} {{eqn | n = 1 | r = e^{i \alpha \theta} e^{-i m \theta} \paren {\dfrac {e^{i \paren {2 m + 1} \theta} - 1} {e^{i \theta} - 1} } | c = [[Sum of Geometric Sequence]] }} {{eqn | r = e^{i \alpha \theta} e^{-i m \theta} \paren {\dfrac {e^{i \paren {2 m + 1} \theta / 2} \paren {e^{i \paren {2 m + 1} \theta / 2} - e^{-i \paren {2 m + 1} \theta / 2} } } {e^{i \theta / 2} \paren {e^{i \theta / 2} - e^{i \theta / 2} } } } | c = extracting factors }} {{eqn | r = e^{i \alpha \theta} \paren {\dfrac {e^{i \paren {2 m + 1} \theta / 2} - e^{-i \paren {2 m + 1} \theta / 2} } {e^{i \theta / 2} - e^{i \theta / 2} } } | c = [[Exponential of Sum]] and some algebra }} {{eqn | r = e^{i \alpha \theta} \frac {\map \sin {\paren {2 m + 2} \theta / 2} } {\map \sin {\theta / 2} } | c = [[Sine Exponential Formulation]] }} {{eqn | ll= \leadsto | l = \sum_{n \mathop = -m}^m \paren {\cos \paren {n + \alpha} \theta + i \sin \paren {n + \alpha} \theta} | r = \paren {\map \cos {\alpha \theta} + i \map \sin {\alpha \theta} } \frac {\map \sin {\paren {m + \frac 1 2} \theta } } {\map \sin {\theta / 2} } | c = [[Euler's Formula]] and simplifying }} {{eqn | n = 2 | ll= \leadsto | l = \sum_{n \mathop = -m}^m \sin \paren {n + \alpha} \theta | r = \map \sin {\alpha \theta} \frac {\map \sin {\paren {m + \frac 1 2} \theta} } {\map \sin {\theta / 2} } | c = equating imaginary parts }} {{end-eqn}} Note that the {{RHS}} at $(1)$ is not defined when $e^{i u} = 1$. This happens when $u = 2 k \pi$ for $k \in \Z$. For the given range of $0 < \theta < 2 \pi$ it is therefore seen that $(1)$ does indeed hold. Then: {{begin-eqn}} {{eqn | l = \int_0^\theta \sin \paren {\alpha + n} u \rd u | r = \intlimits {\dfrac {-\cos \paren {n + \alpha} u} {n + \alpha} } {u \mathop = 0} {u \mathop = \theta} | c = [[Primitive of Sine of a x|Primitive of $\sin a x$]] }} {{eqn | r = \paren {\dfrac {-\cos \paren {n + \alpha} \theta} {n + \alpha} } - \paren {\dfrac {-\cos \paren {n + \alpha} 0} {n + \alpha} } | c = }} {{eqn | r = \dfrac {1 - \cos \paren {n + \alpha} \theta} {n + \alpha} | c = [[Cosine of Zero is One]] }} {{eqn | ll= \leadsto | l = \sum_{n \mathop = -m}^m \dfrac {1 - \cos \paren {n + \alpha} \theta} {n + \alpha} | r = \sum_{n \mathop = -m}^m \int_0^\theta \sin \paren {\alpha + n} u \rd u | c = }} {{eqn | r = \int_0^\theta \sum_{n \mathop = -m}^m \sin \paren {\alpha + n} u \rd u | c = [[Linear Combination of Definite Integrals]] }} {{eqn | r = \int_0^\theta \map \sin {\alpha u} \frac {\map \sin {\paren {m + \frac 1 2} u} } {\map \sin {u / 2} } \rd u | c = from $(2)$, changing the variable name }} {{end-eqn}} Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \sin k x | r = \sin x + \sin 2 x + \sin 3 x + \cdots + \sin n x | c = }} {{eqn | r = \frac {\sin \frac {\paren {n + 1} x} 2 \sin \frac {n x} 2} {\sin \frac x 2} | c = }} {{end-eqn}}	0
:$\displaystyle \int \frac {\sec a x} x \ \mathrm d x = \ln \size x + \frac {\paren {a x}^2} 4 + \frac {5 \paren {a x}^4} {96} + \frac {61 \paren {a x}^6} {4320} + \cdots + \frac {\paren {-1}^n E_n \paren {a x}^{2 n} } {\paren {2 n} \paren {2 n}!} + \cdots + C$ where $E_n$ is the $n$th [[Definition:Euler Numbers|Euler number]].	0
The [[Definition:Integer Multiplication|product]] of the number of [[Definition:Edge of Polyhedron|edges]], [[Definition:Edge of Polyhedron|edges]] per [[Definition:Face of Polyhedron|face]] and [[Definition:Face of Polyhedron|faces]] of a [[Definition:Tetrahedron|tetrahedron]] is $72$.	0
:$\cot 60^\circ = \cot \dfrac {\pi} 3 = \dfrac {\sqrt 3} 3$	0
{{begin-eqn}} {{eqn | l = \tan z | r = \frac {\sin z} {\cos z} | c = {{Defof|Complex Tangent Function}} }} {{eqn | r = \frac {e^{i z} - e^{-i z} } {2 i} / \frac {e^{i z} + e^{-i z} } 2 | c = [[Sine Exponential Formulation]] and [[Cosine Exponential Formulation]] }} {{eqn | r = \frac {e^{i z} - e^{-i z} } {i \paren {e^{i z} + e^{-i z} } } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $2 i$ }} {{end-eqn}} {{qed}}	0
The [[Definition:Perimeter|perimeter]] $C$ of a [[Definition:Circle|circle]] with [[Definition:Radius of Circle|radius]] $r$ is given by: : $C = 2 \pi r$	0
We have: {{begin-eqn}} {{eqn | l = \frac x {e^x - 1} | r = \frac x 2 \left({\frac 2 {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x - e^x + 2} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {\left({e^x + 1}\right) - \left({e^x - 1}\right)} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x + 1} {e^x - 1} - 1}\right) | c = }} {{eqn | r = -\frac x 2 + \frac x 2 \left({\frac {e^x + 1} {e^x - 1} }\right) | c = }} {{eqn | r = -\frac x 2 + \frac x 2 \left({\frac {e^{x/2} + e^{-x/2} } {e^{x/2} - e^{-x/2} } }\right) | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $e^{-x/2}$ }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | n = 1 | l = \frac x 2 \left({\frac {e^{x/2} + e^{-x/2} } {e^{x/2} - e^{-x/2} } }\right) | r = \sum_{n \mathop = 0}^\infty \frac {B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = {{Defof|Bernoulli Numbers/Generating Function|Bernoulli numbers}} }} {{end-eqn}} Replacing $x$ with $2 i x$ in the {{LHS}} $(1)$: {{begin-eqn}} {{eqn | o = | r = \frac {2 i x} 2 \left({\frac {e^{2 i x / 2} + e^{-2 i x / 2} } {e^{2 i x / 2} - e^{-2 i x / 2} } }\right) | c = }} {{eqn | r = i x \left({\frac {e^{i x} + e^{-i x} } {e^{i x} - e^{-i x} } }\right) | c = }} {{eqn | r = x \cot x | c = [[Cotangent Exponential Formulation]] }} {{end-eqn}} Replacing $x$ with $2 i x$ in the {{RHS}} $(1)$: {{begin-eqn}} {{eqn | o = | r = \sum_{n \mathop = 0}^\infty \frac {B_{2 n} } {\left({2 n!}\right)} \left({2 i x}\right)^{2 n} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = }} {{eqn | ll= \leadsto | l = x \cot x | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = }} {{eqn | ll= \leadsto | l = \cot x | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n - 1} | c = }} {{end-eqn}} Then from [[Cotangent Minus Tangent]]: :$\tan x = \cot x - 2 \cot 2 x$ from which: {{begin-eqn}} {{eqn | l = \tan x | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} - 2 \sum_{n \mathop = 0}^\infty \frac {\left({-1}\right)^n 2^{2 n} B_{2 n} } {\left({2 n}\right)!} \left({2 x}\right)^{2 n - 1} | c = by $(1)$ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} (1 - 2^{2 n}) B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} | c = }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\left({- 1}\right)^{n - 1} 2^{2 n} (2^{2 n} - 1) B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} | c = }} {{end-eqn}} {{qed|lemma}}	0
The $4 \times 4$ [[Definition:Square (Geometry)|square]] is the only [[Definition:Square (Geometry)|square]] whose [[Definition:Area|area]] in square units equals its [[Definition:Perimeter|perimeter]] in units. The [[Definition:Area|area]] and [[Definition:Perimeter|perimeter]] of this [[Definition:Square (Geometry)|square]] are $16$.	0
The [[Definition:Internal Angle|internal angles]] of a [[Definition:Square (Geometry)|square]] are [[Definition:Right Angle|right angles]].	0
Let $m \ne 0$. {{begin-eqn}} {{eqn | l = \int \cos m x \rd x | r = \frac {\sin m x} m + C | c = [[Primitive of Cosine of a x|Primitive of $\cos m x$]] }} {{eqn | ll= \leadsto | l = \int_\alpha^{\alpha + 2 \pi} \cos m x \rd x | r = \intlimits {\frac {\sin m x} m} \alpha {\alpha + 2 \pi} | c = }} {{eqn | r = \paren {\frac {\map \sin {m \paren {\alpha + 2 \pi} } } m} - \paren {\frac {\sin m \alpha} m} | c = }} {{eqn | r = \paren {\frac {\sin m \alpha} m} - \paren {\frac {\sin m \alpha} m} | c = [[Sine of Angle plus Full Angle/Corollary|Corollary to Sine of Angle plus Full Angle]] }} {{eqn | r = 0 - 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed|lemma}} Let $m = 0$. {{begin-eqn}} {{eqn | l = \int \cos 0 x \rd x | r = \int 1 \rd x | c = [[Cosine of Zero is One]] }} {{eqn | r = x + C | c = [[Primitive of Constant]] }} {{eqn | ll= \leadsto | l = \int_\alpha^{\alpha + 2 \pi} \cos 0 x \rd x | r = \bigintlimits x \alpha {\alpha + 2 \pi} | c = }} {{eqn | r = \alpha + 2 \pi - \alpha | c = }} {{eqn | r = 2 \pi | c = }} {{end-eqn}} {{qed}}	0
'''Geodesy''' is the branch of [[Definition:Geometry|geometry]] which studies the shape of the [[Definition:Surface|surface]] of the [[Definition:Earth|Earth]].	0
:[[File:Heron3.png|700px]] Let $T$ be the [[Definition:Area|area]] of $\triangle ABC$. Construct the [[Definition:Incircle of Triangle|incircle]] of $\triangle ABC$. Let the [[Definition:Incenter of Triangle|incenter]] of $\triangle ABC$ be $M$. Let the [[Definition:Inradius of Triangle|inradius]] of $\triangle ABC$ be $r$. $\triangle ABC$ is made up of three [[Definition:Triangle (Geometry)|triangles]]: $\triangle AMB$, $\triangle BMC$ and $\triangle CMA$. From [[Area of Triangle in Terms of Side and Altitude]], the [[Definition:Area|areas]] of $\triangle AMB$, $\triangle BMC$ and $\triangle CMA$ are given by: :$\Area \paren {\triangle AMB} = \dfrac {r c} 2$ :$\Area \paren {\triangle BMC} = \dfrac {r a} 2$ :$\Area \paren {\triangle CMA} = \dfrac {r b} 2$ Thus: :$\paren 1: \quad T = \dfrac {r \paren {c + a + b} } 2 = r s$ where $s$ is the [[Definition:Semiperimeter|semiperimeter]] of $\triangle ABC$. Construct the [[Definition:Excircle of Triangle|excircle]] of $\triangle ABC$ with [[Definition:Excenter of Triangle|excenter]] $N$ [[Definition:Tangent Line|tangent]] to $AB$, and to $AC$ and $BC$ [[Definition:Production|produced]] at $D$ and $E$ respectively. We note that $s = CD = CE$. Therefore: :$DA = s - b$ :$EB = s - a$ Note that: :$AF + DA = BG + EB$ and: :$AF + BG = C$ Note also that: :$\triangle NDC$ is [[Definition:Similar Triangles|similar]] to $\triangle MFC$ :$\triangle NDA$ is [[Definition:Similar Triangles|similar]] to $\triangle AFM$ from which: :$\dfrac R r = \dfrac s {s - c}$ :$\dfrac R {s - b} = \dfrac {s - a} r$ Substituting for $R$: $R = \dfrac {r s} {s - c} = \dfrac {\paren {s - a} \paren {s - b} } r$ and so: :$r^2 = \dfrac {\paren {s - a} \paren {s - b} \paren {s - c}} s$ Thus $\paren 1$ becomes: :$T = s \sqrt {\dfrac {\paren {s - a} \paren {s - b} \paren {s - c}} s} = \sqrt {s \paren {s - a} \paren {s - b} \paren {s - c}}$ {{qed}}	0
:$\map {\dfrac \d {\d x} } {\coth^{-1} x} = \dfrac {-1} {x^2 - 1}$	0
:$\cos 4 \theta = \paren {\cos \theta - \cos \dfrac \pi 8} \paren {\cos \theta - \cos \dfrac {3 \pi} 8} \paren {\cos \theta - \cos \dfrac {5 \pi} 8} \paren {\cos \theta - \cos \dfrac {7 \pi} 8}$	0
{{begin-eqn}} {{eqn | l = \int e^{a x} \cos b x \rd x | r = \frac {e^{a x} \cos b x} a + \frac b a \int e^{a x} \sin b x \rd x | c = [[Primitive of Exponential of a x by Cosine of b x/Lemma|Primitive of $e^{a x} \cos b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} \cos b x} a + \frac b a \paren {\frac {e^{a x} \sin b x} a - \frac b a \int e^{a x} \cos b x \rd x} | c = [[Primitive of Exponential of a x by Sine of b x/Lemma|Primitive of $e^{a x} \sin b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} a \cos b x + e^{a x} b \sin b x} {a^2} - \frac {b^2} {a^2} \int e^{a x} \cos b x \rd x | c = simplifying }} {{eqn | ll= \leadsto | l = \paren {1 + \frac {b^2} {a^2} } \int e^{a x} \cos b x \rd x | r = \frac {e^{a x} \paren {a \cos b x + b \sin b x} } {a^2} | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {a^2 + b^2} {a^2} \int e^{a x} \cos b x \rd x | r = \frac {e^{a x} \paren {a \cos b x + b \sin b x} } {a^2} | c = common denominator }} {{eqn | ll= \leadsto | l = \int e^{a x} \cos b x \rd x | r = \frac {e^{a x} \paren {a \cos b x + b \sin b x} } {a^2 + b^2} | c = multiplying by $\dfrac {a^2} {a^2 + b^2}$ }} {{end-eqn}} {{qed}}	0
=== Open Sets Intersect implies Open Sets are Connected === Let $T = \struct {S, \tau}$ be [[Definition:Irreducible Space/Definition 3|irreducible]] in the sense that: :no two [[Definition:Non-Empty Set|non-empty]] [[Definition:Open Set (Topology)|open sets]] of $T$ are [[Definition:Disjoint Sets|disjoint]]. Let $U \subseteq S$ be an [[Definition:Open Set (Topology)|open set]] of $T$. {{AimForCont}} $U$ is not [[Definition:Connected Set (Topology)|connected]]. Then there exist [[Definition:Non-Empty Set|non-empty]] [[Definition:Open Set (Topology)|open sets]] $V, W$ of $U$ that are [[Definition:Disjoint Sets|disjoint]] and whose [[Definition:Set Union|union]] is $U$. By [[Open Set in Open Subspace]], $V$ and $W$ are [[Definition:Open Set (Topology)|open sets]] of $T$. Because $V \cap W = \O$, $T$ is not [[Definition:Irreducible Space|irreducible]]. This is a [[Definition:Contradiction|contradiction]]. Thus $U$ is [[Definition:Connected Set (Topology)|connected]]. Thus $T = \struct {S, \tau}$ is [[Definition:Irreducible Space/Definition 7|irreducible]] in the sense that: :every [[Definition:Open Set (Topology)|open set]] of $T$ is [[Definition:Connected Set (Topology)|connected]].. {{qed|lemma}} === Open Sets are Connected implies Open Sets Intersect === Let $T = \struct {S, \tau}$ be [[Definition:Irreducible Space/Definition 3|irreducible]] in the sense that: :every [[Definition:Open Set (Topology)|open set]] of $T$ is [[Definition:Connected Set (Topology)|connected]]. Let $V$ and $W$ be [[Definition:Open Set (Topology)|open sets]] of $T$. By definition of [[Definition:Topology|topology]], their [[Definition:Set Union|union]] $V \cup W$ is [[Definition:Open Set (Topology)|open]] in $T$. [[Definition:By Hypothesis|By hypothesis]], $V \cup W$ is [[Definition:Connected Set (Topology)|connected]]. By [[Open Set in Open Subspace]], $V$ and $W$ are [[Definition:Open Set (Topology)|open sets]] of $V \cup W$. Because $V \cup W$ is [[Definition:Connected Set (Topology)|connected]], $V \cap W$ is [[Definition:Non-Empty Set|non-empty]]. Because $V$ and $W$ were arbitrary, $T$ is [[Definition:Irreducible Space|irreducible]]. Thus $T = \struct {S, \tau}$ is [[Definition:Irreducible Space/Definition 1|irreducible]] in the sense that: :no two [[Definition:Non-Empty Set|non-empty]] [[Definition:Open Set (Topology)|open sets]] of $T$ are [[Definition:Disjoint Sets|disjoint]].	0
{{begin-eqn}} {{eqn | l = \sin 60 \degrees | r = \map \cos {90 \degrees - 60 \degrees} | c = [[Cosine of Complement equals Sine]] }} {{eqn | r = \cos 30 \degrees | c = }} {{eqn | r = \dfrac {\sqrt 3} 2 | c = [[Cosine of 30 Degrees|Cosine of $30 \degrees$]] }} {{end-eqn}} {{qed}}	0
{{TFAE|def = Complex Inverse Hyperbolic Cotangent}} Let $S$ be the [[Definition:Subset|subset]] of the [[Definition:Complex Plane|complex plane]]: :$S = \C \setminus \left\{{-1 + 0 i, 1 + 0 i}\right\}$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \arcsin \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 {\sqrt {a^2 - x^2} } | c = [[Derivative of Arcsine of x over a|Derivative of $\arcsin \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^2} 2 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \arcsin \frac x a \ \mathrm d x | r = \frac {x^2} 2 \arcsin \frac x a - \int \frac {x^2} 2 \left({\frac 1 {\sqrt {a^2 - x^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arcsin \frac x a - \frac 1 2 \int \frac {x^2 \ \mathrm d x} {\sqrt {a^2 - x^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arcsin \frac x a - \frac 1 2 \left({\frac {-x \sqrt {a^2 - x^2} } 2 + \frac {a^2} 2 \arcsin \frac x a}\right) + C | c = [[Primitive of x squared over Root of a squared minus x squared|Primitive of $\dfrac {x^2} {\sqrt {a^2 - x^2} }$]] }} {{eqn | r = \left({\frac {x^2} 2 - \frac {a^2} 4}\right) \arcsin \frac x a + \frac {x \sqrt {a^2 - x^2} } 4 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
We have $AO = BO$ since they are [[Definition:Radius|radii]]. Therefore $\triangle AOB$ is [[Definition:Isosceles Triangle|isosceles]]. By [[Isosceles Triangle has Two Equal Angles]]: :$\angle OAB = \angle OBA$ By [[Sum of Angles of Triangle equals Two Right Angles]]: :$\angle OAB + \angle OBA + \theta = 180 \degrees$ Therefore $\angle OAB = \dfrac {180 \degrees - \theta} 2 = 90 \degrees - \dfrac \theta 2$. Thus: {{begin-eqn}} {{eqn | l = \dfrac {AB} {\sin \theta} | r = \dfrac {BO} {\sin \angle OAB} | c = [[Law of Sines]] }} {{eqn | l = {AB} | r = \dfrac {BO \sin \theta} {\sin \angle OAB} | c = }} {{eqn | r = \dfrac {2 r \sin \frac \theta 2 \cos \frac \theta 2} {\map \sin {90 \degrees - \frac \theta 2} } | c = [[Double Angle Formula for Sine]] }} {{eqn | r = \dfrac {2 r \sin \frac \theta 2 \cos \frac \theta 2} {\cos \frac \theta 2} | c = [[Sine of Supplementary Angle]] }} {{eqn | r = 2 r \sin \dfrac \theta 2 | c = }} {{end-eqn}} {{qed}}	0
:$\sin 36^\circ = \sin \dfrac \pi 5 = \dfrac {\sqrt {\sqrt 5 / \phi} } 2 = \sqrt {\dfrac 5 8 - \dfrac {\sqrt 5} 8}$ where $\phi$ denotes the [[Definition:Golden Mean|golden mean]].	0
From [[Cotangent is Cosine divided by Sine]]: : $\cot \theta = \dfrac {\cos \theta} {\sin \theta}$ When $\sin \theta = 0$, $\dfrac {\cos \theta} {\sin \theta}$ can be defined only if $\cos \theta = 0$. But there are no such $\theta$ such that both $\cos \theta = 0$ and $\sin \theta = 0$. When $\theta = 0$, $\sin \theta = 0$. Thus $\cot \theta$ is undefined at this value. {{qed}}	0
:$\tan 180^\circ = \tan \pi = 0$	0
=== Sufficient Condition === Let $\mathcal L$ be an affine transformation. Let $L$ be the [[Definition:Tangent Map of Affine Transformation|tangent map]]. Let $r \in \mathcal E$ be any point. Then by definition we have: :$\lambda p + \left({1 - \lambda}\right) q = r + \lambda \vec{r p} + \left({1 - \lambda}\right) \vec{r q}$ Thus we find: {{begin-eqn}} {{eqn | l = \mathcal L \left({\lambda p + \left({1 - \lambda}\right) q}\right) | r = \mathcal L \left({r}\right) + L \left({\lambda p + \left({1 - \lambda}\right) q}\right) | c = Definition of [[Definition:Affine Transformation|Affine Transformation]] }} {{eqn | r = \mathcal L \left({r}\right) + \lambda L \left({p}\right) + \left({1 - \lambda}\right) L \left({q}\right) | c = since $L$ is [[Definition:Linear Transformation|linear]] }} {{eqn | r = \lambda \mathcal L \left({p}\right) + \left({1 - \lambda}\right) \mathcal L \left({q}\right) | c = Definition of [[Definition:Barycenter|Barycenter]] }} {{end-eqn}} {{qed|lemma}} === Necessary Condition === Suppose that for all points $p, q \in \mathcal E$ and all $\lambda \in \R$: :$\mathcal L \left({\lambda p + \left({1 - \lambda}\right) q}\right) = \lambda \mathcal L \left({p}\right) + \left({1 - \lambda}\right) \mathcal L \left({q}\right)$ Let $E$ be the [[Definition:Difference Space|difference space]] of $\mathcal E$. [[Definition:Fixed Point|Fix a point]] $p \in \mathcal E$, and define for all $u \in E$: :$L\left(u\right) = \mathcal L\left(p + u\right) - \mathcal L\left(p\right)$ Let $q = p + u$. Then: :$\mathcal L \left({q}\right) = \mathcal L \left({p}\right) + L \left({u}\right)$ So to show that $\mathcal L$ is affine, we are required to prove that $L$ is [[Definition:Linear Transformation|linear]]. That is, we want to show that for all $\lambda \in k$ and all $u, v \in E$: :$L \left({\lambda u}\right) = \lambda L \left({u}\right)$ and: :$L \left({u + v}\right) = L \left({u}\right) + L \left({v}\right)$ First of all: {{begin-eqn}} {{eqn | l = L \left({\lambda u}\right) | r = \mathcal L \left({p + \lambda u}\right) - \mathcal L \left({p}\right) | c = Definition of $L$ }} {{eqn | r = \mathcal L \left({\left({1 - \lambda}\right) + \lambda \left({p + u}\right)}\right) | c = Definition of [[Definition:Barycenter|Barycenter]] }} {{eqn | r = \left({1 - \lambda}\right) \mathcal L \left({p}\right) + \lambda \mathcal L \left({p + u}\right) - \mathcal L \left({p}\right) | c = [[Definition:By Hypothesis|By Hypothesis]] on $\mathcal L$ }} {{eqn | r = \lambda \left({\mathcal L \left({p + u}\right) - \mathcal L \left({p}\right)}\right) | c = }} {{eqn | r = \lambda L \left({u}\right) | c = Definition of $L$ }} {{end-eqn}} Now it is to be shown that :$L \left({u + v}\right) = L \left({u}\right) + \left({v}\right)$ First: :$p + u + v = \dfrac 1 2 \left({p + 2 u}\right) + \dfrac 1 2 \left({p + 2 v}\right)$ Now: {{begin-eqn}} {{eqn | l = \mathcal L \left({p + u + v}\right) | r = \mathcal L \left({\frac 1 2 \left({p + 2 u}\right) + \frac 1 2 \left({p + 2 v}\right)}\right) | c = }} {{eqn | r = \frac 1 2 \mathcal L \left({p + 2 u}\right) + \frac 1 2 \mathcal L \left({p + 2 v}\right) | c = [[Definition:By Hypothesis|By Hypothesis]] on $\mathcal L$ }} {{eqn | r = \frac 1 2 \left({\mathcal L \left({p + 2 u}\right) - \mathcal L \left({p}\right)}\right) + \frac 1 2 \left({ \mathcal L \left({p + 2 v}\right) - \mathcal L \left({p}\right)}\right) + \mathcal L \left({p}\right) | c = }} {{eqn | r = \frac 1 2 L \left({2 u}\right) + \frac 1 2 L \left({2 v}\right) + \mathcal L \left({p}\right) | c = Definition of $L$ }} {{eqn | r = L \left({u}\right) + L \left({v}\right) + \mathcal L \left({p}\right) | c = as $L$ preserves scalar multiples }} {{end-eqn}} From the above calculation: :$L \left({u + v}\right) = \mathcal L \left({p + u + v}\right) - \mathcal L \left({p}\right) = L \left({u}\right) + L \left({v}\right)$ This shows that $L$ is linear, and therefore concludes the proof. {{Qed}} [[Category:Affine Geometry]] 9splhjisbqidaxnnz69xjkwpx8v7k1c	0
Let $\map f t := \map \Ci t = \displaystyle \int_t^\infty \dfrac {\cos u} u \rd u$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = -\dfrac {\cos t} t | c = }} {{eqn | ll= \leadsto | l = t \map {f'} t | r = -\cos t | c = }} {{eqn | ll= \leadsto | l = \laptrans {t \map {f'} t} | r = -\laptrans {\cos t} | c = }} {{eqn | r = -\dfrac s {s^2 + 1} | c = [[Laplace Transform of Cosine]] }} {{eqn | ll= \leadsto | l = -\dfrac \d {\d s} \laptrans {\map {f'} t} | r = -\dfrac s {s^2 + 1} | c = [[Derivative of Laplace Transform]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d s} } {s \laptrans {\map f t} - \map f 0} | r = \dfrac s {s^2 + 1} | c = [[Laplace Transform of Derivative]] }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \int \dfrac s {s^2 + 1} \rd s | c = $\map f 0 = 0$, and integrating both sides {{WRT|Integration}} $s$ }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{end-eqn}} By the [[Initial Value Theorem of Laplace Transform]]: :$\displaystyle \lim_{s \mathop \to \infty} s \laptrans {\map f t} = \lim_{t \mathop \to 0} \map f t = \map f 0 = 0$ which leads to: :$c = 0$ Thus: {{begin-eqn}} {{eqn | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} | c = }} {{eqn | ll= \leadsto | l = \laptrans {\map f t} | r = \dfrac {\map \ln {s^2 + 1} } {2 s} | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \frac {\cos x} {\sec x + \tan x} | r = \frac {\cos^2 x} {1 + \sin x} | c = [[Sum of Secant and Tangent]] }} {{eqn | r = \frac {1 - \sin^2 x} {1 + \sin x} | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \frac {\paren {1 + \sin x} \paren {1 - \sin x} } {1 + \sin x} | c = [[Difference of Two Squares]] }} {{eqn | r = 1 - \sin x }} {{end-eqn}} {{qed}} [[Category:Trigonometric Identities]] j1dy19ag3ylw8gkc1yh8rx9voltltmz	0
:$\sin 30 \degrees = \sin \dfrac \pi 6 = \dfrac 1 2$	0
=== [[Definition:Quadrilateral/Square|Geometry]] === {{:Definition:Quadrilateral/Square}} === [[Definition:Square/Mapping|Abstract Algebra]] === {{:Definition:Square/Mapping}}	0
:$\displaystyle \int \frac {\mathrm d x} {\left({1 - \cos a x}\right)^2} = \frac {-1} {2a} \cot \frac {a x} 2 - \frac 1 {6 a} \cot^3 \frac {a x} 2 + C$	0
Let $\triangle ABC$ be a [[Definition:Triangle (Geometry)|triangle]]. Let $ACDE$ and $BCFG$ be [[Definition:Parallelogram|parallelograms]] constructed on the [[Definition:Side of Polygon|sides]] $AC$ and $BC$ of $\triangle ABC$. Let $DE$ and $FG$ be [[Definition:Production|produced]] to [[Definition:Intersection (Geometry)|intersect]] at $H$. Let $AJ$ and $BI$ be constructed on $A$ and $B$ [[Definition:Parallel Lines|parallel]] to and equal to $HC$. Then the [[Definition:Area|area]] of the [[Definition:Parallelogram|parallelogram]] $ABIJ$ equals the sum of the [[Definition:Area|areas]] of the [[Definition:Parallelogram|parallelograms]] $ACDE$ and $BCFG$.	0
:[[File:Euclid-XI-25.png|600px]] Let the [[Definition:Parallelepiped|parallelepiped]] $ABCD$ be cut by the [[Definition:Plane|plane]] $FG$ which is [[Definition:Parallel Planes|parallel]] to the [[Definition:Opposite Face of Parallelepiped|opposite planes]] $RA$ and $DH$. It is to be demonstrated that the [[Definition:Ratio|ratio]] of the [[Definition:Base of Parallelepiped|base]] $AEFV$ to the [[Definition:Base of Parallelepiped|base]] $EHCF$ equals the [[Definition:Ratio|ratio]] of the [[Definition:Parallelepiped|parallelepiped]] $ABFU$ to the [[Definition:Parallelepiped|parallelepiped]] $EGCD$. Let $AH$ be produced in each direction. Let any number of [[Definition:Straight Line|straight lines]] $AK, KL$ equal to $AE$ be made. Let any number of [[Definition:Straight Line|straight lines]] $HM, MN$ equal to $EH$ be made. Let the [[Definition:Parallelogram|parallelograms]] $LP, KV, HW, MS$ be completed. Let the [[Definition:Parallelepiped|parallelepipeds]] $LQ, KR, DM, MT$ be completed. We have that the [[Definition:Straight Line|straight lines]] $LK, KA, AE$ are equal. Therefore the [[Definition:Parallelogram|parallelograms]] $LP, KV, AF$ are equal. From {{EuclidPropLink|book = XI|prop = 24|title = Opposite Planes of Solid contained by Parallel Planes are Equal Parallelograms}}: :the [[Definition:Parallelogram|parallelograms]] $LX, KQ, AR$ are equal. For the same reason, the [[Definition:Parallelogram|parallelograms]] $EC, HW, MS$ are equal. Therefore the [[Definition:Parallelepiped|parallelepipeds]] $HG, HI, NT$ are equal. Further, the [[Definition:Parallelogram|parallelograms]] $DH, MY, MT$ are equal. Therefore, in the [[Definition:Parallelepiped|parallelepipeds]] $LQ, KR, AU$, three [[Definition:Plane|planes]] equal three [[Definition:Plane|planes]]. But the three [[Definition:Plane|planes]] equal the three [[Definition:Opposite Face of Parallelepiped|planes opposite]]. Therefore the three [[Definition:Parallelepiped|parallelepipeds]] $LQ, KR, AU$ are equal to one another. For the same reason, the three [[Definition:Parallelepiped|parallelepipeds]] $ED, DM, MT$ are equal to one another. Therefore, whatever multiple the [[Definition:Base of Parallelepiped|base]] $LF$ is of the [[Definition:Base of Parallelepiped|base]] $AF$, the same multiple also is the [[Definition:Parallelepiped|parallelepiped]] $LU$ of the [[Definition:Parallelepiped|parallelepiped]] $AU$. For the same reason, whatever multiple the [[Definition:Base of Parallelepiped|base]] $NF$ is of the [[Definition:Base of Parallelepiped|base]] $FH$, the same multiple also is the [[Definition:Parallelepiped|parallelepiped]] $NU$ of the [[Definition:Parallelepiped|parallelepiped]] $HU$. And if the [[Definition:Base of Parallelepiped|base]] $LF$ equals the [[Definition:Base of Parallelepiped|base]] $NF$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ equals the [[Definition:Parallelepiped|parallelepiped]] $NU$. If the [[Definition:Base of Parallelepiped|base]] $LF$ exceeds the [[Definition:Base of Parallelepiped|base]] $NF$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ exceeds the [[Definition:Parallelepiped|parallelepiped]] $NU$. If the [[Definition:Base of Parallelepiped|base]] $LF$ falls short of the [[Definition:Base of Parallelepiped|base]] $NF$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ falls short of the [[Definition:Parallelepiped|parallelepiped]] $NU$. We have four magnitudes: :the two [[Definition:Base of Parallelepiped|bases]] $AF$ and $FH$ and: :the two [[Definition:Parallelepiped|parallelepipeds]] $AU$ and $HU$. Equimultiples have been taken of the [[Definition:Base of Parallelepiped|base]] $AF$ and the [[Definition:Parallelepiped|parallelepiped]] $AU$: the [[Definition:Base of Parallelepiped|base]] $LF$ and the [[Definition:Parallelepiped|parallelepiped]] $LU$. and equimultiples have been taken of the [[Definition:Base of Parallelepiped|base]] $HF$ and the [[Definition:Parallelepiped|parallelepiped]] $HU$: the [[Definition:Base of Parallelepiped|base]] $NF$ and the [[Definition:Parallelepiped|parallelepiped]] $NU$. It has been proved that: : if the [[Definition:Base of Parallelepiped|base]] $LF$ exceeds the [[Definition:Base of Parallelepiped|base]] $FN$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ exceeds the [[Definition:Parallelepiped|parallelepiped]] $NU$ :if the [[Definition:Base of Parallelepiped|base]] $LF$ equals the [[Definition:Base of Parallelepiped|base]] $NF$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ equals the [[Definition:Parallelepiped|parallelepiped]] $NU$ :if the [[Definition:Base of Parallelepiped|base]] $LF$ falls short of the [[Definition:Base of Parallelepiped|base]] $NF$, the [[Definition:Parallelepiped|parallelepiped]] $LU$ falls short of the [[Definition:Parallelepiped|parallelepiped]] $NU$. Therefore, as the [[Definition:Base of Parallelepiped|base]] $AF$ is to the [[Definition:Base of Parallelepiped|base]] $FH$, so the [[Definition:Parallelepiped|parallelepiped]] $AU$ is to the [[Definition:Parallelepiped|parallelepiped]] $UH$. {{qed}} {{Euclid Note|25|XI}}	0
{{begin-eqn}} {{eqn | l = \map \sin {x + \pi} | r = \sin x \cos \pi + \cos x \sin \pi | c = [[Sine of Sum]] }} {{eqn | r = \sin x \cdot \paren {-1} + \cos x \cdot 0 | c = [[Cosine of Straight Angle]] and [[Sine of Straight Angle]] }} {{eqn | r = -\sin x | c = }} {{end-eqn}} {{qed}}	0
:$\ds \int \frac {\d x} {p \sin a x + q \cos a x + r} = \begin{cases} \ds \frac 2 {a \sqrt {r^2 - p^2 - q^2} } \map \arctan {\frac {p + \paren {r - q} \tan \dfrac {a x} 2} {\sqrt {r^2 - p^2 - q^2} } } + C & : p^2 + q^2 < r^2 \\ \ds \frac 1 {a \sqrt {p^2 + q^2 - r^2} } \ln \size {\frac {p - \sqrt {p^2 + q^2 - r^2} + \paren {r - q} \tan \dfrac {a x} 2} {p + \sqrt {p^2 + q^2 - r^2} + \paren {r - q} \tan \dfrac {a x} 2} } + C & : p^2 + q^2 > r^2 \end{cases}$	0
Let $a, b, c, d \in \R$ be [[Definition:Real Number|real numbers]]. Let $f: \R^* \to \R^*$ be the [[Definition:Möbius Transformation on Real Numbers|Möbius transformation]] [[Definition:Restriction of Mapping|restricted]] to the [[Definition:Real Number|real numbers]]: :$\map f x = \begin {cases} \dfrac {a x + b} {c x + d} & : x \ne -\dfrac d c \\ \infty & : x = -\dfrac d c \\ \dfrac a c & : x = \infty \\ \infty & : x = \infty \text { and } c = 0 \end {cases}$ Then: :$f: \R^* \to \R^*$ is a [[Definition:Bijection|bijection]] {{iff}}: :$a c - b d \ne 0$	0
Recall the definition of the [[Definition:Complex Sine Function|sine function]]: {{begin-eqn}} {{eqn | l = \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = }} {{eqn | r = z - \frac {z^3} {3!} + \frac {z^5} {5!} - \frac {z^7} {7!} + \cdots + \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} + \cdots | c = }} {{end-eqn}} Recall the definition of the [[Definition:Exponential Function/Complex/Sum of Series|exponential as a power series]]: {{begin-eqn}} {{eqn | l = \exp z | r = \sum_{n \mathop = 0}^\infty \frac {z^n} {n!} | c = }} {{eqn | r = 1 + \frac z {1!} + \frac {z^2} {2!} + \frac {z^3} {3!} + \cdots + \frac {z^n} {n!} + \cdots | c = }} {{end-eqn}} Then, starting from the {{RHS}}: {{begin-eqn}} {{eqn | l = \frac {\exp \paren {i z} - \exp \paren {-i x} } {2 i} | r = \frac 1 {2 i} \paren {\sum_{n \mathop = 0}^\infty \frac {\paren {i z}^n} {n!} - \sum_{n \mathop = 0}^\infty \frac {\paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^n - \paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^{2 n} - \paren {-i z}^{2 n} } {\paren {2 n}!} + \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = split into [[Definition:Even Integer|even]] and [[Definition:Odd Integer|odd]] $n$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-i z}^{2 n} = \paren {i z}^{2 n}$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {2 \paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-1}^{2 n + 1} = -1$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = cancel $2$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {i \paren {-1}^n z^{2 n + 1} } {\paren {2 n + 1}!} | c = as $i^{2 n + 1} = i \paren {-1})^n $ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1!} } | c = cancel $i$ }} {{eqn | r = \sin z }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \sec {-x} | r = \frac 1 {\map \cos {-x} } | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 1 {\cos x} | c = [[Cosine Function is Even]] }} {{eqn | r = \sec x | c = [[Secant is Reciprocal of Cosine]] }} {{end-eqn}} {{qed}}	0
From [[Fibonacci Number greater than Golden Section to Power less Two]]: :$F_n \ge \phi^{n - 2}$ From [[Fibonacci Number less than Golden Section to Power less One]]: :$F_n \le \phi^{n - 1}$ {{qed}}	0
:$\dfrac 2 {\pi} x \le \sin x \le x$ for all $x$ in the [[Definition:Real Interval/Closed|interval]] $\left[{0 \,.\,.\, \dfrac {\pi} 2}\right]$	0
{{begin-eqn}} {{eqn | l = \cos 2 \theta + i \sin 2 \theta | r = \paren {\cos \theta + i \sin \theta}^2 | c = [[De Moivre's Formula]] }} {{eqn | r = \cos^2 \theta + i^2 \sin^2 \theta + 2 i \cos \theta \sin \theta }} {{eqn | r = \cos^2 \theta - \sin^2 \theta + 2 i \cos \theta \sin \theta }} {{eqn | ll= \leadsto | l = \sin 2 \theta | r = 2 \cos \theta \sin \theta | c = equating imaginary parts }} {{end-eqn}} {{qed}}	0
:$\sin 75 \degrees = \sin \dfrac {5 \pi} {12} = \dfrac {\sqrt 6 + \sqrt 2} 4$	0
Let $\C$ be the [[Definition:Complex Plane|complex plane]]. Let $C$ be a [[Definition:Circle|circle]] in $\C$ whose [[Definition:Radius of Circle|radius]] is $r \in \R_{>0}$ and whose [[Definition:Center of Circle|center]] is $\alpha \in \C$. Then $C$ may be written as: :$\cmod {z - \alpha} = r$ where $\cmod {\, \cdot \,}$ denotes [[Definition:Complex Modulus|complex modulus]].	0
:[[File:Euclid-XI-22a.png|600px]] Let $\angle ABC, \angle DEF, \angle GHK$ be [[Definition:Plane Angle|plane angles]] such that the sum of any two is greater than the remaining one. That is: :$\angle ABC + \angle DEF > \angle GHK$ :$\angle DEF + \angle GHK > \angle ABC$ :$\angle GHK + \angle ABC > \angle DEF$ Let the [[Definition:Straight Line|straight lines]] $AB, BC, DE, EF, GH, GK$ be equal. Let $AC$, $DF$ and $GK$ be joined. It is to be demonstrated that it is possible to construct a [[Definition:Triangle (Geometry)|triangle]] from [[Definition:Straight Line|straight lines]] equal to $AC$, $DF$ and $HK$. If $\angle ABC, \angle DEF, \angle GHK$ are equal to one another, then it is possible to construct an [[Definition:Equilateral Triangle|equilateral triangle]] from $AC$, $DF$ and $HK$. :[[File:Euclid-XI-22b.png|250px|right]] Otherwise, let $\angle ABC, \angle DEF, \angle GHK$ be unequal. On the [[Definition:Straight Line|straight line]] $HK$ at the point $H$, let: :$\angle KHL$ be constructed equal to $\angle ABC$ :$HL$ be constructed equal to one of $AB, BC, DE, EF, GH, GK$ :$KL$ be joined. We have that: :$AB$ and $BC$ are equal to $KH$ and $HL$ :$\angle ABC = \angle KHL$ Therefore from {{EuclidPropLink|book = I|prop = 4|title = Triangle Side-Angle-Side Equality}}: :$AC = KL$ We have that: :$\angle GHK + \angle ABC > \angle DEF$ while: :$\angle ABC = \angle KHL$ Therefore: :$\angle GHL > \angle DEF$ We have that: :$GH$ and $HL$ are equal to $DE$ and $EF$ :$\angle GHL > \angle DEF$ Therefore from {{EuclidPropLink|book = I|prop = 24|title = Hinge Theorem}}: :$GL > DF$ But: :$GK + KL > GL$ Therefore: :$GK + KL > DF$ But: :$KL = AC$ Therefore: :$AC + GK > DF$ Similarly it can be proved that: :$AC + DF > GK$ and that: :$DF + GK > AC$ Hence the result. {{qed}} {{Euclid Note|22|XI}}	0
From [[Sine of Complex Number]]: :$\map \sin {x + i y} = \sin x \cosh y + i \cos x \sinh y$ The result follows by definition of the [[Definition:Real Part|real part]] of a [[Definition:Complex Number|complex number]]. {{qed}}	0
Let $\C$ be the [[Definition:Complex Plane|complex plane]]. Let $z \in \C$ be subject to the condition: :$\cmod {z - 1} = \cmod {z + 1}$ where $\cmod {\, \cdot \,}$ denotes [[Definition:Complex Modulus|complex modulus]]. Then the [[Definition:Locus|locus]] of $z$ is the [[Definition:Imaginary Axis|imaginary axis]].	0
Let $l_1 \perp l_2$. By definition of [[Definition:Perpendicular Lines|perpendicular lines]], $l_1$ meets $l_2$ at a [[Definition:Right Angle|right angle]]. Hence $l_2$ similarly meets $l_1$ at a [[Definition:Right Angle|right angle]]. That is: :$l_2 \perp l_1$ Thus $\parallel$ is seen to be [[Definition:Symmetric Relation|symmetric]].	0
:[[File:Euclid-XI-24.png|450px]] Let the [[Definition:Polyhedron|solid]] $CDHG$ be [[Definition:Containment of Solid Figure|contained]] by the [[Definition:Parallel Planes|parallel planes]] $AC, GF, AH, DF, BF, AE$. It is to be demonstrated that opposite [[Definition:Plane|planes]] are equal [[Definition:Parallelogram|parallelograms]]. We have that the two [[Definition:Parallel Planes|parallel planes]] $BG$ and $CE$ are cut by the [[Definition:Plane|plane]] $AC$. From {{EuclidPropLink|book = XI|prop = 16|title = Common Sections of Parallel Planes with other Plane are Parallel}}: :the [[Definition:Common Section|common sections]] of $BG$ and $CE$ are [[Definition:Parallel Lines|parallel lines]]. Thus $AB \parallel DC$. Again, we have that the two [[Definition:Parallel Planes|parallel planes]] $BF$ and $AE$ are cut by the [[Definition:Plane|plane]] $AC$. Thus $BC \parallel AD$. But $AB \parallel DC$. Therefore $AC$ is by definition a [[Definition:Parallelogram|parallelogram]]. Similarly it can be shown that each of $GF, AH, DF, BF, AE$ are [[Definition:Parallelogram|parallelograms]]. Let $AH$ and $DF$ be joined. We have that: :$AB \parallel DC$ and: :$BH \parallel CF$ Thus the two [[Definition:Straight Line|straight lines]] $AB$ and $BH$ which meet one another are [[Definition:Parallel Lines|parallel]] to the two [[Definition:Straight Line|straight lines]] $DC$ and $CF$ which also meet one another, but not in the same [[Definition:Plane|plane]]. Therefore by {{EuclidPropLink|book = XI|prop = 10|title = Two Lines Meeting which are Parallel to Two Other Lines Meeting contain Equal Angles}}: :$\angle ABH = \angle DCF$ From {{EuclidPropLink|book = I|prop = 34|title = Opposite Sides and Angles of Parallelogram are Equal}}: :$AB$ and $BH$ are equal to $DC$ and $CF$. and because: :$\angle ABH = \angle DCF$ it follows that: :$AH = DF$ So from {{EuclidPropLink|book = I|prop = 4|title = Triangle Side-Angle-Side Equality}}: :$\triangle ABH = \triangle DCF$ We have that the [[Definition:Parallelogram|parallelogram]] $BG$ is double $\triangle ABH$. From {{EuclidPropLink|book = I|prop = 34|title = Opposite Sides and Angles of Parallelogram are Equal}}: :the [[Definition:Parallelogram|parallelogram]] $CE$ is double the $\triangle DCF$. Therefore the [[Definition:Parallelogram|parallelogram]] $BG$ equals the [[Definition:Parallelogram|parallelogram]] $CE$. Similarly it is shown that: :the [[Definition:Parallelogram|parallelogram]] $AC$ equals the [[Definition:Parallelogram|parallelogram]] $GF$ and: :the [[Definition:Parallelogram|parallelogram]] $AE$ equals the [[Definition:Parallelogram|parallelogram]] $BF$. {{qed}} {{Euclid Note|24|XI}}	0
The [[Definition:Positive Integer|positive integers]] $n$ are represented in the [[Definition:Golden Mean Number System|golden mean number system]] in their [[Definition:Simplest Form of Number in Golden Mean Number System|simplest form]] $S_n$ as follows: :{| border="1" |- ! align="right" style = "padding: 2px 10px" | $n$ ! align="left" style = "padding: 2px 10px" | $S_n$ |- | align="right" style = "padding: 2px 10px" | $1$ | align="left" style = "padding: 2px 10px" | $1$ |- | align="right" style = "padding: 2px 10px" | $2$ | align="left" style = "padding: 2px 10px" | $10 \cdotp 01$ |- | align="right" style = "padding: 2px 10px" | $3$ | align="left" style = "padding: 2px 10px" | $100 \cdotp 01$ |- | align="right" style = "padding: 2px 10px" | $4$ | align="left" style = "padding: 2px 10px" | $101 \cdotp 01$ |- | align="right" style = "padding: 2px 10px" | $5$ | align="left" style = "padding: 2px 10px" | $1000 \cdotp 1001$ |- | align="right" style = "padding: 2px 10px" | $6$ | align="left" style = "padding: 2px 10px" | $1010 \cdotp 0001$ |- | align="right" style = "padding: 2px 10px" | $7$ | align="left" style = "padding: 2px 10px" | $10000 \cdotp 0001$ |- | align="right" style = "padding: 2px 10px" | $8$ | align="left" style = "padding: 2px 10px" | $10001 \cdotp 0001$ |- | align="right" style = "padding: 2px 10px" | $9$ | align="left" style = "padding: 2px 10px" | $10010 \cdotp 0101$ |- | align="right" style = "padding: 2px 10px" | $10$ | align="left" style = "padding: 2px 10px" | $10100 \cdotp 0101$ |- | align="right" style = "padding: 2px 10px" | $11$ | align="left" style = "padding: 2px 10px" | $10101 \cdotp 0101$ |- | align="right" style = "padding: 2px 10px" | $12$ | align="left" style = "padding: 2px 10px" | $100000 \cdotp 101001$ |- | align="right" style = "padding: 2px 10px" | $13$ | align="left" style = "padding: 2px 10px" | $100010 \cdotp 001001$ |- | align="right" style = "padding: 2px 10px" | $14$ | align="left" style = "padding: 2px 10px" | $100100 \cdotp 001001$ |- | align="right" style = "padding: 2px 10px" | $15$ | align="left" style = "padding: 2px 10px" | $100101 \cdotp 001001$ |- | align="right" style = "padding: 2px 10px" | $16$ | align="left" style = "padding: 2px 10px" | $101000 \cdotp 100001$ |}	0
:[[File:Angle-between-Straight-Lines.png|500px]] Let $\psi_1$ and $\psi_2$ be the [[Definition:Angle|angles]] that $L_1$ and $L_2$ make with the [[Definition:X-Axis|$x$-axis]] respectively. Then from [[Slope of Straight Line is Tangent of Angle with Horizontal]]: {{begin-eqn}} {{eqn | l = \tan \psi_1 | r = m_1 }} {{eqn | l = \tan \psi_2 | r = m_2 }} {{end-eqn}} and so: {{begin-eqn}} {{eqn | l = \tan \psi | r = \map \tan {\psi_2 - \psi_1} | c = }} {{eqn | r = \dfrac {\tan \psi_2 - \tan \psi_1} {1 + \tan \psi_1 \tan \psi_2} | c = [[Tangent of Difference]] }} {{eqn | r = \dfrac {m_2 - m_1} {1 + m_1 m_2} | c = Definition of $m_1$ and $m_2$ }} {{end-eqn}} {{qed}}	0
:[[File:Arccosech.png|600px]]	0
The '''[[Definition:Catenary|catenary]]''' is described by the equation: :$y = \dfrac a 2 \paren {e^{x / a} + e^{-x / a} } = a \cosh \dfrac x a$ where $a$ is a [[Definition:Constant|constant]]. The lowest point of the [[Definition:Chain (Physics)|chain]] is at $\tuple {0, a}$.	0
{{begin-eqn}} {{eqn | l = \int \sin^2 a x \cos^2 a x \rd x | r = \int \paren {\sin a x \cos a x}^2 \rd x }} {{eqn | r = \int \paren {\frac 1 2 \sin 2 a x}^2 \rd x | c = [[Double Angle Formula for Sine]] }} {{eqn | r = \frac 1 4 \int \sin^2 2 a x \rd x }} {{eqn | r = \frac 1 4 \paren {\frac x 2 - \frac {\map \sin {2 x \times 2 a} } {4 \times 2 a} } + C | c = [[Primitive of Square of Sine of a x|Primitive of $\sin^2 a x$]] }} {{eqn | r = \frac x 8 - \frac {\sin 4 a x} {32 a} + C }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \sin b x} a - \frac b a \int e^{a x} \cos b x \rd x | c = [[Primitive of Exponential of a x by Sine of b x/Lemma|Primitive of $e^{a x} \sin b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} \sin b x} a - \frac b a \paren {\frac {e^{a x} \cos b x} a + \frac b a \int e^{a x} \sin b x \rd x} | c = [[Primitive of Exponential of a x by Cosine of b x/Lemma|Primitive of $e^{a x} \cos b x$: Lemma]] }} {{eqn | r = \frac {e^{a x} a \sin b x - e^{a x} b \cos b x} {a^2} - \frac {b^2} {a^2} \int e^{a x} \sin b x \rd x | c = simplifying }} {{eqn | ll= \leadsto | l = \paren {1 + \frac {b^2} {a^2} } \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \left({a \sin b x - b \cos b x}\right)} {a^2} | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {a^2 + b^2} {a^2} \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2} | c = common denominator }} {{eqn | ll= \leadsto | l = \int e^{a x} \sin b x \rd x | r = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2 + b^2} | c = multiplying by $\dfrac {a^2} {a^2 + b^2}$ }} {{end-eqn}} {{qed}}	0
From [[Sine of Complex Number]]: :$\sin \paren {x + i y} = \sin x \cosh y + i \cos x \sinh y$ The result follows by definition of the [[Definition:Imaginary Part|imaginary part]] of a [[Definition:Complex Number|complex number]]. {{qed}}	0
Let $S_n$ be a [[Definition:Sphere (Geometry)|spheres]] of [[Definition:Dimension (Geometry)|$n$ dimensions]] with a given [[Definition:Radius of Sphere|radius]] $r$. Let $\map T n$ denote the number of instances of $S_n$ that can touch one other such instance of $S_n$. The [[Definition:Sequence|sequence]] of $\map T n$ begins as follows: :{| border="1" |- ! align="right" style = "padding: 2px 10px" | $n$ ! align="right" style = "padding: 2px 10px" | $\map T n$ |- | align="right" style = "padding: 2px 10px" | $0$ | align="right" style = "padding: 2px 10px" | $0$ |- | align="right" style = "padding: 2px 10px" | $1$ | align="right" style = "padding: 2px 10px" | $2$ |- | align="right" style = "padding: 2px 10px" | $2$ | align="right" style = "padding: 2px 10px" | $6$ |- | align="right" style = "padding: 2px 10px" | $3$ | align="right" style = "padding: 2px 10px" | $12$ |- | align="right" style = "padding: 2px 10px" | $4$ | align="right" style = "padding: 2px 10px" | $24$ |- | align="right" style = "padding: 2px 10px" | $5$ | align="right" style = "padding: 2px 10px" | $40$ |- | align="right" style = "padding: 2px 10px" | $6$ | align="right" style = "padding: 2px 10px" | $72$ |- | align="right" style = "padding: 2px 10px" | $7$ | align="right" style = "padding: 2px 10px" | $126$ |- | align="right" style = "padding: 2px 10px" | $8$ | align="right" style = "padding: 2px 10px" | $240$ |- | align="right" style = "padding: 2px 10px" | $9$ | align="right" style = "padding: 2px 10px" | $272$ |} {{OEIS|A001116}} </onlyinclude>	0
:$\dfrac {1 + \tan \frac x 2} {1 - \tan \frac x 2} = \sec x + \tan x$	0
Let $m \in \Z$ such that $m > 1$. Then: :$\displaystyle \prod_{k \mathop = 1}^{m - 1} \sin \frac {k \pi} m = \frac m {2^{m - 1} }$	0
Let $\tuple {\lambda_1, \lambda_2} = \tuple {\rho \cos \sigma, \rho \sin \sigma}$. Then: {{begin-eqn}} {{eqn | l = r_\alpha \left({\lambda_1, \lambda_2}\right) | r = \tuple {\rho \cos \alpha \cos \sigma - \rho \sin \alpha \sin \sigma, \rho \sin \alpha \cos \sigma + \rho \cos \alpha \sin \sigma} | c = }} {{eqn | r = \tuple {\lambda_1 \cos \alpha - \lambda_2 \sin \alpha, \lambda_1 \sin \alpha + \lambda_2 \cos \alpha} | c = }} {{end-eqn}} The result follows from [[Linear Operator on the Plane]]. {{ProofWanted|This definition requires to be approached from several conceptual directions.}}	0
'''Volume''' is the measure of the extent of a [[Definition:Body|body]]. It has three [[Definition:Dimension (Geometry)|dimensions]] and is specified in units of [[Definition:Length (Linear Measure)|length]] [[Definition:Cube (Algebra)|cubed]].	0
We have: {{begin-eqn}} {{eqn | l = L \left({\Gamma_1, \rho}\right) | r = \inf_{\gamma \mathop \in \Gamma_1} L \left({\gamma, \rho}\right) | c = by definition }} {{eqn | o = \ge | r = \inf_{\gamma \mathop \in \Gamma_2} L \left({\gamma, \rho}\right) | c = since every curve of $\Gamma_1$ contains a curve of $\Gamma_2$ }} {{eqn | r = L \left({\Gamma_2, \rho}\right) | c = by definition }} {{end-eqn}} This proves the second claim. The second claim implies the first by definition. {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan 15 \degrees | r = \tan \frac {30 \degrees} 2 | c = }} {{eqn | r = \frac {1 - \cos 30 \degrees} {\sin 30 \degrees} | c = [[Half Angle Formulas/Tangent/Corollary 2|Half Angle Formula for Tangent: Corollary 2]] }} {{eqn | r = \frac {1 - \frac {\sqrt 3} 2} {\frac 1 2} | c = [[Cosine of 30 Degrees|Cosine of $30 \degrees$]] and [[Sine of 30 Degrees|Sine of $30 \degrees$]] }} {{eqn | r = 2 - \sqrt 3 | c = multiplying top and bottom by $2$ }} {{end-eqn}} {{qed}}	0
Join $A$ with $B$ and $C$ with $D$, as shown in this diagram: :[[File:Euclid-III-35-2.png|310px]] Then we have: {{begin-eqn}} {{eqn | l = \angle AEB | o = \cong | r = \angle DEC | c = [[Two Straight Lines make Equal Opposite Angles]] }} {{eqn | l = \angle BAE | o = \cong | r = \angle CDE | c = [[Angles in Same Segment of Circle are Equal]] }} {{end-eqn}} By [[Triangles with Two Equal Angles are Similar]] we have $\triangle AEB \sim \triangle DEC$. Thus: {{begin-eqn}} {{eqn | l = \frac {AE} {EB} | r = \frac {DE} {EC} | c = }} {{eqn | ll= \leadsto | l = AE \cdot EC | r = DE \cdot EB | c = }} {{end-eqn}} {{qed}}	0
A [[Definition:Square (Geometry)|square]] [[Definition:Polygon Inscribed in Circle|inscribed]] in a [[Definition:Circle|circle]] has an [[Definition:Area|area]] greater than half that of the [[Definition:Circle|circle]].	0
{{begin-eqn}} {{eqn | l = \tan 180^\circ | r = \frac {\sin 180^\circ} {\cos 180^\circ} | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac 0 {-1} | c = [[Sine of Straight Angle]] and [[Cosine of Straight Angle]] }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed}}	0
By definition of an [[Definition:Ordering|ordering]]: :$0 \preceq 0$ Thus from [[Definition:Naturally Ordered Semigroup Axioms|axiom $(\text {NO} 3)$]]: :$\exists p \in S: 0 \circ p = 0$ By the definition of [[Definition:Zero of Naturally Ordered Semigroup|zero]]: :$0 \preceq 0 \circ 0$ and $0 \preceq p$ Thus since $\preceq$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$: :$0 \circ 0 \preceq 0 \circ p = 0$ Thus: :$0 \circ 0 \preceq 0$ and $0 \preceq 0 \circ 0$ Hence, as $\preceq$ is [[Definition:Antisymmetric Relation|antisymmetric]], it follows that: :$0 \circ 0 = 0$ Because $\struct {S, \circ, \preceq}$ is a [[Definition:Semigroup|semigroup]], $\circ$ is [[Definition:Associative|associative]]. So: :$\forall n \in S: \paren {n \circ 0} \circ 0 = n \circ \paren {0 \circ 0} = n \circ 0$ Thus from [[Definition:Naturally Ordered Semigroup Axioms|axiom $(\text {NO} 2)$]]: :$\forall n \in S: n \circ 0 = n$ Similarly: :$\forall n \in S: 0 \circ \paren {0 \circ n} = \paren {0 \circ 0} \circ n = 0 \circ n$ meaning: :$\forall n \in S: 0 \circ n = n$ Thus: :$\forall n \in S: n \circ 0 = n = 0 \circ n$ and so $0$ is the [[Definition:Identity Element|identity]] for $\circ$. {{Qed}}	0
We are given that $\mathcal R$ is a [[Definition:Congruence Relation|congruence relation]] for $\circ$. From [[Congruence Relation iff Compatible with Operation]], we have: : $\forall u \in G: x \mathop {\mathcal R} y \implies \left({x \circ u}\right) \mathop {\mathcal R} \left({y \circ u}\right), \left({u \circ x}\right)\mathop {\mathcal R} \left({u \circ y}\right)$ ==== Proof of being a Subgroup ==== We show that $H$ is a [[Definition:Subgroup|subgroup]] of $G$. First we note that $H$ is not [[Definition:Empty Set|empty]]: :$e \in H \implies H \ne \varnothing$ Then we show $H$ is closed: {{begin-eqn}} {{eqn | l = x, y | o = \in | r = H | c = }} {{eqn | ll= \implies | l = e | o = \mathcal R | r = x | c = }} {{eqn | lo= \land | l = e | o = \mathcal R | r = y | c = by definition of $H$ }} {{eqn | ll= \implies | l = \left({e \circ e}\right) | o = \mathcal R | r = \left({x \circ y}\right) | c = $\mathcal R$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$ }} {{eqn | ll= \implies | l = x \circ y | o = \in | r = H | c = by definition of $H$ }} {{end-eqn}} Next we show that $x \in H \implies x^{-1} \in H$: {{begin-eqn}} {{eqn | l = x | o = \in | r = H | c = }} {{eqn | ll= \implies | l = e | o = \mathcal R | r = x | c = by definition of $H$ }} {{eqn | ll= \implies | l = \left({x^{-1} \circ e}\right) | o = \mathcal R | r = \left({x^{-1} \circ x}\right) | c = $\mathcal R$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$ }} {{eqn | ll= \implies | l = x^{-1} | o = \mathcal R | r = e | c = [[Definition:Group|Group properties]] }} {{eqn | ll= \implies | l = x^{-1} | o = \in | r = H | c = by definition of $H$ }} {{end-eqn}} Thus by the [[Two-Step Subgroup Test]], $H$ is a [[Definition:Subgroup|subgroup]] of $G$. {{qed|lemma}} ==== Proof of Normality ==== Next we show that $H$ is [[Definition:Normal Subgroup|normal]] in $G$. Thus: {{begin-eqn}} {{eqn | l = x | o = \in | r = H | c = }} {{eqn | ll= \implies | l = e | o = \mathcal R | r = h | c = for some $h \in H$, by definition of $H$ }} {{eqn | ll= \implies | l = \left({x \circ e}\right) | o = \mathcal R | r = \left({x \circ h}\right) | c = $\mathcal R$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$ }} {{eqn | ll= \implies | l = \left({x \circ e \circ x^{-1} }\right) | o = \mathcal R | r = \left({x \circ h \circ x^{-1} }\right) | c = $\mathcal R$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$ }} {{eqn | ll= \implies | l = e | o = \mathcal R | r = \left({x \circ h \circ x^{-1} }\right) | c = [[Definition:Group|Group properties]] }} {{eqn | ll= \implies | l = x \circ h \circ x^{-1} | o = \in | r = H | c = by definition of $H$ }} {{end-eqn}} Thus from [[Subgroup is Normal iff Contains Conjugate Elements]], we have that $H$ is [[Definition:Normal Subgroup|normal]]. {{qed}}	0
Let $a, b, c \in G$ and let $a^{-1}$ be the [[Definition:Inverse Element|inverse]] of $a$. Suppose $b a = c a$. Then: {{begin-eqn}} {{eqn | l = \paren {b a} a^{-1} | r = \paren {c a} a^{-1} | c = }} {{eqn | ll= \leadsto | l = b \paren {a a^{-1} } | r = c \paren {a a^{-1} } | c = {{Defof|Associative Operation}} }} {{eqn | ll= \leadsto | l = b e | r = c e | c = {{Defof|Inverse Element}} }} {{eqn | ll= \leadsto | l = b | r = c | c = {{Defof|Identity Element}} }} {{end-eqn}} Thus, the '''right cancellation law''' holds. The proof of the '''left cancellation law''' is analogous. {{qed}}	0
Let $\left({G, \circ}\right)$ be a [[Definition:Group|group]]. Let $S$ be a [[Definition:Generator of Group|generating set]] for $G$ which is closed under inverses (that is, $x^{-1} \in S \iff x\in S$). Let $d_S$ be the associated [[Definition:Word Metric|word metric]]. Then $d_S$ is a [[Definition:Metric|metric]] on $G$.	0
Recall that [[Integers form Ordered Integral Domain]]. Then from [[Relation Induced by Strict Positivity Property is Compatible with Addition]]: :$\forall x, y, z \in \Z: x \le y \implies \paren {x + z} \le \paren {y + z}$ :$\forall x, y, z \in \Z: x \le y \implies \paren {z + x} \le \paren {z + y}$ So: {{begin-eqn}} {{eqn | l = a | o = \le | r = b | c = }} {{eqn | ll= \leadsto | l = a + c | o = \le | r = b + c | c = [[Relation Induced by Strict Positivity Property is Compatible with Addition]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = c | o = \le | r = d | c = }} {{eqn | ll= \leadsto | l = b + c | o = \le | r = b + d | c = [[Relation Induced by Strict Positivity Property is Compatible with Addition]] }} {{end-eqn}} Finally: {{begin-eqn}} {{eqn | l = a + c | o = \le | r = b + c }} {{eqn | l = b + c | o = \le | r = b + d }} {{eqn | ll= \leadsto | l = a + c | o = \le | r = b + d | c = {{Defof|Ordering}} }} {{end-eqn}} {{qed}}	0
Let $\paren {a N}^n$ be the [[Definition:Identity Element|identity]] of $G / N$. Then: {{begin-eqn}} {{eqn | l = \paren {a N}^n | r = N | c = [[Quotient Group is Group]]: {{GroupAxiom|2}} }} {{eqn | ll= \leadstoandfrom | l = \paren {a^n} N | r = N | c = [[Power of Coset Product is Coset of Power]] }} {{eqn | ll= \leadstoandfrom | l = a^n | o = \in | r = N | c = [[Coset Equals Subgroup iff Element in Subgroup]] }} {{end-eqn}} {{qed}}	0
The operation of [[Definition:Natural Number Multiplication|multiplication]] is [[Definition:Left Distributive Operation|left distributive]] over [[Definition:Natural Number Addition|addition]] on the [[Definition:Set|set]] of [[Definition:Natural Numbers|natural numbers]] $\N$: :$\forall x, y, n \in \N_{> 0}: n \times \paren {x + y} = \paren {n \times x} + \paren {n \times y}$	0
Let $\N$ be the [[Definition:Natural Numbers|natural numbers]]. Then: :$\forall n \in \N: 0 + n = n = n + 0$	0
Consider the [[Definition:Relation|relation]] $\mathcal R \subseteq G \times G$ defined as: :$\forall g, h \in G: \tuple {g, h} \in \mathcal R \iff \exists g \in X$ Then: :$\forall S \subseteq G: X \circ S = \map {\mathcal R} S$ Then: {{begin-eqn}} {{eqn | l = X \circ \paren {Y \cup Z} | r = \map {\mathcal R} {Y \cup Z} | c = }} {{eqn | r = \map {\mathcal R} y \cup \map {\mathcal R} Z | c = [[Image of Union under Relation]] }} {{eqn | r = \paren {X \circ Y} \cup \paren {X \circ Z} | c = }} {{end-eqn}} Next, consider the [[Definition:Relation|relation]] $\mathcal R \subseteq G \times G$ defined as: :$\forall g, h \in G: \tuple {g, h} \in \mathcal R \iff \exists h \in X$ Then: :$\forall S \subseteq G: S \circ X = \map {\mathcal R} S$ Then: {{begin-eqn}} {{eqn | l = \paren {Y \cup Z} \circ X | r = \map {\mathcal R} {Y \cup Z} | c = }} {{eqn | r = \map {\mathcal R} Y \cup \map {\mathcal R} Z | c = [[Image of Union under Relation]] }} {{eqn | r = \paren {Y \circ X} \cup \paren {Z \circ X} | c = }} {{end-eqn}} {{qed}}	0
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative Ring|commutative ring]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\struct {N, +, \circ}$ denote the [[Definition:Nilradical of Ring|nilradical]] of $R$. The [[Definition:Quotient Ring|quotient ring]] $R / N$ is a [[Definition:Reduced Ring|reduced ring]].	0
Let $G$ be a [[Definition:Subgroup|subgroup]] of the [[Definition:Additive Group of Real Numbers|additive group of real numbers]]. Then one of the following holds: :$G$ is [[Definition:Everywhere Dense|dense]] in $\R$. :$G$ is [[Definition:Discrete Subgroup of Real Numbers|discrete]] and there exists $a \in \R$ such that $G = a \Z$, that is, $G$ is [[Definition:Cyclic Group|cyclic]].	0
Let $a < b$ and $b < c$. Thus: {{begin-eqn}} {{eqn | o = | r = a < b, b < c | c = }} {{eqn | o = \leadsto | r = \map P {-a + b}, \map P {-b + c} | c = Definition of $<$ }} {{eqn | o = \leadsto | r = \map P {\paren {-a + b} + \paren {-b + c} } | c = {{Defof|Strict Positivity Property}} }} {{eqn | o = \leadsto | r = \map P {-a + c} | c = Properties of $+$ in $D$ }} {{eqn | o = \leadsto | r = a < c | c = Definition of $<$ }} {{end-eqn}} And so $<$ is seen to be [[Definition:Transitive Relation|transitive]]. {{qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $H$ be a [[Definition:Subset|subset]] of $G$. Then $\struct {H, \circ}$ is a [[Definition:Subgroup|subgroup]] of $\struct {G, \circ}$ {{iff}}: : $(1): \quad H \ne \O$, that is, $H$ is [[Definition:Non-Empty Set|non-empty]] : $(2): \quad \forall a, b \in H: a \circ b^{-1} \in H$.	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Then: :$\struct {G, \circ} \le \struct {G, \circ}$ That is, a [[Definition:Group|group]] is always a [[Definition:Subgroup|subgroup]] of itself.	0
Let $\struct {F, +, \times}$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Unity of Field|unity]] is $1_F$ such that $\Char F = p$. Let $P$ be a [[Definition:Prime Subfield|prime subfield]] of $F$. From [[Field has Prime Subfield]], this has been shown to exist. We can consistently define a [[Definition:Mapping|mapping]] $\phi: \Z_p \to F$ by: :$\forall n \in \Z_p: \map \phi {\eqclass n p} = n \cdot 1_F$ Suppose $a, b \in \eqclass n p$. Then: :$a = n + k_1 p, b = n + k_2 p$ So: {{begin-eqn}} {{eqn | l = \map \phi a | r = \map \phi {n + k_1 p} | c = }} {{eqn | r = \paren {n + k_1 p} \cdot 1_F | c = }} {{eqn | r = n \cdot 1_F + k_1 p \cdot 1_F | c = }} {{eqn | r = n \cdot 1_F | c = }} {{end-eqn}} and similarly for $b$, showing that $\phi$ is [[Definition:Well-Defined Mapping|well-defined]]. Let $C_a, C_b \in \Z_p$. Let $a \in C_a, b \in C_b$ such that $a = a' + k_a p, b = b' + k_b p$. Then: {{begin-eqn}} {{eqn | l = \map \phi {C_a} + \map \phi {C_b} | r = \map \phi {a' + k_a p} + \map \phi {b' + k_b p} | c = }} {{eqn | r = \paren {a' + k_a p} \cdot 1_F + \paren {b' + k_b p} \cdot 1_F | c = }} {{eqn | r = \paren {a' + k_a p + b' + k_b p} \cdot 1_F | c = [[Integral Multiple Distributes over Ring Addition]] }} {{eqn | r = \paren {a' + b'} \cdot 1_F + \paren {\paren {k_a p + k_b p} \cdot 1_F} | c = }} {{eqn | r = \paren {a' + b'} \cdot 1_F | c = }} {{eqn | r = \map \phi {C_a +_p C_b} | c = }} {{end-eqn}} Similarly for $\map \phi {C_a} \times \map \phi {C_b}$. So $\phi$ is a [[Definition:Ring Homomorphism|ring homomorphism]]. From [[Ring Homomorphism from Field is Monomorphism or Zero Homomorphism]], it follows that $\phi$ is a [[Definition:Ring Monomorphism|ring monomorphism]]. Thus it follows that $P = \Img \phi$ is a [[Definition:Subfield|subfield]] of $F$ such that $P \cong \Z_p$. Let $K$ be a [[Definition:Subfield|subfield]] of $F$. let $P = \Img \phi$ as defined above. We know that $1_F \in K$. It follows that $1_F \in K \implies P \subseteq K$. Thus $K$ contains a [[Definition:Subfield|subfield]] $P$ such that $P$ is [[Definition:Field Isomorphism|isomorphic]] to $\Z_p$. The [[Definition:Unique|uniqueness]] of $P$ follows from the fact that if $P_1$ and $P_2$ are both minimal [[Definition:Subfield|subfields]] of $F$, then $P_1 \subseteq P_2$ and $P_2 \subseteq P_1$, thus $P_1 = P_2$. {{qed}}	0
Let $a, b \in \Z$. Let $q, r \in \Z$ such that $a = q b + r$. Then: :$\gcd \set {a, b} = \gcd \set {b, r}$ where $\gcd \set {a, b}$ is the [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] of $a$ and $b$.	0
From [[Bézout's Lemma]] we have: Let $a, b \in \Z$ such that $a$ and $b$ are not both [[Definition:Zero (Number)|zero]]. Let $\gcd \set {a, b}$ be the [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] of $a$ and $b$. Then: :$\exists x, y \in \Z: a x + b y = \gcd \set {a, b}$ Furthermore, $\gcd \set {a, b}$ is the [[Definition:Smallest Element|smallest]] [[Definition:Positive Integer|positive]] [[Definition:Integer Combination|integer combination]] of $a$ and $b$. In this instance, $a, b \in \Z_{>0}$ and are therefore both non-[[Definition:Zero (Number)|zero]]. The result then follows by definition of [[Definition:Greatest Common Divisor of Integers|greatest common divisor]]: $d = \gcd \set {a, b}$ {{iff}}: : $(1): \quad d \divides a \land d \divides b$ : $(2): \quad c \divides a \land c \divides b \implies c \divides d$ {{qed}}	0
Let $x$ be an [[Definition:Even Integer|even integer]]. Let $y = 2 n + 5$. Assume $y = x + 5$ is not an [[Definition:Odd Integer|odd integer]]. Then: :$y = x + 5 = 2 n$ where $n \in \Z$. Then: {{begin-eqn}} {{eqn | l = x | r = 2 n - 5 | c = }} {{eqn | r = \paren {2 n - 6} + 1 | c = }} {{eqn | r = 2 \paren {n - 3} + 1 | c = }} {{eqn | r = 2 r + 1 | c = where $r = n - 3 \in \Z$ }} {{end-eqn}} Hence $x$ is [[Definition:Odd Integer|odd]]. That is, it is [[Definition:False|false]] that $x$ is [[Definition:Even Integer|even]]. It follows by the [[Rule of Transposition]] that if $x$ is [[Definition:Even Integer|even]], then $y$ is [[Definition:Odd Integer|odd]]. {{qed}}	0
From [[Kernel is Trivial iff Monomorphism/Group|Kernel is Trivial iff Monomorphism]], $\phi$ is a [[Definition:Group Monomorphism|monomorphism]] {{iff}} $K = \left\{{e_G}\right\}$. By definition, a [[Definition:Group|group]] $G$ is an [[Definition:Group Epimorphism|epimorphism]] is an [[Definition:Group Isomorphism|isomorphism]] {{iff}} $G$ is also a [[Definition:Group Monomorphism|monomorphism]]. Hence the result. {{qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $X$ be the set of all [[Definition:Subgroup|subgroups]] of $G$. Let $*$ be the [[Definition:Conjugacy Action|conjugacy action]] on $H$: :$\forall g \in G, H \in X: g * H = g \circ H \circ g^{-1}$ Then the [[Definition:Stabilizer|stabilizer]] of $H$ in $\powerset G$ is given by: :$\Stab H = \map {N_G} H$ where $\map {N_G} H$ is the [[Definition:Normalizer|normalizer of $H$ in $G$]].	0
Let $A$ and $B$ be [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $f : A \to B$ be a [[Definition:unital Ring Homomorphism|ring homomorphism]]. Let $\mathfrak a \subseteq A$ be an [[Definition:Ideal of Ring|ideal]]. Then $\mathfrak a$ is [[Definition:Set Containment|contained]] in the [[Definition:Contraction of Ideal|contraction]] of its [[Definition:Extension of Ideal|extension]] by $f$: :$\mathfrak a \subseteq \mathfrak a^{ec}$	0
It is shown that $\struct{G, +_G, \circ’}$ satisfies the [[Definition:Left Module Axioms|left module axioms]] By definition of the [[Definition:Opposite Ring|opposite ring]]: :$\forall x, y \in S: x *_R y = y \times_R x$. === $(M \, 1)$ : Scalar Multiplication (Left) Distributes over Module Addition === Let $\lambda \in R$ and $x, y \in G$. {{begin-eqn}} {{eqn | l = \lambda \circ’ \paren{x +_G y} | r = \paren{x +_G y} \circ \lambda | c = Definition of $\circ’$ }} {{eqn | r = x \circ \lambda +_G y \circ \lambda | c = [[Definition:Right Module Axioms|Right module axiom $(RM \, 1)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = \lambda \circ’ x +_G \lambda \circ’ y | c = Definition of $\circ’$ }} {{end-eqn}} {{qed|lemma}} === $(M \, 2)$ : Scalar Multiplication (Right) Distributes over Scalar Addition === Let $\lambda, \mu \in R$ and $x \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda +_R \mu} \circ’ x | r = x \circ \paren {\lambda +_R \mu} | c = Definition of $\circ’$ }} {{eqn | r = x \circ \lambda +_G x \circ \mu | c = [[Definition:Right Module Axioms|Right module axiom $(RM \, 2)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = \lambda \circ’ x +_G \mu \circ’ x | c = Definition of $\circ’$ }} {{end-eqn}} {{qed|lemma}} === $(M \, 3)$ : Associativity of Scalar Multiplication === Let $\lambda, \mu \in R$ and $x \in G$. {{begin-eqn}} {{eqn | l = \paren {\lambda *_R \mu} \circ’ x | r = x \circ \paren {\lambda *_R \mu} | c = Definition of $\circ’$ }} {{eqn | r = x \circ \paren {\mu \times_R \lambda} | c = Definition of $*_R$ }} {{eqn | r = \paren {x \circ \mu} \circ \lambda | c = [[Definition:Right Module Axioms|Right module axiom $(RM \, 3)$]] on $\struct{G, +_G, \circ}$ }} {{eqn | r = \paren {\mu \circ’ x} \circ \lambda | c = Definition of $\circ’$ }} {{eqn | r = \lambda \circ’ \paren {\mu \circ’ x} | c = Definition of $\circ’$ }} {{end-eqn}} {{qed}}	0
We have [[Set of all Self-Maps is Monoid]]. By [[Inverse of Permutation is Permutation]], if $f$ is a [[Definition:Permutation|permutation]] of $S$, then so is its [[Definition:Inverse of Mapping|inverse]] $f^{-1}$. By [[Bijection iff Inverse is Bijection]], it follows that all the [[Definition:Invertible Element|invertible elements]] of $S^S$ are exactly the [[Definition:Permutation|permutations]] on $S$. The result follows from [[Invertible Elements of Monoid form Subgroup of Cancellable Elements]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = {}^{n - 1} P_n | r = \dfrac {n!} {\paren {n - \paren {n - 1} }!} | c = [[Number of Permutations]] }} {{eqn | r = \dfrac {n!} {1!} | c = }} {{eqn | r = n! | c = }} {{eqn | r = {}^n P_n | c = [[Number of Permutations]] }} {{end-eqn}} {{qed}}	0
Let: {{begin-eqn}} {{eqn | l = m | r = {p_1}^{k_1} {p_2}^{k_2} \dotsm {p_r}^{k_r} }} {{eqn | l = n | r = {p_1}^{l_1} {p_2}^{l_2} \dotsm {p_r}r^{l_r} | c = }} {{end-eqn}} From [[LCM from Prime Decomposition]]: :$\lcm \set {m, n} = p_1^{\max \set {k_1, l_1} } p_2^{\max \set {k_2, l_2} } \dotsm p_r^{\max \set {k_r, l_r} }$ From [[GCD from Prime Decomposition]]: :$\gcd \set {m, n} = p_1^{\min \set {k_1, l_1} } p_2^{\min \set {k_2, l_2} } \dotsm p_r^{\min \set {k_r, l_r} }$ From [[Sum of Maximum and Minimum]], for all $i \in \set {1, 2, \ldots, r}$: :$\min \set {k_i, l_i} + \max \set {k_i, l_i} = k_i + l_i$ Hence: {{begin-eqn}} {{eqn | l = \gcd \set {m, n} \times \lcm \set {m, n} | r = p_1^{k_1 + l_1} p_2^{k_2 + l_2} \dotsm p_r^{k_r + l_r} | c = }} {{eqn | r = p_1^{k_1} p_1^{l_1} p_2^{k_2} p_2^{l_2} \dotsm p_r^{k_r} p_r^{l_r} | c = }} {{eqn | r = p_1^{k_1} p_2^{k_2} \dotsm p_r^{k_r} \times p_1^{l_1} p_2^{l_2} \dotsm p_r^{l_r} | c = }} {{eqn | r = m n | c = }} {{end-eqn}} {{qed}}	0
The [[Definition:Ring (Abstract Algebra)|ring]] $\struct {\set {0_R, 1_R}, +, \circ}$ is the smallest [[Definition:Algebraic Structure|algebraic structure]] which is a [[Definition:Field (Abstract Algebra)|field]].	0
By [[Fourier Series for Absolute Value Function over Symmetric Range]], the [[Definition:Real Function|function]] $f: \openint {-\lambda} \lambda \to \R$ defined as: :$\forall x \in \openint {-\lambda} \lambda: \map f x = \size x$ has a [[Definition:Fourier Series|Fourier series]]: :$\map f x \sim \dfrac \lambda 2 - \displaystyle \dfrac {4 \lambda} {\pi^2} \sum_{n \mathop = 0}^\infty \frac 1 {\paren {2 n + 1}^2} \cos \dfrac {\paren {2 n + 1} \pi x} \lambda$ Substituting for $\lambda = \pi$ gives: :$\size x = \dfrac \pi 2 - \displaystyle \dfrac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\map \cos {2 n - 1} x} {\paren {2 n - 1}^2}$ as required. {{qed}}	0
From [[Positive Real Complex Root of Unity]], we have that $1$ is the only [[Definition:Positive Real Number|positive real number]] in $U_n$. {{AimForCont}} $z \in \R$ such that $z \in U_n$ and $z < 0$. From [[Odd Power of Negative Real Number is Negative]], $z^n < 0$. But this [[Definition:Contradiction|contradicts]] the fact that $z_n = 1 > 0$. Hence by [[Proof by Contradiction]] it follows there is no [[Definition:Negative Real Number|negative real number]] $z$ such that $z^n = 1$. Thus $1$ is the only [[Definition:Real Number|real number]] in $U_n$. {{qed}}	0
{{begin-eqn}} {{eqn | r = \paren {x + \omega y + \omega^2 z} \paren {x + \omega^2 y + \omega z} | o = | c = }} {{eqn | r = x^2 + \omega^2 x y + \omega x z + \omega x y + \omega^3 y^2 + \omega^2 y z + x \omega^2 z + \omega^4 y z + \omega^3 z^2 | c = }} {{eqn | r = x^2 + y^2 + z^2 + \paren {\omega + \omega^2} x y + \paren {\omega + \omega^2} x z + \paren {\omega + \omega^2} y z | c = $\omega^3 = 1$ by definition }} {{eqn | r = x^2 + y^2 + z^2 - x y - x z - y z | c = [[Sum of Cube Roots of Unity]]: $\omega + \omega^2 = -1$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | o = | r = \paren {x + y + z} \paren {x^2 + y^2 + z^2 + - x y - x z - y z} | c = }} {{eqn | r = x^3 + x y^2 + x z^2 - x^2 y - x^2 z - x y z | c = }} {{eqn | o = | ro= + | r = y x^2 + y^3 + y z^2 - x y^2 - x y z - y^2 z | c = }} {{eqn | o = | ro= + | r = z x^2 + z y^2 + z^3 - x y x - x z^2 - y z^2 | c = }} {{eqn | r = x^3 + y^3 + z^3 - 3 x y z | c = }} {{end-eqn}} {{qed}}	0
Let $S$ be a [[Definition:Finite Set|finite set]]. Let $f: S \to S$ be an [[Definition:Injection|injection]]. Then $f$ is a [[Definition:Permutation|permutation]].	0
Let $\struct {\Z'_m, \times_m}$ denote the [[Definition:Multiplicative Group of Reduced Residues|multiplicative group of reduced residues modulo $m$]]. The [[Definition:Order of Group|order]] of $\struct {\Z'_m, \times_m}$ is $\map \phi m$, where $\phi$ denotes the [[Definition:Euler Phi Function|Euler $\phi$ function]].	0
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced by $\norm{\,\cdot\,}$]]. Then $d$ is a [[Definition:Metric|metric]].	0
From [[Solution of Linear Congruence/Existence|Solution of Linear Congruence: Existence]]: :the problem of finding all integers satisfying the [[Definition:Linear Congruence|linear congruence]] $a x \equiv b \pmod n$ is the same problem as: :the problem of finding all the $x$ values in the [[Definition:Linear Diophantine Equation|linear Diophantine equation]] $a x - n y = b$. Let: :$\gcd \set {a, n} = 1$ Let $x = x_0, y = y_0$ be one solution to the [[Definition:Linear Diophantine Equation|linear Diophantine equation]]: :$a x - n y = b$ From [[Solution of Linear Diophantine Equation]], the general solution is: :$\forall k \in \Z: x = x_0 + n k, y = y_0 + a k$ But: :$\forall k \in \Z: x_0 + n k \equiv x_0 \pmod n$ Hence $x \equiv x_0 \pmod n$ is the only solution of $a x \equiv b \pmod n$. {{qed}}	0
By definition: :$\displaystyle n \cdot x := \sum_{j \mathop = 1}^n x$ Thus: {{begin-eqn}} {{eqn | l = \left({n \cdot x} \right) \circ y | r = \left({\sum_{j \mathop = 1}^n x} \right) \circ y | c = by definition of [[Definition:Integral Multiple/Rings and Fields|integral multiple]] }} {{eqn | n = 1 | r = \sum_{j \mathop = 1}^n \left({x \circ y} \right) | c = [[General Distributivity Theorem]] }} {{eqn | r = n \cdot \left({x \circ y}\right) | c = by definition of [[Definition:Integral Multiple/Rings and Fields|integral multiple]] }} {{eqn | r = x \circ \left({\sum_{j \mathop = 1}^n y} \right) | c = [[General Distributivity Theorem]] from $(1)$ }} {{eqn | r = x \circ \left({n \cdot y}\right) | c = by definition of [[Definition:Integral Multiple/Rings and Fields|integral multiple]] }} {{end-eqn}} {{qed}} [[Category:Ring Theory]] g02rj9281x192zcs329rxqfssoa0b8q	0
$\gcd \set {h + 1, h} = \gcd \set {h, 1} = \gcd \set {1, 0} = 1$ from the [[Euclidean Algorithm]]. {{qed}}	0
Let $n \in \Z$. We have: {{begin-eqn}} {{eqn | l = n | r = q k + r | c = }} {{eqn | ll= \leadstoandfrom | l = n - r | r = q k | c = }} {{eqn | ll= \leadstoandfrom | l = k | o = \divides | r = \paren {n - r} | c = }} {{end-eqn}} The result follows from [[Equal Powers of Finite Order Element]]. {{qed}}	0
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $H$ be a [[Definition:Subgroup|subgroup]] of $G$. For clarity of expression, we will use the notation: :$\tuple {x, y} \in \mathcal R^l_H$ for: :$x \equiv^l y \pmod H$ From the definition of [[Definition:Left Congruence Modulo Subgroup|left congruence modulo a subgroup]], we have: :$\mathcal R^l_H = \set {\tuple {x, y} \in G \times G: x^{-1} y \in H}$ We show that $\mathcal R^l_H$ is an [[Definition:Equivalence Relation|equivalence]]: === Reflexive === We have that $H$ is a [[Definition:Subgroup|subgroup]] of $G$. From [[Identity of Subgroup]]: :$e \in H$ Hence: :$\forall x \in G: x x^{-1} = e \in H \implies \tuple {x, x} \in \mathcal R^r_H$ and so $\mathcal R^r_H$ is [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetric === {{begin-eqn}} {{eqn | l = \tuple {x, y} | o = \in | r = \mathcal R^r_H | c = }} {{eqn | ll= \leadsto | l = x y^{-1} | o = \in | r = H | c = {{Defof|Right Congruence Modulo Subgroup|Right Congruence Modulo $H$}} }} {{eqn | ll= \leadsto | l = \tuple {x y^{-1} }^{-1} | o = \in | r = H | c = {{GroupAxiom|0}} }} {{end-eqn}} But then: : $\tuple {x y^{-1} }^{-1} = y x^{-1} \implies \tuple {y, x} \in \mathcal R^r_H$ Thus $\mathcal R^r_H$ is [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitive === {{begin-eqn}} {{eqn | l = \tuple {x, y}, \tuple {y, z} | o = \in | r = \mathcal R^r_H | c = }} {{eqn | ll= \leadsto | l = x y^{-1} | o = \in | r = H | c = {{Defof|Right Congruence Modulo Subgroup|Right Congruence Modulo $H$}} }} {{eqn | lo= \land | l = y z^{-1} | o = \in | r = H | c = {{Defof|Right Congruence Modulo Subgroup|Right Congruence Modulo $H$}} }} {{eqn | ll= \leadsto | l = \tuple {x y^{-1} } \tuple {y z^{-1} } = x z^{-1} | o = \in | r = H | c = [[Definition:Group|Group Properties]] }} {{eqn | ll= \leadsto | l = \tuple {x, z} | o = \in | r = R^r_H | c = {{Defof|Right Congruence Modulo Subgroup|Right Congruence Modulo $H$}} }} {{end-eqn}} Thus $\mathcal R^r_H$ is [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} So $\mathcal R^r_H$ is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	0
By definition of the [[Definition:Imaginary Unit|imaginary unit]] $i$: {{begin-eqn}} {{eqn | l = i^2 | r = -1 }} {{eqn | l = i^3 | r = -i }} {{eqn | l = i^4 | r = 1 }} {{end-eqn}} thus demonstrating that $U_\C$ is [[Definition:Generator of Cyclic Group|generated]] by $i$. Thus $\left({U_\C, \times}\right)$ is by definition a [[Definition:Cyclic Group|cyclic group]] of [[Definition:Order of Structure|order $4$]]. {{qed}}	0
Note that $\size a = \pm a$. Suppose that: :$u \divides a$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Then: :$\exists q \in \Z: a = q u$ Then: :$\size a = \pm q u = \paren {\pm q} u \implies u \divides \size a$ So every [[Definition:Divisor of Integer|divisor]] of $a$ is a [[Definition:Divisor of Integer|divisor]] of $\size a$. Similarly, note that: :$a = \pm \size a$ so every [[Definition:Divisor of Integer|divisor]] of $\size a$ is a [[Definition:Divisor of Integer|divisor]] of $a$. So it follows that the [[Definition:Common Divisor of Integers|common divisors]] of $a$ and $b$ are the same as those of $a$ and $\size b$, and so on. In particular: :$\gcd \set {a, b} = \gcd \set {a, \size b}$ and so on. {{Qed}} [[Category:Greatest Common Divisor]] le9bxzi7pir8rzelwcgd7h3vqlvza11	0
We prove the [[Definition:Contrapositive Statement|contrapositive]]: that a [[Definition:Galois Field|Galois field]] cannot be [[Definition:Algebraically Closed Field|algebraically closed]]. Let $F$ be [[Definition:Galois Field|Galois]]. Define the [[Definition:Polynomial (Abstract Algebra)|polynomial]]: :$\displaystyle \map f x = 1 + \prod_{a \mathop \in F} \paren {x - a}$ By definition, a [[Definition:Field (Abstract Algebra)|field]] is a [[Definition:Ring (Abstract Algebra)|ring]]. Thus by [[Ring Product with Zero]]: :$\displaystyle \forall x \in F: \prod_{a \mathop \in F} \paren {x - a} = 0$ But: :$\forall a \in F: \map f a = 1 + 0 \ne 0$ Therefore $\map f x$ cannot have a [[Definition:Root of Polynomial|root]] in $F$. Thus $F$ is not [[Definition:Algebraically Closed Field|algebraically closed]]. The result follows by the [[Rule of Transposition]]. {{qed}} [[Category:Field Theory]] dxwe85ej6e1gfs3xfgm4kzqok4lswql	0
By definition of [[Definition:Generated Subgroup|generated subgroup]], $\gen G$ is the [[Definition:Smallest Set by Set Inclusion|smallest]] [[Definition:Subgroup|subgroup]] of $G$ containing $G$. Hence the result by [[Group is Subgroup of Itself]]. {{qed}}	0
We have that $A \subseteq \R$. By definition, $\struct {A, \tau_d}$ is a [[Definition:Topological Subspace|subspace]] of $\struct {\R, \tau_d}$. Hence the result from [[Topological Subspace is Topological Space]]. {{qed}}	0
Let the [[Definition:Natural Number|(natural) number]] $AB$ be an [[Definition:Aliquant Part|aliquant part]] of the [[Definition:Natural Number|(natural) number]] $C$. Let the [[Definition:Natural Number|(natural) number]] $DE$ be the same [[Definition:Aliquant Part|aliquant part]] of another [[Definition:Natural Number|(natural) number]] $F$ that $AB$ is of $C$. We need to show that $AB + DE$ is the same [[Definition:Aliquant Part|aliquant part]] of $C + F$. :[[File:Euclid-VII-6.png|250px]] We have that whatever [[Definition:Aliquant Part|aliquant part]] $AB$ is of $C$, $DE$ is also the same [[Definition:Aliquant Part|aliquant part]] as $F$. It follows that as many [[Definition:Aliquant Part|aliquant parts]] of $C$ as there are in $AB$, so many [[Definition:Aliquant Part|aliquant parts]] of $F$ are there also in $DE$. Let $AB$ be divided into the [[Definition:Aliquant Part|aliquant parts]] of $C$, namely $AG, GB$, and $DE$ into the [[Definition:Aliquant Part|aliquant parts]] of $F$, namely $DH, HE$. Thus the multitude of $AG, GB$ will be equal to the multitude of $DH, HE$. We have that whatever [[Definition:Aliquot Part|aliquot part]] $AG$ is of $C$, the same [[Definition:Aliquot Part|aliquot part]] is $DH$ of $F$ also. Therefore, from {{EuclidPropLink|book = VII|prop = 5|title = Divisors obey Distributive Law}}, whatever [[Definition:Aliquot Part|aliquot part]] $AG$ is of $C$, the same [[Definition:Aliquot Part|aliquot part]] also is $AG + DH$ of $C + F$ also. For the same reason, whatever [[Definition:Aliquot Part|aliquot part]] $GB$ is of $C$, the same [[Definition:Aliquot Part|aliquot part]] also is $GB + HE$ of $C + F$. Therefore whatever [[Definition:Aliquant Part|aliquant part]] $AB$ is of $C$, the same [[Definition:Aliquant Part|aliquant part]] also is $AB + DE$ of the $C + F$ {{qed}}	0
Define $f: \openint 0 1 \to \R$ by: :$\map f x := \map \ln {\dfrac {1 - x} x}$ Let us show that $f$ is a [[Definition:Bijection|bijection]] by constructing an [[Definition:Inverse Mapping|inverse mapping]] $g: \R \to \openint 0 1$: :$\map g z := \dfrac 1 {1 + \exp z}$ === [[Group/Examples/inv x = 1 - x/Lemma 1|Lemma 1]] === {{:Group/Examples/inv x = 1 - x/Lemma 1}} === [[Group/Examples/inv x = 1 - x/Lemma 2|Lemma 2]] === {{:Group/Examples/inv x = 1 - x/Lemma 2}} {{finish|Might also be necessary to demonstrate the domain of $f$ is the image set of $g$ and vice versa.}} Thus $f$ is a [[Definition:Bijection|bijection]]. Let $\struct {\R, +}$ be the [[Definition:Additive Group of Real Numbers|additive group on $\R$]]. Now define $\circ := +_f$ to be the [[Definition:Operation Induced by Injection|operation induced on $\openint 0 1$ by $f$ and $+$]]: :$x \circ y := \map {f^{-1} } {\map f x + \map f y}$ Let us determine the behaviour of $\circ$ more explicitly: {{begin-eqn}} {{eqn | l = x \circ y | r = \map g {\map f x + \map f y} }} {{eqn | r = \frac 1 {1 + \map \exp {\map \log {\frac {1 - x} x} + \map \log {\frac {1 - y} y} } } | c = }} {{eqn | r = \frac 1 {1 + \map \exp {\map \log {\frac {1 - x} x} } \map \exp {\map \log {\frac {1 - y} y} } } | c = [[Exponential of Sum]] }} {{eqn | r = \frac 1 {1 + \paren {\frac {1 - x} x} \paren {\frac {1 - y} y} } | c = [[Exponential of Natural Logarithm]] }} {{end-eqn}} {{MissingLinks}} We see that $\circ$ is [[Definition:Commutative Operation|commutative]]. Let $x = \dfrac 1 2$, $\dfrac {1 - x} x = 1$ so that: {{begin-eqn}} {{eqn | l = \dfrac 1 2 \circ y | r = \dfrac 1 {1 + \paren {\frac {1 - y} y} } }} {{eqn | r = \frac 1 {\frac y y + \frac {1 - y} y} }} {{eqn | r = \frac 1 {\frac 1 y} }} {{eqn | r = y }} {{end-eqn}} so that $\dfrac 1 2$ is the [[Definition:Identity Element|identity element]] for $\circ$. Furthermore, putting $y = 1 - x$, the following obtains: {{begin-eqn}} {{eqn | l = \frac 1 {1 + \paren {\frac {1 - x} x} \paren {\frac x {1 - x} } } | r = \frac 1 {1 + 1} }} {{eqn | r = \frac 1 2 }} {{end-eqn}} establishing $1 - x$ to be the [[Definition:Inverse Element|inverse]] of $x$, as desired. That $\circ$ in fact determines a [[Definition:Group|group]] on $\openint 0 1$ follows from [[Pullback of Group is Group]]. {{qed}}	0
Let $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ denote the [[Cyclotomic Ring/Examples/5th|$5$th cyclotomic ring]]. Then $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ is not a [[Definition:Unique Factorization Domain|unique factorization domain]]. The following [[Definition:Element|elements]] of $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ are [[Definition:Irreducible Element of Ring|irreducible]]: :$2$ :$3$ :$1 + i \sqrt 5$ :$1 - i \sqrt 5$	0
From [[Power Set is Closed under Symmetric Difference]], we have that $\struct {\powerset S, *}$ is [[Definition:Closed Algebraic Structure|closed]]. The result follows directly from [[Set System Closed under Symmetric Difference is Abelian Group]]. {{Qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $H_1, H_2 \le G$. Then $G$ is the [[Definition:Internal Group Direct Product|internal group direct product]] of $H_1$ and $H_2$ {{iff}}: :$(1): \quad \forall h_1 \in H_1, h_2 \in H_2: h_1 \circ h_2 = h_2 \circ h_1$ :$(2): \quad G = H_1 \circ H_2$ :$(3): \quad H_1 \cap H_2 = \set e$ Condition $(1)$ can also be stated as: :$(1): \quad$ Either $H_1$ or $H_2$ is [[Definition:Normal Subgroup|normal]] in $G$	0
A direct consequence of [[Burnside's Theorem]]: the smallest number with $3$ [[Definition:Distinct|distinct]] [[Definition:Prime Factor|prime factors]] is $2 \times 3 \times 5 = 60$. {{qed}}	0
Let $\struct {\Q, \tau_d}$ be the [[Definition:Rational Number Space|rational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Then every [[Definition:Singleton|singleton]] [[Definition:Subset|subset]] of $\Q$ is [[Definition:Nowhere Dense|nowhere dense]] in $\struct {\Q, \tau_d}$.	0
Recall that by [[Integers form Ordered Integral Domain]], $\struct {\Z, +, \times, \le}$ is an [[Definition:Ordered Integral Domain|ordered integral domain]] By [[Rational Numbers form Field]], $\struct {\Q, +, \times}$ is a [[Definition:Field (Abstract Algebra)|field]]. In the [[Definition:Rational Number/Formal Definition|formal definition of rational numbers]], $\struct {\Q, +, \times}$ is the [[Definition:Field of Quotients|field of quotients]] of $\struct {\Z, +, \times, \le}$ By [[Total Ordering on Field of Quotients is Unique]], it follows that $\struct {\Q, +, \times}$ has a unique [[Definition:Total Ordering|total ordering]] on it that is [[Definition:Ordering Compatible with Ring Structure|compatible with its ring structure]]. {{explain|Review the ordering / total ordering question}} Thus $\struct {\Q, +, \times, \le}$ is an [[Definition:Ordered Field|ordered field]]. {{Qed}}	0
Let $p \in \N$. Let $p^\ge$ be the [[Definition:Upper Closure of Element|upper closure]] of $p$ in $\N$: :$p^\ge := \set {x \in \N: x \ge p} = \set {p, p + 1, p + 2, \ldots}$ Then there exists [[Definition:Exactly One|exactly one]] [[Definition:Mapping|mapping]] $f: p^\ge \to T$ such that: :$\forall x \in p^\ge: \map f x = \begin{cases} a & : x = p \\ \map g {\map f n} & : x = n + 1 \end{cases}$	0
From [[Euclidean Space is Complete Metric Space]], a [[Definition:Euclidean Space|Euclidean space]] is a [[Definition:Metric Space|metric space]]. The result follows from [[Metric Space is Paracompact]]. {{qed}}	0
By [[Product of Cyclotomic Polynomials]]: :$\displaystyle \prod_{d \mathop \divides n} \map {\Phi_d} x = x^n - 1$ for all $n \in \N$. The nonzero [[Definition:Rational Form|rational forms]] form an [[Definition:Abelian Group|abelian group]] under multiplication. By the [[Möbius Inversion Formula/Abelian Group|Möbius inversion formula for abelian groups]], this implies: :$\displaystyle \map {\Phi_n} x = \prod_{d \mathop \divides n} \paren {x^d - 1}^{\map \mu {n / d} }$ for all $n \in \N$. {{qed}} [[Category:Cyclotomic Polynomials]] ic3t80hx736i6vqfc3xi1j06ekyufsm	0
{{begin-eqn}} {{eqn | l = \paren {-a} + a | r = a + \paren {-a} | c = {{Field-axiom|A2}} }} {{eqn | r = 0_F | c = {{Field-axiom|A4}} }} {{eqn | ll= \leadsto | l = a | r = -\paren {-a} | c = {{Defof|Field Negative}} }} {{end-eqn}} {{qed}}	0
From [[Units of Gaussian Integers]], $U_\C$ is the [[Definition:Set|set]] of [[Definition:Unit of Ring|units]] of the [[Definition:Ring of Gaussian Integers|ring of Gaussian integers]]. From [[Group of Units is Group]], $\left({U_\C, \times}\right)$ forms a [[Definition:Group|group]]. It remains to note that: {{begin-eqn}} {{eqn | l = i^2 | r = -1 }} {{eqn | l = i^3 | r = -i }} {{eqn | l = i^4 | r = 1 }} {{end-eqn}} thus demonstrating that $U_\C$ is [[Definition:Cyclic Group|cyclic]]. {{qed}}	0
Let $\phi: \struct {R_1, +_1, \circ_1} \to \struct {R_2, +_2, \circ_2}$ be a [[Definition:Ring Homomorphism|ring homomorphism]]. Then the [[Definition:Kernel of Ring Homomorphism|kernel]] of $\phi$ is a [[Definition:Subring|subring]] of $R_1$.	0
From [[Preimage of Subring under Ring Homomorphism is Subring]] we have that $S_1 = \phi^{-1} \left[{S_2}\right]$ is a [[Definition:subring|subring]] of $R_1$ such that $\ker \left({\phi}\right) \subseteq S_1$. We now need to show that $S_1$ is an [[Definition:Ideal of Ring|ideal]] of $R_1$. Let $s_1 \in S_1, r_1 \in R_1$. Then: {{begin-eqn}} {{eqn | l = \phi \left({r_1 \circ_1 s_1}\right) | r = \phi \left({r_1}\right) \circ_2 \phi \left({s_1}\right) | c = as $\phi$ is a [[Definition:Ring Homomorphism|homomorphism]] }} {{eqn | r = S_2 | c = as $S_2$ is an [[Definition:Ideal of Ring|ideal]] of $R_2$ }} {{end-eqn}} Thus: :$r_1 \circ_1 s_1 \in \phi^{-1} \left[{S_2}\right]= S_1$ Similarly for $s_1 \circ_1 r_1$. So $S_1$ is an [[Definition:Ideal of Ring|ideal]] of $R_1$. {{qed}}	0
The definition of [[Definition:Extremally Disconnected Space|extremally disconnected space]] can be stated as: :A [[Definition:Hausdorff Space|$T_2$ (Hausdorff) topological space]] $T = \struct {S, \tau}$ is '''extremally disconnected''' {{iff}} the [[Definition:Closure (Topology)|closure]] of every [[Definition:Open Set (Topology)|open set]] of $T$ is [[Definition:Open Set (Topology)|open]]. Hence the existence will be demonstrated of an [[Definition:Open Set (Topology)|open set]] in $\struct {\Q, \tau_d}$ whose [[Definition:Closure (Topology)|closure]] is not [[Definition:Open Set (Topology)|open]]. First we establish that $\struct {\Q, \tau_d}$ is indeed a [[Definition:Hausdorff Space|Hausdorff space]]. Indeed, we have: :[[Rational Numbers form Metric Space]] :[[Metric Space is Hausdorff]] We have that $\openint 0 1$ is [[Definition:Open Set (Topology)|open]] in $\struct {\R, \tau_d}$. Thus $\openint 0 1 \cap \Q$ is [[Definition:Open Set (Topology)|open]] in $\struct {\Q, \tau_d}$. We show that $0$ and $1$ are [[Definition:Limit Point of Set|limit points]] of $\openint 0 1 \cap \Q$. For any $\epsilon \in \R{>0}$: :$\openint {-\epsilon} \epsilon \cap \Q \cap \openint 0 1 = \openint 0 \epsilon \cap \Q$ From [[Between two Real Numbers exists Rational Number]]: :$\openint 0 \epsilon \cap \Q \ne \O$ Similarly: :$\openint {1 - \epsilon} 1 \cap \Q \ne \O$ Thus $0$ and $1$ are [[Definition:Limit Point of Set|limit points]] of $\openint 0 1 \cap \Q$. Hence: :$0, 1 \in \paren {\openint 0 1 \cap \Q}'$ Thus: :$\closedint 0 1 \cap \Q \subseteq \paren {\openint 0 1 \cap \Q}^-$ where $\paren {\openint 0 1 \cap \Q}^-$ denotes the [[Definition:Closure (Topology)|closure]] of $\openint 0 1 \cap \Q$. Now we show that for any $x \notin \closedint 0 1$, $x$ is not a [[Definition:Limit Point of Set|limit point]] of $\openint 0 1 \cap \Q$. Suppose $x < 0$. Then $\dfrac x 2 < 0$. Thus: :$\openint {x + \dfrac x 2} {x - \dfrac x 2} \cap \Q \cap \openint 0 1 = \O$ Similarly for $y > 1$: :$\openint {y - \dfrac {y - 1} 2} {y + \dfrac {y - 1} 2} \cap \Q \cap \openint 0 1 = \O$ Thus: :$\map \complement {\closedint 0 1 \cap \Q} \subseteq \map \complement {\paren {\openint 0 1 \cap \Q}^-}$ Hence we have: :$\closedint 0 1 \cap \Q = \paren {\openint 0 1 \cap \Q}^-$ From [[Between two Real Numbers exists Rational Number]]: :$\openint {-\epsilon} \epsilon \cap \Q \cap \map \complement {\closedint 0 1} = \openint {-\epsilon} 0 \cap \Q \ne \O$ A [[Definition:Neighborhood (Topology)|neighborhood]] of $0$ must [[Definition:Set Intersection|intersect]] $\map \complement {\closedint 0 1}$. So $\closedint 0 1 \cap \Q$ is not [[Definition:Open Set (Topology)|open]] in $\struct {\Q, \tau_d}$. Hence $\openint 0 1$ is that [[Definition:Open Set (Topology)|open set]] in $\struct {\Q, \tau}$ whose [[Definition:Closure (Topology)|closure]] is not [[Definition:Open Set (Topology)|open]] of which we were to demonstrate the existence. Thus the result follows from definition of [[Definition:Extremally Disconnected Space|extremally disconnected space]]. {{qed}} [[Category:Rational Numbers]] [[Category:Extremally Disconnected Spaces]] pj3e3go1uulkw6i5pzacjqxou6r42t6	0
{{:Even Integer Plus 5 is Odd}}	0
By definition $T^*$ is a [[Definition:Connected Topological Space|connected space]] of $T^*$ {{iff}} it admits no [[Definition:Separation (Topology)|separation]]. {{AimForCont}} $T^*$ does admit a [[Definition:Separation (Topology)|separation]]. That is, there exist [[Definition:Open Set (Topology)|open sets]] $A, B \in \tau^*$ such that $A, B \ne \O$, $A \cup B = \Q^*$ and $A \cap B = \O$. That is, both $A$ and $B = \relcomp {\Q^*} A$ are [[Definition:Open Set (Topology)|open]] in $T^*$. {{WLOG}}, Let $p \in A$. Then $B$ is a [[Definition:Closed Set (Topology)|closed]] and [[Definition:Compact Topological Subspace|compact subset]] of $\Q$. From [[Compact Set of Rational Numbers is Nowhere Dense]], $B$ is [[Definition:Nowhere Dense|nowhere dense]]. From [[Set is Closed iff Equals Topological Closure]], $B$ equals its [[Definition:Closure (Topology)|closure]]. Thus by definition of [[Definition:Nowhere Dense|nowhere dense]], the [[Definition:Interior (Topology)|interior]] of $B$ is [[Definition:Empty Set|empty]]. We have that $B$ is [[Definition:Open Set (Topology)|open]] in $T^*$ and $p \notin B$. Thus it follows by definition of [[Definition:Alexandroff Extension|Alexandroff extension]] that $B$ is [[Definition:Open Set (Topology)|open]] in $\struct {\Q, \tau_d}$. But from [[Interior of Open Set]], $B$ equals its [[Definition:Interior (Topology)|interior]]. This [[Definition:Contradiction|contradicts]] the deduction above that the [[Definition:Interior (Topology)|interior]] of $B$ is [[Definition:Empty Set|empty]]. Hence by [[Proof by Contradiction]] $B$ cannot be [[Definition:Open Set (Topology)|open]] in $T^*$. That is, $A$ and $B$ cannot form a [[Definition:Separation (Topology)|separation]] of $T^*$. Thus $T^*$ is a [[Definition:Connected Topological Space|connected space]] by definition. {{qed}}	0
Let $x \in \tilde S$. That is: :$\exists s \in \hat S: x = a s a^{-1}$ Then: {{begin-eqn}} {{eqn | l = x^{-1} | r = \paren {a s a^{-1} }^{-1} | c = }} {{eqn | r = a s^{-1} a^{-1} | c = [[Power of Conjugate equals Conjugate of Power]] }} {{end-eqn}} Since $s^{-1} \in \hat S$, it follows that $x^{-1} \in \tilde S$. {{Qed}} [[Category:Conjugacy]] o56izk8q21jzbzwty9uvh72whozxfbh	0
=== Necessary Condition === Let $\struct {G, \circ}$ be an [[Definition:Abelian Group|abelian group]]. Let $a, b \in G$ be arbitrary. Then: {{begin-eqn}} {{eqn | l = \map \phi {a \circ b} | r = \paren {a \circ b}^n | c = Definition of $\phi$ }} {{eqn | r = a^n \circ b^n | c = [[Power of Product of Commutative Elements in Group]] }} {{eqn | r = \map \phi a \circ \map \phi b | c = Definition of $\phi$ }} {{end-eqn}} As $a$ and $b$ are arbitrary, the above holds for all $a, b \in G$. Thus $\phi$ is a [[Definition:Group Homomorphism|group homomorphism]] from $G$ to $G$. So by definition, $\phi$ is a [[Definition:Group Endomorphism|group endomorphism]]. {{qed|lemma}} === Sufficient Condition === Let $\phi: G \to G$ as defined above be a [[Definition:Group Endomorphism|group endomorphism]]. Then: {{begin-eqn}} {{eqn | lo= \forall a, b \in G: | l = \map \phi {a \circ b} | r = \map \phi a \circ \map \phi b | c = {{Defof|Group Endomorphism}} }} {{eqn | ll= \leadsto | lo= \forall a, b \in G: | l = \paren {a \circ b}^n | r = a^n \circ b^n | c = Definition of $\phi$ }} {{end-eqn}} From [[Power of Product of Commutative Elements in Group]] it follows that $G$ is an [[Definition:Abelian Group|abelian group]]. {{qed}}	0
Let $\struct {S, *}$ be an [[Definition:Algebraic Structure|algebraic structure]]. Let $\Aut S$ be the [[Definition:Automorphism Group of Group|automorphism group]] of $\struct {S, *}$. Then $\Aut S$ is a [[Definition:Subgroup|subgroup]] of the [[Definition:Symmetric Group|symmetric group ]] $\struct {\Gamma \paren S, \circ}$ on $S$.	0
Let $\Q$ denote the [[Definition:Rational Number|set of rational numbers]]. Let $S \subseteq \Q$ denote the [[Definition:Set|set]] of [[Definition:Rational Number|set of rational numbers]] of the form $\dfrac p q$ where $q$ is a [[Definition:Integer Power|power of $2$]]: :$S = \set {\dfrac p q: p \in \Z, q \in \set {2^m: m \in \Z_{\ge 0} } }$ Then $\struct {S, +, \times}$ is an [[Definition:Integral Domain|integral domain]].	0
Let $E$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $G \le \Aut E$ be a [[Definition:Subgroup|subgroup]] of its [[Definition:Automorphism Group of Field|automorphism group]]. Let $F = \map {\operatorname {Fix}_E} G$ be its [[Definition:Fixed Field|fixed field]]. Let $\alpha \in E$ have a [[Definition:Finite Set|finite]] [[Definition:Orbit under Group of Permutations|orbit]] under $G$. Then $\alpha$ is [[Definition:Separable Element|separable]] over $F$.	0
Let $\Q_{\ne 0}$ be the [[Definition:Set|set]] of non-[[Definition:Zero (Number)|zero]] [[Definition:Rational Number|rational numbers]]: :$\Q_{\ne 0} = \Q \setminus \set 0$ The [[Definition:Algebraic Structure|structure]] $\struct {\Q_{\ne 0}, \times}$ is a [[Definition:Countably Infinite Group|countably infinite]] [[Definition:Abelian Group|abelian group]].	0
Let $\order a = k$. Then $a^k = e$, and: : $\forall n \in \N_{>0}: n < k \implies a^n \ne e$ by definition of the [[Definition:Order of Group Element|order of $a$ in $G$]] We have: {{begin-eqn}} {{eqn | l = \paren {x \circ a \circ x^{-1} }^k | r = x \circ a^k \circ x^{-1} | c = [[Power of Conjugate equals Conjugate of Power]] }} {{eqn | r = x \circ e \circ x^{-1} | c = }} {{eqn | r = x \circ x^{-1} | c = }} {{eqn | r = e | c = }} {{end-eqn}} Thus $\order {x \circ a \circ x^{-1} } \le \order a$. Now suppose $a^n = y, y \ne e$. Then: : $x \circ a^n \circ x^{-1} = x \circ y \circ x^{-1}$ If $x \circ y = e$, then: : $x \circ a^n \circ x^{-1} = x^{-1}$ If $y \circ x^{-1} = e$, then: : $x \circ a^n \circ x^{-1} = x$ So: : $a^n \ne e \implies x \circ a^n \circ x^{-1} = \paren {x \circ a \circ x^{-1} }^n \ne e$ Thus: : $\order {x \circ a \circ x^{-1} } \ge \order a$ and the result follows. {{qed}}	0
Let $D$ be the [[Definition:Subset|subset]] of $\Z_{>0}$ defined as: :$D = \set {a s + b t: s, t \in \Z, a s + b t > 0}$ Setting $s = 1$ and $t = 0$ it is clear that $a = \paren {a \times 1 + b \times 0} \in D$. So $D \ne \O$. By [[Set of Integers Bounded Below by Integer has Smallest Element]], $D$ has a [[Definition:Smallest Element|smallest element]] $d$, say. Thus $d = a s + b t$ for some $s, t \in \Z$. === Proof of $(1)$ === From the [[Division Theorem]] we can write $a = q d + r$ for some $q, r$ with $0 \le r < d$. If $r \ne 0$ we have: {{begin-eqn}} {{eqn | l = r | r = a - q d | c = }} {{eqn | r = a - q \paren {a s + b t} | c = }} {{eqn | r = a \paren {1 - q s} + b \paren {-q t} | c = }} {{end-eqn}} So by definition of $D$, it is clear that $r \in D$. But $r < d$ which contradicts the stipulation that $d$ is the [[Definition:Smallest Element|smallest element]] of $D$. Thus $r = 0$ and so $a = q d$. That is $d \divides a$. By the same argument, it follows that $d \divides b$ also. {{qed|lemma}} === Proof of $(2)$ === Let $c \divides a$ and $c \divides b$. Then: : $\exists u, v \in \Z: a = c u, b = c v$ Thus: {{begin-eqn}} {{eqn | l = d | r = a s + b t | c = }} {{eqn | r = c u s + c v t | c = }} {{eqn | r = c \paren {u s + v t} | c = }} {{end-eqn}} demonstrating that $c \divides d$. {{qed}}	0
Let $a$ ''not'' be [[Definition:Coprime Integers|coprime]] to $m$. Then it is not necessarily the case that: :$x \equiv y \pmod m$	0
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $N$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$. Let $a \in G$. Then: :$\paren {a N}^n$ is the [[Definition:Identity Element|identity]] of the [[Definition:Quotient Group|quotient group]] $G / N$ {{iff}}: :$a^n \in N$	0
In order to invoke the [[First Isomorphism Theorem for Groups]], we must construct a [[Definition:Group Homomorphism|group homomorphism]] $\phi: \map {N_G} H \to \Aut H$. Consider the mapping $\phi: x \mapsto \paren {g \mapsto x g x^{-1}}$. From [[Inner Automorphism is Automorphism]], $g \mapsto x g x^{-1}$ is an [[Definition:Group Automorphism|automorphism]] of $G$, so $\phi$ is well-defined. To see that $\phi$ is a [[Definition:Group Homomorphism|homomorphism]], notice that for any $x, y \in \map {N_G} H$: {{begin-eqn}} {{eqn | l = \map \phi x \map \phi y | r = \paren {g \mapsto x g x^{-1} } \circ \paren {g \mapsto y g y^{-1} } | c = where $\circ$ denote [[Definition:Composition of Mappings|composition of maps]] }} {{eqn | r = g \mapsto x \paren {y g y^{-1} } x^{-1} }} {{eqn | r = g \mapsto \paren {x y} g \paren {x y}^{-1} | c = [[Inverse of Group Product]] }} {{eqn | r = \map \phi {x y} }} {{end-eqn}} Hence $\phi$ is a [[Definition:Group Homomorphism|homomorphism]]. Now we prove that $\ker \phi = \map {C_G} H$. Note that for $x \in \map {N_G} H$: {{begin-eqn}} {{eqn | l = x | o = \in | r = \ker \phi }} {{eqn | ll = \leadstoandfrom | l = g | r = x g x^{-1} | rr = \forall g \in H | c = $g \mapsto g$ is the [[Definition:Identity Element of Group|identity]] of $\Aut H$ }} {{eqn | ll = \leadstoandfrom | l = g x | r = x g | rr = \forall g \in H }} {{eqn | ll = \leadstoandfrom | l = x | o = \in | r = \map {C_G} H | c = {{Defof|Centralizer of Subgroup}} }} {{end-eqn}} Hence $\ker \phi = \map {C_G} H$. By [[Kernel is Normal Subgroup of Domain]]: :$\map {C_G} H \lhd \map {N_G} H$ By [[First Isomorphism Theorem for Groups]]: :$\map {N_G} H / \map {C_G} H \cong \Img \phi$ By [[Image of Group Homomorphism is Subgroup]]: :$\Img \phi \le \Aut H$ Hence the result. {{qed}}	0
Let the [[Definition:Mapping|mapping]] $\psi: \N \to$ the set of all ideals of $\Z$ be defined as: :$\forall b \in \N: \psi \left({b}\right) = \left({b}\right)$ where $\left({b}\right)$ is the [[Definition:Principal Ideal of Ring|principal ideal]] of $\Z$ generated by $b$. Then $\psi$ is a [[Definition:Bijection|bijection]].	0
Let $m \in \Z_{> 0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\Z'_m$ be the [[Definition:Reduced Residue System|reduced residue system modulo $m$]]: :$\Z'_m = \set {\eqclass k m \in \Z_m: k \perp m}$ Let $S = \struct {\Z'_m, \times_m}$ be the [[Definition:Algebraic Structure|algebraic structure]] consisting of $\Z'_m$ under the [[Definition:Modulo Multiplication|modulo multiplication]]. Then $S$ is [[Definition:Closed Algebraic Structure|closed]], in the sense that: :$\forall a, b \in \Z'_m: a \times_m b \in \Z'_m$	0
Let $\struct {\Q, \tau_d}$ be the [[Definition:Rational Number Space|rational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Let $p$ be a [[Definition:New Element|new element]] not in $\Q$. Let $\Q^* := \Q \cup \set p$. Let $T^* = \struct {\Q^*, \tau^*}$ be the [[Definition:Alexandroff Extension|Alexandroff extension]] on $\struct {\Q, \tau_d}$. Then $T^*$ is a [[Definition:Connected Topological Space|connected space]].	0
{{AimForCont}} $A$ is [[Definition:Finite Class|finite]]. From [[Subset of Naturals is Finite iff Bounded]], it follows that $A$ is [[Definition:Bounded Class|bounded]]. Hence $A$ has a [[Definition:Greatest Element|greatest element]]. This [[Definition:Contradiction|contradicts]] the fact that $A$ has no [[Definition:Greatest Element|greatest element]]. Hence by [[Proof by Contradiction]] it follows that $A$ is not [[Definition:Finite Class|finite]]. From [[Set of Natural Numbers is either Finite or Denumerable]], $A$ is [[Definition:Denumerable Class|denumerable]]. {{Qed}}	0
Let $\epsilon > 0$. Let $\delta = \epsilon$. From the definition of a [[Definition:Limit of Real Function|limit of a function]], we need to show that $\left\vert{f \left({x}\right) - 0}\right\vert < \epsilon$ provided that $0 < \left\vert{x - \xi}\right\vert < \delta$, where $f \left({x}\right) = \left\vert{x - \xi}\right\vert$. Thus, provided $0 < \left\vert{x - \xi}\right\vert < \delta$, we have: {{begin-eqn}} {{eqn | l=\left\vert{x - \xi}\right\vert - 0 | r=\left\vert{x - \xi}\right\vert | c= }} {{eqn | o=< | r=\delta | c= }} {{eqn | r=\epsilon | c= }} {{end-eqn}} Hence the result. {{qed}}	0
From the definition of [[Definition:Factorial|factorial]]: :$n! = 1 \times 2 \times \cdots \times \left({n-1}\right) \times n$ Thus every number less than $n$ appears as a [[Definition:Divisor of Integer|divisor]] of $n!$. The result follows from definition of [[Definition:Congruence (Number Theory)/Integers/Integer Multiple|congruence]]. {{qed}} [[Category:Factorials]] [[Category:Divisors]] 3p1tjojrrsp4gejtpfuz94uxurucs8o	0
Let $S$ be a [[Definition:Set|set]] and let $\powerset S$ be its [[Definition:Power Set|power set]]. Consider the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\powerset S, \cap}$, where $\cap$ denotes [[Definition:Set Intersection|set intersection]]. Then $S$ serves as the [[Definition:Identity Element|identity]] for $\struct {\powerset S, \cap}$.	0
Let $g \in G$. Then: {{begin-eqn}} {{eqn | l = g | o = \in | r = G | c = }} {{eqn | ll= \leadsto | l = e | r = g^{-1} \circ g | c = {{Defof|Inverse Element}} }} {{eqn | ll= \leadsto | l = \paren {g^{-1} }^{-1} \circ e | r = \paren {g^{-1} }^{-1} \circ \paren {g^{-1} \circ g} | c = }} {{eqn | ll= \leadsto | l = \paren {g^{-1} }^{-1} \circ e | r = \paren {\paren {g^{-1} }^{-1} \circ g^{-1} } \circ g | c = {{Defof|Associative Operation}} }} {{eqn | ll= \leadsto | l = \paren {g^{-1} }^{-1} \circ e | r = e \circ g | c = {{Defof|Inverse Element}} }} {{eqn | ll= \leadsto | l = \paren {g^{-1} }^{-1} | r = g | c = {{Defof|Identity Element}} }} {{end-eqn}} {{qed}}	0
We have by definition that a [[Definition:Field (Abstract Algebra)|field]] is a [[Definition:Division Ring|division ring]]. The result can be seen to be an application of [[Ring Homomorphism from Division Ring is Monomorphism or Zero Homomorphism]]. {{qed}}	0
The [[Conjugacy Classes of Symmetric Group]] are determined uniquely by the [[Definition:Cycle Type|cycle type]] of the [[Definition:Element|elements]]. Since any element in $S_n$ is of the same [[Definition:Cycle Type|cycle type]] with its [[Definition:Inverse Element|inverse]], they are in the same [[Definition:Conjugacy Class|conjugacy class]]. Hence they are [[Definition:Conjugate of Group Element|conjugates]] of each other. This implies that $S_n$ is [[Definition:Ambivalent Group|ambivalent]]. {{qed}} [[Category:Ambivalent Groups]] 8dh89ilrc2bqnqj6sgtu3z57lhhvwpt	0
From [[Isomorphism between Ring of Integers Modulo 2 and Parity Ring]]: :$\struct {\set {\text {even}, \text {odd} }, +, \times}$ is [[Definition:Ring Isomorphism|isomorphic]] with $\struct {\Z_2, +_2, \times_2}$ the [[Definition:Ring of Integers Modulo m|ring of integers modulo $2$]]. The result follows from: :[[Modulo Addition is Associative]] and: :[[Isomorphism Preserves Associativity]]. {{qed}}	0
Let $m, n \in \Z$. Let $m \Z$ denote the [[Definition:Set of Integer Multiples|set of integer multiples of $m$]] Then: :$m \Z \cup n \Z \subseteq \gcd \set {m, n} \Z$ where $\gcd$ denotes [[Definition:Greatest Common Divisor of Integers|greatest common divisor]].	0
Let $\left({G, \circ}\right)$ be a [[Definition:group|group]]. Let $S \subseteq G$. Let $S$ be a [[Definition:Normal Subset/Definition 2|normal subset of $G$ by Definition 2]]. Then $S$ is a [[Definition:Normal Subset/Definition 3|normal subset of $G$ by Definition 3]]. That is, if: :$\forall g \in G: g \circ S \circ g^{-1} = S$ or: :$\forall g \in G: g^{-1} \circ S \circ g = S$ then: :$\forall g \in G: g \circ S \circ g^{-1} \subseteq S$ and: :$\forall g \in G: g^{-1} \circ S \circ g \subseteq S$	0
Let $\struct {R, \norm {\,\cdot\,}}$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence in $R$]]. Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent in the norm]] $\norm {\,\cdot\,}$ to the following [[Definition:Limit of Sequence (Metric Space)|limit]]: :$\displaystyle \lim_{n \mathop \to \infty} x_n = l$ Then $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]].	0
Let $a$ and $b$ be [[Definition:Integer|integers]] chosen at random. The [[Definition:Probability|probability]] that $a$ and $b$ are [[Definition:Coprime Integers|coprime]] is given by: :$\map \Pr {a \perp b} = \dfrac 1 {\map \zeta 2} = \dfrac 6 {\pi^2}$ where $\zeta$ denotes the [[Definition:Riemann Zeta Function|zeta function]]. The [[Definition:Decimal Expansion|decimal expansion]] of $\dfrac 1 {\map \zeta 2}$ starts: :$\dfrac 1 {\map \zeta 2} = 0 \cdotp 60792 \, 71018 \, 54026 \, 6 \ldots$ {{OEIS|A059956}}	0
Let $L / K$ be a [[Definition:Field Extension|field extension]]. Let $\alpha \in L$ be [[Definition:Algebraic over Field|algebraic]] over $K$. Then the [[Definition:Minimal Polynomial|minimal polynomial]] in $\alpha$ over $K$ is [[Definition:Unique|unique]] and [[Definition:Irreducible Polynomial|irreducible]].	0
From [[Quotient Structure of Monoid is Monoid]] $\struct {G / \RR, \circ_\RR}$ is a [[Definition:Monoid|monoid]] with $\eqclass e \RR$ as its [[Definition:Identity Element|identity]]. Let $\eqclass x \RR \in S / \RR$. Consider $\eqclass {-x} \RR$ where $-x$ denotes the [[Definition:Inverse Element|inverse]] of $x$ under $\circ$. {{begin-eqn}} {{eqn | l = \eqclass x \RR \circ_{S / \RR} \eqclass {-x} \RR | r = \eqclass {x \circ -x} \RR | c = {{Defof|Operation Induced on Quotient Set|Operation Induced on $S / \RR$ by $\circ$}} }} {{eqn | r = \eqclass e \RR | c = {{Defof|Inverse Element}} }} {{end-eqn}} Furthermore: {{begin-eqn}} {{eqn | l = \eqclass {-x} \RR \circ_{S / \RR} \eqclass x \RR | r = \eqclass {-x \circ x} \RR | c = {{Defof|Operation Induced on Quotient Set|Operation Induced on $S / \RR$ by $\circ$}} }} {{eqn | r = \eqclass e \RR | c = {{Defof|Inverse Element}} }} {{end-eqn}} Hence $\eqclass {-x} \RR$ is the [[Definition:Inverse Element|inverse]] of $\eqclass x \RR$. Hence $\struct {G / \RR, \circ_\RR}$ is a [[Definition:Group|group]]. {{qed}}	0
Let $\struct {G \times H, \circ}$ be the [[Definition:Group Direct Product|group direct product]] of the two [[Definition:Group|groups]] $\struct {G, \circ_1}$ and $\struct {H, \circ_2}$. Let $g^{-1}$ be an [[Definition:Inverse Element|inverse]] of $g \in \struct {G, \circ_1}$. Let $h^{-1}$ be an [[Definition:Inverse Element|inverse]] of $h \in \struct {H, \circ_2}$. Then $\tuple {g^{-1}, h^{-1} }$ is the [[Definition:Inverse Element|inverse]] of $\tuple {g, h} \in \struct {G \times H, \circ}$.	0
A [[Definition:Semiprime Number|semiprime]] with [[Definition:Distinct|distinct]] [[Definition:Prime Factor|prime factors]] is a [[Definition:Square-Free|square-free]] [[Definition:Integer|integer]]. By [[Sigma Function of Square-Free Integer]]: :$\ds \map \sigma n = \prod_{1 \mathop \le i \mathop \le r} p_i + 1$ Hence the result. {{qed}}	0
Let $\left({A, +, \circ}\right)$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $I$ be an [[Definition:Ideal of Ring|ideal]] of $A$ such that the [[Definition:Quotient Ring|quotient ring]] $A / I$ is a [[Definition:Field (Abstract Algebra)|field]]. Let $J$ be an [[Definition:Ideal of Ring|ideal]] of $A$ such that $I \subsetneq J$. Then: :$A = J$	0
[[Proof by Counterexample]]: Consider the [[Definition:Algebraic Structure|algebraic structure]] $\left({S, \circ}\right)$ consisting of: :The [[Definition:Set|set]] $S = \left\{{a, b, e}\right\}$ :The [[Definition:Binary Operation|binary operation]] $\circ$ whose [[Definition:Cayley Table|Cayley table]] is given as follows: :$\begin{array}{c|cccc} \circ & e & a & b \\ \hline e & e & a & b \\ a & a & e & e \\ b & b & e & e \\ \end{array}$ By inspection, we see that $e$ is the [[Definition:Identity Element|identity element]] of $\left({S, \circ}\right)$. We also note that: {{begin-eqn}} {{eqn | l = \left({a \circ a}\right) \circ b | r = e \circ b | c = }} {{eqn | r = b | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = a \circ \left({a \circ b}\right) | r = a \circ e | c = }} {{eqn | r = a | c = }} {{end-eqn}} and so $\circ$ is not [[Definition:Associative|associative]]. Note further that: {{begin-eqn}} {{eqn | l = a \circ b | r = e | c = }} {{eqn | r = b \circ a | c = }} {{end-eqn}} and also: {{begin-eqn}} {{eqn | l = a \circ a | r = e | c = }} {{end-eqn}} So both $a$ and $b$ are [[Definition:Inverse Element|inverses]] of $a$. Hence the result. {{qed}}	0
Let $\struct {\N, +}$ be the [[Natural Numbers under Addition form Commutative Semigroup|semigroup of natural numbers under addition]]. Let $\struct {\N \times \N, \oplus}$ be the [[Definition:External Direct Product|(external) direct product]] of $\struct {\N, +}$ with itself, where $\oplus$ is the [[Definition:Operation Induced by Direct Product|operation on $\N \times \N$ induced by $+$ on $\N$]]. The [[Definition:Relation|relation]] $\boxtimes$ defined on $\N \times \N$ by: :$\tuple {x_1, y_1} \boxtimes \tuple {x_2, y_2} \iff x_1 + y_2 = x_2 + y_1$ is an [[Definition:Equivalence Relation|equivalence relation]] on $\struct {\N \times \N, \oplus}$.	0
From [[Ring of Quaternions is Ring]] we have that $\H$ forms a [[Definition:Ring (Abstract Algebra)|ring]]. From [[Multiplicative Identity for Quaternions]] we have that $\mathbf 1$ is the [[Definition:Identity Element|identity]] for [[Definition:Quaternion Multiplication|quaternion multiplication]]. From [[Multiplicative Inverse of Quaternion]] we have that every element of $\H$ except $\mathbf 0$ has an [[Definition:Inverse Element|inverse]] under [[Definition:Quaternion Multiplication|quaternion multiplication]]. So $\H \setminus \set 0 = \H^*$ is a [[Definition:Group|group]]. Hence $\H$ forms a [[Definition:Division Ring|division ring]]. But [[Definition:Quaternion Multiplication|quaternion multiplication]] is specifically not [[Definition:Commutative Operation|commutative]], for example: :$\mathbf i \mathbf j = \mathbf k, \ \mathbf j \mathbf i = -\mathbf k$ So $\H$ forms a [[Definition:Skew Field|skew field]] under [[Definition:Quaternion Addition|addition]] and [[Definition:Quaternion Multiplication|multiplication]]. {{qed}}	0
From [[Additive Group of Rationals is Subgroup of Reals]], $\struct {\Q, +} \lhd \struct {\R, +}$. From [[Additive Group of Reals is Subgroup of Complex]], $\struct {\R, +} \lhd \struct {\C, +}$. Thus $\struct {\Q, +} \le \struct {\C, +}$. From [[Complex Numbers under Addition form Abelian Group]], $\struct {\C, +}$ is [[Definition:Abelian Group|abelian]]. From [[Subgroup of Abelian Group is Normal]], it follows that $\struct {\Q, +} \lhd \struct {\C, +}$. {{qed}} [[Category:Additive Group of Rational Numbers]] [[Category:Additive Group of Complex Numbers]] [[Category:Normal Subgroups]] 4boyza8yljy6ipg9wn24xyz3fvfxgft	0
Suppose $x = b a = c a$. By [[Group has Latin Square Property]], there exists [[Definition:Exactly One|exactly one]] $y \in G$ such that $x = y a$. That is, $x = b a = c a \implies b = c$. Similarly, suppose $x = a b = a c$. Again by [[Group has Latin Square Property]], there exists [[Definition:Exactly One|exactly one]] $y \in G$ such that $x = a y$. That is, $a b = a c \implies b = c$. {{qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $S \subseteq G$ such that: :$\forall x, y \in S: x \circ y = y \circ x$ Then the [[Definition:Generated Subgroup|subgroup generated]] by $S$ is [[Definition:Abelian Group|abelian]].	0
The [[Definition:Klein Four-Group|Klein $4$-group]] $K_4$ is a [[Definition:Group|group]].	0
{{begin-eqn}} {{eqn | lo= \forall x \in S: | l = \map {\paren {\paren {f \times g} \times h} } x | r = \paren {\map f x \times \map g x} \times \map h x | c = {{Defof|Pointwise Multiplication of Rational-Valued Functions}} }} {{eqn | r = \map f x \times \paren {\map g x \times \map h x} | c = [[Rational Multiplication is Associative]] }} {{eqn | r = \map {\paren {f \times \paren {g \times h} } } x | c = {{Defof|Pointwise Multiplication of Rational-Valued Functions}} }} {{end-eqn}} {{qed}} [[Category:Pointwise Multiplication]] [[Category:Rational Multiplication]] [[Category:Associativity]] bufaz948k0tqlqxxyso14zg62al4bih	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $G$ be a [[Definition:Finite Group|finite]] [[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e$. Let $G$ have at least $p$ [[Definition:Element|elements]] of [[Definition:Order of Group Element|order]] $p$. Then: : $p^2 \divides \order G$ where: :$\divides$ denotes [[Definition:Divisor of Integer|divisibility]] :$\order G$ denotes the [[Definition:Order of Structure|order]] of $G$.	0
If $p = 2$ the result is obvious. Therefore we assume that $p$ is an [[Definition:Odd Prime|odd prime]]. Let $p$ be [[Definition:Prime Number|prime]]. Consider $n \in \Z, 1 \le n < p$. As $p$ is [[Definition:Prime Number|prime]], $n \perp p$. From [[Law of Inverses (Modulo Arithmetic)]], we have: :$\exists n' \in \Z, 1 \le n' < p: n n' \equiv 1 \pmod p$ By [[Solution of Linear Congruence]], for each $n$ there is '''exactly one''' such $n'$, and $\paren {n'}' = n$. So, provided $n \ne n'$, we can pair any given $n$ from $1$ to $p$ with another $n'$ from $1$ to $p$. We are then left with the numbers such that $n = n'$. Then we have $n^2 \equiv 1 \pmod p$. Consider $n^2 - 1 = \paren {n + 1} \paren {n - 1}$ from [[Difference of Two Squares]]. So either $n + 1 \divides p$ or $n - 1 \divides p$. Observe that these cases do not occur simultaneously, as their difference is $2$, and $p$ is an [[Definition:Odd Prime|odd prime]]. From [[Negative Number is Congruent to Modulus minus Number]]: :$p - 1 \equiv -1 \pmod p$ Hence $n = 1$ or $n = p - 1$. So, we have that $\paren {p - 1}!$ consists of numbers multiplied together as follows: :in pairs whose product is [[Definition:Congruence Modulo Integer|congruent]] to $1 \pmod p$ :the numbers $1$ and $p - 1$. The product of all these numbers is therefore [[Definition:Congruence Modulo Integer|congruent]] to $1 \times 1 \times \cdots \times 1 \times p - 1 \pmod p$ by [[Definition:Modulo Multiplication|modulo multiplication]]. From [[Negative Number is Congruent to Modulus minus Number]]: :$\paren {p - 1}! \equiv -1 \pmod p$ {{qed}}	0
From [[Order of Symmetric Group]]: :$\order {S_n} = n!$ where: :$\order {S_n}$ denotes the [[Definition:Order of Structure|order]] of $S_n$ :$n!$ denotes the [[Definition:Factorial|factorial]] of $n$. By definition: :$\card {R_e} + \card {R_o} = order {S_n}$ From [[Odd and Even Permutations of Set are Equivalent]]: :$\card {R_e} = \card {R_o}$ The result follows. {{qed}}	0
{{begin-eqn}} {{eqn | l = c | o = \divides | r = a }} {{eqn | ll= \leadsto | lo= \exists x \in \Z: | l = a | r = x c | c = {{Defof|Divisor of Integer}} }} {{eqn | l = c | o = \divides | r = b }} {{eqn | ll= \leadsto | lo= \exists y \in \Z: | l = b | r = y c | c = {{Defof|Divisor of Integer}} }} {{eqn | ll= \leadsto | l = a - b | r = x c - y c | c = substituting for $a$ and $b$ }} {{eqn | r = \paren {x - y} c | c = [[Integer Multiplication Distributes over Addition]] }} {{eqn | ll= \leadsto | lo= \exists z \in \Z: | l = a - b | r = z c | c = where $z = x - y$ }} {{eqn | ll= \leadsto | l = c | o = \divides | r = \paren {a - b} | c = {{Defof|Divisor of Integer}} }} {{end-eqn}} {{qed}}	0
:$1 \cdotp 1 \bmod 1 = 0 \cdotp 1$	0
Let $a$ be an [[Definition:Integer|integer]] chosen at random. The [[Definition:Probability|probability]] that $a$ is [[Definition:Square-Free Integer|square-free]] is given by: :$\map \Pr {\neg \exists b \in \Z: b^2 \divides a} = \dfrac 1 {\map \zeta 2} = \dfrac 6 {\pi^2}$ where $\zeta$ denotes the [[Definition:Riemann Zeta Function|zeta function]]. The [[Definition:Decimal Expansion|decimal expansion]] of $\dfrac 1 {\map \zeta 2}$ starts: :$\dfrac 1 {\map \zeta 2} = 0 \cdotp 60792 \, 71018 \, 54026 \, 6 \ldots$ {{OEIS|A059956}}	0
By definition of [[Definition:Cardinality|cardinality]] and the [[Definition:Trivial Subgroup|trivial subgroup]]: :$\order {\set e} = 1$ From [[Lagrange's Theorem (Group Theory)|Lagrange's Theorem]]: :$\index G {\set e} = \dfrac {\order G} {\order {\set e} } = \dfrac {\order G} 1 = \order G$ {{qed}}	0
Let $x H$ denote the [[Definition:Left Coset|left coset]] of $H$ by $x$. Then: :$x H = y H \iff x^{-1} y \in H$	0
By [[Zero Strictly Precedes One]], $0 \prec 1$, where $0$ is the [[Definition:Zero of Naturally Ordered Semigroup|zero]] of $S$. Hence from [[Strict Ordering of Naturally Ordered Semigroup is Strongly Compatible]]: :$n \circ 0 \prec n \circ 1$ and by [[Zero is Identity in Naturally Ordered Semigroup]], $n \circ 0 = n$. Now suppose that $n \prec m$. Then by [[Definition:Naturally Ordered Semigroup Axioms|axiom $(NO3)$]], there exists $p \in S$ such that: :$n \circ p = m$ Moreover, since $n \ne m$, it follows that $p \ne 0$. Hence $0 \prec p$ by definition of [[Definition:Zero of Naturally Ordered Semigroup|zero]]. Therefore, by definition of [[Definition:One of Naturally Ordered Semigroup|one]]: :$1 \preceq p$ Now by [[Definition:Relation Compatible with Operation|compatibility]] of $\preceq$ with $\circ$: :$n \circ 1 \preceq n \circ p = m$ as desired. Conversely, if $n \circ 1 \preceq m$, it is immediate from: :$n \prec n \circ 1$ that $n \prec m$. {{qed}}	0
From [[Absolute Value Function on Integers induces Equivalence Relation]], $\RR$ is an [[Definition:Equivalence Relation|equivalence relation]]. Let: :$\size {x_1} = \size {x_2}$ :$\size {y_1} = \size {y_2}$ Then by definition of [[Definition:Absolute Value|absolute value]]: {{begin-eqn}} {{eqn | l = \size {x_1 y_1} | r = \size {x_1} \size {y_1} | c = }} {{eqn | r = \size {x_2} \size {y_2} | c = }} {{eqn | r = \size {x_2 y_2} | c = }} {{end-eqn}} That is: :$\paren {x_1 y_1, x_2 y_2} \in \RR$ That is, $\RR$ is a [[Definition:Congruence Relation|congruence relation]] for [[Definition:Integer Multiplication|integer multiplication]]. {{qed}}	0
The [[Definition:Integer|integers]] have no [[Definition:Zero Divisor of Ring|zero divisors]]: :$\forall x, y, \in \Z: x \times y = 0 \implies x = 0 \lor y = 0$	0
A direct application of [[Cardinality of Set of Bijections]]. {{qed}}	0
By the [[Index Laws for Monoids/Sum of Indices|Index Law for Sum of Indices]] and [[Powers of Ring Elements]], we have $\left({n 1_R}\right) \left({m 1_R}\right) = n \left({m 1_R}\right) = \left({n m}\right) 1_R$. Thus $g$ is an [[Definition:Ring Epimorphism|epimorphism]] from $\Z$ onto $S$. Assume that $R$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. By [[Kernel of Ring Epimorphism is Ideal]], the [[Definition:Kernel of Ring Homomorphism|kernel]] of $g$ is an ideal of $\Z$. By [[Ring of Integers is Principal Ideal Domain]], the [[Definition:Kernel of Ring Homomorphism|kernel]] of $g$ is $\left({p}\right)$ for some $p \in \Z_{>0}$. By [[Kernel of Ring Epimorphism is Ideal]] (don't think this is the correct reference - check it), $S$ is [[Definition:Isomorphism (Abstract Algebra)|isomorphic]] to $\Z_p$ and also has no [[Definition:Proper Zero Divisor|proper zero divisors]]. So from [[Integral Domain of Prime Order is Field]] either $p = 0$ or $p$ is [[Definition:Prime Number|prime]]. Now we need to show that $g$ is unique. Let $h$ be a non-zero [[Definition:Ring Homomorphism|(ring) homomorphism]] from $\Z$ into $R$. As $h \left({1}\right) = h \left({1^2}\right) = \left({h \left({1}\right)}\right)^2$, either $h \left({1}\right) = 1_R$ or $h \left({1}\right) = 0_R$ by [[Idempotent Elements of Ring with No Proper Zero Divisors]]. But, by [[Homomorphism of Powers/Integers|Homomorphism of Powers: Integers]], $\forall n \in \Z: h \left({n}\right) = h \left({n 1}\right) = n h \left({1}\right)$ So if $h \left({1}\right) = 0_R$, then $\forall n \in \Z: h \left({n}\right) = n 0_R = 0_R$. Hence $h$ would be a [[Definition:Zero Homomorphism|zero homomorphism]], which contradicts our stipulation that it is not. So $h \left({1}\right) = 1_R$, and thus $\forall n \in \Z: h \left({n}\right) = n 1 = g \left({n}\right)$. {{qed}} {{Proofread}}	0
By [[Compact Subspace of Hausdorff Space is Closed]], $S$ is [[Definition:Closed Set (Topology)|closed]] in $\Q$. By [[Set is Closed iff Equals Topological Closure]], $S = S^-$. {{AimForCont}} $S$ is not [[Definition:Nowhere Dense|nowhere dense]] in $\Q$. Then $S^-$ contains some [[Definition:Non-Empty Set|non-empty]] [[Definition:Open Set (Topology)|open set]]. From [[Basis for Euclidean Topology on Real Number Line]], the set of all [[Definition:Open Real Interval|open real intervals]] of $\R$ form a basis for $\struct {\R, \tau_d}$. By [[Basis for Topological Subspace]], the set of all intersections of $\Q$ and [[Definition:Open Real Interval|open real intervals]] of $\R$ form a basis for $\struct {\Q, \tau_d}$. So there exists $V = \Q \cap \openint {a'} {b'} \subseteq S^- = S$ for some $a' < b'$. By [[Between two Real Numbers exists Irrational Number]] we have: :$\exists r \in \R \setminus \Q: a' < r < b'$ By [[Between two Real Numbers exists Rational Number]] we have: :$\exists a, b \in \Q: a' < a < r < b < b'$ Thus $\Q \cap \closedint a b$ is a [[Definition:Closed Set (Topology)|closed]] [[Definition:Subset|subset]] of $S$ in $\Q$. By [[Closed Subspace of Compact Space is Compact]], $\Q \cap \closedint a b$ is [[Definition:Compact Topological Space|compact]]. Now we construct an [[Definition:Open Cover|open cover]] for $\Q \cap \closedint a b$: Let $C = \openint r {b + 1} \cap \Q$. Let $C_q = \openint {a - 1} q \cap \Q$ for each $q \in \openint a r \cap \Q$. Then $C$ and $C_q$ are [[Definition:Open Set (Topology)|open sets]]. Let $\CC = \set C \cup \set {C_q: q \in \openint a r \cap \Q}$. To show that $\CC$ is an [[Definition:Open Cover|open cover]] for $\Q \cap \closedint a b$: :Pick any $x \in \Q \cap \closedint a b$. :Suppose $x > r$. :Then $r < x \le b < b + 1$. :Therefore $x \in \openint r {b + 1} = C \in \CC$. :Suppose $x < r$. :By [[Between two Real Numbers exists Rational Number]] we have: ::$\exists q \in \Q: a - 1 < a \le x < q < r$ :Then $x \in \openint {a - 1} q$ and $q \in \openint a r$. :Thus $x \in C_q \in \CC$. :Therefore $x \in \bigcup \CC$, showing that $\CC$ is an [[Definition:Open Cover|open cover]] for $\Q \cap \closedint a b$. Since $\Q \cap \closedint a b$ is [[Definition:Compact Topological Space|compact]], $\CC$ has a [[Definition:Finite Subcover|finite subcover]], $\CC'$. This [[Definition:Finite Subcover|finite subcover]] can only contain a finite number of sets in $\set {C_q: q \in \openint a r \cap \Q}$. Therefore $\set {q: C_q \in \CC'}$ has a greatest element, $q'$. We have that $q' \in \openint a r \cap \Q$. By [[Between two Real Numbers exists Rational Number]] we have: :$\exists q^* \in \Q: a < q' < q^* < r < b$ So $q^* \in \Q \cap \closedint a b$. But for all $q \in \hointl a {q'} \cap \Q$: :$q^* \notin C_q = \openint {a - 1} q \cap \Q$ We also have $q^* \notin C = \openint r {b + 1} \cap \Q$. Hence $q^* \notin \bigcup \CC'$. So $\CC'$ is not a [[Definition:Cover of Set|cover]] for $\Q \cap \closedint a b$. This is a contradiction. Thus $S$ is [[Definition:Nowhere Dense|nowhere dense]] in $\Q$. {{qed}}	0
Let $\left({G, \circ, \preceq}\right)$ be an [[Definition:Ordered Group|ordered group]]. An '''ordered group automorphism''' from $\left({G, \circ, \preceq}\right)$ to itself is a [[Definition:Mapping|mapping]] $\phi: G \to G$ that is both: :$(1): \quad$ A [[Definition:Group Automorphism|group automorphism]], that is, a [[Definition:Group Isomorphism|group isomorphism]] from the [[Definition:Group|group]] $\left({G, \circ}\right)$ to itself :$(2): \quad$ An [[Definition:Order Isomorphism|order isomorphism]] from the [[Definition:Ordered Set|ordered set]] $\left({G, \preceq}\right)$ to itself.	0
The [[Definition:Real Number|set of real numbers]] $\R$ forms an [[Definition:Ordered Field|ordered field]] under [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]]: $\struct {\R, +, \times, \le}$.	0
From [[Null Ring is Ring]], the [[Definition:Null Ring|null ring]] $\struct {\set {0_R}, +, \circ}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. We also have that $\set {0_R}$ is a [[Definition:Subset|subset]] of $R$ Hence the result by definition of [[Definition:Subring|subring]]. {{qed}}	0
Applying the definition of $<$, the theorem becomes: Exactly one of the following is true: :$(1): \quad a = b$ :$(2): \quad \exists n \in \N_{>0} : b + n = a$ :$(3): \quad \exists n \in \N_{>0} : a + n = b$ We will use the [[Principle of Mathematical Induction|principle of Mathematical Induction]]. Let $P \left({a}\right)$ be the proposition that exactly one of the above is true, for all natural numbers $b$, for fixed natural number $a$. === Basis for the Induction === We will prove that the proposition is true for $a = 0$. Using [[Proof by Cases]], we divide the proposition into two cases. ==== Case 1: $b = 0$ ==== ===== $(1)$ is true ===== It follows trivially from the values of $a$ and $b$. ===== $(2)$ is false ===== {{AimForCont}} $c$ is a non-zero natural number such that: :$b + c = a$ Substituting the values of $a$ and $b$, we obtain: :$0 + c = 0$ By [[Natural Number Addition Commutes with Zero]], we can simplify the equation to: :$c = 0$ which is a contradiction of the assumption that $c$ is non-zero. ===== $(3)$ is false ===== {{AimForCont}} $c$ is an non-zero natural number such that: :$a + c = b$ Substituting the values of $a$ and $b$, we obtain: :$0 + c = 0$ By [[Natural Number Addition Commutes with Zero]], we can simplify the equation to: :$c = 0$ which is a contradiction of the assumption that $c$ is non-zero. ==== Case 2: $b \ne 0$. ==== ===== $(1)$ is false ===== It follows trivially from the fact that $a = 0$. ===== $(2)$ is false ===== {{AimForCont}} $c$ is an non-zero natural number such that: :$b + c = a$ Substituting the values of $a$, we obtain: :$b + c = 0$ By [[Non-Successor Element of Peano Structure is Unique]], there exists a natural number $d$ such that: :$s \left({d}\right) = c$ where $s$ denotes the [[Definition:Successor Mapping|successor mapping]]. Therefore, we have: :$b + s \left({d}\right) = 0$ Applying the definition of [[Definition:Addition in Peano Structure|addition in Peano structure]], we get: :$s \left({b + d}\right) = 0$ which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. ===== $(3)$ is true ===== By [[Natural Number Addition Commutes with Zero]], we have: :$a + b = b$ The result follows. === Inductive hypothesis === It is now assumed that the proposition is true for $a = k$, where $k$ is a natural number. That is, for all natural numbers $b$, exactly one of the following is true: :$(1): \quad k = b$ :$(2): \quad k > b$ :$(3): \quad k < b$ Then, it is to be proved that the proposition is true for $a = s \left({k}\right)$. That is, for all natural numbers $b$, exactly one of the following is true: :$(1'): \quad s \left({k}\right) = b$ :$(2'): \quad s \left({k}\right) > b$ :$(3'): \quad s \left({k}\right) < b$ === Inductive step === We have: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = s \left({k + 0}\right) | c = {{Defof|Addition in Peano Structure}} }} {{eqn | r = k + s \left({0}\right) | c = {{Defof|Addition in Peano Structure}} }} {{eqn | r = k + 1 | c = {{Defof|One}} }} {{end-eqn}} ==== Case 1: $k = b$ ==== In this case: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = k + 1 }} {{eqn | r = b + 1 }} {{end-eqn}} ===== $(1')$ is false ===== {{AimForCont}} $(1')$ is true. Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = b | c = Assumption }} {{eqn | l = b + 1 | r = b | c = }} {{eqn | l = 1 | r = 0 | c = [[Natural Number Addition is Cancellable]] }} {{eqn | l = s \left({0}\right) | r = 0 | c = {{Defof|One}} }} {{end-eqn}} which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. ===== $(2')$ is true ===== This is apparent from the definition of $>$. ===== $(3')$ is false ===== {{AimForCont}} $(3')$ is true. Let $c$ be a non-zero natural number such that: :$s \left({k}\right) + c = b$ Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) + c | r = b | c = Assumption }} {{eqn | l = \left({b + 1}\right) + c | r = b | c = }} {{eqn | l = b + \left({c + 1}\right) | r = b | c = [[Natural Number Addition is Associative]] and [[Natural Number Addition is Commutative]] }} {{eqn | l = c + 1 | r = 0 | c = [[Natural Number Addition is Cancellable]] }} {{eqn | l = c + s \left({0}\right) | r = 0 | c = {{Defof|One}} }} {{eqn | l = s \left({c}\right) | r = 0 | c = {{Defof|Addition in Peano Structure}} }} {{end-eqn}} which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. ==== Case 2: $k < b$ and $s \left({k}\right) = b$ ==== ===== $(1')$ is true ===== This follows from the assumption. ===== $(2')$ is false ===== {{AimForCont}} $(2')$ is true. Let $c$ be a non-zero natural number such that: :$s \left({k}\right) = b + c$ Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = b + c | c = Assumption }} {{eqn | l = b | r = b + c | c = Assumption }} {{eqn | l = 0 | r = c | c = [[Natural Number Addition is Cancellable]] }} {{end-eqn}} which is a contradiction of our assumption that $c$ is non-zero. ===== $(3')$ is false ===== {{AimForCont}} $(3')$ is true. Let $c$ be a non-zero natural number such that: :$s \left({k}\right) + c = b$ Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) + c | r = b | c = Assumption }} {{eqn | l = b + c | r = b | c = Assumption }} {{eqn | l = c | r = 0 | c = [[Natural Number Addition is Cancellable]] }} {{end-eqn}} which is a contradiction of our assumption that $c$ is non-zero. ==== Case 3: $k < b$ and $s \left({k}\right) \ne b$ ==== From the definition of $<$, we have the existence of a non-zero natural number $m$ such that: :$k + m = b$ By [[Non-Successor Element of Peano Structure is Unique]], there exists a natural number $p$ such that: :$s \left({p}\right) = m$ Substituting: :$k + s \left({p}\right) = b$ Applying the definition of [[Definition:Addition in Peano Structure|addition in Peano structure]], we get: :$s \left({k + p}\right) = b$ ===== $(1')$ is false ===== It is assumed in this case that: :$s \left({k}\right) \ne b$ Therefore, $(1')$ is false. ===== $(2')$ is false ===== {{AimForCont}} $(2')$ is true. Let $c$ be a non-zero natural number such that: :$s \left({k}\right) = b + c$ By [[Non-Successor Element of Peano Structure is Unique]], there exists a natural number $d$ such that: :$s \left({d}\right) = c$ Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = b + c | c = Assumption }} {{eqn | r = \left({k + s \left({p}\right)}\right) + s \left({d}\right) | c = }} {{eqn | r = k + \left({s\left({p}\right) + s \left({d}\right)}\right) | c = [[Natural Number Addition is Commutative]] }} {{eqn | r = k + s\left({p + s \left({d}\right)}\right) | c = {{Defof|Addition in Peano Structure}} }} {{eqn | r = s\left({k + \left({p + s \left({d}\right)}\right)}\right) | c = {{Defof|Addition in Peano Structure}} }} {{eqn | r = s\left({k + \left({s \left({p + d}\right)}\right)}\right) | c = {{Defof|Addition in Peano Structure}} }} {{eqn | l = k | r = k + \left({s \left({p + d}\right)}\right) | c = [[Axiom:Peano's Axioms/Formulation 1|$(P3)$]]$:$ $s$ is [[Definition:Injective|injective]] }} {{eqn | l = 0 | r = s \left({p + d}\right) | c = [[Natural Number Addition is Cancellable]] }} {{end-eqn}} which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. ===== $(3')$ is true ===== We have: {{begin-eqn}} {{eqn | l = s \left({k + p}\right) | r = b | c = }} {{eqn | l = s \left({p + k}\right) | r = b | c = [[Natural Number Addition is Commutative]] }} {{eqn | l = p + s \left({k}\right) | r = b | c = {{Defof|Addition in Peano Structure}} }} {{eqn | l = s \left({k}\right) + p | r = b | c = [[Natural Number Addition is Commutative]] }} {{end-eqn}} The result follows. ==== Case 4: $k > b$ ==== From the definition of $>$, we have the existence of a non-zero natural number $m$ such that: :$k = b + m$ By [[Non-Successor Element of Peano Structure is Unique]], there exists a natural number $p$ such that: :$s \left({p}\right) = m$ Substituting: :$k = b + s \left({p}\right)$ By applying $s$ to both sides, we obtain: :$s \left({k}\right) = s \left({b + s \left({p}\right)}\right)$ Applying the definition of [[Definition:Addition in Peano Structure|addition in Peano structure]], we get: :$s \left({k}\right) = b + s \left({s \left({p}\right)}\right)$ ===== $(1')$ is false ===== {{AimForCont}} $(1')$ is true. Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) | r = b | c = Assumption }} {{eqn | l = b + s \left({s \left({p}\right)}\right) | r = b | c = }} {{eqn | l = s \left({s \left({p}\right)}\right) | r = 0 | c = [[Natural Number Addition is Cancellable]] }} {{end-eqn}} which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. ===== $(2')$ is true ===== We have: :$s \left({k}\right) = b + s \left({s \left({p}\right)}\right)$ The result follows. ===== $(3')$ is false ===== {{AimForCont}} $(3')$ is true. Let $c$ be a non-zero natural number such that: :$s \left({k}\right) + c = b$ Then: {{begin-eqn}} {{eqn | l = s \left({k}\right) + c | r = b | c = Assumption }} {{eqn | l = \left({b + s \left({s \left({p}\right)}\right)}\right) + c | r = b | c = }} {{eqn | l = b + \left({c + s \left({s \left({p}\right)}\right)}\right) | r = b | c = [[Natural Number Addition is Associative]] and [[Natural Number Addition is Commutative]] }} {{eqn | l = \left({c + s \left({s \left({p}\right)}\right)}\right) | r = 0 | c = [[Natural Number Addition is Cancellable]] }} {{eqn | l = s \left({c + s \left({p}\right)}\right) | r = 0 | c = {{Defof|Addition in Peano Structure}} }} {{end-eqn}} which is a contradiction of [[Axiom:Peano's Axioms/Formulation 1|$(P4)$]]: $0$ is not in the [[Definition:Image of Mapping|image]] of $s$. {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = 144 \degrees | r = 180 \degrees - 36 \degrees | c = }} {{eqn | ll= \leadsto | l = \cos 36 \degrees | r = -\cos 144 \degrees | c = [[Cosine of Supplementary Angle]] }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = 108 \degrees | r = 180 \degrees - 72 \degrees | c = }} {{eqn | ll= \leadsto | l = \cos 72 \degrees | r = -\cos 108 \degrees | c = [[Cosine of Supplementary Angle]] }} {{end-eqn}} Thus: :$\cos 36 \degrees + \cos 72 \degrees + \cos 108 \degrees + \cos 144 \degrees = 0$ :[[File:Sum of Cosines of k pi over 5.png|500px]] {{qed}}	0
Let $\eqclass {a, b} {}$ and $\eqclass {c, d} {}$ be [[Definition:Integer|integers]], as defined by the [[Definition:Integer/Formal Definition|formal definition of integers]]. Then exactly one of the following holds: {{begin-eqn}} {{eqn | l = \eqclass {a, b} {} | o = < | r = \eqclass {c, d} {} | c = }} {{eqn | l = \eqclass {a, b} {} | o = = | r = \eqclass {c, d} {} | c = }} {{eqn | l = \eqclass {a, b} {} | o = > | r = \eqclass {c, d} {} | c = }} {{end-eqn}} That is, [[Definition:Strict Ordering on Integers|strict ordering]] is a [[Definition:Trichotomy|trichotomy]].	0
By definition, $\struct {K, \circ_R}$ and $\struct {L, \circ_S}$ are [[Definition:Inverse Completion|inverse completions]] of $\struct {R, \circ_R}$ and $\struct {S, \circ_S}$ respectively. {{Questionable|this is not the definition}} So by the [[Extension Theorem for Homomorphisms]], there is one and only one [[Definition:Ring Monomorphism|monomorphism]] $\psi: \struct {K, \circ_R} \to \struct {L, \circ_S$ extending $\phi$. Thus: :$\forall x \in R, y \in R^*: \map \psi {\dfrac x y} = \dfrac {\map \phi x} {\map \phi y}$ By the [[Extension Theorem for Isomorphisms]], $\psi$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]] if $\phi$ is. Thus, $\forall x, y \in R, z, w \in R^*$: {{begin-eqn}} {{eqn | l = \map \psi {\frac x z +_R \frac y w} | r = \map \psi {\frac {\paren {x \circ_R w} +_R \paren {y \circ_R z} } {z \circ_R w} } | c = [[Addition of Division Products]] }} {{eqn | r = \frac {\map \phi {\paren {x \circ_R w} +_R \paren {y \circ_R z} } } {\map \phi {z \circ_R w} } | c = Definition of $\psi$ }} {{eqn | r = \frac {\paren {\map \phi x \circ_S \map \phi w} +_S \paren {\map \phi y \circ_S \map \phi z} } {\map \phi z \circ_S \map \phi w} | c = [[Definition:Morphism Property|Morphism Property]] }} {{eqn | r = \frac {\map \phi x} {\map \phi z} +_S \frac {\map \phi y} {\map \phi w} | c = [[Addition of Division Products]] }} {{eqn | r = \map \psi {\frac x z} +_S \map \psi {\frac y w} | c = Definition of $\psi$ }} {{end-eqn}} Thus $\psi: K \to L$ is a [[Definition:Ring Monomorphism|monomorphism]]. {{qed}}	0
By definition, if $a \divides b$ then $\exists d \in \Z: a d = b$. Then: :$\paren {a d} c = b c$ that is: :$\paren {a c} d = b c$ which follows because [[Integer Multiplication is Commutative]] and [[Integer Multiplication is Associative]]. Hence the result. {{qed}}	0
From [[Subgroup is Subgroup of Normalizer]] we have that $H \le \map {N_G} H$. It remains to show that $H$ is [[Definition:Normal Subgroup|normal]] in $\map {N_G} H$. Let $a \in H$ and $b \in \map {N_G} H$. By the definition of [[Definition:Normalizer|normalizer]]: : $b a b^{-1} \in H$ Thus $H$ is [[Definition:Normal Subgroup|normal]] in $\map {N_G} H$. {{qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $N$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$. Then [[Definition:Congruence Modulo Subgroup|congruence modulo $N$]] is a [[Definition:Congruence Relation|congruence relation]] for the [[Definition:Group Operation|group operation]] $\circ$.	0
{{proof wanted}} [[Category:Quotient Rings]] exe0820b4r8fbvc9qgptqjs40gh6a28	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $S \subseteq G$. Then the [[Definition:Conjugate of Group Subset|conjugate of $S$ by $e$]] is $S$: :$S^e = S$	0
Let $\struct{S, \circ}$ and $\struct{T, *}$ be [[Definition:Algebraic Structure|algebraic structures]]. Let $\phi: \struct{S, \circ} \to \struct{T, *}$ be a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]]. Let $\struct{S, \circ}$ have an [[Definition:Identity Element|identity]] $e_S$. Let $\struct{T, *}$ have an [[Definition:Identity Element|identity]] $e_T$. Let every element of $\struct{T, *}$ be [[Definition:Cancellable Element|cancellable]]. Then $\map \phi {e_S}$ is the [[Definition:Identity Element|identity]] $e_T$.	0
Let $g_1 = g^n$ and $g_2 = g^m$. By [[Powers of Group Element Commute]], $g_1$ and $g_2$ [[Definition:Commuting Elements|commute]]. We have: :${g_1}^m = g^{n m} = e$ :${g_2}^n = g^{m n} = e$ It follows that: :the [[Definition:Order of Group Element|order]] of $g_1$ is $m$ :the [[Definition:Order of Group Element|order]] of $g_2$ is $n$ otherwise if either $g_1$ or $g_2$ were of a smaller [[Definition:Order of Group Element|order]] then $g$ would also be of a smaller [[Definition:Order of Group Element|order]]. By [[Bézout's Lemma]]: :$\exists u, v \in \Z: u n + v m = 1$ as $m \perp n$. Thus: :$g = g^{u n + v m} = \paren {g^n}^u \paren {g^m}^v = {g_1}^u {g_2}^v$ Also by [[Bézout's Lemma]]: :$u \perp m$ and: :$v \perp n$ Thus by [[Order of Group Element equals Order of Coprime Power]]: :$\order { {g_1}^u} = m$ and: :$\order { {g_2}^v} = n$ We have that $g_1$ and $g_2$ [[Definition:Commute|commute]]. So by [[Commutativity of Powers in Group]], ${g_1}^u$ and ${g_2}^v$ also [[Definition:Commuting Elements|commute]]. Putting $a = g_1^u$ and $b = g_2^v$, we have $a$ and $b$ which satisfy the required conditions. It remains to prove [[Definition:Unique|uniqueness]]. Suppose that: :$(1): \quad g = r_1 r_2 = s_1 s_2$ where: :$r_1$ and $r_2$ [[Definition:Commuting Elements|commute]] :$s_1$ and $s_2$ [[Definition:Commuting Elements|commute]] :$\order {r_1} = \order {s_1} = m$ :$\order {r_2} = \order {s_2} = n$ Raising $(1)$ to the [[Definition:Power of Group Element|$n u$th power]]: :$g^{n u} = {r_1}^{n u} {r_2}^{n u} = {s_1}^{n u} {s_2}^{n u}$ and so: {{begin-eqn}} {{eqn | l = {r_1}^{n u} {r_2}^{n u} | r = {s_1}^{n u} {s_2}^{n u} | c = }} {{eqn | l = {r_1}^{n u} \paren { {r_2}^n}^u | r = {s_1}^{n u} \paren { {s_2}^n}^u | c = }} {{eqn | l = {r_1}^{n u} e^u | r = {s_1}^{n u} e^u | c = as $\order {r_2} = \order {s_2} = n$ }} {{eqn | ll= \leadsto | l = {r_1}^{n u} | r = {s_1}^{n u} | c = }} {{eqn | ll= \leadsto | l = {r_1}^{1 - m v} | r = {s_1}^{1 - m v} | c = }} {{eqn | ll= \leadsto | l = r_1 \paren { {r_1}^m}^{-v} | r = s_1 \paren { {s_1}^m}^{-v} | c = }} {{eqn | ll= \leadsto | l = r_1 e^{-v} | r = s_1 e^{-v} | c = }} {{eqn | ll= \leadsto | l = r_1 | r = s_1 | c = }} {{end-eqn}} It follows directly from $(1)$ that $r_2 = s_2$. Hence the result. {{qed}}	0
Let $\dfrac p q$ denote a [[Definition:Proper Fraction|proper fraction]] expressed in [[Definition:Canonical Form of Rational Number|canonical form]]. Then it is always possible to express $\dfrac p q$ as the [[Definition:Integer Addition|sum]] of a [[Definition:Finite Set|finite number]] of [[Definition:Distinct Elements|distinct]] [[Definition:Unit Fraction|unit fractions]]: {{begin-eqn}} {{eqn | l = \dfrac p q | r = \sum_{\substack {1 \mathop \le k \mathop \le m \\ n_j \mathop \le n_{j + 1} } } \dfrac 1 {n_k} | c = }} {{eqn | r = \dfrac 1 {n_1} + \dfrac 1 {n_2} + \dotsb + \dfrac 1 {n_m} | c = }} {{end-eqn}}	0
Let $\left({S, \circ}\right)$ be an [[Definition:Algebraic Structure|algebraic structure]]. If $\left({S, \circ}\right)$ has more than one [[Definition:Right Identity|right identity]], then it has no [[Definition:Left Identity|left identity]].	0
By inspection and investigation. {{qed}}	0
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $a \in R$. Then the [[Definition:Ring Negative|ring negative]] $-a$ of $a$ is unique.	0
{{begin-eqn}} {{eqn | l = \map \phi {a -_1 b} | r = \map \phi {a +_1 \paren {-b} } | c = {{Defof|Ring Subtraction}} }} {{eqn | r = \map \phi a +_2 \map \phi {-b} | c = {{Defof|Ring Homomorphism}} }} {{eqn | r = \map \phi a +_2 \paren {-\map \phi b} | c = [[Ring Homomorphism Preserves Negatives]] }} {{eqn | r = \map \phi a -_2 \map \phi b | c = {{Defof|Ring Subtraction}} }} {{end-eqn}} {{qed}}	0
First we establish the result for when $\left({R, \odot}\right)$ has an [[Definition:Identity Element|identity element]] $e$. For $n = 0$ we have: :$\displaystyle \odot^0 \left({x + y}\right) = e = {0 \choose 0} \left({\odot^{0 - 0} x}\right) \odot \left({\odot^0 y}\right) = \sum_{k \mathop = 0}^0 {0 \choose k} x^{0 - k} \odot y^k$ For $n = 1$ we have: :$\displaystyle \odot^1 \left({x + y}\right) = \left({x + y}\right) = {0 \choose 1} \left({\odot^{1 - 0} x}\right) \odot \left({\odot^0 y}\right) + {1 \choose 1} \left({\odot^{1 - 1} x}\right) \odot \left({\odot^1 y}\right) = \sum_{k \mathop = 0}^1 {1 \choose k} x^{1 - k} \odot y^k$ === Basis for the Induction === For $n = 2$ we have: {{begin-eqn}} {{eqn | l = \odot^2 \left({x + y}\right) | r = \left({x + y}\right) \odot \left({x + y}\right) | c = }} {{eqn | r = \left({x \odot x}\right) + \left({x \odot y}\right) + \left({y \odot x}\right) + \left({y \odot y}\right) | c = }} {{eqn | r = \left({x \odot x}\right) + 2 \left({x \odot y}\right) + \left({y \odot y}\right) | c = $+$ is [[Definition:Commutative Operation|commutative]] in $R$ }} {{eqn | r = \odot^2 x + 2 \left({\odot^1 x}\right) \odot \left({\odot^1 y}\right) + \odot^2 y | c = }} {{eqn | r = \odot^2 x + {2 \choose 1} \left({\odot^{2-1} x}\right) \odot \left({\odot^1 y}\right) + \odot^2 y | c = }} {{end-eqn}} This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === This is our [[Principle of Mathematical Induction#Induction Hypothesis|inductive hypothesis]]: :$\displaystyle \forall n \ge 2: \odot^n \left({x + y}\right) = \odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \odot^{n + 1} \left({x + y}\right) | r = \left({x + y}\right) \odot \left({\odot^n \left({x + y}\right)}\right) | c = }} {{eqn | r = x \odot \left({\odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y}\right) | c = }} {{eqn | o = | ro= + | r = ~ y \odot \left({\odot^n x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^k y}\right) + \odot^n y}\right) | c = [[Binomial Theorem/Ring Theory#Inductive Hypothesis|Inductive Hypothesis]] }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + x \odot \left({\odot^n y}\right) | c = }} {{eqn | o = | ro= + | r = ~ y \odot \left({\odot^n x}\right) + \sum_{k \mathop = 1}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \sum_{k \mathop = 0}^{n - 1} {n \choose k} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \sum_{k \mathop = 1}^n {n \choose k - 1} \left({\odot^{n - k} x}\right) \odot \left({\odot^{k + 1} y}\right) + \odot^{n + 1} y | c = }} {{eqn | r = \odot^{n + 1} x + \sum_{k \mathop = 1}^n {n + 1 \choose k} \left({\odot^{n + 1 - k} x}\right) \odot \left({\odot^k y}\right) + \odot^{n + 1} y | c = [[Pascal's Rule]] }} {{end-eqn}} The result follows by the [[Principle of Mathematical Induction]]. {{qed}}	0
The [[Definition:Integer|integers]] $\Z$ form an [[Definition:Ordered Integral Domain|ordered integral domain]] under [[Definition:Integer Addition|addition]] and [[Definition:Integer Multiplication|multiplication]].	0
Let $\struct {G, \oplus}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e_G$. Let $S$ be a [[Definition:Set|set]]. Let $\struct {G^S, \oplus}$ be the structure on $G^S$ [[Definition:Induced Structure|induced]] by $\oplus$. Let $f \in G^S$. Let $f^* \in G^S$ be defined as follows: :$\forall f \in G^S: \forall x \in S: \map {f^*} x = \paren {\map f x}^{-1}$ Then $f^*$ is the [[Definition:Inverse Element|inverse]] of $f$ for the [[Definition:Pointwise Operation|pointwise operation induced]] on $G^S$ by $\oplus$.	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Then: :$\paren {p - 1}! \equiv -1 \pmod p$	0
{{AimForCont}} $p \in A$ is the [[Definition:Greatest Element|greatest element]] of $A$. Then by definition of $A$: :$p^2 < 2$ Let $h \in \Q$ be a [[Definition:Rational Number|rational number]] such that $0 < h < 1$ such that: :$h < \dfrac {2 - p^2} {2 p + 1}$ This is always possible, because by definition $2 - p^2 > 0$ and $2 p + 1 > 0$. Let $q = p + h$. Then $q > p$, and: {{begin-eqn}} {{eqn | l = q^2 | r = p^2 + \paren {2 p + h} h | c = }} {{eqn | o = < | r = p^2 + \paren {2 p + 1} h | c = }} {{eqn | o = < | r = p^2 + \paren {2 - p^2} | c = }} {{eqn | r = 2 | c = }} {{end-eqn}} That means $q \in A$. This [[Definition:Contradiction|contradicts]] our assertion that $p$ is the [[Definition:Greatest Element|greatest element]] of $A$. It follows that $A$ can have no [[Definition:Greatest Element|greatest element]]. {{qed}}	0
We are to show that: :$\left({x + y}\right) \times z = \left({x \times z}\right) + \left({y \times z}\right)$ for all $x, y, z \in \N$. From the definition of [[Definition:Natural Number Multiplication|natural number multiplication]], we have by definition that: {{begin-eqn}} {{eqn | ll= \forall m, n \in \N: | l = m \times 0 | r = 0 }} {{eqn | l = m \times n^+ | r = \left({m \times n}\right) + m }} {{end-eqn}} Let $x, y \in \N$ be arbitrary. For all $z \in \N$, let $P \left({z}\right)$ be the [[Definition:Proposition|proposition]]: :$\forall x, y \in \N: \left({x + y}\right) \times z = \left({x \times z}\right) + \left({y \times z}\right)$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \left({x + y}\right) \times 0 | r = 0 | c = Definition of [[Definition:Natural Number Multiplication|Natural Number Multiplication]] }} {{eqn | r = 0 + 0 | c = Definition of [[Definition:Natural Number Addition|Natural Number Addition]] }} {{eqn | r = x \times 0 + y \times 0 | c = Definition of [[Definition:Natural Number Multiplication|Natural Number Multiplication]] }} {{end-eqn}} and so $P \left({0}\right)$ holds. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 0$, then it logically follows that $P \left({k^+}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\forall x, y \in \N: \left({x + y}\right) \times k = \left({x \times k}\right) + \left({y \times k}\right)$ Then we need to show: :$\forall x, y \in \N: \left({x + y}\right) \times k^+ = \left({x \times k^+}\right) + \left({y \times k^+}\right)$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \left({x + y}\right) \times k^+ | r = \left({x + y}\right) \times k + \left({x + y}\right) | c = Definition of [[Definition:Natural Number Multiplication|Natural Number Multiplication]] }} {{eqn | r = \left({x \times k}\right) + \left({y \times k}\right) + \left({x + y}\right) | c = [[Natural Number Multiplication Distributes over Addition/Proof 2#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \left({\left({x \times k}\right) + x}\right) + \left({\left({y \times k}\right) + y}\right) | c = [[Natural Number Addition is Commutative/Proof 2|Natural Number Addition is Commutative]] and [[Natural Number Addition is Associative/Proof 2|Associative]] }} {{eqn | r = \left({x \times k^+}\right) + \left({y \times k^+}\right) | c = Definition of [[Definition:Natural Number Multiplication|Natural Number Multiplication]] }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k^+}\right)$ and the result follows by the [[Principle of Mathematical Induction]]: :$\forall x, y, z \in \N: \left({x + y}\right) \times n = \left({x \times z}\right) + \left({y \times z}\right)$ {{qed|lemma}} Next we need to show that: :$z \times \left({x + y}\right) = \left({z \times x}\right) + \left({z \times y}\right)$ for all $x, y, z \in \N$. So: {{begin-eqn}} {{eqn | l = z \times \left({x + y}\right) | r = \left({x + y}\right) \times z | c = [[Natural Number Multiplication is Commutative]] }} {{eqn | r = \left({x \times z}\right) + \left({y \times z}\right) | c = from above }} {{eqn | r = \left({z \times x}\right) + \left({z \times y}\right) | c = [[Natural Number Multiplication is Commutative]] }} {{end-eqn}} Thus we have proved: :$\forall x, y, z \in \N: z \times \left({x + y}\right) = \left({z \times x}\right) + \left({z \times y}\right)$ {{qed}}	0
From [[Center of Group of Prime Power Order is Non-Trivial]], $\map Z G$ is not the [[Definition:Trivial Subgroup|trivial subgroup]]. From [[Quotient of Group by Center Cyclic implies Abelian]], $G / \map G Z$ cannot be [[Definition:Cyclic Group|cyclic]] and [[Definition:Non-Trivial Subgroup|non-trivial]]. Thus $\order {G / \map G Z}$ cannot be $p$ and so must be $p^2$. Thus $\order {\map G Z} = p$. Let $N$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$ of [[Definition:Order of Group|order]] $p$. Then from [[Normal Subgroup of p-Group of Order p is Subset of Center]]: :$N \subseteq \map G Z$ Thus there is no [[Definition:Normal Subgroup|normal subgroup]] of [[Definition:Order of Group|order]] $p$ different from $\map G Z$. Hence the result. {{qed}}	0
Let $S$ be a [[Definition:Set|set]], and let $\mathcal P \left({S}\right)$ be its [[Definition:Power Set|power set]]. Denote with $*$ and $\cap$ [[Definition:Symmetric Difference|symmetric difference]] and [[Definition:Set Intersection|intersection]], respectively. Then $\left({S, *, \cap}\right)$ is a [[Definition:Boolean Ring|Boolean ring]].	0
Using the [[Axiom:Axiomatization of 1-Based Natural Numbers|following axioms]]: {{:Axiom:Axiomatization of 1-Based Natural Numbers}} Suppose that $\exists n \in \N_{>0}: a < n < a + 1$. Then by the definition of [[Definition:Ordering on Natural Numbers|ordering on natural numbers]]: {{begin-eqn}} {{eqn | l = a + x | r = n | c = Definition of [[Definition:Ordering on Natural Numbers|Ordering on Natural Numbers]]: $a < n$ }} {{eqn | l = n + y | r = a + 1 | c = Definition of [[Definition:Ordering on Natural Numbers|Ordering on Natural Numbers]]: $a < n$ }} {{eqn | ll= \implies | l = \left({a + x}\right) + y | r = a + 1 | c = substitution for $n$ }} {{eqn | ll= \implies | l = a + \left({x + y}\right) | r = a + 1 | c = [[Natural Number Addition is Associative]] }} {{eqn | ll= \implies | l = x + y | r = 1 | c = [[Addition on 1-Based Natural Numbers is Cancellable|Addition on $1$-Based Natural Numbers is Cancellable]] }} {{end-eqn}} By [[Axiom:Axiomatization of 1-Based Natural Numbers|Axiom $D$]], either: :$y = 1$ or: :$y = t + 1$ for some $t \in \N_{>0}$ Then either: :$x + 1 = 1$ when $y = 1$ or: :$x + \left({t + 1}\right) = \left({x + t}\right) + 1 = 1$ when $y = t + 1$ Both of these conclusions violate [[Natural Number is Not Equal to Successor]]. Hence the result. {{qed}}	0
Let $\R \sqbrk X$ be the [[Definition:Ring of Polynomials in Ring Element|ring of polynomials]] in an [[Definition:Indeterminate|indeterminate]] $X$ over $\R$. Then $\R \sqbrk X$ is not a [[Definition:Field (Abstract Algebra)|field]].	0
By definition of [[Definition:Ring with Unity|ring with unity]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {R, \circ}$ is a [[Definition:Monoid|monoid]]. The result follows from [[Inverse in Monoid is Unique]]. {{qed}}	0
Let $a, z \in \R$. Then $a$ is [[Definition:Congruence (Number Theory)|congruent]] to $0$ modulo $z$ {{iff}} $a$ is an [[Definition:Integer Multiple|integer multiple]] of $z$. :$\exists k \in \Z: k z = a \iff a \equiv 0 \pmod z$ If $z \in \Z$, then further: :$z \divides a \iff a \equiv 0 \pmod z$	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $g \in G$. Let $m, n \in \N_{>0}$. Then: :$\forall m, n \in \N_{>0}: g^n \circ g^m = g^m \circ g^n$	0
Let $\dfrac {x_k} {y_k}$ and $\dfrac {x_{k + 1} } {y_{k + 1} }$ be consecutive stages of the calculation of the [[Definition:Unit Fraction|unit fractions]] accordingly: :$\dfrac {x_k} {y_k} - \dfrac 1 {\ceiling {y_n / x_n} } = \dfrac {x_{k + 1} } {y_{k + 1} }$ By definition of [[Fibonacci's Greedy Algorithm]]: :$\dfrac {x_{k + 1} } {y_{k + 1} } = \dfrac {\paren {-y_k} \bmod {x_k} } {y_k \ceiling {y_k / x_k} }$ It is established during the processing of [[Fibonacci's Greedy Algorithm]] that: :$\paren {-y_k} \bmod {x_k} < x_k$ Hence successive [[Definition:Numerator|numerators]] decrease by at least $1$. Hence there can be no more [[Definition:Unit Fraction|unit fractions]] than there are [[Definition:Natural Number|natural numbers]] between $1$ and $p$. Hence the result. {{Qed}}	0
Let $a, b, c, \in \Z$ be [[Definition:Integer|integers]]. Let: :$a \divides b, a \divides c$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Then: :$a^2 \divides b c$	0
Let $\mathcal L$ be a [[Definition:Logical Language|logical language]]. Let $\mathscr M$ be a [[Definition:Formal Semantics|formal semantics]] for $\mathcal L$. Let $\mathcal F$ be an [[Definition:Satisfiable Set of Formulas|$\mathscr M$-satisfiable set of formulas]] from $\mathcal L$. Let $\phi \in \mathcal F$. Then $\mathcal F \setminus \left\{{\phi}\right\}$ is also [[Definition:Satisfiable Set of Formulas|$\mathscr M$-satisfiable]].	0
:$\vdash p \land q \implies p$	0
{{BeginTableau|\vdash p \iff \left({p \lor q}\right) \land \left({p \lor \neg q}\right)}} {{Assumption|1|p}} {{Addition|2|1|p \lor q|1|1}} {{Addition|3|1|p \lor \neg q|1|2}} {{Conjunction|4|1|\left({p \lor q}\right) \land \left({p \lor \neg q}\right)|2|3}} {{Implication|5||p \implies \left({p \lor q}\right) \land \left({p \lor \neg q}\right)|1|4}} {{Assumption|6|\left({p \lor q}\right) \land \left({p \lor \neg q}\right)}} {{SequentIntro|7|6|p \lor \left({q \land \neg q}\right)|6|[[Disjunction Distributes over Conjunction]]}} {{Assumption|8|p}} {{TheoremIntro|9|\neg \left({q \land \neg q}\right)|[[Principle of Non-Contradiction/Sequent Form/Formulation 2|Principle of Non-Contradiction: Formulation 2]]}} {{ModusTollendoPonens|10|6|p|7|9|2}} {{ProofByCases|11|6|p|6|8|8|9|10}} {{Implication|12||\left({p \lor q}\right) \land \left({p \lor \neg q}\right) \implies p|6|11}} {{BiconditionalIntro|13||p \iff \left({p \lor q}\right) \land \left({p \lor \neg q}\right)|5|12}} {{EndTableau}} {{qed}}	0
Let $P$ be a [[Definition:URM Program|URM program]]. Let $l = \map \lambda P$ be the [[Definition:Unlimited Register Machine#Length of Program|number of basic instructions]] in $P$. Let $u = \map \rho P$ be the [[Definition:Unlimited Register Machine#Number of Registers Used|number of registers used]] by $P$. Then $P$ can be modified as follows: :Every <tt>Jump</tt> of the form $\map J {m, n, q}$ where $q > l$ may be replaced by $\map J {m, n, l + 1}$ :If $u > 0$, a [[Clear Registers Program]] $\map Z {2, u}$ can be appended to the end of $P$ at lines $l + 1$ to $l + u - 1$. The new program $P'$ that results from the above modifications produces exactly the same [[Definition:Unlimited Register Machine#Output|output]] as $P$ for each [[Definition:Unlimited Register Machine#Input|input]]. Note now though that $\map \lambda {P'} = l + u - 1$. Such a program as $P'$ is called a '''normalized URM program'''. The point of doing this is so as to make programs easier to [[Definition:Concatenation of URM Programs|concatenate]]. Once the above have been done, each program has a well-defined [[Definition:Unlimited Register Machine#Termination|exit line]] which can be used as the start line of the program that immediately follows it.	0
{{BeginTableau| \vdash ((p \land q) \lor (\lnot p \land q)) \lor ( (p \land \lnot q) \lor (\lnot p \land \lnot q)) }} {{ExcludedMiddle|1|p \lor \lnot p}} {{ExcludedMiddle|2|q \lor \lnot q}} {{Conjunction|3||(p \lor \lnot p) \land (q \lor \lnot q)|1|2}} {{SequentIntro|4||((p \lor \lnot p) \land q) \lor ((p \lor \lnot p) \land \lnot q)|3|[[Rule of Distribution/Conjunction Distributes over Disjunction/Left Distributive/Formulation 1|Conjunction Distributes over Disjunction]]}} {{Assumption|5|(p \lor \lnot p) \land q}} {{SequentIntro|6|5|(p \land q) \lor (\lnot p \land q)|5|[[Rule of Distribution/Conjunction Distributes over Disjunction/Right Distributive/Formulation 1|Conjunction Distributes over Disjunction]]}} {{Implication|7||(p \lor \lnot p) \land q \implies (p \land q) \lor (\lnot p \land q)|5|6 }} {{Assumption|8|(p \lor \lnot p) \land \lnot q}} {{SequentIntro|9|8| (p \land \lnot q) \lor (\lnot p \land \lnot q)|8| [[Rule of Distribution/Conjunction Distributes over Disjunction/Right Distributive/Formulation 1|Conjunction Distributes over Disjunction]] }} {{Implication|10|| (p \lor \lnot p) \land \lnot q \implies (p \land \lnot q) \lor (\lnot p \land \lnot q)|8|9}} {{SequentIntro|11|| ((p \lor \lnot p) \land q) \lor ((p \lor \lnot p) \land \lnot q) \implies ((p \land q) \lor (\lnot p \land q)) \lor ( (p \land \lnot q) \lor (\lnot p \land \lnot q))|7,10|[[Constructive Dilemma/Formulation 1|Constructive Dilemma]]}} {{ModusPonens|12|| ((p \land q) \lor (\lnot p \land q)) \lor ( (p \land \lnot q) \lor (\lnot p \land \lnot q))|11|4 }} {{EndTableau}} {{qed}} {{LEM}} [[Category:Propositional Logic]] 48jjx3jhedwa77osqylwqo68vmo427a	0
Denote with $\mathscr H_2 - (\text A 4)$ the [[Definition:Proof System|proof system]] resulting from $\mathscr H_2$ by removing [[Definition:Axiom (Formal Systems)|axiom]] $(\text A 4)$. Consider $\mathscr C_5$, [[Definition:Constructed Semantics/Instance 5|Instance 5]] of [[Definition:Constructed Semantics|constructed semantics]]. We will prove that: * $\mathscr H_2 - (\text A 4)$ is [[Definition:Sound Proof System|sound]] for $\mathscr C_5$; * [[Definition:Axiom (Formal Systems)|Axiom]] $(\text A 4)$ is not a [[Definition:Tautology (Formal Semantics)|tautology]] in $\mathscr C_5$ which leads to the conclusion that $(\text A 4)$ is not a [[Definition:Theorem (Formal Systems)|theorem]] of $\mathscr H_2 - (\text A 4)$. === Soundness of $\mathscr H_2 - (\text A 4)$ for $\mathscr C_5$ === Starting with the [[Definition:Axiom (Formal Systems)|axioms]]: {{begin-axiom}} {{axiom | n = \text A 1 | lc= [[Rule of Idempotence/Disjunction/Formulation 2/Reverse Implication|Rule of Idempotence]] | m = \paren {p \lor p} \implies p | rc= [[Definition:Constructed Semantics/Instance 5/Rule of Idempotence|Proof of Tautology]] }} {{axiom | n = \text A 2 | lc= [[Rule of Addition/Sequent Form/Formulation 2/Form 2|Rule of Addition]] | m = q \implies \paren {p \lor p} | rc= [[Definition:Constructed Semantics/Instance 5/Rule of Addition|Proof of Tautology]] }} {{axiom | n = \text A 3 | lc= [[Rule of Commutation/Disjunction/Formulation 2/Forward Implication|Rule of Commutation]] | m = \paren {p \lor q} \implies \paren {q \lor p} | rc= [[Definition:Constructed Semantics/Instance 5/Rule of Commutation|Proof of Tautology]] }} {{end-axiom}} Next it needs to be shown that the [[Definition:Hilbert Proof System/Instance 2|rules of inference of $\mathscr H_2$]] preserve $\mathscr C_5$-[[Definition:Tautology (Formal Semantics)|tautologies]]. ==== Rule $\text {RST} 1$: Rule of Uniform Substitution ==== By definition, any [[Definition:WFF of Propositional Logic|WFF]] is assigned a value from $\set {0, 1, 2, 3}$. Thus, in applying [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 1$]], we are introducing $0$, $1$, $2$ or $3$ in the position of a [[Definition:Propositional Variable|propositional variable]]. But all possibilities of assignments to such [[Definition:Propositional Variable|propositional variables]] were shown not to affect the resulting values of the axioms. Hence [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 1$]] preserves $\mathscr C_5$-[[Definition:Tautology (Formal Semantics)|tautologies]]. ==== Rule $\text {RST} 2$: Rule of Substitution by Definition ==== Because the definition of $\mathscr C_5$ was given in terms of [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 2$]], it cannot affect any of its results. ==== Rule $\text {RST} 3$: Rule of Detachment ==== Suppose $\mathbf A$ and $\mathbf A \implies \mathbf B$ both take value $0$. Then using [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 2$]], definition $(2)$, we get: :$\neg \mathbf A \lor \mathbf B$ taking value $0$ by assumption. But $\neg \mathbf A$ takes value $1$ by definition of $\neg$. So from the definition of $\lor$, it must be that $\mathbf B$ takes value $0$. Hence [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 3$]] also produces only [[Definition:WFF of Propositional Logic|WFFs]] of value $0$. ==== Rule $\text {RST} 4$: Rule of Adjunction ==== Suppose $\mathbf A$ and $\mathbf B$ take value $0$. Then using the definitional abbreviations: :$\mathbf A \land \mathbf B =_{\text {def} } \neg \paren {\neg \mathbf A \lor \neg \mathbf B}$ We compute: {{begin-eqn}} {{eqn | l = \mathbf A \land \mathbf B | r = 0 \land 0 }} {{eqn | r = \neg \paren {\neg 0 \lor \neg 0} | c = [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 2 \, (1)$]] }} {{eqn | r = \neg \paren {1 \lor 1} }} {{eqn | r = \neg 1 }} {{eqn | r = 0 }} {{end-eqn}} proving that [[Definition:Hilbert Proof System/Instance 2|Rule $\text {RST} 4$]] also produces only $0$s from $0$s. Hence $\mathscr H_2 - (\text A 4)$ is [[Definition:Sound Proof System|sound]] for $\mathscr C_5$. === $(\text A 4)$ is not a $\mathscr C_5$-tautology === Recall [[Definition:Axiom (Formal Systems)|axiom]] $(\text A 4)$, the [[Factor Principles/Disjunction on Left/Formulation 2|Factor Principle]]: :$\paren {p \lor q} \implies \paren {q \lor p}$ Under $\mathscr C_5$, we can use definitional abbreviations to arrive at: :$\neg \paren {\neg p \lor q} \lor \paren {\neg \paren {r \lor p} \lor \paren {r \lor q} }$ Applying the definition of $\mathscr C_5$, we have the following: ::$\begin{array}{|ccccc|c|cccccccc|} \hline \neg & (\neg & p & \lor & q) & \lor & (\neg & (r & \lor & p) & \lor & (r & \lor & q)) \\ \hline 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 1 & 3 & 2 & 0 & 0 & 0 & 1 & 0 & 0 & 2 & 0 & 0 & 0 & 0 \\ 1 & 0 & 3 & 0 & 0 & 0 & 1 & 0 & 0 & 3 & 0 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\ 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 \\ 1 & 3 & 2 & 0 & 0 & 0 & 3 & 1 & 2 & 2 & 0 & 1 & 0 & 0 \\ 1 & 0 & 3 & 0 & 0 & 0 & 0 & 1 & 3 & 3 & 0 & 1 & 0 & 0 \\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 2 & 0 & 0 & 0 & 2 & 0 & 0 \\ 1 & 0 & 1 & 0 & 0 & 0 & 3 & 2 & 2 & 1 & 0 & 2 & 0 & 0 \\ 1 & 3 & 2 & 0 & 0 & 0 & 3 & 2 & 2 & 2 & 0 & 2 & 0 & 0 \\ 1 & 0 & 3 & 0 & 0 & 0 & 1 & 2 & 0 & 3 & 0 & 2 & 0 & 0 \\ 1 & 1 & 0 & 0 & 0 & 0 & 1 & 3 & 0 & 0 & 0 & 3 & 0 & 0 \\ 1 & 0 & 1 & 0 & 0 & 0 & 0 & 3 & 3 & 1 & 0 & 3 & 0 & 0 \\ 1 & 3 & 2 & 0 & 0 & 0 & 1 & 3 & 0 & 2 & 0 & 3 & 0 & 0 \\ 1 & 0 & 3 & 0 & 0 & 0 & 0 & 3 & 3 & 3 & 0 & 3 & 0 & 0 \\ 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\ 0 & 3 & 2 & 3 & 1 & 0 & 1 & 0 & 0 & 2 & 0 & 0 & 0 & 1 \\ 1 & 0 & 3 & 0 & 1 & 0 & 1 & 0 & 0 & 3 & 0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 1 \\ 0 & 3 & 2 & 3 & 1 & 0 & 3 & 1 & 2 & 2 & 3 & 1 & 1 & 1 \\ 1 & 0 & 3 & 0 & 1 & 0 & 0 & 1 & 3 & 3 & 0 & 1 & 1 & 1 \\ 0 & 1 & 0 & 1 & 1 & 0 & 1 & 2 & 0 & 0 & 2 & 2 & 2 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 3 & 2 & 2 & 1 & 0 & 2 & 2 & 1 \\ 0 & 3 & 2 & 3 & 1 & 0 & 3 & 2 & 2 & 2 & 0 & 2 & 2 & 1 \\ 2 & 1 & 2 & 0 & 3 & 2 & 2 & 2 & 1 & 1 & 0 & 3 & 0 & 1 \\ 0 & 1 & 0 & 1 & 1 & 0 & 1 & 3 & 0 & 0 & 3 & 3 & 3 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 0 & 3 & 3 & 1 & 0 & 3 & 3 & 1 \\ 0 & 3 & 2 & 3 & 1 & 0 & 1 & 3 & 0 & 2 & 3 & 3 & 3 & 1 \\ 1 & 0 & 3 & 0 & 1 & 0 & 0 & 3 & 3 & 3 & 0 & 3 & 3 & 1 \\ 3 & 1 & 0 & 2 & 2 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 2 \\ 1 & 0 & 1 & 0 & 2 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 2 \\ 1 & 3 & 2 & 0 & 2 & 0 & 1 & 0 & 0 & 2 & 0 & 0 & 0 & 2 \\ 1 & 0 & 3 & 0 & 2 & 0 & 1 & 0 & 0 & 3 & 0 & 0 & 0 & 2 \\ 3 & 1 & 0 & 2 & 2 & 0 & 1 & 1 & 0 & 0 & 2 & 1 & 2 & 2 \\ 1 & 0 & 1 & 0 & 2 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 2 & 2 \\ 1 & 3 & 2 & 0 & 2 & 0 & 3 & 1 & 2 & 2 & 0 & 1 & 2 & 2 \\ 1 & 0 & 3 & 0 & 2 & 0 & 0 & 1 & 3 & 3 & 0 & 1 & 2 & 2 \\ 3 & 1 & 0 & 2 & 2 & 0 & 1 & 2 & 0 & 0 & 2 & 2 & 2 & 2 \\ 1 & 0 & 1 & 0 & 2 & 0 & 3 & 2 & 2 & 1 & 0 & 2 & 2 & 2 \\ 1 & 3 & 2 & 0 & 2 & 0 & 3 & 2 & 2 & 2 & 0 & 2 & 2 & 2 \\ 1 & 0 & 3 & 0 & 2 & 2 & 1 & 2 & 0 & 3 & 2 & 2 & 2 & 2 \\ 3 & 1 & 0 & 2 & 2 & 0 & 1 & 3 & 0 & 0 & 0 & 3 & 0 & 2 \\ 1 & 0 & 1 & 0 & 2 & 0 & 0 & 3 & 3 & 1 & 0 & 3 & 0 & 2 \\ 1 & 3 & 2 & 0 & 2 & 0 & 1 & 3 & 0 & 2 & 0 & 3 & 0 & 2 \\ 1 & 0 & 3 & 0 & 2 & 0 & 0 & 3 & 3 & 3 & 0 & 3 & 0 & 2 \\ 0 & 1 & 0 & 3 & 3 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 3 \\ 1 & 0 & 1 & 0 & 3 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 3 \\ 0 & 3 & 2 & 3 & 3 & 0 & 1 & 0 & 0 & 2 & 0 & 0 & 0 & 3 \\ 1 & 0 & 3 & 0 & 3 & 0 & 1 & 0 & 0 & 3 & 0 & 0 & 0 & 3 \\ 0 & 1 & 0 & 3 & 3 & 0 & 1 & 1 & 0 & 0 & 3 & 1 & 3 & 3 \\ 1 & 0 & 1 & 0 & 3 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 3 & 3 \\ 0 & 3 & 2 & 3 & 3 & 0 & 3 & 1 & 2 & 2 & 3 & 1 & 3 & 3 \\ 1 & 0 & 3 & 0 & 3 & 0 & 0 & 1 & 3 & 3 & 0 & 1 & 3 & 3 \\ 0 & 1 & 0 & 3 & 3 & 0 & 1 & 2 & 0 & 0 & 0 & 2 & 0 & 3 \\ 1 & 0 & 1 & 0 & 3 & 0 & 3 & 2 & 2 & 1 & 0 & 2 & 0 & 3 \\ 0 & 3 & 2 & 3 & 3 & 0 & 3 & 2 & 2 & 2 & 0 & 2 & 0 & 3 \\ 1 & 0 & 3 & 0 & 3 & 0 & 1 & 2 & 0 & 3 & 0 & 2 & 0 & 3 \\ 0 & 1 & 0 & 3 & 3 & 0 & 1 & 3 & 0 & 0 & 3 & 3 & 3 & 3 \\ 1 & 0 & 1 & 0 & 3 & 0 & 0 & 3 & 3 & 1 & 0 & 3 & 3 & 3 \\ 0 & 3 & 2 & 3 & 3 & 0 & 1 & 3 & 0 & 2 & 3 & 3 & 3 & 3 \\ 1 & 0 & 3 & 0 & 3 & 0 & 0 & 3 & 3 & 3 & 0 & 3 & 3 & 3 \\ \hline \end{array}$ Hence according to the definition of $\mathscr C_5$, $(\text A 4)$ is not a [[Definition:Tautology (Formal Semantics)|tautology]]. Therefore $(\text A 4)$ is independent from $(\text A 1)$, $(\text A 2)$, $(\text A 3)$. {{qed}}	0
Consider the [[Definition:Categorical Statement|categorical statements]]: :$\map {\mathbf A} {S, P}: \quad$ The [[Definition:Universal Affirmative|universal affirmative]]: $\forall x: \map S x \implies \map P x$ :$\map {\mathbf I} {P, S}: \quad$ The [[Definition:Particular Affirmative|particular affirmative]]: $\exists x: \map P x \land \map S x$ Then: :$\map {\mathbf A} {S, P} \implies \map {\mathbf I} {P, S}$ {{iff}}: :$\exists x: \map S x$ Using the [[Definition:Symbolic Logic|symbology]] of [[Definition:Predicate Logic|predicate logic:]] :$\exists x: \map S x \iff \paren {\paren {\forall x: \map S x \implies \map P x} \implies \paren {\exists x: \map P x \land \map S x} }$ This law has the traditional name '''conversion per accidens of $\mathbf A$'''. Thus the $\mathbf A$ form '''converts per accidens''' to the $\mathbf I$ form.	0
: $\neg q \implies p \vdash \neg p \implies q$	0
We apply the [[Method of Truth Tables]] to the proposition. As can be seen by inspection, the [[Definition:Truth Value|truth values]] in the appropriate columns match. $\begin{array}{|c||cc|} \hline \top & \neg & \bot \\ \hline T & T & F \\ \hline \end{array}$ {{qed}}	0
We apply the [[Method of Truth Tables]] to the proposition. As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connective]] is [[Definition:True|true]] for all [[Definition:Boolean Interpretation|boolean interpretations]], proving a [[Definition:Tautology (Boolean Interpretations)|tautology]]. $\begin{array}{|ccccccc|} \hline (p & \implies & q) & \lor & (q & \implies & p) \\ \hline F & T & F & T & F & T & F \\ F & T & T & T & T & F & F \\ T & F & F & T & F & T & T \\ T & T & T & T & T & T & T \\ \hline \end{array}$ {{qed}}	0
Let $\mathbf 2$ be the [[Definition:Boolean Algebra|Boolean algebra]] [[Definition:Two (Boolean Algebra)|two]], and let $X$ be a [[Definition:Set|set]]. Let $\mathbf 2^X$ be the [[Definition:Set of All Mappings|set of all mappings]] $p: X \to \mathbf 2$. Define the operations $\vee$, $\wedge$ and $\neg$ on $\mathbf 2^X$ in [[Definition:Pointwise Operation|pointwise]] fashion thus: :$\vee: \mathbf 2^X \times \mathbf 2^X \to \mathbf 2^X, \left({p \vee q}\right) (x) := p (x) \vee q (x)$ :$\wedge: \mathbf 2^X \times \mathbf 2^X \to \mathbf 2^X, \left({p \wedge q}\right) (x) := p (x) \wedge q (x)$ :$\neg: \mathbf 2^X \to \mathbf 2^X, \left({\neg p}\right) (x) := \neg p (x)$ Furthermore, write $\bot$ and $\top$ for the [[Definition:Constant Mapping|constant mappings]] with these values, viz: :$\bot: X \to \mathbf 2, \bot (x) := \bot$ :$\top: X \to \mathbf 2, \top (x) := \top$ Then $\left({\mathbf 2^X, \vee, \wedge, \neg}\right)$ is a [[Definition:Boolean Algebra|Boolean algebra]], with $\bot$ and $\top$ as [[Definition:Identity Element|identities]] for $\vee$ and $\wedge$, respectively.	0
From [[Functionally Complete Logical Connectives/Negation and Conjunction|Functionally Complete Logical Connectives: Negation and Conjunction]], any boolean expression can be expressed in terms of $\land$ and $\neg$. From [[NAND with Equal Arguments]]: :$\neg p \dashv \vdash p \uparrow p$ From [[Conjunction in terms of NAND]]: :$p \land q \dashv \vdash \paren {p \uparrow q} \uparrow \paren {p \uparrow q}$ demonstrating that $p \land q$ is expressed solely in terms of $\uparrow$. Thus any boolean expression can be represented solely in terms of $\uparrow$. That is, $\set {\uparrow}$ is [[Definition:Functionally Complete|functionally complete]]. {{qed}}	0
{{BeginTableau|\left({p \vdash \left({q \land \neg q}\right)}\right) \vdash \neg p}} {{Premise|1|p \vdash \left({q \land \neg q}\right)}} {{Assumption|2|p}} {{SequentIntro|3|1, 2|q \land \neg q|1, 2|[[Definition:By Hypothesis|by hypothesis]]}} {{Simplification|4|1, 2|q|3|1}} {{Simplification|5|1, 2|\neg q|3|2}} {{NonContradiction|6|1, 2|4|5}} {{Contradiction|7|1|\neg p|2|6}} {{EndTableau}} {{Qed}}	0
Let $k \in \Z_{>0}$ be a [[Definition:Positive Integer|positive integer]]. Let $f: \Z_{>0} \to \Z_{>0}$ be the [[Definition:Mapping|mapping]] defined as: :$\forall m \in \Z_{>0}: \map f m = $ the [[Definition:Integer Addition|sum]] of the [[Definition:Cube (Algebra)|cubes]] of the [[Definition:Digit|digits]] of $n$. Let $n_0 \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]] which is a [[Definition:Multiple of Integer|multiple]] of $3$. Consider the sequence: :$s_n = \begin{cases} n_0 & : n = 0 \\ \map f {s_{n - 1} } & : n > 0 \end{cases}$ Then: :$\exists r \in \N_{>0}: s_r = 153$ That is, by performing $f$ repeatedly on a [[Definition:Multiple of Integer|multiple]] of $3$ eventually results in the [[Definition:Pluperfect Digital Invariant|pluperfect digital invariant]] $153$.	0
Let $S \subseteq \N$ be a [[Definition:Subset|subset]] of the [[Definition:Natural Numbers|natural numbers]]. Suppose that: :$(1): \quad 0 \in S$ :$(2): \quad \forall n \in \N : n \in S \implies n + 1 \in S$ Then: :$S = \N$	0
The [[Law of Excluded Middle]] can be symbolised by the [[Definition:Sequent|sequent]]: :$\vdash p \lor \neg p$	0
The [[Definition:Characteristic Function of Set|characteristic function]] $\chi_\N: \N \to \N$ is defined as: :$\forall n \in \N: \chi_\N \left({n}\right) = 1$. So: : $\chi_\N \left({n}\right) = f^1_1 \left({n}\right)$ The [[Constant Function is Primitive Recursive|constant function $f^1_1$ is primitive recursive]]. Hence the result. {{qed}} [[Category:Primitive Recursive Functions]] [[Category:Natural Numbers]] 10vvk2mb2nm8d17tlbp91j9l7rcdjpc	0
Defining $1$ as $\map s 0$ and $2$ as $\map s {\map s 0}$, the statement to be proven becomes: :$\map s 0 + \map s 0 = \map s {\map s 0}$ By the [[Definition:Addition/Peano Structure|definition of addition]]: :$\forall m \in P: \forall n \in P: m + \map s n = \map s {m + n}$ Letting $m = \map s 0$ and $n = 0$: {{begin-eqn}} {{eqn | n = 1 | l = \map s 0 + \map s 0 | r = \map s {\map s 0 + 0} }} {{end-eqn}} By the [[Definition:Addition/Peano Structure|definition of addition]]: :$\forall m: m + 0 = m$ Letting $m = \map s 0$: :$\map s 0 + 0 = \map s 0$ Taking the [[Definition:Successor Mapping on Natural Numbers|successor]] of both sides: {{begin-eqn}} {{eqn | n = 2 | l = \map s {\map s 0 + 0} | r = \map s {\map s 0} }} {{end-eqn}} Applying [[Equality is Transitive]] to $(1)$ and $(2)$ we have: :$\map s 0 + \map s 0 = \map s {\map s 0}$ Hence the result. {{qed}}	0
Let $\mathbf A$ be a [[Definition:WFF of Predicate Logic|WFF of predicate logic]]. Let $x \in \mathrm{VAR}$ be a [[Definition:Variable (Logic)|variable]]. Let $\tau$ be a [[Definition:Term (Predicate Logic)|term of predicate logic]] which is [[Definition:Freely Substitutable|freely substitutable]] for $x$ in $\mathbf A$. Let $\mathbf A \left({x \gets \tau}\right)$ be the [[Definition:Substitution Instance of Well-Formed Formula|substitution instance of $\mathbf A$ substituting $\tau$ for $x$]]. Let $\mathcal A$ be a [[Definition:Structure for Predicate Logic|structure for predicate logic]]. Let $\sigma$ be an [[Definition:Assignment for Structure|assignment]] for $\mathbf A$ and $\tau$. Suppose that: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\tau}\right) } \left[{\sigma}\right] = a$ where $\mathop{ \operatorname{val}_{\mathcal A} \left({\tau}\right) } \left[{\sigma}\right]$ is the [[Definition:Value of Term under Assignment|value of $\tau$ under $\sigma$]]. Then: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A \left({x \gets \tau}\right) }\right) } \left[{\sigma}\right] = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma + \left({x / a}\right)}\right]$ where $\sigma + \left({x / a}\right)$ is the [[Definition:Extension of Assignment|extension of $\sigma$ by mapping $x$ to $a$]].	0
:$(1): \quad p \vdash p \lor q$ :$(2): \quad q \vdash p \lor q$	0
Let $\mathbf A$ be a [[Definition:Semantic Consequence|semantic consequence]] of $\mathbf H$ for [[Definition:Boolean Interpretation|boolean interpretations]]. That is, if $v \models_{\mathrm{BI}} \mathbf H$, also $v \models_{\mathrm{BI}} \mathbf A$. By the [[Definition:Logical Not/Truth Function|truth function for $\neg$]], it follows that for such $v$: :$v \not\models_{\mathrm{BI}} \neg \mathbf A$ Therefore, $\mathbf H' := \mathbf H \cup \left\{{\mathbf A}\right\}$ is [[Definition:Unsatisfiable|unsatisfiable]] for [[Definition:Boolean Interpretation|boolean interpretations]]. Since $\mathbf H'$ is [[Definition:Countable Set|countable]], it follows from the [[Compactness Theorem for Boolean Interpretations]] that: :Some [[Definition:Finite Set|finite]] $\mathbf H'' \subseteq \mathbf H'$ is [[Definition:Unsatisfiable|unsatisfiable]]. By the [[Tableau Extension Lemma]], there exists a [[Definition:Finite Propositional Tableau|finite]] [[Definition:Finished Propositional Tableau|finished tableau]] $T$ for $\mathbf H''$. By definition of [[Definition:Finished Propositional Tableau|finished tableau]], every [[Definition:Branch (Graph Theory)|branch]] of $T$ is [[Definition:Finished Branch of Propositional Tableau|finished]] or [[Definition:Contradictory Branch|contradictory]]. From the [[Finished Branch Lemma/Corollary|Corollary to the Finished Branch Lemma]], $\Phi \left[{\Gamma}\right]$ is [[Definition:Satisfiable|satisfiable]] for any [[Definition:Finished Branch of Propositional Tableau|finished branch]] $\Gamma$. But since $\mathbf H'' \subseteq \Phi \left[{\Gamma}\right]$, this would imply $\mathbf H''$ is also [[Definition:Satisfiable|satisfiable]], which is a contradiction. It follows that every [[Definition:Branch (Graph Theory)|branch]] of $T$ is [[Definition:Contradictory Branch|contradictory]]. Since $\mathbf H'' \subseteq \mathbf H'$, replacing the [[Definition:Hypothesis Set|hypothesis set]] $\mathbf H'$ of $T$ with $\mathbf H''$ yields another [[Definition:Propositional Tableau|propositional tableau]] $T'$. Since every [[Definition:Branch (Graph Theory)|branch]] of $T'$ is [[Definition:Contradictory Branch|contradictory]], $T'$ is a [[Definition:Tableau Confutation|tableau confutation]] of $\mathbf H'$. Recalling that $\mathbf H' = \mathbf H \cup \left\{{\neg\mathbf A}\right\}$, we conclude that $T'$ is a [[Definition:Tableau Proof (Propositional Tableaus)|tableau proof]] of $\mathbf A$ from $\mathbf H$: :$\mathbf H \vdash_{\mathrm{PT}} \mathbf A$ {{qed}}	0
A '''logical argument''' (or just '''argument''') is a process of creating a new [[Definition:Statement|statement]] from one or more existing [[Definition:Statement|statements]]. An '''argument''' proceeds from a set of [[Definition:Premise|premises]] to a [[Definition:Conclusion|conclusion]], by means of [[Definition:Logical Implication|logical implication]], via a procedure called [[Definition:Logical Inference|logical inference]]. An '''argument''' may have more than one [[Definition:Premise|premise]], but only one [[Definition:Conclusion|conclusion]]. While [[Definition:Statement|statements]] may be classified as either '''[[Definition:True|true]]''' or '''[[Definition:False|false]]''', an '''argument''' may be classified as either [[Definition:Valid Argument|valid]] or [[Definition:Invalid Argument|invalid]].	0
Let $\mathscr H$ be [[Definition:Hilbert Proof System/Instance 1|instance 1 of a Hilbert proof system]]. Let $\mathrm{BI}$ be the [[Definition:Formal Semantics of Boolean Interpretations|formal semantics of boolean interpretations]]. Then $\mathscr H$ is a [[Definition:Sound Proof System|sound proof system]] for $\mathrm{BI}$: :Every [[Definition:Theorem (Formal Systems)|$\mathscr H$-theorem]] is a [[Definition:Tautology (Boolean Interpretations)|tautology]].	0
:$\neg p \vdash p \implies q$	0
This proof is done by constructing a model using a method known as a Henkin construction. This results in a model all of whose elements are the interpretations of constant symbols from some language. The construction for this proof in particular is done so that the theory this model satisfies asserts that each tuple of constants fails to satisfy at least one $\phi$ from each type $p_i$. As a result, the model we construct will not realize any of the types. We will prove the theorem for the special case where we only omit one type. That is, assume $\left\{{p_i: i \in \N}\right\}$ is $\left\{{p}\right\}$ for some $p$. Comments on the general case will be made after the proof. Our goal is to eventually be able to apply [[Maximal Finitely Satisfiable Theory with Witness Property is Satisfiable]] to a suitable $\mathcal L^*$-theory $T^*$, where $\mathcal L^*$ contains $\mathcal L$ and $T^*$ contains $T$. Let $\mathcal L^*$ be $\mathcal L \cup \left\{{c_i : i \in \N}\right\}$ where each $c_i$ is a new constant symbol. Note that $\mathcal L^*$ is still countable. We will recursively construct $T^*$ by adding a sentence $\theta_k$ at each stage. In order to ensure that it has the necessary properties, we will construct it in stages, each of which has three steps. The goal of the first step of each stage is ensure that $T^*$ will be maximal upon completion of the definition. The goal of the second step of each stage is to ensure that $T^*$ will satisfy the [[Definition:Witness Property|witness property]]. The goal of the third step of each stage is to ensure that $p$ is not realized by any tuple of constants. Since $\mathcal L^*$ is countable, the set of $\mathcal L$-formulas is countable, and hence in particular, the $\mathcal L^*$-sentences can be listed as $\phi_0, \phi_1, \ldots$ Similarly, since $\mathcal L^*$ is countable, the set of $n$-tuples of its constant symbols can be listed as $\bar d_0, \bar d_1, \dots$ === Recursive definition of $T^*$ === We now recursively define sets $T_k$ whose union will be $T^*$. ==== Stage $k = 0$ ==== Let $\theta_0$ be any $\mathcal L$-sentence which is a [[Definition:Tautology|tautology]]; for example, $\forall x : x = x$. Let $T_0 = T \cup \left\{{\theta_0}\right\}$. Note that $T_0$ is satisfiable since $T$ is satisfiable, and $T \models \theta_0$ trivially. Suppose that $\theta_k$ and $T_k$ have been defined and $T_k$ is satisfiable. We now handle the three steps of the $k$-th stage as mentioned above. ==== Stage $k + 1 = 3 i + 1$ ==== In this step we ensure that $T^*$ will be maximal. If $T_k \cup \left\{{\phi_i}\right\}$ is satisfiable, let $\theta_{k+1}$ be $\phi_i$. Otherwise, since $T_k$ is satisfiable, $T_k \cup \left\{{\neg \phi_i}\right\}$ must be satisfiable. In this case we let $\theta_{k+1}$ be $\neg \phi_i$. Let $T_{k+1} = T_k \cup \left\{{\theta_{k+1} }\right\}$. ==== Stage $k + 1 = 3 i + 2$ ==== In this step we ensure that $T^*$ will have the witness property. Suppose that: :$T_k \models \phi_j$ for some set of $j \le i$ and that: :$\phi_j$ are each of the form $\exists v \psi_j \left({v}\right)$. Let $\theta_{k+1}$ be $\bigwedge \psi \left({c}\right)$ where $c$ is a new constant which doesn't appear in $T_k$. This is possible since $T_k$ only has finitely many $\mathcal L^*$-sentences which are not $\mathcal L$-sentences. Let $T_{k+1} = T_k \cup \{\theta_{k+1}\}$. Since $T_k$ is satisfiable and $T_k \models \bigwedge \exists v \psi_j \left({v}\right)$, any model $\mathcal M$ of $T_k$ has an element $a_j$ such that $\mathcal M \models \psi_j (a_j)$. By interpreting $c_j$ as $a_j$, we can view $\mathcal M$ as a model of $T_{k+1}$. Thus $T_{k+1}$ is satisfiable. If $T_k$ doesn't satisfy any existential statements $\phi_j$ for $j \le i$, let $\theta_{k+1}$ be $\theta_k$ and let $T_{k+1}$ be $T_{k}$. ==== Stage $k + 1 = 3 i + 3$ ==== In this step we ensure that $p$ will be omitted by any model of $T^*$. Let $(e_1,\dots,e_n)$ be the $i$-th $n$-tuple of constants $\bar d_i$. We will add a sentence to ensure that $\bar d_i$ fails to realize $p$. Let $\psi$ be the $\mathcal L$-formula obtained from $\theta_0 \wedge \cdots \theta_k$ by: :$(1): \quad$ replacing [[Definition:Occurrence (Predicate Logic)|occurrences]] of the constants $e_1, \dotsc, e_n$ with variables $v_1, \dotsc, v_n$ :$(2): \quad$ replacing occurrences of $c_i$ besides $e_1, \dotsc, e_n$ by variables $v_{c_i}$ other than $v_1, \dotsc, v_n$ and finally: :$(3): \quad$ adding an existential quantifier $\exists v_{c_i}$ for each of the $c_i$ that were replaced other than $e_1, \dotsc, e_n$. This results in $\psi$ being an $\mathcal L$-formula with $n$ free variables. The free variables are those that correspond to the $e_i$. Since $p$ is not isolated, there must be some $\phi\in p$ such that: :$T \not \models \forall \bar v \left({\psi \left({\bar v}\right) \to \phi \left({\bar v}\right)}\right)$ Thus: :$T \models \exists \bar v \left({\psi \left({\bar v}\right) \wedge \neg \phi \left({\bar v}\right)}\right)$ Let $\theta_{k+1}$ be $\neg \phi \left({\bar d_i}\right)$. Let $T_{k+1} = T_k \cup \left\{ {\theta_{k+1} }\right\}$. Since $T$ is satisfiable, it has a model $\mathcal M$. By the above: :$\mathcal M \models \exists \bar v \left({\psi \left({\bar v}\right) \wedge \neg \phi \left({\bar v}\right)}\right)$ Hence there is some tuple $\bar a$ in $\mathcal M$ such that: :$\mathcal M \models \psi \left({\bar a}\right) \wedge \neg \phi \left({\bar a}\right)$ Let the constants $e_1, \dotsc, e_n$ be interpreted as the constants in the tuple $\bar a$. Let the the constants $c_i$ other than $e_1, \dotsc, e_n$ be interpreted as the elements from $\mathcal M$ which satisfy the existential quantifiers in $\psi \left({\bar a}\right)$. Then we have that $\mathcal M$ can be viewed as a model of $\theta_0 \wedge \dotsb \wedge \theta_k \wedge \neg \phi \left({\bar d_i}\right)$ as well. Thus $T_{k+1}$ is satisfiable. === Verification of properties of $T^*$ === Now, let $\displaystyle T^* = \bigcup_{k \mathop \in \N} T_k$, or equivalently: :$T^* = T \cup \left\{ {\theta_k: k \in \N}\right\}$ We verify that $T^*$ is a maximal finitely satisfiable theory with the witness property. Thus we may apply [[Maximal Finitely Satisfiable Theory with Witness Property is Satisfiable]] which is the theorem that will construct our desired model. ==== $T^*$ is finitely satisfiable ==== Let $\Delta$ be a finite subset of $T^*$. Then $\Delta$ is contained in one of the $T_k$. But by construction each $T_k$ was satisfiable. Heence by the [[Compactness Theorem]] $\Delta$ is satisfiable. ==== $T^*$ is a maximal theory ==== Let $\phi$ is an $\mathcal L^*$-sentence. Then it is some $\phi_i$. Hence either $\phi$ or $\neg \phi$ is contained in $T_{3i + 1}$. ==== $T^*$ has the witness property ==== Let $\psi \left({v}\right)$ be an $\mathcal L^*$-formula with one free variable. Then $\exists v \psi \left({v}\right)$ is an $\mathcal L^*$-sentence and hence is some $\phi_j$. Suppose $T^*\models \exists v \psi \left({v}\right)$. There must be a finite subset $\Delta$ of $T^*$ such that $\Delta \models \exists v \psi \left({v}\right)$. Otherwise, by the [[Compactness Theorem]], since $\Delta \cup \left\{ {\neg \exists v \psi \left({v}\right)}\right\}$ is always satisfiable, $T^* \cup \left\{ {\neg \exists v \psi \left({v}\right)}\right\}$ is satisfiable. Since such a $\Delta$ must be contained in one of the $T_k$, there is some $T_k$ such that $T_k \models \exists v \psi \left({v}\right)$. Since each $T_k$ is contained in $T_{k+1}$, we can assume that $k = 3 i + 2$ for some $i \le j$. But, $T_{3 i + 2}$ contains $\psi \left({c}\right)$ for some constant $c$ since $j \le i$ and $T_{3i+2}\models \phi_j$. === Verification that the constructed model omits $p$ === Recall that $\mathcal L$ was assumed to be countable. Thus $\mathcal L^*$ is countable as well. Thus, by [[Maximal Finitely Satisfiable Theory with Witness Property is Satisfiable]] applied to $T^*$: :there is an $\mathcal L^*$-structure $\mathcal M$ such that $\mathcal M \models T^*$ :every element of $\mathcal M$ is the interpretation of one of the constant symbols from $\mathcal L^*$ and consequently: :$\mathcal M$ is at most countable. We verify that $\mathcal M$ omits $p$. That is, every $n$-tuple of elements in $\mathcal M$ fails some sentence in $p$. Let $\left({a_1, \dotsc, a_n}\right)$ be an $n$-tuple of elements from $\mathcal M$. Then each of $a_1, \dotsc, a_n$ is the interpretation of constant symbol $e_1, \dotsc, e_n$ respectively from $\mathcal L^*$. In turn, $\left({e_1, \dotsc, e_n}\right)$ is one of the $\bar d_i$. But $T_{3 i + 3} \models \neg \phi \left({\bar d_i}\right)$ for some $\phi \in p$. Thus $\mathcal M \models \neg \phi \left({a_1, \dots, a_n}\right)$ for some $\phi \in p$. Hence, $\mathcal M$ omits $p$. === Generalization to countable set of non-isolated types === The above proof can be generalized by extending stage $k + 1 = 3i + 3$. As it is written above, we force $\bar d_i$ to fail to satisfy some $\phi$ in a single type $p$. But suppose we list the pairs $\left\{ {\left({\bar d_\alpha, p_\beta}\right) : \alpha, \beta \in \N}\right\}$ as a set $\left\{ {\pi_i: i \in \N}\right\}$. This is possible since the set of such pairs is countable. Then at stage $3 i + 3$, where $\pi_i = \left({\bar d_\alpha, p_\beta}\right)$, we can force $\bar d_\alpha$ to fail to satisfy some $\phi$ in $p_\beta$. This is achieved analogously to how it is done in the above proof. This ensures that for each $\bar d_\alpha$ with $\alpha \in \N$, and for each $p_\beta$, eventually some $T_k$ includes a sentence $\neg \phi \left({\bar d_\alpha}\right)$ for some $\phi \in p_\beta$. That is, eventually some $T_k$ ensures that $\bar d_\alpha$ does not realize $p_\beta$. {{qed}} {{finish}} [[Category:Model Theory]] [[Category:Named Theorems]] g07qco0li1v0r7459nvaquev67uvukv	0
Firstly, we will prove that $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$, where $n \in \N$. Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: : $\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ === Basis for the Induction === $\map P 1$ is true, as this says $\displaystyle \frac {\sin z} z = \frac {\sin z} z$. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 0$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i}$ Then we need to show: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \paren {\frac {2^k} z} \sin \frac z {2^k} \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Sine of X over X as Infinite Product#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {\frac {2^k} z} \paren {2 \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = [[Double Angle Formulas/Sine|Double Angle Formula for Sine]] }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \cos \frac z {2^{k + 1} } \prod_{i \mathop = 1}^k \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\frac {2^{k + 1} } z} \sin \frac z {2^{k + 1} } \prod_{i \mathop = 1}^{k + 1} \cos \frac z {2^i} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \frac {\sin z} z = \paren {\frac {2^n} z} \sin \frac z {2^n} \prod_{i \mathop = 1}^n \cos \frac z {2^i}$ And then: {{begin-eqn}} {{eqn | l = \frac {\sin z} z | r = \lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } \prod_{i \mathop = 1}^n \cos \frac z {2^i} | c = }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {\frac {2^n} z} \paren {\sin \frac z {2^n} } } \prod_{i \mathop = 1}^{\infty} \cos \frac z {2^i} | c = }} {{eqn | r = \paren 1 \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = [[Limit of Sine of X over X]] }} {{eqn | r = \prod_{i \mathop = 1}^\infty \cos \frac z {2^i} | c = }} {{end-eqn}} {{qed}}	0
Let $S \left({\MM}\right)$ denote the [[Definition:Set|set]] containing all [[Definition:Complete Type|complete types]] over $M$ of every number of [[Definition:Free Variable|free variables]]. Let $\kappa = \left\vert{S \left({\MM}\right)}\right\vert$. Use a [[Definition:Bijection|bijection]] between $\kappa$ and $S \left({\MM}\right)$ to write the [[Definition:Element|elements]] of $S \left({\MM}\right)$ as $p_\alpha$ for $\alpha < \kappa$. For each $\alpha < \kappa$, let $N_\alpha$ be an [[Definition:Elementary Extension|elementary extension]] of $\MM$ which has an [[Definition:Ordered Tuple|ordered tuple]] $\bar a_\alpha$ [[Definition:Realization|realizing]] $p_\alpha$. Such [[Definition:Elementary Extension|extensions]] and [[Definition:Ordered Tuple|tuples]] exist by [[Type is Realized in some Elementary Extension]]. We will construct the [[Definition:Elementary Extension|extension]] claimed by the theorem as the [[Definition:Union|union]] over a [[Definition:Chain of Elementary Extensions|chain of elementary extensions]] defined using [[Transfinite Induction]]. At each step, we use the [[Elementary Amalgamation Theorem]] to add on $N_\alpha$. Base case $\alpha = 0$: Let $\BB_0 = \MM$. Note that $\BB_0$ is an [[Definition:Elementary Extension|elementary extension]] of $\MM$ by choice of $\NN_0$. [[Definition:Limit Ordinal|Limit ordinals]] $\alpha \le \kappa$: Let $\displaystyle \BB_\alpha = \bigcup_{\beta \mathop < \alpha} \BB_\beta$ $\BB_\alpha$ is an [[Definition:Elementary Extension|elementary extension]] of $\MM$ by [[Union of Elementary Chain is Elementary Extension]]. [[Definition:Successor Ordinal|Successor ordinals]] $\alpha + 1$ for $\alpha < \kappa$: We have that $\BB_\alpha$ and $\NN_\alpha$ are both [[Definition:Elementary Extension|elementary extensions]] of $\MM$. By the [[Elementary Amalgamation Theorem]], there is: :an [[Definition:Elementary Extension|elementary extension]] $\BB_{\alpha + 1}$ of $\BB_\alpha$ and: :an [[Definition:Elementary Embedding|elementary embedding]] $g_\alpha: \NN_\alpha \to \BB_{\alpha + 1}$ which is the [[Definition:Identity Mapping|identity]] on $M$ viewed as a [[Definition:Subset|subset]] of $\NN_\alpha$. Note that $\map {g_\alpha} {\bar a_\alpha}$ [[Definition:Realization|realizes]] $p_\alpha$ in $\BB_{\alpha + 1}$ since $g_\alpha$ is [[Definition:Elementary Embedding|elementary]]. Thus $\BB_{\alpha + 1}$ is an [[Definition:Elementary Extension|elementary extension]] of $\MM$ which contains $\map {g_\alpha} {\bar a_\alpha}$ [[Definition:Realization|realizing]] $p_\alpha$. Now, let $\displaystyle \BB = \bigcup_{\alpha < \kappa} \BB_\alpha$. $\BB$ is an [[Definition:Elementary Extension|elementary extension]] of each $\BB_\alpha$ by [[Union of Elementary Chain is an Elementary Extension]]. In particular, $\BB$ is an [[Definition:Elementary Extension|elementary extension]] of $\BB_0 = \MM$ We have that: :each $p_\alpha$ is [[Definition:Realization|realized]] in $\BB_{\alpha + 1}$ by the corresponding $\map {g_\alpha} {\bar a_\alpha}$ and: :$\BB$ is an [[Definition:Elementary Extension|elementary extension]] of each $\BB_{\alpha + 1}$ Thus we have that $\map {g_\alpha} {\bar a_\alpha}$ [[Definition:Realization|realizes]] $p_\alpha$ in $\BB$. {{qed}} [[Category:Model Theory]] nydyqgv6evfgf48xvhhom5vczk5retb	0
The '''(rule of the) hypothetical syllogism''' is a [[Definition:Valid Argument|valid]] deduction [[Definition:Sequent|sequent]] in [[Definition:Propositional Logic|propositional logic]]: :If we can conclude that $p$ implies $q$, and if we can also conclude that $q$ implies $r$, then we may infer that $p$ implies $r$.	0
We apply the [[Method of Truth Tables]] to the propositions in turn. As can be seen for all [[Definition:Boolean Interpretation|boolean interpretations]] by inspection, where the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connective]] on the {{LHS}} (that is, the rightmost $\land$) is $T$, that under the instance of $r$ on the {{RHS}} is also $T$: $\begin{array}{|ccccccccc||c|} \hline ((p & \implies & q) & \land & (q & \implies & r)) & \land & p & r \\ \hline F & T & F & T & F & T & F & F & F & F \\ F & T & F & T & F & T & T & F & F & T \\ F & T & T & F & T & F & F & F & F & F \\ F & T & T & T & T & T & T & F & F & T \\ T & F & F & F & F & T & F & F & T & F \\ T & F & F & F & F & T & T & F & T & T \\ T & T & T & F & T & F & F & F & T & F \\ T & T & T & T & T & T & T & T & T & T \\ \hline \end{array}$ Hence the result. {{qed}}	0
:$p \uparrow q \dashv \vdash q \uparrow p$	0
This is an immediate consequence of [[Semantic Consequence of Superset]]. {{qed}}	0
{{BeginTableau|\forall x: \neg \map P x \vdash \neg \exists x: \map P x}} {{Premise|1|\forall x: \neg \map P x}} {{Assumption|2|\exists x: \map P x}} {{TableauLine|n = 3|pool = 2|f = \map P {\mathbf a}|rlnk = Existential Instantiation|rtxt = Existential Instantiation|dep = 2|c = for an arbitrary $\mathbf a$}} {{TableauLine|n = 4|pool = 1|f = \neg \map P {\mathbf a}|rlnk = Universal Instantiation|rtxt = Universal Instantiation|dep = 3}} {{NonContradiction|5|1, 2|3|4}} {{Contradiction|6|1|\neg \exists x: \map P x|2|5}} {{EndTableau|lemma}} {{BeginTableau|\neg \exists x: \map P x \vdash \forall x: \neg \map P x}} {{Premise|1|\neg \exists x: \map P x}} {{Assumption|2|\map P {\mathbf a}|for some arbitrary $\mathbf a$}} {{TableauLine|n = 3|pool = 2|f = \exists x: \map P x|rlnk = Existential Generalisation|rtxt = Existential Generalisation|dep = 2}} {{NonContradiction|4|1, 2|1|3}} {{Contradiction|5|1|\neg \map P {\mathbf a}|2|4}} {{TableauLine|n = 6|pool = 1|f = \forall x: \neg \map P x|rlnk = Universal Generalisation|rtxt = Universal Generalisation|dep = 5|c = as $\mathbf a$ was arbitrary}} {{EndTableau|qed}} {{Namedfor|Augustus De Morgan|cat = De Morgan}}	0
Let $\struct {S, \vee, \wedge, \neg}$ be a [[Definition:Boolean Algebra|Boolean algebra]]. Let $a, b, c \in S$. Let: {{begin-eqn}} {{eqn | l = a \wedge c | r = b \wedge c }} {{eqn | l = a \wedge \neg c | r = b \wedge \neg c }} {{end-eqn}} Then: : $a = b$	0
Let $n \in \Z: n \ge 1$. Let $n$ be expressed in [[Definition:Binary Notation|binary notation]]: :$n = 2^{e_1} + 2^{e_2} + \cdots + 2^{e_r}$ where $e_1 > e_2 > \cdots > e_r \ge 0$. Let $n!$ be the [[Definition:Factorial|factorial]] of $n$. Then $n!$ is [[Definition:Divisor of Integer|divisible]] by $2^{n-r}$, but not by $2^{n-r+1}$.	0
{{BeginTableau|\vdash \neg p \implies \left({p \implies q}\right)}} {{Assumption|1|\neg p}} {{SequentIntro|2|1|p \implies q|1|[[False Statement implies Every Statement/Formulation 1|False Statement implies Every Statement: Formulation 1]]}} {{Implication|3||\neg p \implies \left({p \implies q}\right)|1|2}} {{EndTableau}} {{qed}}	0
This follows directly from: * [[Kleene's Normal Form Theorem]]; * [[Universal URM Computable Functions]]. {{qed}} [[Category:URM Programs]] 557hxwrpy6pu9y2am1yzmuwcl59xj3w	0
Let $p \implies q$ be a [[Definition:Conditional|conditional]]. Then the [[Definition:Inverse Statement|inverse]] of $p \implies q$ is the [[Definition:Contrapositive Statement|contrapositive]] of its [[Definition:Converse Statement|converse]].	0
Let $\downarrow$ signify the [[Definition:Logical NOR|NOR]] operation. Then there exist [[Definition:Proposition|propositions]] $p,q,r$ such that: :$p \downarrow \left({q \downarrow r}\right) \not \vdash \left({p \downarrow q}\right) \downarrow r$ That is, [[Definition:Logical NOR|NOR]] is not [[Definition:Associative|associative]].	0
Let $\LL$ be a [[Definition:Logical Language|logical language]]. Let $\mathscr M$ be a [[Definition:Formal Semantics|formal semantics]] for $\LL$. Let $\FF$ be a [[Definition:Set|set]] of [[Definition:Logical Formula|logical formulas]] from $\LL$. Let $\phi$ be an [[Definition:Semantic Consequence|$\mathscr M$-semantic consequence]] of $\FF$. Let $\FF'$ be another [[Definition:Set|set]] of [[Definition:Logical Formula|logical formulas]]. Then: :$\FF \cup \FF' \models_{\mathscr M} \phi$ that is, $\phi$ is also a [[Definition:Semantic Consequence|semantic consequence]] of $\FF \cup \FF'$.	0
{{BeginTableau|p \oplus \bot \vdash p}} {{Premise|1|p \oplus \bot}} {{SequentIntro|2|1|\left({p \lor \bot} \right) \land \neg \left({p \land \bot}\right)|1| {{Defof|Exclusive Or}} }} {{SequentIntro|3|1|p \land \neg \left({p \land \bot}\right)|2|[[Disjunction with Contradiction]]}} {{SequentIntro|4|1|p \land \neg \bot|3|[[Conjunction with Contradiction]]}} {{SequentIntro|5|1|p \land \top|4|[[Tautology is Negation of Contradiction]]}} {{SequentIntro|6|1|p|5|[[Conjunction with Tautology]]}} {{EndTableau}} {{qed|lemma}} {{BeginTableau|p \vdash p \oplus \bot}} {{Premise|1|p}} {{SequentIntro|2|1|p \land \top|1|[[Conjunction with Tautology]]}} {{SequentIntro|3|1|\left({p \lor \bot}\right) \land \top|2|[[Disjunction with Contradiction]]}} {{SequentIntro|4|1|\left({p \lor \bot}\right) \land \neg \bot|3|[[Tautology is Negation of Contradiction]]}} {{SequentIntro|5|1|\left({p \lor \bot}\right) \land \neg \left({p \land \bot}\right)|4|[[Conjunction with Contradiction]]}} {{SequentIntro|6|1|p \oplus \bot|5| {{Defof|Exclusive Or}} }} {{EndTableau}} {{qed}}	0
We note that: : $n < m \iff m \mathop{\dot -} n > 0$ : $n \ge m \iff m \mathop{\dot -} n = 0$ So it can be seen that the [[Definition:Characteristic Function of Relation|characteristic function]] of $<$ is given by: :$\chi_< \left({n, m}\right) = \operatorname{sgn} \left({m \mathop{\dot -} n}\right)$ So $\chi_{<}$ is defined by [[Definition:Substitution (Mathematical Logic)|substitution]] from: : the [[Signum Function is Primitive Recursive|primitive recursive function $\operatorname{sgn}$]] : the [[Cut-Off Subtraction is Primitive Recursive|primitive recursive function $\dot -$]] Thus $\chi_<$ is [[Definition:Primitive Recursive Function|primitive recursive]]. So $<$ is a [[Definition:Primitive Recursive Relation|primitive recursive relation]]. Next we see that $n \le m \iff n < m \lor n = m$ from the definition of [[Definition:Strictly Precede|strictly precedes]]. From [[Equality Relation is Primitive Recursive]], we have that $=$ is [[Definition:Primitive Recursive Relation|primitive recursive]]. From above, we have that $<$ is [[Definition:Primitive Recursive Relation|primitive recursive]]. Thus $\le$ is [[Definition:Primitive Recursive Relation|primitive recursive]] from [[Set Operations on Primitive Recursive Relations]]. We could use the same reasoning for $>$ and $\ge$ but there's a different approach. Note that $n \le m \iff n \not > m$, and so $>$ is [[Definition:Primitive Recursive Relation|primitive recursive]] from [[Set Operations on Primitive Recursive Relations]]. Finally the same applies to $\ge$. Hence the result. {{qed}} [[Category:Primitive Recursive Functions]] j4jqu1qfaeq6n6rjpbyvsxw36m8yess	0
:$\vdash \left({p \iff q}\right) \iff \left({\left({p \implies q}\right) \land \left({q \implies p}\right)}\right)$	0
: $\neg p \vdash p \implies q$	0
{{BeginTableau|\left ({p \land q}\right) \implies r \vdash p \implies \left ({q \implies r}\right)}} {{Premise|1|\left ({p \land q}\right) \implies r}} {{Assumption|2|p}} {{Assumption|3|q}} {{Conjunction|4|2, 3|p \land q|2|3}} {{ModusPonens|5|1, 2, 3|r|1|4}} {{Implication|6|1, 2|q \implies r|3|5}} {{Implication|7|1|p \implies \left ({q \implies r}\right)|2|6}} {{EndTableau}} {{qed}}	0
Let $\mathbf H$ be a [[Definition:Countable Set|countable set]] (either [[Definition:Finite Set|finite]] or [[Definition:Countably Infinite|infinite]]) of [[Definition:WFF of Propositional Logic|WFFs of propositional logic]]. The following [[Definition:Statement|statements]] are [[Definition:Logical Equivalence|logically equivalent]]: :$(1): \quad$ $\mathbf H$ has a [[Definition:Model (Boolean Interpretations)|model]]. :$(2): \quad$ $\mathbf H$ is [[Definition:Consistent Set of Formulas|consistent]] for the [[Definition:Proof System of Propositional Tableaus|proof system of propositional tableaus]]. :$(3): \quad$ $\mathbf H$ has no [[Definition:Tableau Confutation|tableau confutation]].	0
: $p \land q \vdash \neg \left({\neg p \lor \neg q}\right)$	0
From [[Functionally Complete Logical Connectives/Negation and Conjunction|Functionally Complete Logical Connectives: Negation and Conjunction]], $\set {\neg, \land}$ is [[Definition:Functionally Complete|functionally complete]]. That is: any expression can be expressed in terms of $\neg$ and $\land$. From [[De Morgan's Laws (Logic)/Conjunction|De Morgan's laws: Conjunction]], we have that: :$p \land q \dashv \vdash \neg \paren {\neg p \lor \neg q}$ Thus all occurrences of $\land$ can be replaced by $\lor$ and $\neg$. Thus any expression can be expressed in terms of $\neg$ and $\lor$. That is: $\set {\neg, \lor}$ is [[Definition:Functionally Complete|functionally complete]]. {{qed}}	0
:$p \land q \dashv \vdash q \land p$	0
It is sufficient to consider the case $a_n = 1$: :$\displaystyle \map P x = \prod_{k \mathop = 1}^n \paren {x - z_k}$ The proof proceeds by [[Principle of Mathematical Induction|induction]]. Let $\map {\Bbb P} n$ be the statement that the identity below holds for all sets $\set {z_1, \ldots, z_n}$. {{begin-eqn}} {{eqn | l = \prod_{j \mathop = 1}^n \paren {x - z_j} | r = x^n + \sum_{j \mathop = 1}^n \paren {-1}^{n - j + 1} e_{n - j + 1} \paren {\set {z_1, \ldots, z_n} } \, x^{j - 1} }} {{eqn | r = x^n + \paren {-1} \, e_1 \paren {\set {z_1, \ldots, z_n} } \, x^{n - 1} + \paren {-1}^2 \, e_2 \paren {\set {z_1, \ldots, z_n} } \, x^{n - 2} + \cdots + \paren {-1}^n e_n \paren {\set {z_1, \ldots, z_n} } }} {{end-eqn}} [[Definition:Basis for the Induction|Basis for the Induction]]: $\map {\Bbb P} 1$ holds because $\map {e_1} {\set {z_1} } = z_1$. [[Definition:Induction Step|Induction Step]] $\map {\Bbb P} n$ implies $\map {\Bbb P} {n + 1}$: Assume $\map {\Bbb P} n$ holds and $n \ge 1$. Let for given values $\set {z_1, \ldots, z_n, z_{n + 1} }$: :$\displaystyle \map Q x = \paren {x - z_{n + 1} } \prod_{k \mathop = 1}^n \paren {x - z_k}$ Expand the right side product above using induction hypothesis $\map {\Bbb P} n$. Then $\map Q x$ equals $x^{n + 1}$ plus terms for $x^{j - 1}$, $1 \le j \le n + 1$. If $j = 1$, then one term occurs for $x^{j - 1}$: :$\displaystyle \paren {-x_{n + 1} } \, \paren {\paren {-1}^{n - 1 + 1} \map {e_{n - 1 + 1} } {\set {z_1, \ldots, z_n} } x^{1 - 1} } = \paren {-1}^{n + 1} \map {e_n} {\set {z_1, \ldots, z_n, z_{n + 1} } }$ If $2 \le j \le n + 1$, then two terms $T_1$ and $T_2$ occur for $x^{j - 1}$: {{begin-eqn}} {{eqn | l = T_1 | r = \paren x \paren {\paren {-1}^{n - j + 2} \, \map {e_{n - j + 2} } {\set {z_1, \ldots, z_n} } x^{j - 2} } }} {{eqn | l = T_2 | r = \paren {-z_{n + 1 } } \, \paren {\paren {-1}^{n - j + 1} \, \map {e_{n - j + 1} } {\set {z_1, \ldots, z_n} } x^{j - 1} } }} {{end-eqn}} The coefficient $c$ of $x^{j - 1}$ for $2 \le j \le n + 1$ is: {{begin-eqn}} {{eqn | l = c | r = \dfrac {T_1 + T_2} {x^{j - 1} } }} {{eqn | r = \paren {-1}^m \, \map {e_m} {\set {z_1, \ldots, z_n} } + \paren {-1}^m \, \map {e_{m - 1} } {\set {z_1, \ldots, z_n} } z_{n + 1} | c = where $m = n - j + 2$. }} {{end-eqn}} Use [[Elementary Symmetric Function/Examples/Recursion|recursion identity]] to simplify the expression for $c$: {{begin-eqn}} {{eqn | l = \map {e_m} {\set {z_1, \ldots, z_n, z_{n + 1} } } | r = z_{n + 1} \, \map {e_{m - 1} } {\set {z_1, \ldots, z_n} } + \map {e_m} {\set {z_1, \ldots, z_n} } }} {{eqn | ll= \leadsto | l = c | r = \paren {-1}^{n - j + 2} \, \map {e_{n - j + 2} } {\set {z_1, \ldots, z_n, z_{n + 1} } } }} {{end-eqn}} Thus $\map {\Bbb P} {n + 1}$ holds and the induction is complete. Set equal the two identities for $\map P x$: :$\displaystyle x^n + \sum_{k \mathop = 0}^{n - 1} a_k x^k = x^n + \paren {-1} \, \map {e_1} {\set {z_1, \ldots, z_n} } x^{n - 1} + \paren {-1}^2 \, \map {e_2} {\set {z_1, \ldots, z_n} } x^{n - 2} + \cdots + \paren {-1}^n \map {e_n} {\set {z_1, \ldots, z_n} }$ Linear independence of the powers $1, x, x^2, \ldots$ implies polynomial coefficients match left and right. Hence the coefficient $a_k$ of $x^k$ on the {{LHS}} matches $\paren {-1}^{n - k} \, \map {e_{n - k} } {\set {z_1, \ldots, z_n} }$ on the {{RHS}}. {{qed}} {{proofread}}	0
By the [[Definition:Definitional Abbreviation|definitional abbreviation]] for the [[Definition:Conditional|conditional]]: :$\mathbf A \implies \mathbf B =_{\text{def}} \neg \mathbf A \lor \mathbf B$ the [[Rule of Commutation/Disjunction/Formulation 2/Forward Implication|Rule of Commutation]] can be written as: :$\neg \left({p \lor q}\right) \lor \left({q \lor p}\right)$ This evaluates as follows: :$\begin{array}{|cccc|c|ccc|} \hline \neg & (p & \lor & q) & \lor & (q & \lor & p) \\ \hline 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 2 & 0 & 2 & 0 & 0 \\ 1 & 0 & 0 & 3 & 0 & 3 & 0 & 0 \\ 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\ 0 & 1 & 1 & 1 & 0 & 1 & 1 & 1 \\ 3 & 1 & 2 & 2 & 0 & 2 & 2 & 1 \\ 0 & 1 & 3 & 3 & 0 & 3 & 3 & 1 \\ 1 & 2 & 0 & 0 & 0 & 0 & 0 & 2 \\ 3 & 2 & 2 & 1 & 0 & 1 & 2 & 2 \\ 3 & 2 & 2 & 2 & 0 & 2 & 2 & 2 \\ 1 & 2 & 0 & 3 & 0 & 3 & 0 & 2 \\ 1 & 3 & 0 & 0 & 0 & 0 & 0 & 3 \\ 0 & 3 & 3 & 1 & 0 & 1 & 3 & 3 \\ 1 & 3 & 0 & 2 & 0 & 2 & 0 & 3 \\ 0 & 3 & 3 & 3 & 0 & 3 & 3 & 3 \\ \hline \end{array}$ {{qed}} [[Category:Formal Semantics]] emlja2kbsygf1qleayueqncwq850mjn	0
All [[Primitive Recursive Function is URM Computable|primitive recursive functions are URM computable]]. The set of $\mathbf U$ of [[URM Programs are Countably Infinite|URM programs is countably infinite]]. The set of $\Bbb F$ of [[Natural Number Functions are Uncountable|natural number functions is uncountably infinite]]. Hence there is no [[Definition:Surjection|surjection]] from $\mathbf U \to \Bbb F$. Hence $\mathbf U \subsetneq \Bbb F$. Hence $\exists f \in \Bbb F: f \notin \mathbf U$. {{qed}} [[Category:Primitive Recursive Functions]] 0urfc305jilu8erji55syap7v41nfft	0
:$p \lor \left ({p \land q}\right) \dashv \vdash p$	0
Let $S_n$ denote the $n$th [[Definition:Fibonacci String|Fibonacci string]]. Then: :$(1):\quad$ There are no instances of $2$ $\text a$'s together :$(2):\quad$ There are no instances of $3$ $\text b$'s together in $S_n$.	0
{{BeginTableau |\vdash \paren {p \implies q} \implies \paren {\neg q \implies \neg p} }} {{Assumption |1|p \implies q}} {{Assumption |2|\neg q}} {{ModusTollens |3|1, 2|\neg p|1|2}} {{Implication |4|1|\neg q \implies \neg p|2|3}} {{Implication |5||\paren {p \implies q} \implies \paren {\neg q \implies \neg p}|1|4}} {{EndTableau}} {{Qed}}	0
A '''unary logical connective''' (or '''one-place connective''') is a [[Definition:Logical Connective|connective]] whose effect on its [[Definition:Compound Statement|compound statement]] is determined by the [[Definition:Truth Value|truth value]] of ''one'' [[Definition:Substatement of Compound Statement|substatement]]. In standard [[Definition:Aristotelian Logic|Aristotelian logic]], there are four of these. The only non-trivial one is [[Definition:Logical Not|logical not]], as shown on [[Unary Truth Functions]].	0
{{BeginTableau|p \implies q, r \implies s \vdash \neg q \lor \neg s \implies \neg p \lor \neg r}} {{Premise|1|p \implies q}} {{Premise|2|r \implies s}} {{Assumption|3|\neg q \lor \neg s}} {{Assumption|4|\neg q}} {{ModusTollens|5|1, 4|\neg p|1|4}} {{Addition|6|1, 4|\neg p \lor \neg r|5|1}} {{Assumption|7|\neg s}} {{ModusTollens|8|2, 7|\neg r|2|7}} {{Addition|9|2, 7|\neg p \lor \neg r|8|2}} {{ProofByCases|10|1, 2, 3|\neg p \lor \neg r|3|4|6|7|9}} {{Implication|11|1, 2|\neg q \lor \neg s \implies \neg p \lor \neg r|3|10}} {{EndTableau}} {{qed}}	0
{{:De Morgan's Laws (Logic)/Conjunction of Negations/Formulation 1}}	0
: $\left({\neg \left({p \lor q}\right)}\right) \implies \left({\neg p \land \neg q}\right)$	0
Apply the [[Method of Truth Tables]]: :$\begin{array}{|ccccc||ccccc|} \hline p & \downarrow & (q & \downarrow & r) & (p & \downarrow & q) & \downarrow & r \\ \hline F & F & F & T & F & F & T & F & F & F \\ F & T & F & F & T & F & T & F & F & T \\ F & T & T & F & F & F & F & T & T & F \\ F & T & T & F & T & F & F & T & F & T \\ T & F & F & T & F & T & F & F & T & F \\ T & F & F & F & T & T & F & F & F & T \\ T & F & T & F & F & T & F & T & T & F \\ T & F & T & F & T & T & F & T & F & T \\ \hline \end{array}$ As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] do not match for all [[Definition:Boolean Interpretation|boolean interpretations]]. {{qed}}	0
Let $n = \left({n_1, n_2, \ldots, n_{i-1}, n_i \ldots, n_k}\right)$. We see that: :$g \left({n_1, n_2, \ldots, n_k}\right) = f \left({\operatorname{pr}^k_1 \left({n}\right), \operatorname{pr}^k_2 \left({n}\right), \ldots, \operatorname{pr}^k_{i-1} \left({n}\right), f_a \left({n}\right), \operatorname{pr}^k_i \left({n}\right), \ldots, \operatorname{pr}^k_k \left({n}\right)}\right)$ We have that: * $\operatorname{pr}^k_j$ is a [[Definition:Basic Primitive Recursive Function/Projection Function|basic primitive recursive function]] for all $j$ such that $1 \ne j \le k$ * [[Constant Function is Primitive Recursive|$f_a$ is a primitive recursive function]]. So $g$ is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from [[Definition:Primitive Recursive Function|primitive recursive functions]] and so is [[Definition:Primitive Recursive Function|primitive recursive]]. {{qed}} [[Category:Primitive Recursive Functions]] d9j44atmv91ffg8o7430zpvybkrx5pk	0
We apply the [[Method of Truth Tables]]. As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|cccc||ccccccccc|} \hline \neg & (p & \iff & q) & (\neg & p & \land & q) & \lor & (p & \land & \neg & q) \\ \hline F & F & T & F & T & F & F & F & F & F & F & T & F \\ T & F & F & T & T & F & T & T & T & F & F & F & T \\ T & T & F & F & F & T & F & F & T & T & T & T & F \\ F & T & T & T & F & T & F & T & F & T & F & F & T \\ \hline \end{array}$ {{qed}}	0
:$\vdash \paren {q \implies r} \implies \paren {\paren {p \implies q} \implies \paren {p \implies r} }$	0
We apply the [[Method of Truth Tables]]. As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|ccccc||ccccc|} \hline p & \oplus & (q & \oplus & r) & (p & \oplus & q) & \oplus & r \\ \hline F & F & F & F & F & F & F & F & F & F \\ F & T & F & T & T & F & F & F & T & T \\ F & T & T & T & F & F & T & T & T & F \\ F & F & T & F & T & F & T & T & F & T \\ T & T & F & F & F & T & T & F & T & F \\ T & F & F & T & T & T & T & F & F & T \\ T & F & T & T & F & T & F & T & F & F \\ T & T & T & F & T & T & F & T & T & T \\ \hline \end{array}$ {{qed}}	0
{{begin-eqn}} {{eqn | lo= \forall n \in \Z_{>0}: | l = F_n | r = \sum_{k \mathop = 0}^{\floor {\frac {n - 1} 2} } \dbinom {n - k - 1} k | c = }} {{eqn | r = \binom {n - 1} 0 + \binom {n - 2} 1 + \binom {n - 3} 2 + \dotsb + \binom {n - j} {j - 1} + \binom {n - j - 1} j | c = where $j = \floor {\frac {n - 1} 2}$ }} {{end-eqn}}	0
Let $D$ be a [[Definition:Minimal (Model Theory)|strongly minimal]] set in $\mathcal M$. Let $A$ be a [[Definition:Subset|subset]] of $D$. Let $b, c \in D$. If $b$ is [[Definition:Algebraic (Model Theory)|algebraic]] over $A \cup \left\{ {c}\right\}$ but not over $A$, then $c$ is [[Definition:Algebraic (Model Theory)|algebraic]] over $A \cup \left\{ {b}\right\}$.	0
: $\vdash \left({\neg q \implies p}\right) \implies \left({\neg p \implies q}\right)$	0
An '''argumentum ad baculum''' is a [[Definition:Logical Argument|logical argument]] that, rather than [[Definition:Proof|prove]] or present evidence for a claim, threatens any who dare argue with the person or group making the claim. The presence of such a threat is often a good reason to avoid ''stating'' an opposing view, but manifestly does not support the [[Definition:True|truth]] or [[Definition:False|falsity]] of the claim under consideration.	0
The result follows directly from the [[Definition:Truth Table|truth table]] for the [[Definition:Biconditional|biconditional]]: $\begin{array}{|cc||ccc|} \hline p & q & p & \iff & q \\ \hline F & F & F & T & F \\ F & T & F & F & T \\ T & F & F & F & F \\ T & T & F & T & T \\ \hline \end{array}$ By inspection, it is seen that $\mathcal M \left({p \iff q}\right) = T$ precisely when $\mathcal M \left({p}\right) = \mathcal M \left({q}\right)$. {{qed}}	0
The [[Definition:Eluding Game|eluding game]] has no [[Definition:Saddle Point (Game Theory)|saddle point]].	0
{{BeginTableau|p \iff \bot \vdash \neg p}} {{Premise|1|p \iff \bot}} {{BiconditionalElimination|2|1|p \implies \bot|1|1}} {{SequentIntro|3|1|\neg p|2|[[Contradictory Consequent]]}} {{EndTableau}} {{qed|lemma}} {{BeginTableau|\neg p \vdash p \iff \bot}} {{Assumption|1|\neg p}} {{SequentIntro|2|1|p \implies \bot|1|[[Contradictory Consequent]]}} {{TopIntro|3}} {{SequentIntro|4||\bot \implies p|3|[[Contradictory Antecedent]]}} {{BiconditionalIntro|5|1|p \iff \bot|2|4}} {{EndTableau}} {{qed}}	0
{{begin-eqn}} {{eqn | o = | r = \map {\mathbf I} {S, P} | c = }} {{eqn | ll= \leadsto | o = | r = \exists x: \map S x \land \map P x | c = {{Defof|Particular Affirmative}} }} {{eqn | ll= \leadsto | o = | r = \exists x: \map P x \land \map S x | c = [[Conjunction is Commutative]] }} {{eqn | ll= \leadsto | o = | r = \map {\mathbf I} {P, S} | c = {{Defof|Particular Affirmative}} }} {{end-eqn}} {{qed}}	0
A '''contradiction''' is a [[Definition:Statement|statement]] which is ''always [[Definition:False|false]]'', independently of any relevant circumstances that could theoretically influence its [[Definition:Truth Value|truth value]]. This has the form: :$p \land \neg p$ or, equivalently: :$\neg p \land p$ that is: :'''$p$ is [[Definition:True|true]] [[Definition:Conjunction|and, at the same time]], $p$ is [[Definition:False|not true]].'''	0
{{BeginTableau|p \land q \vdash q \land p}} {{Premise|1|p \land q}} {{Simplification|2|1|p|1|1}} {{Simplification|3|1|q|1|2}} {{Conjunction|4|1|q \land p|3|2}} {{EndTableau}} {{qed|lemma}} {{BeginTableau|q \land p \vdash p \land q}} {{Premise|1|q \land p}} {{Simplification|2|1|q|1|1}} {{Simplification|2|1|p|1|2}} {{Conjunction|4|1|p \land q|3|2}} {{EndTableau}} {{qed}}	0
{{BeginTableau|p \implies \left({q \land r}\right) \vdash \left({p \implies q}\right) \land \left({p \implies r}\right)}} {{Premise|1|p \implies \left({q \land r}\right)}} {{Assumption|2|p}} {{ModusPonens|3|1, 2|q \land r|1|2}} {{Simplification|4|1, 2|q|3|1}} {{Simplification|5|1, 2|r|3|2}} {{Implication|6|1|p \implies q|2|4}} {{Implication|7|1|p \implies r|2|5}} {{Conjunction|8|1|\left({p \implies q}\right) \land \left({p \implies r}\right)|6|7}} {{EndTableau}} {{qed}}	0
Consider the statement: :'''Socrates is a man.''' This means: :'''The [[Definition:Object|object]] named Socrates has the [[Definition:Property|property]] of being a man.''' Thus we see that '''is''' here means '''has the property of being'''. In this context, '''is''' here is called '''the ''is'' of predication'''.	0
Denote with $\bar{\mathbf A}$ and $\bar U$ the [[Definition:Logical Complement|logical complement]] of a [[Definition:WFF of Propositional Logic|WFF]] $\mathbf A$ and [[Definition:Set|set]] $U$, respectively. === Necessary Condition === We aim to prove this result using the [[Second Principle of Mathematical Induction]], applied to the minimal length of a [[Definition:Formal Proof|formal proof]] of $U$. Suppose first that $U$ is an [[Definition:Axiom (Formal Systems)|axiom]] of $\mathscr G$. Then $U$ contains a [[Definition:Complementary Pair|complementary pair]] of [[Definition:Literal|literals]]. Hence, so does $\bar U$, by definition of [[Definition:Logical Complement|logical complement]]. Therefore, the [[Definition:Semantic Tableau|semantic tableau]] comprising only a [[Definition:Root Node|root node]] labeled $\bar U$ is [[Definition:Closed Tableau|closed]]. Next, suppose that the last step in proving $U$ was an instance of the [[Definition:Gentzen Proof System/Instance 1/Alpha-Rule|$\alpha$-rule]]. That is, for some [[Definition:Alpha-Formula|$\alpha$-formula]] $\mathbf A$ and corresponding $\mathbf A_1, \mathbf A_2$: :$U = U_1 \cup U_2 \cup \left\{{\mathbf A}\right\}$ where $U_1 \cup \left\{{\mathbf A_1}\right\}$ and $U_2 \cup \left\{{\mathbf A_2}\right\}$ are [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorems]]. By induction hypothesis, the [[Definition:Logical Complement|logical complements]]: :$\bar U_1 \cup \left\{{\bar{\mathbf A}_1}\right\}$ :$\bar U_2 \cup \left\{{\bar{\mathbf A}_@}\right\}$ of these [[Definition:Theorem (Formal Systems)|theorems]] have [[Definition:Closed Tableau|closed tableaus]]. From the [[Soundness Theorem for Semantic Tableaus]], it follows that these sets are [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. Now by [[Superset of Unsatisfiable Set is Unsatisfiable]], so are: :$\bar U' := \bar U_1 \cup \bar U_2 \cup \left\{{\bar{\mathbf A}_1}\right\}$ :$\bar U'' := \bar U_1 \cup \bar U_2 \cup \left\{{\bar{\mathbf A}_1}\right\}$ By the [[Completeness Theorem for Semantic Tableaus]], these have [[Definition:Closed Tableau|closed tableaus]]. Since $\mathbf A$ is an [[Definition:Alpha-Formula|$\alpha$-formula]], it is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\mathbf A_1 \land \mathbf A_2$. By [[De Morgan's Laws (Logic)/Disjunction of Negations|De Morgan's Laws]], it follows that: :$\bar{\mathbf A}$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\bar{\mathbf A}_1 \lor \bar{\mathbf A}_2$. Therefore, $\bar{\mathbf A}$ is a [[Definition:Beta-Formula|$\beta$-formula]], and $\bar{\mathbf A}_1, \bar{\mathbf A}_2$ correspond to it as in the [[Definition:Table of Beta-Formulas|table of $\beta$-formulas]]. This allows the [[Semantic Tableau Algorithm]] to expand a [[Definition:Leaf Node|leaf]] labeled $\bar U$ into two leaves labeled $\bar U'$ and $\bar U''$. Because $\bar U'$ and $\bar U''$ have [[Definition:Closed Tableau|closed tableaus]], so does $\bar U$. Finally, suppose that the last step in proving $U$ was an instance of the [[Definition:Gentzen Proof System/Instance 1/Beta-Rule|$\beta$-rule]]. That is, for some [[Definition:Beta-Formula|$\beta$-formula]] $\mathbf B$ and corresponding $\mathbf B_1, \mathbf B_2$: :$U = U_1 \cup \left\{{\mathbf B}\right\}$ where $U' := U_1 \cup \left\{{\mathbf B_1, \mathbf B_2}\right\}$ is a [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]]. By induction hypothesis, the [[Definition:Logical Complement|logical complement]]: :$\bar U' = \bar U_1 \cup \left\{{\bar{\mathbf B}_1, \bar{\mathbf B}_2}\right\}$ of this [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]] has a [[Definition:Closed Tableau|closed tableau]]. Since $\mathbf B$ is a [[Definition:Beta-Formula|$\beta$-formula]], it is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\mathbf B_1 \lor \mathbf B_2$. By [[De Morgan's Laws (Logic)/Conjunction of Negations|De Morgan's Laws]], it follows that: :$\bar{\mathbf B}$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\bar{\mathbf B}_1 \land \bar{\mathbf B}_2$. Therefore, $\bar{\mathbf B}$ is an [[Definition:Alpha-Formula|$\alpha$-formula]], and $\bar{\mathbf B}_1, \bar{\mathbf B}_2$ correspond to it as in the [[Definition:Table of Alpha-Formulas|table of $\alpha$-formulas]]. This allows the [[Semantic Tableau Algorithm]] to expand a [[Definition:Leaf Node|leaf]] labeled $\bar U$ into a leaf labeled $\bar U'$. Because $\bar U'$ has a [[Definition:Closed Tableau|closed tableau]], so does $\bar U$. The result follows by the [[Second Principle of Mathematical Induction]]. {{qed|lemma}} === Sufficient Condition === Let $T$ be a [[Definition:Closed Tableau|closed tableau]] for $\bar U$. We prove that $U$ is a [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]] by the [[Second Principle of Mathematical Induction]], applied to the number of [[Definition:Node (Graph Theory)|nodes]] of $T$. Suppose $T$ has only one [[Definition:Node (Graph Theory)|node]]. Then $\bar U$ has to contain a [[Definition:Complementary Pair|complementary pair]] of [[Definition:Literal|literals]] for $T$ to be [[Definition:Closed Tableau|closed]]. But then $U$ also contains a [[Definition:Complementary Pair|complementary pair]]. Hence, it is an [[Definition:Axiom (Formal Systems)|axiom]] of $\mathscr G$. If $T$ has more than one [[Definition:Node (Graph Theory)|node]], the first iteration of the [[Semantic Tableau Algorithm]] must have selected either an [[Definition:Alpha-Formula|$\alpha$-formula]] $\mathbf A$ or a [[Definition:Beta-Formula|$\beta$-formula]] $\mathbf B$ from $\bar U$. First the case that an [[Definition:Alpha-Formula|$\alpha$-formula]] $\mathbf A$ was selected. Define, for convenience, $\bar U' = \bar U \setminus \left\{{\mathbf A}\right\}$. Then the rest of $T$ is a [[Definition:Semantic Tableau|semantic tableau]] for $\bar U' \cup \left\{{\mathbf A_1, \mathbf A_2}\right\}$. This [[Definition:Semantic Tableau|tableau]] has fewer [[Definition:Node (Graph Theory)|nodes]] than $T$. For $T$ to be [[Definition:Closed Tableau|closed]], this [[Definition:Subtree|subtree]] must also be a [[Definition:Closed Tableau|closed tableau]]. By induction hypothesis, this means that $U' \cup \left\{{\bar{\mathbf A}_1, \bar{\mathbf A}_2}\right\}$ is a [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]]. Now, since $\mathbf A$ is an [[Definition:Alpha-Formula|$\alpha$-formula]], it is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\mathbf A_1 \land \mathbf A_2$. Hence by [[De Morgan's Laws (Logic)/Disjunction of Negations|De Morgan's Laws]], $\bar{\mathbf A}$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\bar{\mathbf A}_1 \lor \bar{\mathbf A}_2$. That is, $\bar{\mathbf A}$ is a [[Definition:Beta-Formula|$\beta$-formula]], so that the [[Definition:Gentzen Proof System/Instance 1/Beta-Rule|$\beta$-rule]] may be applied to it. Hence, $U = U' \cup \left\{{\bar{\mathbf A}}\right\}$ is a [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]], as desired. Now the case that a [[Definition:Beta-Formula|$\beta$-formula]] $\mathbf B$ was selected. Define, for convenience, $\bar U' = \bar U \setminus \left\{{\mathbf B}\right\}$. Then the rest of our [[Definition:Closed Tableau|closed tableau]] $T$ comprises two [[Definition:Semantic Tableau|semantic tableaus]] for $\bar U' \cup \left\{{\mathbf B_1}\right\}$ and $\bar U' \cup \left\{{\mathbf B_2}\right\}$, respectively. Each of these has fewer [[Definition:Node (Graph Theory)|nodes]] than $T$. For $T$ to be [[Definition:Closed Tableau|closed]], these [[Definition:Subtree|subtrees]] must also be [[Definition:Closed Tableau|closed tableaus]]. By induction hypothesis, this means that $U' \cup \left\{{\bar{\mathbf B}_1}\right\}$ and $U' \cup \left\{{\bar{\mathbf B}_2}\right\}$ are [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorems]]. Now, since $\mathbf B$ is a [[Definition:Beta-Formula|$\beta$-formula]], it is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\mathbf B_1 \lor \mathbf B_2$. Hence by [[De Morgan's Laws (Logic)/Conjunction of Negations|De Morgan's Laws]], $\bar{\mathbf B}$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|equivalent]] to $\bar{\mathbf B}_1 \land \bar{\mathbf B}_2$. That is, $\bar{\mathbf B}$ is an [[Definition:Alpha-Formula|$\alpha$-formula]], so that the [[Definition:Gentzen Proof System/Instance 1/Alpha-Rule|$\alpha$-rule]] may be applied to it. Hence, $U = U' \cup \left\{{\bar{\mathbf B}}\right\}$ is a [[Definition:Theorem (Formal Systems)|$\mathscr G$-theorem]], as desired. The result now follows from the [[Second Principle of Mathematical Induction]]. {{qed}}	0
Because $a_n = O(b_n)$, there exists $M\geq0$ and $n_0 \in\N$ such that $|a_n| \leq M \cdot |b_n|$ for $n\geq n_0$. Because $n_k$ [[Definition:Divergent Sequence|diverges]], there exists $k_0\in\N$ such that $n_k\geq n_0$ for $k\geq k_0$. Then $|a_{n_k}| \leq M\cdot |b_{n_k}|$ for $k\geq k_0$. Thus $a_{n_k} = O(b_{n_k})$. {{qed}} [[Category:Asymptotic Notation]] q917p4cyyh16q4kjx1g18nd1hkq82oy	0
The proof proceeds by [[Second Principle of Mathematical Induction|strong induction]]. For all $n \in \Z_{\ge 3}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$S_n$ has $F_{n - 2}$ instances of $\text a$ and $F_{n - 1}$ instances of $\text b$. === Basis for the Induction === $\map P 3$ is the case: :$S_n = \text {ba}$ It can be seen that $S_n$ has $F_1 = 1$ instance of $\text a$ and $F_2 = 1$ instance of $\text b$. Thus $\map P 3$ is seen to hold. $\map P 4$ is the case: :$S_n = \text {bab}$ It can be seen that $S_n$ has $F_2 = 1$ instance of $\text a$ and $F_3 = 2$ instances of $\text b$. Thus $\map P 4$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P j$ is true, for all $j$ such that $4 \le j \le k$, then it logically follows that $\map P {k + 1}$ is true. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$S_k$ has $F_{k - 2}$ instances of $\text a$ and $F_{k - 1}$ instances of $\text b$ and: :$S_{k - 1}$ has $F_{k - 3}$ instances of $\text a$ and $F_{k - 2}$ instances of $\text b$ from which it is to be shown that: :$S_{k + 1}$ has $F_{k - 1}$ instances of $\text a$ and $F_k$ instances of $\text b$. === Induction Step === This is the [[Definition:Induction Step|induction step]]: By definition of [[Definition:Fibonacci String|Fibonacci string]]: :$S_{k + 1} = S_k S_{k - 1}$ [[Definition:Concatenation (Formal Systems)|concatenated]]. By the [[Count of a's and b's in Fibonacci String#Induction Hypothesis|induction hypothesis]]: :$S_k$ has $F_{k - 2}$ instances of $\text a$ and $F_{k - 1}$ instances of $\text b$ and: :$S_{k - 1}$ has $F_{k - 3}$ instances of $\text a$ and $F_{k - 2}$ instances of $\text b$ So: :$S_{k + 1}$ has $F_{k - 2} + F_{k - 3}$ instances of $\text a$ and so by definition of [[Definition:Fibonacci Number|Fibonacci numbers]]: :$S_{k + 1}$ has $F_{k - 1}$ instances of $\text a$ and: :$S_{k + 1}$ has $F_{k - 1} + F_{k - 2}$ instances of $\text b$ and so by definition of [[Definition:Fibonacci Number|Fibonacci numbers]]: :$S_{k + 1}$ has $F_k$ instances of $\text b$. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Second Principle of Mathematical Induction]]. Therefore: :for all $n \in \Z$ such that $n \ge 3$: $S_n$ has $F_{n - 2}$ instances of $\text a$ and $F_{n - 1}$ instances of $\text b$. {{qed}}	0
Let $\mathcal M$ be an $\mathcal L$-[[Definition:First-Order Structure|structure]]. Let $\kappa$ be a [[Definition:Cardinal|cardinal]]. If $\mathcal M$ is $\kappa$-[[Definition:Big Model|big]], then it is $\kappa$-[[Definition:Saturated Model|saturated]].	0
: $p \implies \left({q \land r}\right) \vdash \left({p \implies q}\right) \land \left({p \implies r}\right)$	0
Proceed by the [[Principle of Structural Induction]] applied to the [[Definition:Bottom-Up Specification of Predicate Logic|bottom-up specification of predicate logic]]. If $\mathbf A = p \left({\tau_1, \ldots, \tau_n}\right)$, then: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right] = p_{\mathcal A} \left({ \mathop{ \operatorname{val}_{\mathcal A} \left({\tau_1}\right) } \left[{\sigma}\right], \ldots, \mathop{ \operatorname{val}_{\mathcal A} \left({\tau_n}\right) } \left[{\sigma}\right] }\right)$ Because $\mathbf A$ contains no [[Definition:Quantifier|quantifiers]], all its variables are [[Definition:Free Variable|free]], and hence are in the [[Definition:Domain of Mapping|domain]] of $\sigma, \sigma'$ as [[Definition:Assignment for Formula|assignments]]. Thus $\sigma, \sigma'$ are [[Definition:Assignment for Term|assignments]] for each $\tau_i$, and by [[Value of Term under Assignment Determined by Variables]]: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\tau_i}\right) } \left[{\sigma}\right] = \mathop{ \operatorname{val}_{\mathcal A} \left({\tau_i}\right) } \left[{\sigma'}\right]$ for each $\tau_i$. It is immediate that: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right] = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma'}\right]$ If $\mathbf A = \neg \mathbf B$ and the induction hypothesis applies to $\mathbf B$, then: {{begin-eqn}} {{eqn|l = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right] |r = f^\neg \left({ \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma}\right] }\right) |c = Definition of [[Definition:Value of Formula under Assignment|value under $\sigma$]] }} {{eqn|r = f^\neg \left({ \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma'}\right] }\right) |c = Induction Hypothesis }} {{eqn|r = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma'}\right] |c = Definition of [[Definition:Value of Formula under Assignment|value under $\sigma'$]] }} {{end-eqn}} If $\mathbf A = \mathbf B \circ \mathbf B'$ for $\circ$ one of $\land, \lor, \implies, \iff$ and the induction hypothesis applies to $\mathbf B, \mathbf B'$: {{begin-eqn}} {{eqn|l = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right] |r = f^\circ \left({ \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma}\right], \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B'}\right) } \left[{\sigma}\right] }\right) |c = Definition of [[Definition:Value of Formula under Assignment|value under $\sigma$]] }} {{eqn|r = f^\circ \left({ \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma'}\right], \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B'}\right) } \left[{\sigma'}\right] }\right) |c = Induction Hypothesis }} {{eqn|r = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma'}\right] |c = Definition of [[Definition:Value of Formula under Assignment|value under $\sigma'$]] }} {{end-eqn}} If $\mathbf A = \exists x: \mathbf B$ or $\mathbf A = \forall x : \mathbf B$, and the induction hypothesis applies to $\mathbf B$, then from the definition of [[Definition:Value of Formula under Assignment|value under $\sigma$]]: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right]$ is determined by the [[Definition:Value of Formula under Assignment|values]]: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma + \left({x / a}\right)}\right]$ where $a$ ranges over $\mathcal A$, and $\sigma + \left({x / a}\right)$ is the [[Definition:Extension of Assignment|extension]] of $\sigma$ mapping $x$ to $a$. Now, for a [[Definition:Free Variable|free variable]] $y$ of $\mathbf B$: {{begin-eqn}} {{eqn|l = \left({\sigma + \left({x / a}\right)}\right) \left({y}\right) |r = \begin{cases} a &: \text{if $y = x$} \\ \sigma \left({y}\right) &: \text{otherwise} \end{cases} |c = Definition of [[Definition:Extension of Assignment|Extension]] }} {{eqn|r = \begin{cases} a &: \text{if $y = x$} \\ \sigma' \left({y}\right) &: \text{otherwise} \end{cases} |c = Assumption on $\sigma, \sigma'$ }} {{eqn|r = \left({\sigma' + \left({x / a}\right)}\right) \left({y}\right) |c = Definition of [[Definition:Extension of Assignment|Extension]] }} {{end-eqn}} Hence, by the induction hypothesis: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma + \left({x / a}\right)}\right] = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf B}\right) } \left[{\sigma' + \left({x / a}\right)}\right]$ It follows that: :$\mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma}\right] = \mathop{ \operatorname{val}_{\mathcal A} \left({\mathbf A}\right) } \left[{\sigma'}\right]$ The result follows from the [[Principle of Structural Induction]]. {{qed}} [[Category:Predicate Logic]] 72pyybgluhk8a7ghrccyz86nqc2qh1q	0
We can define $p$ recursively by: :$p \left({n + 1}\right) = \text{the smallest } y \in \N \text { such that } y \text { is prime and } p \left({n}\right) \le y$ Hence we can express it as: :$p \left({n + 1}\right) = \mu y \left({\chi_\Bbb P \left({y}\right) = 1 \land p \left({n}\right) \le y}\right)$ where: * $\chi_\Bbb P \left({y}\right)$ is the [[Definition:Characteristic Function of Set|characteristic function]] of the [[Definition:Set|set]] of [[Definition:Prime Number|prime numbers]] $\Bbb P$ * $\mu y \left({\mathcal R}\right)$ means '''the smallest $y \in \N$ such that the [[Definition:Relation|relation]] $\mathcal R$ holds'''. {{questionable|Sure about the less-than-or equal-to at the end of those expressions? Surely it should be less-than? See talk page.}} Now consider the [[Definition:Relation|relation]] $\mathcal S$ given by: :$\mathcal S \left({m, y}\right) \iff \chi_\Bbb P \left({y}\right) = 1$. We have a reason for making $\mathcal S$ a binary relation, even though $m$ is not actually invoked in its definition. Then we have: :$\chi_\mathcal S \left({m, y}\right) = \chi_{\operatorname{eq}} \left({\chi_\Bbb P \left({y}\right), 1}\right)$. So $\chi_\mathcal S$ is [[Definition:Primitive Recursive Function|primitive recursive]] as it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: * the [[Equality Relation is Primitive Recursive|primitive recursive function $\chi_{\operatorname{eq}}$]] * the [[Set of Prime Numbers is Primitive Recursive|primitive recursive function $\chi_\Bbb P$]]. Then we have that [[Ordering Relations are Primitive Recursive|$<$ is primitive recursive]]. So we define the [[Definition:Relation|relation]] $\mathcal R$ by: :$\mathcal R \left({m, y}\right) \iff \mathcal S \left({m, y}\right) \land m < y \iff \chi_\Bbb P \left({y}\right) = 1 \land m < y$. This is [[Definition:Primitive Recursive Relation|primitive recursive]] from the above and [[Set Operations on Primitive Recursive Relations]]. Now let us define the [[Definition:Function|function]] $g: \N^2 \to \N$ as: :$g \left({m, z}\right) = \mu y \le z \left({\chi_\Bbb P \left({y}\right) = 1 \land m < y}\right)$ which is [[Definition:Primitive Recursive Function|primitive recursive]] by [[Bounded Minimization is Primitive Recursive]]. We note that $g \left({p \left({n}\right), z}\right) = p \left({n + 1}\right)$ as long as $p \left({n + 1}\right) \le z$. Next, let $h: \N \to \N$ be defined as $h \left({n}\right) = \exp \left({2, \exp \left({2, n}\right)}\right)$. Then $h$ is [[Definition:Primitive Recursive Function|primitive recursive]] since it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: * the [[Exponentiation is Primitive Recursive|primitive recursive function $\exp$]]; * the [[Constant Function is Primitive Recursive|constant $2$]]. From [[Upper Bounds for Prime Numbers]], we have $p \left({n+1}\right) \le 2^{2^n} = h \left({n}\right)$. It follows that: :$p \left({n+1}\right) = g \left({p \left({n}\right), h \left({n}\right)}\right)$ where $g$ and $h$ are both [[Definition:Primitive Recursive Function|primitive recursive]]. So using the definition of $p$ as given above, we have: :$p \left({0}\right) = 1$ :$p \left({n+1}\right) = g \left({p \left({n}\right), h \left({n}\right)}\right)$. So $p$ is defined by [[Definition:Primitive Recursion|primitive recursion]] from the [[Constant Function is Primitive Recursive|constant $1$]] and the [[Definition:Primitive Recursive Function|primitive recursive functions]] $g$ and $h$. {{qed}}	0
=== Sufficient Condition === Let $\exists x: \map S x$. Let $\map {\mathbf A} {S, P}$ be [[Definition:True|true]]. As $\map {\mathbf A} {S, P}$ is [[Definition:True|true]], then by [[Modus Ponendo Ponens]]: :$\map P x$ From the [[Rule of Conjunction/Proof Rule|Rule of Conjunction]]: :$\map S x \land \map P x$ Thus $\map {\mathbf I} {S, P}$ holds. So by the [[Rule of Implication]]: :$\map {\mathbf A} {S, P} \implies \map {\mathbf I} {S, P}$ {{qed|lemma}} === Necessary Condition === Let $\map {\mathbf A} {S, P} \implies \map {\mathbf I} {S, P}$. Suppose: :$\neg \exists x: \map S x$ that is, $\map S x$ is [[Definition:Vacuous Truth|vacuous]]. From [[De Morgan's Laws (Predicate Logic)/Denial of Existence|De Morgan's Laws: Denial of Existence]]: :$\forall x: \neg \map S x \dashv \vdash \neg \exists x: \map S x$ it follows that $\forall x: \map S x$ is [[Definition:False|false]]. From [[False Statement implies Every Statement]]: :$\forall x: \map S x \implies \map P x$ is [[Definition:True|true]]. So $\map {\mathbf A} {S, P}$ holds. Again, $\neg \exists x: \map S x$. Then by the [[Rule of Conjunction/Proof Rule|Rule of Conjunction]]: :$\neg \paren {\exists x: \map S x \land \map P x}$ That is, $\mathbf I$ does not hold. So $\map {\mathbf A} {S, P}$ is [[Definition:True|true]] and $\map {\mathbf I} {S, P}$ is [[Definition:False|false]]. This [[Proof by Contradiction|contradicts]] $\map {\mathbf A} {S, P} \implies \map {\mathbf I} {S, P}$ by definition of [[Definition:Conditional|implication]]. Thus $\exists x: \map S x$ must hold. {{qed}}	0
Let $\mathcal R$ be a [[Definition:Relation|$k+1$-ary relation]] on $\N^{k+1}$. Then the [[Definition:Function|function]] $g: \N^{k+1} \to \N$ defined as: :$g \left({n_1, n_2, \ldots, n_k, z}\right) = \mu y \ \mathcal R \left({n_1, n_2, \ldots, n_k, y}\right)$ where $\mu y \ \mathcal R \left({n_1, n_2, \ldots, n_k, y}\right)$ is the [[Definition:Minimization/Relation|minimization operation on $\mathcal R$]] is equivalent to [[Definition:Minimization/Relation|minimization]] on a [[Definition:Function|total function]].	0
We note that $m \divides n \iff n = q m$ where $q \in \Z$. So we see that $m \divides n \iff \map \rem {n, m} = 0$ (see [[Remainder is Primitive Recursive]]). Thus we define the [[Definition:Function|function]] $\operatorname{div}: \N^2 \to \N$ as: :$\map {\operatorname {div} } {n, m} = \map {\chi_{\operatorname {eq} } } {\map \rem {n, m}, 0}$ where $\map {\chi_{\operatorname {eq} } } {n, m}$ denotes the [[Definition:Characteristic Function of Relation|characteristic function]] of the equality relation. So we have: :$\map {\operatorname {div} } {n, y} = \begin{cases} 1 & : y \divides n \\ 0 & : y \nmid n \end{cases}$ So $\map {\operatorname {div} } {n, m}$ is defined by [[Definition:Substitution (Mathematical Logic)|substitution]] from: :the [[Remainder is Primitive Recursive|primitive recursive function $\rem$]] :the [[Equality Relation is Primitive Recursive|primitive recursive relation $\operatorname {eq}$]] :the [[Constant Function is Primitive Recursive|constants]] $1$ and $0$. Thus $\operatorname {div}$ is [[Definition:Primitive Recursive Function|primitive recursive]]. Hence the result. {{qed}} [[Category:Primitive Recursive Functions]] 8wsr1duvmpkzioguxnb1iq0ghwdqwlv	0
{{TFAE|def = Semantic Equivalence for Boolean Interpretations}} Let $\mathbf A, \mathbf B$ be [[Definition:WFF of Propositional Logic|WFFs of propositional logic]].	0
'''Mathematical logic''' is a sub-branch of [[Definition:Symbolic Logic|symbolic logic]] in which the foundations of the assumptions upon which rest mathematics itself are investigated and made rigorous.	0
We apply the [[Method of Truth Tables]] to the proposition $\vdash p \lor \neg p$. As can be seen by inspection, the [[Definition:Truth Value|truth value]] of the [[Definition:Main Connective (Propositional Logic)|main connective]], that is $\lor$, is $T$ for each [[Definition:Boolean Interpretation|boolean interpretation]] for $p$. $\begin{array}{|cccc|} \hline p & \lor & \neg & p \\ \hline F & T & T & F \\ T & T & F & T \\ \hline \end{array}$ {{qed}}	0
Let $\map P n$ be a [[Definition:Propositional Function|propositional function]] depending on $n \in \N$. If: :$(1): \quad \map P n$ is true for all $n \le d$ for some $d \in \N$ :$(2): \quad \forall m \in \N: \paren {\forall k \in \N, m \le k < m + d: \map P k} \implies \map P {m + d}$ then $\map P n$ is true for all $n \in \N$.	0
From the [[Definition:Stronger Statement|stronger]] results: :[[Functionally Complete Logical Connectives/Negation and Disjunction|Functionally Complete Logical Connectives: Negation and Disjunction]]: ::the [[Definition:Set|set]] of [[Definition:Logical Connective|logical connectives]]: $\set {\neg, \lor}$ is [[Definition:Functionally Complete|functionally complete]] :[[Functionally Complete Logical Connectives/Negation and Conjunction|Functionally Complete Logical Connectives: Negation and Conjunction]]: ::the [[Definition:Set|set]] of [[Definition:Logical Connective|logical connectives]]: $\set {\neg, \land}$ is [[Definition:Functionally Complete|functionally complete]] it follows directly that $\set {\neg, \land, \lor}$ is likewise [[Definition:Functionally Complete|functionally complete]]. {{qed}} [[Category:Functional Completeness]] eh8et9jrx7lv8b1gvg1ipjzycdrvr03	0
{{BeginTableau|\bot \implies p \vdash \top}} {{Premise|1|\bot \implies p}} {{TopIntro|2}} {{EndTableau}} {{qed|lemma}} {{BeginTableau|\top \vdash \bot \implies p}} {{Assumption|1|\bot}} {{Premise|2|\top}} {{Explosion|3|1|p|1}} {{Implication|4||\bot \implies p|1|3}} {{EndTableau}} {{qed}}	0
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $\tuple {a_0, a_1, \ldots, a_n}$ be a [[Definition:Finite Continued Fraction|finite continued fraction]] of [[Definition:Length of Continued Fraction|length]] $n \ge 0$. Let $p_n$ and $q_n$ be its $n$th [[Definition:Numerator and Denominator of Continued Fraction|numerator and denominator]]. Then the [[Definition:Value of Continued Fraction|value]] $\sqbrk {a_0, a_1, \ldots, a_n}$ equals $\dfrac {p_n} {q_n}$.	0
Let $\mathcal A$ be the [[Definition:Set|set]] of finitely satisfiable extensions of $T$. By the [[Finitely Satisfiable Theory has Maximal Finitely Satisfiable Extension/Lemma|lemma]], for each element $S$ of $\mathcal A$ and each $\mathcal L$-sentence $\phi$, either $S \cup \left\{ {\phi}\right\} \in \mathcal A$ or $S \cup \left\{ {\neg \phi}\right\} \in \mathcal A$. $\mathcal A$ has [[Definition:Finite Character|finite character]], by the following argument: Let $S \in \mathcal A$. Let $F$ be a [[Definition:Finite Subset|finite subset]] of $S$. Then $S$ is satisfiable and hence finitely satisfiable. Thus in $\mathcal A$. Let $S$ be a theory on $\mathcal L$. Let every [[Definition:Finite Subset|finite subset]] of $S$ be finitely satisfiable. Then every [[Definition:Finite Subset|finite subset]] of $S$ is satisfiable. Therefore $S$ is finitely satisfiable. Thus $\mathcal A$ has finite character. By the [[Restricted Tukey-Teichmüller Theorem]], $\mathcal A$ has an element $T'$ such that: :for each $\mathcal L$-sentence $\phi$, either $\phi \in T'$ or $\neg \phi \in T'$. {{qed}}	0
{{BeginTableau|p \lor \paren {q \land r} \vdash \paren {p \lor q} \land \paren {p \lor r} }} {{Premise | 1|p \lor \paren {q \land r} }} {{Assumption | 2|p}} {{Addition | 3|2|p \lor q|2|1}} {{Addition | 4|2|p \lor r|2|1}} {{Conjunction | 5|2|\paren {p \lor q} \land \paren {p \lor r}|3|4}} {{Assumption | 6|q \land r}} {{Simplification | 7|6|q|6|1}} {{Simplification | 8|6|r|6|2}} {{Addition | 9|6|p \lor q|7|2}} {{Addition |10|6|p \lor r|8|2}} {{Conjunction |11|6|\paren {p \lor q} \land \paren {p \lor r}|7|8}} {{ProofByCases |12|1|\paren {p \lor q} \land \paren {p \lor r}|1|2|5|6|11}} {{EndTableau}} {{qed}} [[Category:Rule of Distribution]] i25mmyxvwb862jvl4ipx120n568xzpk	0
We apply the [[Method of Truth Tables]]. As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|ccc||ccccccccc|} \hline p & \oplus & q & (p & \lor & q) & \land & (\neg & p & \lor & \neg & q) \\ \hline F & F & F & F & F & F & F & T & F & T & T & F \\ F & T & T & F & T & T & T & T & F & T & F & T \\ T & T & F & T & T & F & T & F & T & T & T & F \\ T & F & T & T & T & T & F & F & T & F & F & T \\ \hline \end{array}$ {{qed}}	0
: $1:$ Suppose $\phi$ is an [[Definition:Simple Statement|atom]] $p$. Then we need to show that $p \vdash p$ and $\neg p \vdash \neg p$. These are proved in one line in the proof of the [[Law of Identity#Proof|Law of Identity]]. : $2:$ Suppose $\phi$ is of the form $\neg \phi_1$. There are two cases to consider: * Suppose $\phi$ evaluates to $T$. {{finish|More hard work. I'll come back to this once I've got some more basics done.}} [[Category:Propositional Logic]] odpu8tmuf1zc50moxxiynr9im73pl6r	0
: $p \implies q, q \implies r, p \vdash r$	0
Let $v$ be an arbitrary [[Definition:Boolean Interpretation|interpretation]]. Then by definition of [[Definition:Interderivable|interderivable]]: :$\map v {p \iff q}$ {{iff}} $\map v p = \map v q$ Since $v$ is arbitrary, $\map v p = \map v q$ holds in all [[Definition:Boolean Interpretation|interpretations]]. That is: :$p \dashv \vdash q$ {{qed}}	0
{{ProofWanted|Can be seen to be [[Definition:Logical Equivalence|logically equivalent]] to [[Conditional is not Left Self-Distributive/Formulation 1]] by application of the [[Rule of Implication]] and [[Modus Ponendo Ponens]].}}	0
From the [[Rule of Exportation]]: :$\paren {p \land q} \implies r \dashv \vdash p \implies \paren {q \implies r}$ Then by [[Self-Distributive Law for Conditional]]: :$p \implies \paren {q \implies r} \dashv \vdash \paren {p \implies q} \implies \paren {p \implies r}$ {{qed}} [[Category:Implication]] [[Category:Conjunction]] oc5bpk4w3hco5d9sobteg96jn3b38gs	0
We apply the [[Method of Truth Tables]] to the proposition. As can be seen by inspection, in each case, the [[Definition:Truth Value|truth values]] in the appropriate columns match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|c|ccc||c|} \hline p & \bot & \implies & p & \top \\ \hline F & F & T & F & T \\ T & F & T & T & T \\ \hline \end{array}$ {{qed}}	0
Let $M$ be a [[Definition:Class (Class Theory)|class]]. Let $g: M \to M$ be a [[Definition:Mapping (Class Theory)|mapping]] on $M$. Let $b \in M$ such that $M$ is [[Definition:Minimally Closed Class|minimally closed under $g$ with respect to $b$]]. Let $P: M \to \set {\T, \F}$ be a [[Definition:Propositional Function|propositional function]] on $M$. Suppose that: :$(1): \quad \map P b = \T$ :$(2): \quad \forall x \in M: \map P x = \T \implies \map P {\map g x} = \T$ Then: :$\forall x \in M: \map P x = \T$	0
By assumption, some [[Definition:Logical Formula|logical formula]] $\phi$ is not an $\mathscr M$-[[Definition:Tautology (Formal Semantics)|tautology]]. Since $\mathscr P$ is [[Definition:Sound Proof System|sound]] for $\mathscr M$, $\phi$ is also not a $\mathscr P$-[[Definition:Theorem (Formal Systems)|theorem]]. But then by definition $\mathscr P$ is [[Definition:Consistent Proof System|consistent]]. {{qed}}	0
{{BeginTableau|\neg \left({\neg p \land \neg q}\right) \vdash p \lor q}} {{Premise|1|\neg \left({\neg p \land \neg q}\right)}} {{Assumption|2|\neg \left ({p \lor q}\right)}} {{DeMorgan|3|2|\neg p \land \neg q|2|Conjunction of Negations}} {{NonContradiction|4|1, 2|3|1}} {{Reductio|5|1|p \lor q|2|4}} {{EndTableau}} {{qed}} {{LEM|Reductio ad Absurdum}}	0
Let $\struct {S, \vee, \wedge}$ be a [[Definition:Boolean Algebra/Definition 1|Boolean algebra, defined as in Definition 1]]. Then for all $a, b \in S$: :$a = a \vee \paren {a \wedge b}$ :$a = a \wedge \paren {a \vee b}$ That is, $\vee$ [[Definition:Absorb|absorbs]] $\wedge$, and $\wedge$ [[Definition:Absorb|absorbs]] $\vee$.	0
{{BeginTableau|\vdash q \implies \paren {\paren {p \implies q} \land \paren {\neg p \implies q} } }} {{Assumption|1|q|}} {{SequentIntro|2|1|\paren {p \implies q} \land \paren {\neg p \implies q}|1|[[Principle of Dilemma/Formulation 1/Reverse Implication|Principle of Dilemma: Formulation 1: Reverse Implication]]}} {{Implication|3||q \implies \paren {\paren {p \implies q} \land \paren {\neg p \implies q} }|1|2}} {{EndTableau}} {{qed}} [[Category:Principle of Dilemma]] d9q4uvsexr57d799yi2aupfipflfpf2	0
=== [[Rule of Transposition/Variant 1/Formulation 1/Forward Implication/Proof|Proof of Forward Implication]] === {{:Rule of Transposition/Variant 1/Formulation 1/Forward Implication/Proof}} === [[Rule of Transposition/Variant 1/Formulation 1/Reverse Implication/Proof|Proof of Reverse Implication]] === {{:Rule of Transposition/Variant 1/Formulation 1/Reverse Implication/Proof}}	0
{{BeginTableau|\left({p \lor q}\right) \implies r \vdash \left({p \implies r}\right) \land \left({q \implies r}\right)}} {{Premise|1|\left({p \lor q}\right) \implies r}} {{Assumption|2|p}} {{Addition|3|2|p \lor q|2|1}} {{ModusPonens|4|1, 2|r|1|3}} {{Implication|5|1|p \implies r|2|4}} {{Assumption|6|q}} {{Addition|7|6|p \lor q|6|2}} {{ModusPonens|8|1, 6|r|1|7}} {{Implication|9|1|q \implies r|6|8}} {{Conjunction|10|1|\left({p \implies r}\right) \land \left({q \implies r}\right)|5|9}} {{EndTableau}} {{qed}} [[Category:Proof by Cases]] mmig5x5s1otdg7m1su7k3gl33tj9fq7	0
Let $C$ be a [[Definition:Linear Code|linear code]] whose [[Definition:Master Code|master code]] is $V$. Let $c \in C$ be a [[Definition:Transmitted Codeword|transmitted codeword]]. Let $v$ be the [[Definition:Received Word|received word]] from $c$. By definition, $v$ is an [[Definition:Element|element]] of $V$. Let $v$ have a [[Definition:Distance between Linear Codewords|distance]] $e$ from $c$, where $2 e + 1 \le d$. Thus there have been $e$ [[Definition:Transmission Error|transmission errors]]. {{AimForCont}} $c_1$ is a [[Definition:Codeword of Linear Code|codeword]] of $C$, [[Definition:Distinct Objects|distinct]] from $c$, such that $\map d {v, c_1} \le e$. Then: {{begin-eqn}} {{eqn | l = \map d {c, c_1} | o = \le | r = \map d {c, v} + \map d {v, c_1} | c = }} {{eqn | o = \le | r = e + e | c = }} {{eqn | o = < | r = d | c = }} {{end-eqn}} So $c_1$ has a [[Definition:Distance between Linear Codewords|distance]] from $c$ less than $d$. But $C$ has a [[Definition:Minimum Distance of Linear Code|minimum distance]] $d$. Thus $c_1$ cannot be a [[Definition:Codeword of Linear Code|codeword]] of $C$. From this [[Proof by Contradiction|contradiction]] it follows that there is no [[Definition:Codeword of Linear Code|codeword]] of $C$ closer to $v$ than $c$. Hence there is a [[Definition:Unique|unique]] [[Definition:Codeword of Linear Code|codeword]] of $C$ which has the smallest [[Definition:Distance between Linear Codewords|distance]] from $v$. Hence it can be understood that $C$ has corrected the [[Definition:Transmission Error|transmission errors]] of $v$. {{Qed}}	0
: $p \implies q, q \implies r, p \vdash r$	0
Let $E \subseteq \N$ be the [[Definition:Set|set]] of all [[Definition:Even Integer|even]] [[Definition:Natural Numbers|natural numbers]]. Then $E$ is [[Definition:Primitive Recursive Set|primitive recursive]].	0
: $\vdash p \iff p$	0
While this holds: :$\vdash \paren {\paren {p \implies q} \implies r} \implies \paren {\paren {p \implies r} \implies \paren {q \implies r} }$ its converse does not: :$\not \vdash \paren {\paren {p \implies r} \implies \paren {q \implies r} } \implies \paren {\paren {p \implies q} \implies r}$	0
{{BeginTableau|\vdash p \iff p}} {{TheoremIntro|1|p \implies p|[[Law of Identity/Formulation 2|Law of Identity: Formulation 2]]}} {{BiconditionalIntro|2||p \iff p|1|1}} {{EndTableau|qed}}	0
{{BeginTableau|\neg p \implies q \vdash \neg q \implies p}} {{Premise|1|\neg p \implies q}} {{Assumption|2|\neg q}} {{ModusTollens|3|1, 2|\neg \neg p|1|2}} {{DoubleNegElimination|4|1, 2|p|3}} {{Implication|5|1|\neg q \implies p|2|4}} {{EndTableau}} {{Qed}} {{LEM|Double Negation Elimination|3}}	0
{{BeginTableau|\neg \left({\neg p \lor \neg q}\right) \vdash p \land q}} {{Premise|1|\neg \left({\neg p \lor \neg q}\right)}} {{Assumption|2|\neg \left ({p \land q}\right)}} {{DeMorgan|3|2|\neg p \lor \neg q|2|Disjunction of Negations}} {{NonContradiction|4|1, 2|3|1}} {{Reductio|5|1|p \land q|2|4}} |} {{qed}} {{LEM|Reductio ad Absurdum}}	0
{{BeginTableau|\neg p \land \neg q \vdash \neg \paren {p \lor q} }} {{Premise|1|\neg p \land \neg q}} {{Simplification|2|1|\neg p|1|1}} {{Simplification|3|1|\neg q|1|2}} {{Assumption|4|p \lor q}} {{Assumption|5|p}} {{NonContradiction|6|1, 5|5|2}} {{Assumption|7|q}} {{NonContradiction|8|1, 7|7|3}} {{ProofByCases|9|1, 4|\bot|4|5|6|7|8}} {{Contradiction|10|1|\neg \paren {p \lor q}|4|9}} {{EndTableau|qed}}	0
Let $n$ be a [[Definition:Positive Integer|(positive) integer]] expressed in [[Definition:Hexadecimal Notation|hexadecimal notation]] as: :$n = \sqbrk {a_r a_{r - 1} \dotso a_1 a_0}_H$ Then $n$ can be expressed in [[Definition:Binary Notation|binary notation]] as: :$n = \sqbrk {b_{r 3} b_{r 2} b_{r 1} b_{r 0} b_{\paren {r - 1} 3} b_{\paren {r - 1} 2} b_{\paren {r - 1} 1} b_{\paren {r - 1} 0} \dotso b_{1 3} b_{1 2} b_{1 1} b_{1 0} b_{0 3} b_{0 2} b_{0 1} b_{0 0} }_2$ where $\sqbrk {b_{j 3} b_{j 2} b_{j 1} b_{j 0} }_2$ is the expression of the [[Definition:Hexadecimal Notation|hexadecimal digit]] $a_j$ in [[Definition:Binary Notation|binary notation]]. That is, you take the [[Definition:Binary Notation|binary expression]] of each [[Definition:Hexadecimal Notation|hexadecimal digit]], padding them out with [[Definition:Zero Digit|zeroes]] to make them $4$ [[Definition:Bit|bits]] long, and simply [[Definition:Concatenation (Formal Systems)|concatenate]] them.	0
It is useful to first state the [[Definition:Cayley Table|Cayley tables]] for the three logical operations $\lor$, $\land$ and $\neg$: :$\begin{array}{c|cc} \lor & \bot & \top \\ \hline \bot & \bot & \top \\ \top & \top & \top \end{array} \qquad \begin{array}{c|cc} \land & \bot & \top \\ \hline \bot & \bot & \bot \\ \top & \bot & \top \end{array} \qquad \begin{array}{c|cc} & \bot & \top \\ \hline \neg & \top & \bot \end{array}$ Let us now verify the axioms for a [[Definition:Boolean Algebra|Boolean algebra]] in turn. === $(BA \ 0)$: Closure === It is immediate from the [[Definition:Cayley Table|Cayley tables]] that $S$ is [[Definition:Closed Algebraic System|closed]] under $\lor$, $\land$ and $\neg$. {{qed|lemma}} === $(BA \ 1)$: Commutativity === Follows from the [[Rule of Commutation]]. {{qed|lemma}} === $(BA \ 2)$: Distributivity === Follows from the [[Rule of Distribution]] {{qed|lemma}} === $(BA \ 3)$: Identities === Follows from [[Conjunction with Tautology]] and [[Disjunction with Contradiction]]. {{qed|lemma}} === $(BA \ 4)$: Complements === Follows from [[Contradiction is Negation of Tautology]] and [[Tautology is Negation of Contradiction]]. {{qed|lemma}} Having verified all axioms, we conclude $\mathbf 2$ is a [[Definition:Boolean Algebra|Boolean algebra]]. {{qed}}	0
{{BeginTableau|\neg \paren {p \iff q} \vdash \paren {p \iff \neg q} }} {{Premise | 1|\neg \paren {p \iff q} }} {{SequentIntro | 2|1|\neg \paren {\paren {p \implies q} \land \paren {q \implies p} }|1|[[Rule of Material Equivalence]]}} {{DeMorgan | 3|1|\neg \paren {p \implies q} \lor \neg \paren {q \implies p} |2|Disjunction of Negations}} {{Assumption | 4|p}} {{IdentityLaw | 5|4|p|4}} {{Assumption | 6|q}} {{Implication | 7|6|p \implies q|4|6}} {{DoubleNegIntro | 8|6|\neg \neg \paren {p \implies q}|7}} {{ModusTollendoPonens | 9|1, 6|\neg \paren {q \implies p}|3|8|1}} {{SequentIntro |10|1, 6|q \land \neg p|9|[[Conjunction with Negative Equivalent to Negation of Implication]]}} {{Simplification |11|1, 6|\neg p|10|2}} {{NonContradiction |12|1, 4, 6|4|11}} {{Contradiction |13|1, 4|\neg q|6|12}} {{Implication |14|1|p \implies \neg q|4|13}} {{Assumption |15|\neg q}} {{IdentityLaw |16|15|\neg q|15}} {{Assumption |17|\neg p}} {{Implication |18|17|\neg q \implies \neg p|16|17}} {{SequentIntro |19|17|p \implies q|18|[[Rule of Transposition]]}} {{ModusTollendoPonens |20|1, 17|\neg \paren {q \implies p} |3|19|1}} {{SequentIntro |21|1, 17|q \land \neg p|20|[[Conjunction with Negative Equivalent to Negation of Implication]]}} {{Simplification |22|1, 17|q|21|1}} {{NonContradiction |23|1, 15, 17|15|22}} {{Contradiction |24|1, 15|p|17|23}} {{Implication |25|1|\neg q \implies p|15|24}} {{BiconditionalIntro |26|1|\paren {p \iff \neg q}|14|25}} {{EndTableau|qed}} {{LEM||3}}	0
Let $\left({T, \mathbf H, \Phi}\right)$ be a [[Definition:Tableau Confutation|tableau confutation]] of $\mathbf H$. Suppose that $v$ were a [[Definition:Boolean Interpretation|boolean interpretation]] [[Definition:Model (Boolean Interpretations)|model]] for $\mathbf H$, i.e.: :$v \models_{\mathrm{BI}} \mathbf H$ By [[Model of Root of Propositional Tableau is Model of Branch]], it follows that: :$v \models_{\mathrm{BI}} \Phi \left[{\Gamma}\right]$ for some [[Definition:Branch (Graph Theory)|branch]] $\Gamma$ of $T$. Since $T$ is a [[Definition:Tableau Confutation|tableau confutation]], there is some [[Definition:WFF of Propositional Logic|WFF]] $\mathbf A$ such that: :$\mathbf A, \neg\mathbf A \in \Phi \left[{\Gamma}\right]$ Hence $v \models_{\mathrm{BI}} \mathbf A$, i.e.: :$v \left({\mathbf A}\right) = T$ But by the [[Definition:Logical Not/Truth Table|truth table for $\neg$]], this means: :$v \left({\neg\mathbf A}\right) = F$ which contradicts that $v \models_{\mathrm{BI}} \neg\mathbf A$. Hence, no such [[Definition:Boolean Interpretation|boolean interpretation]] can exist. That is, $\mathbf H$ is [[Definition:Unsatisfiable|unsatisfiable]] for [[Definition:Boolean Interpretation|boolean interpretations]]. {{qed}}	0
{{BeginTableau|\vdash p \implies p}} {{Premise|1|p}} {{Implication|2||p \implies p|1|1}} {{EndTableau}} {{qed}}	0
We will prove inductively the following claim for every [[Definition:Node (Graph Theory)|node]] $t$ of $T$: :If all [[Definition:Leaf Node|leaves]] that are [[Definition:Descendant Node|descendants]] of $t$ are [[Definition:Marked Closed Leaf|marked closed]], then $U \left({t}\right)$ is [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. By the [[Semantic Tableau Algorithm]], we know this statement to hold for the [[Definition:Leaf Node|leaf nodes]] themselves. For, a [[Definition:Leaf Node|leaf]] $t$ is [[Definition:Marked Closed Leaf|marked closed]] [[Definition:Iff|iff]] $U \left({t}\right)$ contains a [[Definition:Complementary Pair|complementary pair]]. The assertion follows from [[Set of Literals Satisfiable iff No Complementary Pairs]]. Inductively, suppose that all [[Definition:Child Node|children]] of a [[Definition:Node (Graph Theory)|node]] $t$ satisfy the mentioned condition. If all [[Definition:Descendant Node|descendant]] [[Definition:Leaf Node|leaf nodes]] of $t$ are [[Definition:Marked Closed Leaf|marked closed]], this evidently holds for the [[Definition:Child Node|children]] $t',t''$ of $t$ as well. Hence by hypothesis, $U \left({t'}\right)$ and $U \left({t''}\right)$ are [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. Let $\mathbf B$ be the [[Definition:WFF of Propositional Logic|WFF]] used by the [[Semantic Tableau Algorithm]] at $t$. Let $\mathbf B_1, \mathbf B_2$ be the formulas added to $t'$ and $t''$. First, the case that $\mathbf B$ is an [[Definition:Alpha-Formula|$\alpha$-formula]]. Then $t' = t''$, and $\mathbf B$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|semantically equivalent]] to $\mathbf B_1 \land \mathbf B_2$. It follows that if: :$v \models_{\mathrm{BI}} U \left({t}\right)$ for some [[Definition:Boolean Interpretation|boolean interpretation]] $v$, then also: :$v \models_{\mathrm{BI}} U \left({t'}\right)$ which contradicts our hypothesis. Thus, $U \left({t}\right)$ is [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. Next, the case that $\mathbf B$ is a [[Definition:Beta-Formula|$\beta$-formula]]. Then $\mathbf B$ is [[Definition:Semantic Equivalence (Boolean Interpretations)|semantically equivalent]] to $\mathbf B_1 \lor \mathbf B_2$. It follows that if: :$v \models_{\mathrm{BI}} U \left({t}\right)$ for some [[Definition:Boolean Interpretation|boolean interpretation]] $v$, then also one of the following must hold: :$v \models_{\mathrm{BI}} U \left({t'}\right)$ :$v \models_{\mathrm{BI}} U \left({t''}\right)$ which contradicts our hypothesis. Thus, $U \left({t}\right)$ is [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. {{handwaving|"It follows", is obvious, and is tedious to write down}} This proves our claim: :If all [[Definition:Leaf Node|leaves]] that are [[Definition:Descendant Node|descendants]] of $t$ are [[Definition:Marked Closed Leaf|marked closed]], then $U \left({t}\right)$ is [[Definition:Unsatisfiable (Boolean Interpretations)|unsatisfiable]]. Applying this claim to the [[Definition:Root Node|root node]] of $T$, we obtain the desired result. {{qed}}	0
: $\vdash \left({\neg \left({p \land q}\right)}\right) \iff \left({p \implies \neg q}\right)$	0
Since the [[Union of Primitive Recursive Sets]] is itself [[Definition:Primitive Recursive Set|primitive recursive]], all we need to do is show that each of $\operatorname{Zinstr}$, $\operatorname{Sinstr}$, $\operatorname{Cinstr}$ and $\operatorname{Jinstr}$ are [[Definition:Primitive Recursive Set|primitive recursive]]. First we consider $\operatorname{Zinstr}$. :$\operatorname{Zinstr} = \left\{{\beta \left({Z \left({n}\right)}\right): n \in \N^*}\right\} = \left\{{6 n - 3: n \in \N^*}\right\}$. So $\operatorname{Zinstr}$ is the set of [[Definition:Natural Numbers|natural numbers]] which are [[Definition:Divisor of Integer|divisible]] by $3$ but not $6$. Thus its [[Definition:Characteristic Function of Set|characteristic function]] is given by: :$\chi_{\operatorname{Zinstr}} \left({k}\right) = \operatorname{div} \left({k, 3}\right) \times \overline{\operatorname{sgn}}\left({\operatorname{div} \left({k, 6}\right)}\right)$ where: : [[Divisor Relation is Primitive Recursive|$\operatorname{div}$ is primitive recursive]] : [[Signum Function is Primitive Recursive|$\overline{\operatorname{sgn}}$ is primitive recursive]] : [[Multiplication is Primitive Recursive]] : [[Constant Function is Primitive Recursive|$3$ and $6$ are constants]] Hence $\operatorname{Zinstr}$ is [[Definition:Primitive Recursive Set|primitive recursive]]. Next we consider $\operatorname{Sinstr}$. :$\operatorname{Sinstr} = \left\{{\beta \left({S \left({n}\right)}\right): n \in \N^*}\right\} = \left\{{6 n: n \in \N^*}\right\}$. So $\operatorname{Sinstr}$ is the set of [[Definition:Natural Numbers|natural numbers]] which are [[Definition:Divisor of Integer|divisible]] by $6$. Thus its [[Definition:Characteristic Function of Set|characteristic function]] is given by: :$\chi_{\operatorname{Sinstr}} \left({k}\right) = \operatorname{sgn} \left({k}\right) \times \operatorname{div} \left({k, 6}\right)$ where: : [[Divisor Relation is Primitive Recursive|$\operatorname{div}$ is primitive recursive]] : [[Signum Function is Primitive Recursive|$\operatorname{sgn}$ is primitive recursive]] : [[Multiplication is Primitive Recursive]] : [[Constant Function is Primitive Recursive|$6$ is constant]] Hence $\operatorname{Sinstr}$ is [[Definition:Primitive Recursive Set|primitive recursive]]. Next we consider $\operatorname{Cinstr}$. We have that $\beta \left({C \left({n, m}\right)}\right) = 2^m 3^n + 1$. Hence $k \in \operatorname{Cinstr}$ iff $k \equiv 1 \pmod 3$ and $k - 1$ [[Definition:Sequence Coding|codes]] a pair of [[Definition:Positive Integer|positive integers]]. That is: {{begin-eqn}} {{eqn | l = k \in \operatorname{Cinstr} | o = \iff | r = \operatorname{rem} \left({k, 3}\right) = 1 | c = }} {{eqn | o = \land | r = \chi_{\operatorname{Seq} } \left({k \, \dot - \, 1}\right) = 1 | c = }} {{eqn | o = \land | r = \operatorname{len} \left({k \, \dot - \, 1}\right) = 2 | c = }} {{end-eqn}} We can introduce two properties: : $P \left({k}\right) \iff \operatorname{eq} \left({\operatorname{rem} \left({k, 3}\right), 1}\right)$ : $Q \left({k}\right) \iff \operatorname{eq} \left({\operatorname{len} \left({k \, \dot - \, 1}\right), 2}\right)$ $\chi_{P}$ is [[Definition:Primitive Recursive Function|primitive recursive]] since it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: : [[Equality Relation is Primitive Recursive|$\operatorname{eq}$ is primitive recursive]] : [[Remainder is Primitive Recursive|$\operatorname{rem}$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$3$ and $1$ are constants]] $\chi_{Q}$ is [[Definition:Primitive Recursive Function|primitive recursive]] since it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: : [[Equality Relation is Primitive Recursive|$\operatorname{eq}$ is primitive recursive]] : [[Length Function is Primitive Recursive|$\operatorname{len}$ is primitive recursive]] : [[Cut-Off Subtraction is Primitive Recursive|$k \, \dot - \, 1$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$2$ and $1$ are constants]] Thus the [[Definition:Characteristic Function of Set|characteristic function]] of $\operatorname{Cinstr}$ is given by: :$\chi_{\operatorname{Cinstr}} = \chi_{P} \left({k}\right) \chi_{Q} \left({k}\right) \chi_{\operatorname{Seq}} \left({k \, \dot - \, 1}\right)$ where: : $\chi_{P}$ is [[Definition:Primitive Recursive Function|primitive recursive]] : $\chi_{Q}$ is [[Definition:Primitive Recursive Function|primitive recursive]] : [[Multiplication is Primitive Recursive]] : [[Set of Sequence Codes is Primitive Recursive|$\chi_{\operatorname{Seq}}$ is primitive recursive]] : [[Cut-Off Subtraction is Primitive Recursive|$k \, \dot - \, 1$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$1$ is constant]] Hence $\operatorname{Cinstr}$ is [[Definition:Primitive Recursive Set|primitive recursive]]. Finally we consider $\operatorname{Jinstr}$. We have that $\beta \left({J \left({n, m, q}\right)}\right) = 2^m 3^n 5^n + 2$. Hence $k \in \operatorname{Jinstr}$ iff $k \equiv 2 \pmod 3$ and $k - 1$ [[Definition:Sequence Coding|codes]] a triad of [[Definition:Positive Integer|positive integers]]. That is: {{begin-eqn}} {{eqn | l = k \in \operatorname{Jinstr} | o = \iff | r = \operatorname{rem} \left({k, 3}\right) = 2 | c = }} {{eqn | o = \land | r = \chi_{\operatorname{Seq} } \left({k \, \dot - \, 2}\right) = 1 | c = }} {{eqn | o = \land | r = \operatorname{len} \left({k \, \dot - \, 2}\right) = 3 | c = }} {{end-eqn}} We can introduce two properties: : $R \left({k}\right) \iff \operatorname{eq} \left({\operatorname{rem} \left({k, 3}\right), 2}\right)$ : $S \left({k}\right) \iff \operatorname{eq} \left({\operatorname{len} \left({k \, \dot - \, 2}\right), 3}\right)$ $\chi_R$ is [[Definition:Primitive Recursive Function|primitive recursive]] since it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: : [[Equality Relation is Primitive Recursive|$\operatorname{eq}$ is primitive recursive]] : [[Remainder is Primitive Recursive|$\operatorname{rem}$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$3$ and $2$ are constants]] $\chi_S$ is [[Definition:Primitive Recursive Function|primitive recursive]] since it is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: : [[Equality Relation is Primitive Recursive|$\operatorname{eq}$ is primitive recursive]] : [[Length Function is Primitive Recursive|$\operatorname{len}$ is primitive recursive]] : [[Cut-Off Subtraction is Primitive Recursive|$k \, \dot - \, 2$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$2$ and $3$ are constants]] Thus the [[Definition:Characteristic Function of Set|characteristic function]] of $\operatorname{Jinstr}$ is given by: :$\chi_{\operatorname{Jinstr}} = \chi_R \left({k}\right) \chi_S \left({k}\right) \chi_{\operatorname{Seq}} \left({k \, \dot - \, 2}\right)$ where: : $\chi_R$ is [[Definition:Primitive Recursive Function|primitive recursive]] : $\chi_S$ is [[Definition:Primitive Recursive Function|primitive recursive]] : [[Multiplication is Primitive Recursive]] : [[Set of Sequence Codes is Primitive Recursive|$\chi_{\operatorname{Seq}}$ is primitive recursive]] : [[Cut-Off Subtraction is Primitive Recursive|$k \, \dot - \, 2$ is primitive recursive]] : [[Constant Function is Primitive Recursive|$2$ is constant]] Hence $\operatorname{Jinstr}$ is [[Definition:Primitive Recursive Set|primitive recursive]]. The result follows. {{qed}} [[Category:URM Programs]] [[Category:Primitive Recursive Functions]] lx6sqfd2svyrgvh97vgzh6pvnffq772	0
Let $P$ be a [[Definition:CNF Satisfiability Problem|CNF SAT problem]]. === CNF SAT is NP === A potential solution to $P$ can be verified in polynomial time by checking every clause in $L$ to see if they all have at least one true un-negated variable or one false negated variable. Because a solution can be verified or rejected in polynomial time the problem is [[Definition:NP Complexity Class|NP]] === CNF SAT is NP-hard === We will show this by reducing the [[Definition:Boolean Satisfiability Problem|boolean satisfiability (SAT) problem]] to [[Definition:CNF Satisfiability Problem|CNF SAT]]. The algorithm to convert the [[Definition:Boolean Satisfiability Problem|SAT) problem]] to [[Definition:CNF Satisfiability Problem|CNF SAT]] is recursive. Wherever $A$, $B$,and $C$ are seen in the output it is understood that the algorithm would call itself on those formulas and convert them into [[Definition:Conjunctive Normal Form|CNF]]. ==== iff and xor ==== The first step will be to remove every instance of mutual implication and exclusive or from the problem. In every clause with one of these functions there must be at least one that does not have any instance of mutual implication or exclusive or in its arguments. Introduce a new variable $x$ and replace either: :$A \implies B$ with $x$ or: :$\neg \left({A \implies B}\right)$ with $\neg x$ as appropriate. Then add the following four clauses to $L$: : $A \lor B \lor x$ : $A \lor \neg B \lor \neg x$ : $\neg A \lor B \lor \neg x$ : $\neg A \lor \neg B \lor x$ Repeat this procedure until there are no more mutual implications or exclusive ors in the list of clauses. It is important to note that the amount this procedure adds to the size of the final problem is bounded. Specifically, every instance of mutual implication or exclusive or increases the number of variables by one, increases the number of "new" symbols in $L$ by a fixed number and reproduces every "old" symbol in $L$ at most four times. In total, this step increases the size of the problem at most linearly. The remaining functions that need to be considered can be converted to [[Definition:Conjunctive Normal Form|CNF]] with only a constant amount of increase to the size by a constant amount per production. Given a logical expression it will either be a variable, a unary function, or a binary function. ==== Single Variable ==== If $A$ is a single variable then it is in [[Definition:Conjunctive Normal Form|CNF]] and the task is complete. ==== Unary functions ==== With the two constant functions, one can simply replace $f_F \left({A}\right)$ and $f_T \left({A}\right)$ with $F$ and $T$ respectively. These can then be removed from the final expression as appropriate. {{explain|Link to the results linking $F$ and $T$ with disjunctions / conjunctions etc.}} For the identity function, replace $f_I \left({A}\right)$ with the [[Definition:Conjunctive Normal Form|CNF]] conversion of $A$. With the $\neg$ function the replacement depends on the argument. If the argument is a single variable $x$ then the output $\neg x$ is in [[Definition:Conjunctive Normal Form|CNF]]. If the argument is one of the constant functions then replace $\neg f_F \left({A}\right)$ and $\neg f_T \left({A}\right)$ with $T$ and $F$ respectively. If the argument is a binary function of the form $f \left({A, B}\right)$ then replace the function with the appropriate one, following the list below: : $\neg f_F \left({A, B}\right) \to f_T \left({A, B}\right)$ : $\neg f_T \left({A, B}\right) \to f_F \left({A, B}\right)$ : $\neg A \land B \to A \uparrow B$ : $\neg A \uparrow B \to A \land B$ : $\neg \left({A \implies B}\right) \to \neg \left({A \implies B}\right)$ : $\neg \neg \left({A \implies B}\right) \to \left({A \implies B}\right)$ : $\neg \operatorname{pr}_1 \left({A, B}\right) \to \overline {\operatorname{pr}_1} \left({A, B}\right)$ : $\neg \overline {\operatorname{pr}_1} \left({A, B}\right) \to \operatorname{pr}_1 \left({A, B}\right)$ : $\neg \left({B \implies A}\right) \to \neg \left({B \implies A}\right)$ : $\neg \neg \left({B \implies A}\right) \to \left({B \implies A}\right)$ : $\neg \operatorname{pr}_2 \to \overline {\operatorname{pr}_2}$ : $\neg \overline {\operatorname{pr}_2} \to \operatorname{pr}_2$ : $\neg A \lor B \to A \downarrow B$ : $\neg A \downarrow B \to A \lor B$ ==== Binary Functions ==== Replace $f_T \left({A, B}\right)$ with $T$. $A \lor B$ is already in [[Definition:Conjunctive Normal Form|CNF]]. Replace $A \implies B$ with $\neg A \lor B$. Replace $\operatorname{pr}_1 \left({A, B}\right)$ with $A$. Replace $B \implies A$ with $A \lor \neg B$. Replace $\operatorname{pr}_2 \left({A, B}\right)$ with $B$. In the case of $A \land B$ the clause takes either the form: : $A \land B$ or: :$\left({A \land B}\right) \lor C$ In the first case simply replace the clause with the two clauses $A$ and $B$. In the second case introduce the new variable $x$ and replace the clause with the three clauses: : $x \lor C$ : $\neg x \lor A$ : $\neg x \lor B$ A simple case analysis will show that the above clauses have a solution {{iff}} $\left({A \land B}\right) \lor C$. Replace $A \uparrow B$ with $\neg A \lor \neg B$ Replace $\overline {\operatorname{pr}_2} \left({A, B}\right)$ with $\neg B$. Replace $\neg \left({A \implies B}\right)$ with $A \land \neg B$ and use the rule for $A \land B$. Replace $\overline {\operatorname{pr}_2} \left({A, B}\right)$ with $\neg A$. Replace $\neg \left({B \implies A}\right)$ with $\neg A \land B$ and use the rule for $A \land B$. Replace $A \downarrow B$ with $\neg A \land \neg B$ and use the rule for $A \land B$. Replace $f_F \left({A, B}\right)$ with $F$. At this point there may be some instances of $T$ and $F$ in the clauses. They can be removed by: :deleting any clause from $L$ that contains $T$ in any of its conjunctions and: :removing $F$ from any clause that contains it. Note that if any clause simplifies to $F$ the problem is trivially unsatisfiable and the task is complete. In the end the size and number of variables in the CNF problem is $O(n)$ where $n$ is the number of symbols required to write down the original [[Definition:Boolean Satisfiability Problem|SAT problem]]. Hence the conversion can be done in polynomial time. We have that [[Boolean Satisfiability Problem is NP-hard]]. But [[Definition:Boolean Satisfiability Problem|SAT]] is reducible to [[Definition:CNF Satisfiability Problem|CNF SAT]]. Therefore [[Definition:CNF Satisfiability Problem|CNF SAT]] is [[Definition:NP-Hard|NP-hard]]. [[Definition:CNF Satisfiability Problem|CNF SAT]] is [[Definition:NP-Complete|NP-complete]]. {{explain|The logical steps need to be clarified in the above.}} {{qed}} [[Category:Mathematical Logic]] 8fhj0sypbhdkrrdnp5fv8a0zfherp05	0
The '''predecessor function''' $\operatorname{pred}: \N \to \N$ defined as: :$\operatorname{pred} \left({n}\right) = \begin{cases} 0 & : n = 0 \\ n-1 & : n > 0 \end{cases}$ is [[Definition:Primitive Recursive Function|primitive recursive]].	0
Let $Q$ be a [[Definition:Valid Argument|valid]] [[Definition:Categorical Syllogism|categorical syllogism]] in [[Definition:Third Figure of Categorical Syllogism|Figure $\text {III}$]]. Then it is a [[Definition:Necessary Condition|necessary condition]] that: :The [[Definition:Conclusion of Syllogism|conclusion]] of $Q$ be a [[Definition:Particular Categorical Statement|particular categorical statement]] and: :If the [[Definition:Conclusion of Syllogism|conclusion]] of $Q$ be a [[Definition:Negative Categorical Statement|negative categorical statement]], then so is the [[Definition:Major Premise of Syllogism|major premise]] of $Q$.	0
{{BeginTableau|p \iff \top \vdash p}} {{Premise|1|p \iff \top}} {{TopIntro|2}} {{BiconditionalElimination|3|1|\top \implies p|1|2}} {{ModusPonens|4|1|p|2|3}} {{EndTableau}} {{qed|lemma}} {{BeginTableau|p \vdash p \iff \top}} {{Premise|1|\top}} {{Assumption|2|p}} {{TopIntro|3}} {{Implication|4||p \implies \top|2|3}} {{Implication|5|2|\top \implies p|1|2}} {{BiconditionalIntro|6|2|p \iff \top|4|5}} {{EndTableau}} {{qed}}	0
{{BeginTableau|p \implies q, \neg q \vdash \neg p}} {{Premise|1|p \implies q}} {{Premise|2|\neg q}} {{Assumption|3|p|Assume $p$ ...}} {{ModusPonens|4|1, 3|q|1|3|... and derive $q$ ...}} {{NonContradiction|5|1, 2, 3|4|2| ... demonstrating a contradiction}} {{Contradiction|6|1, 2|\neg p|3|5}} {{EndTableau}} {{Qed}}	0
Proof by [[Principle of Mathematical Induction|induction]]. Consider the [[Definition:Natural Numbers|natural numbers]] $\N$ defined as the elements of the [[Definition:Minimal Infinite Successor Set|minimal infinite successor set]] $\omega$. From the definition of [[Definition:Addition in Minimal Infinite Successor Set|addition in $\omega$]], we have that: {{begin-eqn}} {{eqn | ll= \forall m, n \in \N: | l = m + 0 | r = m }} {{eqn | l = m + n^+ | r = \paren {m + n}^+ }} {{end-eqn}} For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\forall m \in \N: m + n = n + m$ === Basis for the Induction === From [[Natural Number Addition Commutes with Zero]], we have: :$\forall m \in \N: m + 0 = m = 0 + m$ Thus $\map P 0$ is seen to be true. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 0$, then it logically follows that $\map P {k^+}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]] $\map P k$: :$\forall m \in \N: m + k = k + m$ Then we need to show that $\map P {k^+}$ follows directly from $\map P k$: :$\forall m \in \N: m + k^+ = k^+ + m$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = k^+ + m | r = \paren {k + m}^+ | c = [[Natural Number Addition Commutativity with Successor]] }} {{eqn | r = \paren {m + k}^+ | c = from the [[Natural Number Addition is Commutative/Proof 2#Induction Hypothesis|induction hypothesis]] }} {{eqn | r = m + k^+ | c = by definition }} {{end-eqn}} So $\map P k \implies \map P {k^+}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall m, n \in \N: m + n = n + m$ {{qed}}	0
Let $\N$ be the [[Definition:Natural Numbers|0-based natural numbers]]: :$\N = \left\{{0, 1, 2, \ldots}\right\}$ Let $s: \N \to \N: \map s n = n + 1$ be the [[Definition:Successor Function|successor function]]. Then: :$\forall n \in \N \setminus \set 0 \paren {\exists m \in \N: \map s m = n }$	0
Since $\mathcal F$ is [[Definition:Satisfiable Set of Formulas|$\mathscr M$-satisfiable]], there exists some [[Definition:Model of Set of Formulas|model]] $\mathcal M$ of $\mathcal F$: :$\mathcal M \models_{\mathscr M} \mathcal F$ Since $\psi$ is a [[Definition:Tautology (Formal Semantics)|tautology]], also: :$\mathcal M \models_{\mathscr M} \psi$ Therefore, we conclude that: :$\mathcal M \models_{\mathscr M} \mathcal F \cup \left\{{\phi}\right\}$ i.e., $\mathcal F \cup \left\{{\phi}\right\}$ is [[Definition:Satisfiable Set of Formulas|satisfiable]].	0
An '''inductive argument''' is a form of [[Definition:Logical Argument|argument]] in which, if all the [[Definition:Premise|premises]] are true, the [[Definition:Conclusion|conclusion]] is ''probably true'', but might not be. Such lines of reasoning are ubiquitous in everyday life and in most human endeavors. However, '''inductive arguments''' are only [[Definition:Conjecture|conjectures]] in the field of [[Definition:Mathematics|mathematics]]. Such arguments are not [[Definition:Truth Preservation|truth preserving]] and therefore they are not [[Definition:Proof|proofs]].	0
:$\left({p \vdash q}\right) \vdash p \implies q$	0
Consider $\N$ defined as a [[Definition:Peano Structure|Peano structure]]. The result follows from [[Principle of Mathematical Induction for Peano Structure]]. {{qed}}	0
{{BeginTableau|\left({\neg p \land \neg q}\right) \implies \left({\neg \left({p \lor q}\right)}\right)}} {{Assumption|1|\neg p \land \neg q}} {{SequentIntro|2|1|\neg \left({p \lor q}\right)|1|[[De Morgan's Laws (Logic)/Conjunction of Negations/Formulation 1/Forward Implication|De Morgan's Laws (Logic): Conjunction of Negations: Formulation 1]]}} {{Implication|3||\left({\neg p \land \neg q}\right) \implies \left({\neg \left({p \lor q}\right)}\right)|1|2}} {{EndTableau}} {{qed}}	0
Using a [[Definition:Tableau Proof (Formal Systems)|tableau proof]] for [[Definition:Hilbert Proof System/Instance 1|instance 1 of a Hilbert proof system]]: {{BeginTableau|p \implies p|nohead=1}} {{TableauLine|n = 1 | f = \paren {p \implies \paren {\paren {p \implies p} \implies p} } \implies \paren {\paren {p \implies \paren {p \implies p} } \implies \paren {p \implies p} } | rtxt = Axiom 2 | c = $\mathbf A = p, \mathbf B = p \implies p, \mathbf C = p$ }} {{TableauLine|n = 2 | f = p \implies \paren {\paren {p \implies p} \implies p} | rtxt = Axiom 1 | c = $\mathbf A = p, \mathbf B = p \implies p$ }} {{ModusPonens|3||\paren {p \implies \paren {p \implies p} } \implies \paren {p \implies p}|1|2}} {{TableauLine|n = 4 | f = p \implies \paren {p \implies p} | rtxt = Axiom 1 | c = $\mathbf A = p, \mathbf B = p$ }} {{ModusPonens|5||p \implies p|3|4}} {{EndTableau}} {{qed}}	0
First of all, we need to prove === Lemma === Following the definitions on [[Definition:Ultraproduct]] :$\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$, $k = 1, \dotsc, n$ {{iff}} :$\left\{ i : \left({m_{1, i}, \dots, m_{n, i} }\right) = \left({m'_{1, i}, \dots, m'_{n, i} }\right) \right \} \in \mathcal U$ === Proof === Let :$I_k := \left\{ i \in I : m_{k, i} = m'_{k, i} \right\}$ :$I^* := \left\{ i : \left({m_{1, i}, \dots, m_{n, i} }\right) = \left({m'_{1, i}, \dots, m'_{n, i} }\right) \right \}= \displaystyle \bigcap^n_{k = 1} I_k $ Suppose :$\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$ for $k = 1, \dotsc, n$ we have :$I_k \in \mathcal U$ for $k = 1, \dotsc, n$ Since $\mathcal U$ is closed under [[Definition:Set Intersection|intersection]] :$I^* \in \mathcal U$ On the other hand, suppose :$I^* \in \mathcal U$ Since $\mathcal U$ is [[Definition:Upper Set|upward-closed]] :$I_k \in \mathcal U$ for $k = 1, \dotsc, n$ Therefore :$\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$ {{qed}} === Proposition 1 === ''The definition of $f^\mathcal M$ is consistent.'' i.e. for $\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$, $k = 1, \dotsc, n$ :$\left(f^{\mathcal M_i}\left(m_{1, i}, \dots, m_{n, i}\right)\right)_\mathcal U = \left(f^{\mathcal M_i}\left(m'_{1, i}, \dots, m'_{n, i}\right)\right)_\mathcal U$ === Proof === Firstly note that: :$\{i : f^{\mathcal M_i}\left(m_{1, i}, \dots, m_{n, i}\right) = f^{\mathcal M_i}\left(m'_{1, i}, \dots, m'_{n, i}\right) \} \supseteq \{i : \left(m_{1, i}, \dots, m_{n, i}\right) = \left(m'_{1, i}, \dots, m'_{n, i}\right) \} $ and by $\mathcal U$ is an [[Definition:Ultrafilter_on_Set|ultrafilter]] on $I$, we have :$\{i : \left(m_{1, i}, \dots, m_{n, i}\right) = \left(m'_{1, i}, \dots, m'_{n, i}\right) \} \in \mathcal U$ implies :$\{i : f^{\mathcal M_i}\left(m_{1, i}, \dots, m_{n, i}\right) = f^{\mathcal M_i}\left(m'_{1, i}, \dots, m'_{n, i}\right) \} \in \mathcal U$ Therefore, :$\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$, $k = 1, \dotsc, n$, by [[#Lemma]], which is equvalent to $\{i : \left(m_{1, i}, \dots, m_{n, i}\right) = \left(m'_{1, i}, \dots, m'_{n, i}\right) \} \in \mathcal U$ implies :$\left(f^{\mathcal M_i}\left(m_{1, i}, \dots, m_{n, i}\right)\right)_\mathcal U = \left(f^{\mathcal M_i}\left(m'_{1, i}, \dots, m'_{n, i}\right)\right)_\mathcal U$ {{qed}} === Proposition 2 === ''The definition of $R^\mathcal M$ is consistent.'' i.e. for $\left(m_{k, i}\right)_\mathcal U = \left(m'_{k, i}\right)_\mathcal U$, $k = 1, \dotsc, n$ :$\left\{i \in I: \left({m_{1, i}, \dots, m_{n, i} }\right) \in R^\mathcal M_i\right\} \in \mathcal U$ {{iff}} :$\left\{i \in I: \left({m'_{1, i}, \dots, m'_{n, i} }\right) \in R^\mathcal M_i\right\} \in \mathcal U$ === Proof === Let :$S := \left\{i \in I: \left({m_{1, i}, \dots, m_{n, i} }\right) \in R^\mathcal M_i\right\}$ :$S' := \left\{i \in I: \left({m'_{1, i}, \dots, m'_{n, i} }\right) \in R^\mathcal M_i\right\}$ :$I^* := \left\{ i : \left({m_{1, i}, \dots, m_{n, i} }\right) = \left({m'_{1, i}, \dots, m'_{n, i} }\right) \right \}$ :$T := I^* \cap S$ :$T' := I^* \cap S'$ As [[#Lemma]] implies :$I^* \in \mathcal U$ therefore :$S \in \mathcal U$ implies $T \in \mathcal U$ Note that :$\left({m_{1, i}, \dots, m_{n, i} }\right) = \left({m'_{1, i}, \dots, m'_{n, i} }\right)$ for $i \in I^*$ we have :$T = T'$ Hence :$T' \in \mathcal U$ and :$S' \in \mathcal U$ since $S' \supseteq T'$ So far we have proved :$S \in \mathcal U$ implies $S' \in \mathcal U$ By symmetry, :$S' \in \mathcal U$ implies $S \in \mathcal U$ {{qed}} [[Category:Model Theory]] pxaqnmgw5qct26nsoe4koay2w3aghan	0
{{BeginTableau|\paren {p \vdash \bot} \vdash \neg p}} {{Premise |1|p \vdash \bot}} {{Assumption |2|p}} {{Contradiction|3|1|\neg p|2|2}} {{EndTableau}} {{Qed}} [[Category:Proof by Contradiction]] s213zzvegzbnzr2r7p55j1sgvbbcsk8	0
We apply the [[Method of Truth Tables]] to the proposition. As can be seen by inspection, in each case, the [[Definition:Truth Value|truth values]] in the appropriate columns match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|c|c||ccc|} \hline p & \top & p & \lor & \top \\ \hline F & T & F & T & T \\ T & T & T & T & T \\ \hline \end{array}$ {{qed}}	0
Let $M$ be a [[Definition:Class (Class Theory)|class]]. Let $g: M \to M$ be a [[Definition:Mapping (Class Theory)|mapping]] on $M$. Let $M$ be [[Definition:Minimally Inductive Class under General Mapping|minimally inductive under $g$]]. Let $P: M \to \set {\T, \F}$ be a [[Definition:Propositional Function|propositional function]] on $M$. Suppose that: :$(1): \quad \map P \O = \T$ :$(2): \quad \forall x \in M: \map P x = \T \implies \map P {\map g x} = \T$ Then: :$\forall x \in M: \map P x = \T$	0
Let $T$ be a [[Definition:Finite Propositional Tableau|finite propositional tableau]]. Let $\Gamma$ be a [[Definition:Branch (Graph Theory)|branch]] of $T$. Then $\Gamma$ is a [[Definition:Finite Branch|finite branch]].	0
Let $\mathcal M, \mathcal N$ be [[Definition:Structure|$\mathcal L$-structures]] such that $\mathcal M$ is a [[Definition:Substructure|substructure]] of $\mathcal N$. {{wtd|The page [[Definition:Structure]] is a disambiguation page, in which the form in which it is used here may not be included. The level of clarity in this page generally needs improving. Hence the invocation of the Disambiguate template.}} {{Disambiguate|Definition:Structure}} Then $\mathcal M$ is an [[Definition:Elementary Substructure|elementary substructure]] of $\mathcal N$ {{iff}}: :for every [[Definition:Logical Formula|$\mathcal L$-formula]] $\phi \left({x, \bar v}\right)$ and for every $\bar a$ in $\mathcal M$: ::if there exists an $n$ in $\mathcal N$ such that $\mathcal N \models \phi \left({n, \bar a}\right)$ ::then there exists an $m$ in $\mathcal M$ such that $\mathcal N \models \phi \left({m, \bar a}\right)$. {{wtd|Before sense can be made of this page, [[Definition:Substructure]] and [[Definition:Elementary Substructure]] need to be written.}} {{Disambiguate|Definition:Logical Formula}} The condition on the right side of the {{iff}} statement above can be rephrased as: :Every existential statement with parameters from $\mathcal M$ which is satisfied in $\mathcal N$ can be witnessed by an element from the substructure $\mathcal M$.	0
Let us use the following abbreviations {{begin-eqn}} {{eqn | l = \phi | o = \text{ for } | r = p \implies q }} {{eqn | l = \psi | o = \text{ for } | r = q \implies r }} {{eqn | l = \chi | o = \text{ for } | r = p \implies r }} {{end-eqn}} {{BeginTableau|\left({p \implies q}\right) \implies \left({\left({q \implies r}\right) \implies \left({p \implies r}\right)}\right)}} {{TheoremIntro|1|\left({\phi \land \psi}\right) \implies \chi|[[Hypothetical Syllogism/Formulation 3|Hypothetical Syllogism: Formulation 3]]}} {{SequentIntro|2||\phi \implies \left({\psi \implies \chi}\right)|1|[[Rule of Exportation]]}} {{EndTableau}} Expanding the abbreviations leads us back to: : $\vdash \left({p \implies q}\right) \implies \left({\left({q \implies r}\right) \implies \left({p \implies r}\right)}\right)$ {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \map f {n_1, n_2, \ldots, n_l} | r = \map {g_1} {n_1, n_2, \ldots, n_l} \times \map {\chi_{\mathcal R_1} } {n_1, n_2, \ldots, n_l} | c = }} {{eqn | o = | ro= + | r = \map {g_2} {n_1, n_2, \ldots, n_l} \times \map {\chi_{\mathcal R_2} } {n_1, n_2, \ldots, n_l} | c = }} {{eqn | o = | ro= + | r = \cdots | c = }} {{eqn | o = | ro= + | r = \map {g_k} {n_1, n_2, \ldots, n_l} \times \map {\chi_{\mathcal R_k} } {n_1, n_2, \ldots, n_l} | c = }} {{end-eqn}} because if $\tuple {n_1, n_2, \ldots, n_l} \in \N^k$, there is a unique $r$ such that $\map {\mathcal R_r} {n_1, n_2, \ldots, n_l}$. Then $\map {\chi_{\mathcal R_r} } {n_1, n_2, \ldots, n_l} = 1$ and $\map {\chi_{\mathcal R_s} } {n_1, n_2, \ldots, n_l} = 0$ for $s \ne r$. Then the value of the {{RHS}} is $\map {g_r} {n_1, n_2, \ldots, n_l}$ as required. Since $\mathcal R_1, \mathcal R_2, \ldots, \mathcal R_k$ are [[Definition:Primitive Recursive Relation|primitive recursive]], the functions $\chi_{\mathcal R_1}, \chi_{\mathcal R_2}, \ldots, \chi_{\mathcal R_k}$ are [[Definition:Primitive Recursive Function|primitive recursive]] as well. Hence $f$ is obtained by [[Definition:Substitution (Mathematical Logic)|substitution]] from: :the [[Addition is Primitive Recursive|primitive recursive function $\operatorname{add}$]] :the [[Definition:Primitive Recursive Function|primitive recursive functions]] $g_j$ :the [[Definition:Primitive Recursive Function|primitive recursive functions]] $\chi_{\mathcal R_j}$. Hence the result. {{Qed}} === Proof of Corollary === Immediate from the main proof and [[Set Operations on Primitive Recursive Relations]]. {{qed}} [[Category:Primitive Recursive Functions]] 0o99l3ecimzdexg51bsckelv6epgk5z	0
The proof proceeds by [[Second Principle of Mathematical Induction|strong induction]]. For all $n \in \Z_{>0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$\len \left({S_n}\right) = F_n$ === Basis for the Induction === $P \left({1}\right)$ is the case: {{begin-eqn}} {{eqn | l = \len \left({S_1}\right) | r = \len \left({\text a}\right) | c = {{Defof|Fibonacci String}} }} {{eqn | r = 1 | c = {{Defof|Length of String}} }} {{eqn | r = F_1 | c = {{Defof|Fibonacci Number}}: $F_1 = 1$ }} {{end-eqn}} Thus $P \left({1}\right)$ is seen to hold. $P \left({2}\right)$ is the case: {{begin-eqn}} {{eqn | l = \len \left({S_2}\right) | r = \len \left({\text b}\right) | c = {{Defof|Fibonacci String}} }} {{eqn | r = 1 | c = {{Defof|Length of String}} }} {{eqn | r = F_2 | c = {{Defof|Fibonacci Number}}: $F_2 = 1$ }} {{end-eqn}} Thus $P \left({2}\right)$ is seen to hold. This is the [[Second Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({j}\right)$ is true, for all $j$ such that $1 \le j \le k$, then it logically follows that $P \left({k + 1}\right)$ is true. This is the [[Second Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\len \left({S_k}\right) = F_k$ and: :$\len \left({S_{k - 1} }\right) = F_{k - 1}$ from which it is to be shown that: :$\len \left({S_{k + 1} }\right) = F_{k + 1}$ === Induction Step === This is the [[Second Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \len \left({S_{k + 1} }\right) | r = \len \left({S_k S_{k - 1} }\right) | c = {{Defof|Fibonacci String}} }} {{eqn | r = \len \left({S_k}\right) + \len \left({S_{k - 1} }\right) | c = {{Defof|Length of String}} }} {{eqn | r = F_k + F_{k - 1} | c = [[Length of Fibonacci String is Fibonacci Number#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = F_{k + 1} | c = {{Defof|Fibonacci Number}} }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Second Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{>0}: \len \left({S_n}\right) = F_n$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \int_0^{\pi/2} \paren {\map \ln {\cos x} }^2 \rd x | r = \int_0^{\pi/2} \paren {\map \ln {\map \cos {\frac \pi 2 - x} } }^2 \rd x }} {{eqn | r = \int_0^{\pi/2} \paren {\map \ln {\sin x} }^2 \rd x | c = [[Cosine of Complement equals Sine]] }} {{eqn | r = \frac \pi 2 \paren {\ln 2}^2 + \frac {\pi^3} {24} | c = [[Definite Integral from 0 to Half Pi of Square of Logarithm of Sine x|Definite Integral from $0$ to $\dfrac \pi 2$ of $\paren {\map \ln {\sin x} }^2$]] }} {{end-eqn}} {{qed}}	0
By [[Riemann Zeta Function in terms of Dirichlet Eta Function]], it coincides with $\zeta$ for $\map \Re s > 1$. By [[Dirichlet Eta Function is Analytic]], it is [[Definition:Analytic Complex Function|analytic]] for $\map \Re s > 0$, except at $s = 1$. {{qed}}	0
Let $x, y \in \Q$ such that $x \ne y$. From [[Between two Rational Numbers exists Irrational Number]], there exists $\alpha \in \R \setminus \Q$ such that $x < \alpha < y$. From [[Rational Numbers are not Connected]], it follows that $x$ and $y$ belong to different [[Definition:Component (Topology)|components]] of $\Q$. As $x$ and $y$ are arbitrary, it follows that no [[Definition:Rational Number|rational number]] is in the same [[Definition:Component (Topology)|component]] as any other [[Definition:Rational Number|rational number]]. That is, the [[Definition:Component (Topology)|components]] of $\Q$ are [[Definition:Singleton|singetons]]. Hence the result, by definition of a [[Definition:Totally Disconnected Space|totally disconnected space]]. {{qed}}	0
This proof depends on the [[Definition:Exponential Function/Real/Extension of Rational Exponential|continuous extension definition of the exponential function]]. Let $\exp$ be the unique [[Definition:Continuous Extension|continuous extension]] of $e^x$ from $\Q$ to $\R$. By definition, $\exp$ is continuous. Hence the result. {{qed}}	0
For all $n \in \N_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$ === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \dfrac {x^1 - 1} {x - 1} | r = 1 | c = }} {{eqn | r = 2^0 | c = }} {{eqn | r = \sum_{j \mathop = 0}^{1 - 1} x^j | c = }} {{end-eqn}} so $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 0}^{k - 1} x^j = \frac {x^k - 1} {x - 1}$ Then we need to show: :$\displaystyle \sum_{j \mathop = 0}^k x^j = \frac {x^{k + 1} - 1} {x - 1}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^k x^j | r = \sum_{j \mathop = 0}^{k - 1} x^j + x^k | c = }} {{eqn | r = \frac {x^k - 1} {x - 1} + x^k | c = }} {{eqn | r = \frac {x^k - 1 + \paren {x - 1} x^k} {x - 1} | c = }} {{eqn | r = \frac {x^k - 1 + x^{k + 1} - x^k} {x - 1} | c = }} {{eqn | r = \frac {x^{k + 1} - 1} {x - 1} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \N_{> 0}: \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$ {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \frac x {e^x - 1} | r = \frac x 2 \left({\frac 2 {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x - e^x + 2} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {\left({e^x + 1}\right) - \left({e^x - 1}\right)} {e^x - 1} }\right) | c = }} {{eqn | r = \frac x 2 \left({\frac {e^x + 1} {e^x - 1} - 1}\right) | c = }} {{eqn | r = -\frac x 2 + \frac x 2 \left({\frac {e^x + 1} {e^x - 1} }\right) | c = }} {{eqn | r = -\frac x 2 + \frac x 2 \left({\frac {e^{x/2} + e^{-x/2} } {e^{x/2} - e^{-x/2} } }\right) | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $e^{-x/2}$ }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | n = 1 | l = \frac x 2 \left({\frac {e^{x/2} + e^{-x/2} } {e^{x/2} - e^{-x/2} } }\right) | r = \sum_{n \mathop = 0}^\infty \frac {B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = {{Defof|Bernoulli Numbers/Generating Function|Bernoulli numbers}} }} {{end-eqn}} Replacing $x$ with $2 i x$ in the {{LHS}} $(1)$: {{begin-eqn}} {{eqn | o = | r = \frac {2 i x} 2 \left({\frac {e^{2 i x / 2} + e^{-2 i x / 2} } {e^{2 i x / 2} - e^{-2 i x / 2} } }\right) | c = }} {{eqn | r = i x \left({\frac {e^{i x} + e^{-i x} } {e^{i x} - e^{-i x} } }\right) | c = }} {{eqn | r = x \cot x | c = [[Cotangent Exponential Formulation]] }} {{end-eqn}} Replacing $x$ with $2 i x$ in the {{RHS}} $(1)$: {{begin-eqn}} {{eqn | o = | r = \sum_{n \mathop = 0}^\infty \frac {B_{2 n} } {\left({2 n!}\right)} \left({2 i x}\right)^{2 n} | c = }} {{eqn | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = }} {{eqn | ll= \leadsto | l = x \cot x | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n} | c = }} {{eqn | ll= \leadsto | l = \cot x | r = \sum_{n \mathop = 0}^\infty \left({-1}\right)^n \frac {2^{2 n} B_{2 n} } {\left({2 n!}\right)} x^{2 n - 1} | c = }} {{end-eqn}} Then from [[Cotangent Minus Tangent]]: :$\tan x = \cot x - 2 \cot 2 x$ from which: {{begin-eqn}} {{eqn | l = \tan x | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} - 2 \sum_{n \mathop = 0}^\infty \frac {\left({-1}\right)^n 2^{2 n} B_{2 n} } {\left({2 n}\right)!} \left({2 x}\right)^{2 n - 1} | c = by $(1)$ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} (1 - 2^{2 n}) B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} | c = }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\left({- 1}\right)^{n - 1} 2^{2 n} (2^{2 n} - 1) B_{2 n} } {\left({2 n}\right)!} x^{2 n - 1} | c = }} {{end-eqn}} {{qed|lemma}}	0
Let $\sequence {f_n}$ denote the [[Definition:Sequence|sequence]] of mappings $f_n: \R_{>0} \to \R$ defined as: :$\map {f_n} x = n \paren {\sqrt [n] x - 1}$ Fix $x \in \R_{>0}$. We first show that $\forall n \in \N : n \paren {\sqrt [n] x - 1} < x - 1 $ === Case 1: $0 < x < 1$ === Suppose $0 < x < 1$. Then: {{begin-eqn}} {{eqn | l = 0 | o = < | m = x | mo= < | r = 1 }} {{eqn | ll= \leadsto | l = 0 | o = < | m = \sqrt [n] x^{n - k} | mo= < | r = 1 | c = [[Power Function on Base between Zero and One is Strictly Decreasing/Rational Number]] | cc= $\forall k \in \set {0, 1, \ldots, n - 1}$ }} {{eqn | ll= \leadsto | l = 0 | o = < | m = \sum_{k \mathop = 0}^{n - 1} \sqrt [n] x^{n - k} | mo= < | r = n | c = [[Real Number Ordering is Compatible with Addition]] }} {{eqn | ll= \leadsto | o = | m = \frac 1 n | mo= < | r = \frac 1 {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = [[Ordering of Reciprocals]] }} {{eqn | ll= \leadsto | o = | m = 1 | mo= < | r = \frac n {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = multiplying both sides by $n$ }} {{eqn | ll= \leadsto | o = | m = x - 1 | mo= > | r = \frac {n \paren {x - 1} } {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = multiplying both sides by $x - 1 < 0$ }} {{eqn | ll= \leadsto | o = | m = y^n - 1 | mo= > | r = \frac {\paren {y^n - 1} } {1 + y + \cdots + y^{n - 1} } | c = substituting $y = \sqrt [n] x$ }} {{eqn | ll= \leadsto | o = | m = y^n - 1 | mo= > | r = n \paren {y - 1} | c = [[Sum of Geometric Sequence]] }} {{eqn | ll= \leadsto | o = | m = x - 1 | mo= > | r = n \paren {\sqrt [n] x - 1} | c = substituting $\sqrt [n] x = y$ }} {{end-eqn}} {{qed|lemma}} === Case 2: $x = 1$ === Suppose $x = 1$. Then: {{begin-eqn}} {{eqn | l = \map \ln x | r = \map \ln 1 }} {{eqn | r = 0 | c = [[Natural Logarithm of 1 is 0/Proof 3]] }} {{eqn | r = 1 - 1 }} {{eqn | r = x - 1 }} {{end-eqn}} {{qed|lemma}} === Case 3: $x > 1$ === Suppose $x > 1$. Then: {{begin-eqn}} {{eqn | l = x | o = > | r = 1 }} {{eqn | ll= \leadsto | l = \sqrt [n] x^{n - k} | o = > | r = 1 | c = [[Power Function on Base Greater than One is Strictly Increasing/Rational Number]] | cc = $\forall k \in \set { 0, 1, \ldots, n - 1}$ }} {{eqn | ll= \leadsto | l = \sum_{k \mathop = 0}^{n - 1} \sqrt [n] x^{n - k} | o = > | r = n | c = [[Real Number Ordering is Compatible with Addition]] }} {{eqn | ll= \leadsto | l = \frac 1 n | o = > | r = \frac 1 {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = [[Ordering of Reciprocals]] }} {{eqn | ll= \leadsto | l = 1 | o = > | r = \frac n {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = Multiply both sides by $n$ }} {{eqn | ll= \leadsto | l = x - 1 | o = > | r = \frac {n \paren {x - 1} } {1 + \sqrt [n] x + \cdots + \sqrt [n] x^{n - 1} } | c = multiplying both sides by $x - 1 > 1$ }} {{eqn | ll= \leadsto | l = y^n - 1 | o = > | r = \frac {n \paren {y^n - 1} } {1 + y + \cdots + y^{n - 1} } | c = substituting $y = \sqrt [n] x$ }} {{eqn | ll= \leadsto | l = y^n - 1 | o = > | r = n \paren {y - 1} | c = [[Sum of Geometric Sequence]] }} {{eqn | ll= \leadsto | l = x - 1 | o = > | r = n \paren {\sqrt [n] x - 1} | c = Substitute $\sqrt [n] x = y$ }} {{end-eqn}} {{qed|lemma}} Thus: :$\forall n \in \N: n \paren {\sqrt [n] x - 1} \le x - 1$ by [[Proof by Cases]]. Thus: :$\displaystyle \lim_{n \mathop \to \infty} \paren {\sqrt [n] x - 1 } \le x - 1$ from [[Limit of Bounded Convergent Sequence is Bounded]]. Hence the result, from the [[Definition:Real Natural Logarithm|definition of $\ln$]]. {{qed}}	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \ \mathrm d x} {\left({a x + b}\right)^2} | r = \int \frac 1 a \frac {u - b} {a u^2} \ \mathrm d u | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 {a^2} \int \frac {\mathrm d u} u - \frac b {a^2} \int \frac {\mathrm d u} {u^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^2} \ln \left\vert{u}\right\vert + C - \frac b {a^2} \int \frac {\mathrm d u} u | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^2} \ln \left\vert{u}\right\vert - \frac b {a^2} \frac {-1} u + C | c = [[Primitive of Power]] }} {{eqn | r = \frac b {a^2 \left({a x + b}\right)} + \frac 1 {a^2} \ln \left\vert{a x + b}\right\vert + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
By [[Zerofree Analytic Function on Simply Connected Set has Logarithm]], there exists an [[Definition:Entire Function|entire function]] $g$ with $f = \exp g$. {{ProofWanted}}	0
The [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] under [[Definition:Rational Multiplication|multiplication]] $\struct {\Q, \times}$ forms a [[Definition:Monoid|monoid]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sin a x + \cos a x} | r = \frac 1 a \int \frac {\dfrac {2 \rd u} {1 + u^2} } {\dfrac {2 u} {1 + u^2} + \dfrac {1 - u^2} {1 + u^2} } | c = [[Weierstrass Substitution]]: $u = \tan \dfrac {a x} 2$ }} {{eqn | r = \frac 2 a \int \frac {\d u} {- u^2 + 2 u + 1} | c = simplifying }} {{eqn | r = \frac 2 a \paren {\frac 1 {\sqrt 8} \ln \size {\frac {-2 u + 2 - \sqrt 8} {-2 u + 2 + \sqrt 8} } } + C | c = [[Primitive of Reciprocal of a x squared plus b x plus c|Primitive of $\dfrac 1 {a x^2 + b x + c}$]] }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {u - 1 + \sqrt 2} {u - 1 - \sqrt 2} } + C | c = simplifying }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {\tan \dfrac {a x} 2 - \paren {1 - \sqrt 2} } {\tan \dfrac {a x} 2 - \paren {1 + \sqrt 2} } } + C | c = substituting for $u$ }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {\tan \dfrac {a x} 2 - \tan \dfrac \pi 8} {\tan \dfrac {a x} 2 - \tan \dfrac {3 \pi} 8} } + C | c = [[Tangent of 22.5 Degrees|Tangent of $\dfrac \pi 8$]] and [[Tangent of 67.5 Degrees|Tangent of $\dfrac {3 \pi} 8$]] }} {{end-eqn}}	0
Let $f$ and $g$ be [[Definition:Multiplicative Arithmetic Function|multiplicative]]. Let $m \perp n$. Then: {{begin-eqn}} {{eqn | l = \map {f \times g} {m \times n} | r = \map f {m \times n} \times \map g {m \times n} | c = {{Defof|Pointwise Multiplication of Integer-Valued Functions}} }} {{eqn | r = \map f m \times \map f n \times \map g m \times \map g n | c = {{Defof|Multiplicative Arithmetic Function}} }} {{eqn | r = \map f m \times \map g m \times \map f n \times \map g n | c = [[Integer Multiplication is Commutative]] }} {{eqn | r = \paren {\map {f \times g} n} \times \paren {\map {f \times g} n} | c = {{Defof|Pointwise Multiplication of Integer-Valued Functions}} }} {{end-eqn}} Hence the result by definition of [[Definition:Multiplicative Arithmetic Function|multiplicative function]]. {{qed}}	0
{{begin-eqn}} {{eqn | r = x \in Y_i | o = }} {{eqn | ll = \leadstoandfrom | r = \forall j \in I : x_j = \begin{cases} z_j & j \ne i \\ x_i \in X_i & i = j \end{cases} | o = | c = Definition of $Y_i$ }} {{eqn | ll = \leadstoandfrom | r = \forall j \in I : x_j \in Z_j | o = | c = Definition of $Z_j$ for all $j \in I$ }} {{eqn | ll = \leadstoandfrom | r = x \in \prod_{j \mathop \in I} Z_j | o = | c = {{Defof|Cartesian Product}} }} {{end-eqn}} The result follows by definition of [[Definition:Set Equality|set equality]]. {{qed}} [[Category:Subspace of Product Space Homeomorphic to Factor Space]] igffiro4fexfs4m4dwkepn6z6uoobie	0
{{begin-eqn}} {{eqn | l = \int \csc^3 x \rd x | r = \frac {-\csc a x \cot a x} {2 a} + \frac 1 2 \int \csc a x \rd x | c = [[Primitive of Power of Cosecant of a x|Primitive of $\csc^n a x$]] where $n = 3$ }} {{eqn | r = \frac {-\csc a x \cot a x} {2 a} + \frac 1 2 \paren {\frac 1 a \ln \size {\tan \dfrac {a x} 2} } | c = [[Primitive of Cosecant of a x/Tangent Form|Primitive of $\csc a x$: Tangent Form]] }} {{eqn | r = \frac {-\csc a x \cot a x} {2 a} + \frac 1 {2 a} \ln \size {\tan \dfrac {a x} 2} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $z \in \R$ be a [[Definition:Real Number|real number]]. Let $a, b \in \R$ such that $a$ is [[Definition:Congruence (Number Theory)|congruent modulo $z$]] to $b$, that is: :$a \equiv b \pmod z$ Let $m \in \R$ such that $z$ is an [[Definition:Integer Multiple|integer multiple]] of $m$: :$\exists k \in \Z: z = k m$ Then: : $a \equiv b \pmod m$	0
Let $\C$ denote the [[Definition:Complex Number|set of complex numbers]]. Let $N: \C \to \R_{\ge 0}$ denote the [[Definition:Field Norm of Complex Number|field norm on complex numbers]]: :$\forall z \in \C: \map N z = \cmod z^2$ where $\cmod z$ denotes the [[Definition:Complex Modulus|complex modulus]] of $z$. Then $N$ is a [[Definition:Multiplicative Function on Ring|multiplicative function]] on $\C$.	0
There are three cases to consider: === Points in $A$ === Consider $x \in \R$ such that $x \in A$. That is, $x = \dfrac 1 n$ for some $n \in \N$. Let: :$d = \dfrac 1 n - \dfrac 1 {n + 1}$ Consider the [[Definition:Open Real Interval|open real interval]]: :$I := \openint {\dfrac 1 n - d} {\dfrac 1 n + d} \subseteq \R$ By definition, $I$ is an [[Definition:Open Set (Real Analysis)|open set of $\R$]]. Thus $I$ is an [[Definition:Open Set (Topology)|open set]] of $\R$ which contains no element of $A$ distinct from $x$. Thus $x$ is not a [[Definition:Limit Point of Set|limit point]] of $A$ in $\R$. {{qed|lemma}} === Non-Zero Points not in $A$ === Let $x \in \R$ such that $x \ne 0$ and $x \notin A$. Let: :$d := \min \set {\size {x - m}: m \in A}$ that is, the smallest distance from $x$ to an [[Definition:Element|element]] of $A$. Consider the [[Definition:Open Real Interval|open real interval]]: :$I := \openint {x - d} {x + d} \subseteq \R$ By definition, $I$ is an [[Definition:Open Set (Real Analysis)|open set of $\R$]]. Thus $I$ is an [[Definition:Open Set (Topology)|open set]] of $\R$ which contains no [[Definition:Element|element]] of $A$ (distinct from $x$ or not). Thus $x$ is not a [[Definition:Limit Point of Set|limit point]] of $A$ in $\R$. {{qed|lemma}} === Zero === Finally, consider the point $0$. Let $U$ be an [[Definition:Open Set (Topology)|open set]] of $\R$ which contains $0$. From [[Open Sets in Real Number Line]], there exists an [[Definition:Open Real Interval|open interval]] $I$ of the form: :$I := \openint {-a} b \subseteq U$ By the [[Archimedean Principle]]: :$\exists n \in \N: n > \dfrac 1 b$ and so: :$\exists n \in \N: \dfrac 1 n < b$ But $\dfrac 1 n \in A$. Thus an [[Definition:Open Set (Topology)|open set]] $U$ which contains $0$ contains at least one element of $A$ (distinct from $0$). Thus, by definition, $0$ is a [[Definition:Limit Point of Set|limit point]] of $A$ in $\R$. {{qed|lemma}} Thus the '''only''' [[Definition:Limit Point of Set|limit point]] of $A$ in $\R$ is $0$. {{qed}} [[Category:Integer Reciprocal Space]] [[Category:Limit Points]] kkrn344ko442q6qhfupxqkb5uf7hryz	0
:$\displaystyle \int \frac {x \ \mathrm d x} {\left({x^3 + a^3}\right)^2} = \frac {x^2} {3 a^3 \left({x^3 + a^3}\right)} + \frac 1 {18 a^4} \ln \left({\frac {x^2 - a x + a^2} {\left({x + a}\right)^2} }\right) + \frac 1 {3 a^4 \sqrt 3} \arctan \frac {2 x - a} {a \sqrt 3}$	0
Let $D \subset \C$ be an [[Definition:Open Set (Complex Analysis)|open]] [[Definition:Connected Topological Space|connected set]]. Let $\left\langle{f_n}\right\rangle$ be a [[Definition:Sequence|sequence]] of [[Definition:Analytic Function|analytic functions]] $f_n: D \to \C$ that are not [[Definition:Identically Zero|identically zero]]. Let $\displaystyle \sum_{n \mathop = 1}^\infty \left({f_n - 1}\right)$ [[Definition:Locally Uniform Absolute Convergence|converge locally uniformly absolutely]] on $D$. Then: :$(1): \quad f = \displaystyle \prod_{n \mathop = 1}^\infty f_n$ [[Definition:Local Uniform Absolute Convergence of Product|converges locally uniformly absolutely]] on $D$ :$(2): \quad f$ is [[Definition:Analytic Function|analytic]] :$(3): \quad$ For each $z \in D$, $f_n \left({z}\right) = 0$ for [[Definition:Finite Set|finitely many]] $n \in \N$ :$(4): \quad$ For each $z \in D$, $\operatorname {mult}_z \left({f}\right) = \displaystyle \sum_{n \mathop = 1}^\infty \operatorname{mult}_z \left({f_n}\right)$ where $\operatorname{mult}$ denotes [[Definition:Multiplicity (Complex Analysis)|multiplicity]].	0
From [[Square of Small-Digit Palindromic Number is Palindromic/Examples/11|Square of 11 is Palindromic]]: :$11^2 = 121$ Thus: <pre> 121 x 11 ----- 121 1210 ----- 1331 </pre>{{qed}}	0
Over the complex numbers, $P$ is a product of prime factors: :$\displaystyle \map P z = \alpha \prod_{i \mathop = 1}^n (z-a_i)$ where: :the complex numbers $a_1, a_2, \ldots, a_n$ are the (not necessary distinct) zeros of the polynomial $P$ :the complex number $\alpha$ is the leading coefficient of $P$ :$n$ is the degree of $P$. Let $z$ be any complex number for which $\map P z \ne 0$. Then we have for the [[Definition:Logarithmic Derivative|logarithmic derivative]]: :$\displaystyle \frac {\map {P'} z} {\map P z} = \sum_{i \mathop = 1}^n \frac 1 {z - a_i}$ In particular, if $z$ is a zero of $P'$ and $\map P z \ne 0$, then: :$\displaystyle \sum_{i \mathop = 1}^n \frac 1 {z - a_i} = 0$ or: :$\displaystyle \sum_{i \mathop = 1}^n \frac {\overline z - \overline {a_i} } {\size {z - a_i}^2} = 0$ This may also be written as: :$\displaystyle \paren {\sum_{i \mathop = 1}^n \frac 1 {\size {z - a_i}^2} } \overline z = \paren {\sum_{i \mathop = 1}^n \frac 1 {\size {z - a_i}^2} \overline {a_i} }$ Taking their conjugates, we see that $z$ is a weighted sum with positive coefficients that sum to one, or the [[Definition:Barycentric Coordinates (Astronomy)|barycenter on affine coordinates]], of the complex numbers $a_i$ (with different mass assigned on each root whose weights collectively sum to $1$). If $\map P z = \map {P'} z = 0$, then: :$z = 1 \cdot z + 0 \cdot a_i$ and is still a [[Definition:Convex Combination|convex combination]] of the roots of $P$. {{qed}} {{Namedfor|Carl Friedrich Gauss|name2 = François Édouard Anatole Lucas|cat = Gauss|cat2 = Lucas}} [[Category:Complex Analysis]] pbpg6q87ucyirl7spobfu56x1sv47pf	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {p^2 \sin^2 a x + q^2 \cos^2 a x} | r = \int \frac {\sec^2 a x \d x} {p^2 \tan^2 a x + q^2} | c = multiplying by $\dfrac {\sec^2 a x} {\sec^2 a x}$ }} {{eqn | r = \frac 1 a \int \frac {\d t} {p^2 t^2 + q^2} | c = [[Integration by Substitution|substituting]] $t = \tan a x$ }} {{eqn | r = \frac 1 {a p^2} \int \frac {\d t} {t^2 + \paren {\frac q p}^2} }} {{eqn | r = \frac 1 {a p^2} \times \frac p q \map \arctan {\frac {p t} q} + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | r = \frac 1 {a p q} \map \arctan {\frac {p \tan a x} q} + C | c = substituting back for $t$ }} {{end-eqn}} {{qed}}	0
Let $a$ and $b$ be [[Definition:Real Number|real numbers]]. Let $i$ be the [[Definition:Imaginary Unit|imaginary unit]]. Then: :$\sin \paren {a + b i} = \sin a \cosh b + i \cos a \sinh b$ where: :$\sin$ denotes the [[Definition:Sine Function|sine function]] ([[Definition:Real Sine Function|real]] and [[Definition:Complex Sine Function|complex]]) :$\cos$ denotes the [[Definition:Real Cosine Function|real cosine function]] :$\sinh$ denotes the [[Definition:Hyperbolic Sine|hyperbolic sine function]] :$\cosh$ denotes the [[Definition:Hyperbolic Cosine|hyperbolic cosine function]].	0
Let $\map f x$ be the [[Definition:Real Function|real function]] defined on $\openint 0 \pi$ as: :$\map f x = \cos \lambda x$ From [[Half-Range Fourier Cosine Series/Cosine of Non-Integer Multiple of x over 0 to Pi|Half-Range Fourier Cosine Series: $\cos \lambda x$ over $\openint 0 \pi$]] its [[Definition:Fourier Series over Range 2 Pi|Fourier series]] can be expressed as: :$\displaystyle \cos \lambda x \sim \frac {2 \lambda \sin \lambda \pi} \pi \paren {\frac 1 {2 \lambda^2} + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\cos n x} {\lambda^2 - n^2} }$ Because of the nature of this expansion, we have that: :$\map f \pi = \map f {-\pi}$ and so the expansion holds for $x = \pi$. Also note that because $\lambda$ is not an [[Definition:Integer|integer]], $\sin \lambda \pi \ne 0$ and so $\cot \pi \lambda$ is defined. So, setting $x = \pi$: {{begin-eqn}} {{eqn | l = \cos \lambda \pi | r = \frac {2 \lambda \sin \lambda \pi} \pi \paren {\frac 1 {2 \lambda^2} + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\cos n \pi} {\lambda^2 - n^2} } | c = }} {{eqn | ll= \leadsto | l = \pi \cot \pi \lambda | r = 2 \lambda \paren {\frac 1 {2 \lambda^2} + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {\cos n \pi} {\lambda^2 - n^2} } | c = {{Defof|Real Cotangent Function}} }} {{eqn | r = \frac 1 \lambda + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {2 \lambda \cos n \pi} {\lambda^2 - n^2} | c = simplification }} {{eqn | r = \frac 1 \lambda + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {2 \lambda \paren {-1}^n} {\lambda^2 - n^2} | c = [[Cosine of Multiple of Pi]] }} {{eqn | r = \frac 1 \lambda + \sum_{n \mathop = 1}^\infty \frac {2 \lambda} {\lambda^2 - n^2} | c = simplification }} {{end-eqn}} {{qed}}	0
From [[Topological Closure is Closed]], $S^-$ is [[Definition:Closed Set (Complex Analysis)|closed]]. From [[Sequence of Imaginary Reciprocals/Boundedness|Sequence of Imaginary Reciprocals: Boundedness]], $S$ is [[Definition:Bounded Subset of Complex Plane|bounded in $\C$]]. It follows trivially that $S^-$ is also [[Definition:Bounded Subset of Complex Plane|bounded in $\C$]]. Hence the result by definition of [[Definition:Compact Subset of Complex Plane|compact]]. {{qed}}	0
By [[Form of Geometric Sequence of Integers from One]], the general [[Definition:Term of Geometric Sequence|term]] of $G_n$ can be expressed as: :$a_j = q^j$ for some $q \in \Z$. Let $k, m \in \set {1, 2, \ldots, n}$ such that $k \divides m$. By definition of [[Definition:Divisor of Integer|divisibility]]: :$\exists r \in \Z: m = r k$ Then: {{begin-eqn}} {{eqn | l = a_m | r = q^m | c = }} {{eqn | r = q^{r k} | c = }} {{eqn | r = \paren {q^r}^k | c = }} {{end-eqn}} That is, $a_m$ is a [[Definition:Integer Power|power]] of $k$. {{qed}} {{Euclid Note|8|IX}}	0
From the definition of [[Definition:Complex Number/Definition 2|complex numbers]], we define the following: {{begin-eqn}} {{eqn | l = z | o = := | r = \tuple {x_1, y_1} }} {{eqn | l = w | o = := | r = \tuple {x_2, y_2} }} {{end-eqn}} where $x_1, x_2, y_1, y_2 \in \R$. Then: {{begin-eqn}} {{eqn | l = z + w | r = \tuple {x_1, y_1} + \tuple {x_2, y_2} | c = {{Defof|Complex Number|index = 2}} }} {{eqn | r = \tuple {x_1 + x_2, y_1 + y_2} | c = {{Defof|Complex Number/Definition 2/Addition|Complex Addition}} }} {{eqn | r = \tuple {x_2 + x_1, y_2 + y_1} | c = [[Real Addition is Commutative]] }} {{eqn | r = \tuple {x_2, y_2} + \tuple {x_1, y_1} | c = {{Defof|Complex Number/Definition 2/Addition|Complex Addition}} }} {{eqn | r = w + z | c = {{Defof|Complex Number|index = 2}} }} {{end-eqn}} {{qed}}	0
Let $\struct{X, \norm{\,\cdot\,} }$ be a [[Definition:Normed Vector Space|normed vector space]]. Every [[Definition:Convergent Sequence in Normed Vector Space|convergent sequence]] in $X$ is a [[Definition:Cauchy Sequence in Normed Vector Space|Cauchy sequence]].	0
The [[Definition:Rational Multiplication|multiples]] of $\dfrac 1 {13}$ from $\dfrac 1 {13}$ to $\dfrac {12} {13}$ can be divided into two [[Definition:Set|sets]] of equal [[Definition:Cardinality|size]]: :one where the [[Definition:Digit|digits]] of the [[Definition:Recurring Part|recurring part]] consists of a [[Definition:Cyclic Permutation|cyclic permutation]] of $076923$ :one where the [[Definition:Digit|digits]] of the [[Definition:Recurring Part|recurring part]] consists of a [[Definition:Cyclic Permutation|cyclic permutation]] of $153846$. :[[File:ReciprocalOf13Cycles.png|300px]]	0
We have $a^0 = e$ so it follows trivially that $a^{-0} = \paren {a^{-1} }^0$. From the [[Inverse of Product/Monoid/General Result|general inverse of product]], we have: :$\paren {a_1 \circ a_2 \circ \cdots \circ a_n}^{-1} = a_n^{-1} \circ \cdots \circ a_2^{-1} \circ a_1^{-1}$ where $a_1, a_2, \ldots, a_n \in S$ are all [[Definition:Invertible Element|invertible]] for $\circ$. So we can put $a_1, a_2, \ldots, a_n = a$ and we have that : $a^n$ is [[Definition:Invertible Element|invertible]] for all $n \in \N$ : $\forall n \in \N: \paren {a^n}^{-1} = \paren {a^{-1 }}^n$ From the above: :$a^{-n} = \paren {a^{-1} }^n$ Thus: {{begin-eqn}} {{eqn | l = \paren {a^{-n} }^{-1} | r = \paren {\paren {a^{-1} }^n}^{-1} | c = }} {{eqn | r = \paren {\paren {a^{-1} }^{-1} }^n | c = }} {{eqn | r = a^{-\paren {-n} } | c = }} {{end-eqn}} Similarly, if $a$ is [[Definition:Invertible Element|invertible]] then $a^{-1}$ is also [[Definition:Invertible Element|invertible]]. So we also have: :$\circ^{-n} \paren {a^{-1} } = \circ^n \paren {\paren {a^{-1} }^{-1} }$ Thus: {{begin-eqn}} {{eqn | l = a^{-\paren {-n} } | r = \paren {\paren {a^{-1} }^{-1} }^n | c = }} {{eqn | r = \paren {a^{-1} }^{-n} | c = }} {{end-eqn}} Thus the result holds for all $n \in \Z$. {{qed}}	0
{{begin-eqn}} {{eqn | l = z | r = \tan a x | c = }} {{eqn | ll= \implies | l = \frac {\d z} {\d x} | r = a \sec^2 a x | c = [[Derivative of Tangent Function/Corollary|Derivative of Tangent Function: Corollary]] }} {{eqn | ll= \implies | l = \int \tan^n a x \sec^2 a x \rd x | r = \int \frac 1 a z^n \rd z | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 a \frac {z^{n + 1} } {n + 1} | c = [[Primitive of Power]] }} {{eqn | r = \frac {\tan^{n + 1} a x} {\paren {n + 1} a} + C | c = substituting for $z$ and simplifying }} {{end-eqn}} {{qed}}	0
Let $S \left({x}\right)$ [[Definition:Convergent Series|converge]] when $x = x_0$. Then by [[Existence of Interval of Convergence of Power Series]], $x_0$ is in the [[Definition:Interval of Convergence|interval of convergence]] of $S \left({x}\right)$ whose [[Definition:Midpoint of Interval|midpoint]] is $0$. The result follows by definition of [[Definition:Interval of Convergence|interval of convergence]]. {{qed}}	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Then: :$z^n - 1 = \displaystyle \prod_{k \mathop = 0}^{n - 1} \paren {z - \alpha^k}$ where $\alpha$ is a [[Definition:Primitive Complex Root of Unity|primitive complex $n$th root of unity]].	0
Let $x \in \Z$ be an [[Definition:Integer|integer]]. : If $x$ is [[Definition:Even Integer|even]] then: ::: $x^2 \equiv 0 \pmod 8$ or $x^2 \equiv 4 \pmod 8$ : If $x$ is [[Definition:Odd Integer|odd]] then: ::: $x^2 \equiv 1 \pmod 8$	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^2 - a^2} | r = \int \frac {\d x} {\paren {x - a} \paren {x + a} } | c = [[Difference of Two Squares]] }} {{eqn | r = \int \frac {\d x} {2 a \paren {x - a} } - \int \frac {\d x} {2 a \paren {x + a} } | c = [[Primitive of Reciprocal of x squared minus a squared/Logarithm Form/Proof 2/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac 1 {2 a} \int \frac {\d x} {x - a} - \frac 1 {2 a} \int \frac {\d x} {x + a} | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 {2 a} \ln \size {x - a} - \frac 1 {2 a} \ln \size {x + a} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \dfrac 1 {2 a} \ln \size {\dfrac {x - a} {x + a} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
:$f * g: \struct{S, \tau_{_S} } \to \struct {G, \tau_{_G} }$ is a [[Definition:Continuous Mapping on Set|continuous mapping]].	0
From [[Primitive of Power of a x + b|Primitive of Power of $a x + b$]]: :$\displaystyle \int \left({a x + b}\right)^n \ \mathrm d x = \frac {\left({a x + b}\right)^{n + 1} } {\left({n + 1}\right) a} + C$ where $n \ne 1$. The result follows by setting $n = -2$. {{qed}}	0
No point of $S$ is an [[Definition:Interior Point (Complex Analysis)|interior point]].	0
Let $f, g, h: S \to \R$ be [[Definition:Real-Valued Function|real-valued functions]]. Let $f + g: S \to \R$ denote the [[Definition:Pointwise Addition of Real-Valued Functions|pointwise sum of $f$ and $g$]]. Then: :$\paren {f + g} + h = f + \paren {g + h}$	0
From [[Real Number Line is Complete Metric Space]], $\R$ under the [[Definition:Usual Metric|usual metric]] is a [[Definition:Metric Space|metric space]]. The result then follows as a special case of [[Convergent Sequence in Metric Space is Cauchy Sequence]]. {{qed}}	0
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be [[Definition:Metric Space|metric spaces]]. Let $M_1$ and $M_2$ be [[Definition:Homeomorphic Metric Spaces|homeomorphic]]. Then it is not necessarily the case that $M_1$ and $M_2$ are [[Definition:Isometric Metric Spaces|isometric]].	0
Let $n \in \Z$ be an [[Definition:Integer|integer]] such that $n \ge 118$. Then between $n$ and $\dfrac {4 n} 3$ there exists at least one [[Definition:Prime Number|prime number]] of each of the forms: :$4 m - 1, 4 m + 1, 6 m - 1, 6 m + 1$	0
Let $\map f t := \map \Ci t = \displaystyle \int_t^\infty \dfrac {\cos u} u \rd u$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = -\dfrac {\cos t} t | c = }} {{eqn | ll= \leadsto | l = t \map {f'} t | r = -\cos t | c = }} {{eqn | ll= \leadsto | l = \laptrans {t \map {f'} t} | r = -\laptrans {\cos t} | c = }} {{eqn | r = -\dfrac s {s^2 + 1} | c = [[Laplace Transform of Cosine]] }} {{eqn | ll= \leadsto | l = -\dfrac \d {\d s} \laptrans {\map {f'} t} | r = -\dfrac s {s^2 + 1} | c = [[Derivative of Laplace Transform]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d s} } {s \laptrans {\map f t} - \map f 0} | r = \dfrac s {s^2 + 1} | c = [[Laplace Transform of Derivative]] }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \int \dfrac s {s^2 + 1} \rd s | c = $\map f 0 = 0$, and integrating both sides {{WRT|Integration}} $s$ }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{end-eqn}} By the [[Initial Value Theorem of Laplace Transform]]: :$\displaystyle \lim_{s \mathop \to \infty} s \laptrans {\map f t} = \lim_{t \mathop \to 0} \map f t = \map f 0 = 0$ which leads to: :$c = 0$ Thus: {{begin-eqn}} {{eqn | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} | c = }} {{eqn | ll= \leadsto | l = \laptrans {\map f t} | r = \dfrac {\map \ln {s^2 + 1} } {2 s} | c = }} {{end-eqn}} {{qed}}	0
From the definition of the [[Definition:Exponential Function/Complex/Real Functions|complex exponential function]]: :$\exp z := e^x \paren {\cos y + i \sin y}$ The result follows by definition of the [[Definition:Real Part|real part]] of a [[Definition:Complex Number|complex number]]. {{qed}}	0
Let $\displaystyle \map f x := \sum_{n \mathop = 0}^\infty a_n \paren {x - \xi}^n$ be a [[Definition:Power Series|power series]] about a point $\xi$. Let $R$ be the [[Definition:Radius of Convergence|radius of convergence]] of $S$. Then $\map f x$ is a [[Definition:Continuous Function|continuous function]] on $\set {x: \size {x - \xi} < R}$.	0
Proof by [[Principle of Mathematical Induction|induction]]: For all $m \in \Z_{> 0}$, let $P \left({m}\right)$ be the [[Definition:Proposition|proposition]]: :$\Gamma \left({m + \dfrac 1 2}\right) = \dfrac {\left({2 m}\right)!} {2^{2 m} m!} \sqrt \pi$ === Basis for the Induction === $P \left({1}\right)$ is the case: {{begin-eqn}} {{eqn | l = \Gamma \left({1 + \frac 1 2}\right) | r = \frac 1 2 \Gamma \left({\frac 1 2}\right) | c = [[Gamma Difference Equation]] }} {{eqn | r = \frac {\sqrt \pi} 2 | c = [[Gamma Function of One Half]] }} {{eqn | r = \frac 2 4 \sqrt \pi | c = }} {{eqn | r = \frac {2!} {2^{2 \times 1} 1!} \sqrt \pi | c = Definition of [[Definition:Factorial|Factorial]] }} {{eqn | r = \frac {\left({2 m}\right)!} {2^{2 m} m!} \sqrt \pi | c = where $m = 1$ }} {{end-eqn}} and so $P(1)$ holds. This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\Gamma \left({k + \dfrac 1 2}\right) = \dfrac {\left({2 k}\right)!} {2^{2 k} k!} \sqrt \pi$ Then we need to show: :$\Gamma \left({k + 1 + \dfrac 1 2}\right) = \dfrac {\left({2 \left({k + 1}\right)}\right)!} {2^{2 \left({k + 1}\right)} \left({k + 1}\right)!} \sqrt \pi$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \Gamma \left({k + 1 + \frac 1 2}\right) | r = \left({k + \frac 1 2}\right) \Gamma \left({k + \frac 1 2}\right) | c = [[Gamma Difference Equation]] }} {{eqn | r = \left({k + \frac 1 2}\right) \dfrac {\left({2 k}\right)!} {2^{2 k} k!} \sqrt \pi | c = [[Gamma Function of Positive Half-Integer#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \frac {\left({2 k + 1}\right) \left({2 k}\right)!} {2 \times 2^{2 k} k!} \sqrt \pi | c = simplifying }} {{eqn | r = \frac {\left({2 k}\right)! \left({2 k + 1}\right) \left({2 k + 2}\right)} {2 \left({2 k + 2}\right) 2^{2 k} k!} \sqrt \pi | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $2 k + 2$ }} {{eqn | r = \frac {\left({2 k + 2}\right)!} {2^{2 k + 1} \left({2 \left({k + 1}\right)}\right) k!} \sqrt \pi | c = }} {{eqn | r = \frac {\left({2 \left({k + 1}\right)}\right)!} {2^{2 \left({k + 1}\right)} \left({k + 1}\right)!} \sqrt \pi | c = }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k+1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Finally: {{begin-eqn}} {{eqn | l = \frac {\left({2 m}\right)!} {2^{2 m} m!} | r = \frac {1 \times 2 \times 3 \times \cdots \times 2 m} {2^{2 m} \ 1 \times 2 \times 3 \times \cdots \times m} | c = }} {{eqn | r = \frac {1 \times 2 \times 3 \times \cdots \times \left({2 m - 1}\right) \times 2 m} {2^m \ 2^m \ \left({1 \times 2 \times 3 \times \cdots \times m}\right)} | c = }} {{eqn | r = \frac {1 \times 2 \times 3 \times \cdots \times \left({2 m - 1}\right) \times 2 m} {2^m \left({\left({2 \times 1}\right) \times \left({2 \times 2}\right) \times \left({2 \times 3}\right) \times \cdots \times 2 m}\right)} | c = }} {{eqn | r = \frac {1 \times 2 \times 3 \times \cdots \times \left({2 m - 1}\right) \times 2 m} {2^m \left({2 \times 4 \times 6 \times \cdots \times 2 m}\right)} | c = }} {{eqn | r = \frac {1 \times 3 \times 5 \times \cdots \times \left({2 m - 1}\right)} {2^m} | c = }} {{end-eqn}} Therefore: {{begin-eqn}} {{eqn | ll= \forall m \in \Z_{>0}: | l = \Gamma \left({m + \frac 1 2}\right) | r = \frac {\left({2 m}\right)!} {2^{2 m} m!} \sqrt \pi | c = }} {{eqn | r = \frac {1 \times 3 \times 5 \times \cdots \times \left({2 m - 1}\right)} {2^m} \sqrt \pi | c = }} {{end-eqn}} {{qed}}	0
We are given that $f$ is [[Definition:Bounded Piecewise Continuous Function|piecewise continuous and bounded]] on $\closedint a b$. Therefore, there exists a [[Definition:Finite Subdivision|finite subdivision]] $\set {x_0, x_1, \ldots, x_n}$ of $\closedint a b$, where $x_0 = a$ and $x_n = b$, such that for all $i \in \set {1, 2, \ldots, n}$: :$f$ is [[Definition:Continuous Real Function on Open Interval|continuous]] on $\openint {x_{i - 1} } {x_i}$ :$f$ is [[Definition:Bounded Real-Valued Function|bounded]] on $\closedint {x_{i - 1} } {x_i}$. Note that $n$ is the number of [[Definition:Open Real Interval|intervals]] $\openint {x_{i - 1} } {x_i}$ defined from the [[Definition:Finite Subdivision|(finite) subdivision]] $\set {x_0, x_1, \ldots, x_n}$. We shall use [[Principle of Mathematical Induction|proof by induction]] on these $n$ [[Definition:Open Real Interval|intervals]]. For all $k \in \set {1, 2, \ldots, n}$, let $\map P k$ be the [[Definition:Proposition|proposition]]: :$f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_k}$. === Basis for the Induction === $\map P 1$ is the case: :$f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_{i - 1} } {x_i}$ for an arbitrary $i \in \set {1, 2, \ldots, k}$. $f$ is [[Definition:Bounded Piecewise Continuous Function|piecewise continuous and bounded]] for the case $n = 1$ means that: :$f$ is [[Definition:Continuous Real Function on Interval|continuous]] on $\openint {x_{i - 1} } {x_i}$ :$f$ is [[Definition:Bounded Real-Valued Function|bounded]] on $\closedint {x_{i - 1} } {x_i}$. By [[Bounded Function Continuous on Open Interval is Darboux Integrable]], $f$ is [[Definition:Definite Integral|Darboux integrable]] on $\closedint {x_{i - 1} } {x_i}$. Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_k}$ from which it is to be shown that: :$f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_{k + 1} }$. === Induction Step === This is the [[Definition:Induction Step|induction step]]: By definition of a [[Definition:Bounded Piecewise Continuous Function|bounded piecewise continuous function]], for every $i \in \set {1, 2, \ldots, k, k + 1}$: :$f$ is [[Definition:Continuous Real Function on Open Interval|continuous]] on $\openint {x_{i - 1} } {x_i}$ :$f$ is [[Definition:Bounded Real-Valued Function|bounded]] on $\closedint {x_{i - 1} } {x_i}$. By the [[Bounded Piecewise Continuous Function is Darboux Integrable#Induction Hypothesis|induction hypothesis]], $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_k}$. From the [[Bounded Piecewise Continuous Function is Darboux Integrable#Basis for the Induction|basis for the induction]], $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_k} {x_{k + 1} }$. We have that $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_k}$ and $\closedint {x_k} {x_{k + 1} }$. Therefore, $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_k} \cup \closedint {x_k} {x_{k + 1} }$ by [[Existence of Integral on Union of Adjacent Intervals]]. We have that: :$\closedint {x_0} {x_{k + 1} } = \closedint {x_0} {x_k} \cup \closedint {x_k} {x_{k + 1} }$. Accordingly, $f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint {x_0} {x_{k + 1} }$. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$f$ is [[Definition:Darboux Integrable Function|Darboux integrable]] on $\closedint a b$. {{qed}}	0
:$\displaystyle \int \frac {x^2 \rd x} {\paren {\sqrt {x^2 - a^2} }^3} = \frac {-x} {\sqrt {x^2 - a^2} } + \ln \size {x + \sqrt {x^2 - a^2} } + C$	0
:$\displaystyle \int \frac {\d x} {x^3 + a^3} = \frac 1 {6 a^2} \ln \size {\frac {\paren {x + a}^2} {x^2 - a x + a^2} } + \frac 1 {a^2 \sqrt 3} \arctan \frac {2 x - a} {a \sqrt 3}$	0
:$\displaystyle \int \frac {\d x} {p + q e^{a x} } = \frac x p - \frac 1 {a p} \ln \size {p + q e^{a x} } + C$	0
By [[Ordering on 1-Based Natural Numbers is Trichotomy|Ordering on $1$-Based Natural Numbers is Trichotomy]], one and only one of the following holds: :$a = b$ :$a < b$ :$b < a$ Let $a + c < b + c$. Suppose $a = b$. Then by [[Ordering on 1-Based Natural Numbers is Compatible with Addition|Ordering on $1$-Based Natural Numbers is Compatible with Addition]]: :$a + c = b + c$ By [[Ordering on 1-Based Natural Numbers is Trichotomy|Ordering on $1$-Based Natural Numbers is Trichotomy]], this contradicts the fact that $a + c < b + c$. Similarly, suppose $b < a$. Then by [[Ordering on 1-Based Natural Numbers is Compatible with Addition|Ordering on $1$-Based Natural Numbers is Compatible with Addition]]: :$b + c < a + c$ By [[Ordering on 1-Based Natural Numbers is Trichotomy|Ordering on $1$-Based Natural Numbers is Trichotomy]], this also contradicts the fact that $a + c < b + c$. The only other possibility is that $a < b$. So :$\forall a, b, c \in \N_{>0}: a + c = b + c \implies a < b$ and so $+$ is [[Definition:Right Cancellable Operation|right cancellable]] on $\N_{>0}$ for $<$. Let $a + b < a + c$. Suppose $b = c$. Then by [[Ordering on 1-Based Natural Numbers is Compatible with Addition|Ordering on $1$-Based Natural Numbers is Compatible with Addition]]: :$a + b = a + c$ By [[Ordering on 1-Based Natural Numbers is Trichotomy|Ordering on $1$-Based Natural Numbers is Trichotomy]], this contradicts the fact that $a + b < a + c$. Similarly, suppose $c < b$. Then by [[Ordering on 1-Based Natural Numbers is Compatible with Addition|Ordering on $1$-Based Natural Numbers is Compatible with Addition]]: :$a + c < a + b$ By [[Ordering on 1-Based Natural Numbers is Trichotomy|Ordering on $1$-Based Natural Numbers is Trichotomy]], this also contradicts the fact that $a + b < a + c$. The only other possibility is that $b < c$. So :$\forall a, b, c \in \N_{>0}: a + b < a + c \implies b < c$ and so $+$ is [[Definition:Left Cancellable Operation|left cancellable]] on $\N_{>0}$ for $<$. From [[Natural Number Addition is Commutative]] and [[Right Cancellable Commutative Operation is Left Cancellable]]: :$\forall a, b, c \in \N_{>0}: a + b = a + c \implies b = c$ So $+$ is both [[Definition:Right Cancellable Operation|right cancellable]] and [[Definition:Left Cancellable Operation|left cancellable]] on $\N_{>0}$. Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {\left({1 + \cos a x}\right)^2} | r = \int \left({\frac 1 2 \sec^2 \frac {a x} 2}\right)^2 \ \mathrm d x | c = [[Reciprocal of One Plus Cosine]] }} {{eqn | r = \frac 1 4 \int \sec^4 \frac {a x} 2 \ \mathrm d x | c = simplifying }} {{eqn | r = \frac 1 4 \left({\frac{\sec^2 \dfrac {a x} 2 \tan \dfrac {a x} 2} {\dfrac {3 a} 2} + \frac 2 3 \int \sec^2 \frac {a x} 2 \ \mathrm d x}\right) + C | c = [[Primitive of Power of Secant of a x|Primitive of $\sec^n a x$]] }} {{eqn | r = \frac 1 {6 a} \sec^2 \frac {a x} 2 \tan \dfrac {a x} 2 + \frac 1 6 \int \sec^2 \frac {a x} 2 \ \mathrm d x + C | c = simplifying }} {{eqn | r = \frac 1 {6 a} \sec^2 \frac {a x} 2 \tan \dfrac {a x} 2 + \frac 1 6 \left({\frac 2 a \tan \frac {a x} 2}\right) + C | c = [[Primitive of Square of Secant of a x|Primitive of $\sec^2 a x$]] }} {{eqn | r = \frac 1 {6 a} \left({1 + \tan^2 \frac {a x} 2}\right) \tan \dfrac {a x} 2 + \frac 2 {6 a} \tan \frac {a x} 2 + C | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = \frac 1 {2a} \tan \frac {a x} 2 + \frac 1 {6 a} \tan^3 \frac {a x} 2 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $\dfrac p q$ denote a [[Definition:Proper Fraction|proper fraction]] expressed in [[Definition:Canonical Form of Rational Number|canonical form]]. Let $\dfrac p q$ be expressed as the [[Definition:Integer Addition|sum]] of a [[Definition:Finite Set|finite number]] of [[Definition:Distinct Elements|distinct]] [[Definition:Unit Fraction|unit fractions]] using [[Fibonacci's Greedy Algorithm]]. Then $\dfrac p q$ is expressed using no more than $p$ [[Definition:Unit Fraction|unit fractions]].	0
From [[Cardano's Formula]], the [[Definition:Root of Polynomial|roots]] of $P$ are: :$(1): \quad x_1 = S + T - \dfrac b {3 a}$ :$(2): \quad x_2 = - \dfrac {S + T} 2 - \dfrac b {3 a} + \dfrac {i \sqrt 3} 2 \paren {S - T}$ :$(3): \quad x_3 = - \dfrac {S + T} 2 - \dfrac b {3 a} - \dfrac {i \sqrt 3} 2 \paren {S - T}$ where: :$S = \sqrt [3] {R + \sqrt {Q^3 + R^2} }$ :$T = \sqrt [3] {R - \sqrt {Q^3 + R^2} }$ Let $D = Q^3 + R^2 < 0$. Then $S^3 = R + i \sqrt {\size {Q^3 + R^2} }$. We can express this in [[Definition:Polar Form of Complex Number|polar form]]: :$S^3 = r \paren {\cos \theta + i \sin \theta}$ where: :$r = \sqrt {R^2 + \paren {\sqrt {Q^3 + R^2} }^2} = \sqrt {R^2 - \paren {Q^3 + R^2} } = \sqrt {-Q^3}$ :$\tan \theta = \dfrac {\sqrt {\size {Q^3 + R^2} } } R$ Then: :$\cos \theta = \dfrac R {\sqrt {-Q^3} }$ Similarly for $T^3$. The result: :$(1): \quad x_1 = 2 \sqrt {-Q} \, \map \cos {\dfrac \theta 3} - \dfrac b {3 a}$ :$(2): \quad x_2 = 2 \sqrt {-Q} \, \map \cos {\dfrac \theta 3 + \dfrac {2 \pi} 3} - \dfrac b {3 a}$ :$(3): \quad x_3 = 2 \sqrt {-Q} \, \map \cos {\dfrac \theta 3 + \dfrac {4 \pi} 3} - \dfrac b {3 a}$ follows after some algebra. {{qed}}	0
{{:Euclid:Proposition/V/17}} That is: :$a : b = c : d \implies \left({a - b}\right) : b = \left({c - d}\right) : d$	0
From the definition, the [[Definition:Real Number|real numbers]] are the set of all [[Definition:Equivalence Class|equivalence classes]] $\eqclass {\sequence {x_n} } {}$ of [[Definition:Cauchy Sequence|Cauchy sequences]] of [[Definition:Rational Number|rational numbers]]. Let $x = \eqclass {\sequence {x_n} } {}, y = \eqclass {\sequence {y_n} } {}, z = \eqclass {\sequence {z_n} } {}$, where $\eqclass {\sequence {x_n} } {}$, $\eqclass {\sequence {y_n} } {}$ and $\eqclass {\sequence {z_n} } {}$ are such [[Definition:Equivalence Class|equivalence classes]]. From the definition of [[Definition:Real Multiplication|real multiplication]], $x \times y$ is defined as $\eqclass {\sequence {x_n} } {} \times \eqclass {\sequence {y_n} } {} = \eqclass {\sequence {x_n \times y_n} } {}$. Thus we have: {{begin-eqn}} {{eqn | l = x \times \paren {y \times z} | r = \eqclass {\sequence {x_n} } {} \times \paren {\eqclass {\sequence {y_n} } {} \times \eqclass {\sequence {z_n} } {} } | c = }} {{eqn | r = \eqclass {\sequence {x_n} } {} \times \eqclass {\sequence {y_n \times z_n} } {} | c = }} {{eqn | r = \eqclass {\sequence {x_n \times \paren {y_n \times z_n} } } {} | c = }} {{eqn | r = \eqclass {\sequence {\paren {x_n \times y_n} \times z_n} } {} | c = [[Rational Multiplication is Associative]] }} {{eqn | r = \eqclass {\sequence {x_n \times y_n} } {} \times \eqclass {\sequence {z_n} } {} | c = }} {{eqn | r = \paren {\eqclass {\sequence {x_n} } {} \times \eqclass {\sequence {y_n} } {} } \times \eqclass {\sequence {z_n} } {} | c = }} {{eqn | r = \paren {x \times y} \times z | c = }} {{end-eqn}} {{qed}}	0
Let $x \in \R$ be a [[Definition:Real Number|real number]]. Let $0 < x < 1$. Let [[Definition:Set|set]] $S = \set {x^n: n \in \N}$. Then: :$\inf S = 0$ and: :$\sup S = 1$ where $\inf S$ and $\sup S$ are the [[Definition:Infimum of Set|infimum]] and [[Definition:Supremum of Set|supremum]] of $S$ respectively.	0
:$\map \sin {-z} = -\sin z$ That is, the [[Definition:Complex Sine Function|sine function]] is [[Definition:Odd Function|odd]].	0
:$\laptrans {\map \Ci t} = \dfrac {\map \ln {s^2 + 1} } {2 s}$ where: :$\laptrans f$ denotes the [[Definition:Laplace Transform|Laplace transform]] of the [[Definition:Real Function|function]] $f$ :$\Ci$ denotes the [[Definition:Cosine Integral Function|cosine integral function]].	0
{{begin-eqn}} {{eqn | l = \sin a \cos b + \cos a \sin b | r = \paren {\frac {e^{i a} - e^{-i a} }{2 i} } \cos b + \cos a \paren {\frac {e^{i b} - e^{-i b} }{2 i} } | c = [[Sine Exponential Formulation]] }} {{eqn | r = \paren {\frac {e^{i a} - e^{-i a} } {2 i} } \paren {\frac {e^{i b} + e^{-i b} } 2} + \paren {\frac {e^{i a} + e^{-i a} } 2} \paren {\frac {e^{i b} - e^{-i b} } {2 i} } | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \frac {e^{i a} e^{i b} + e^{i a} e^{-i b} - e^{-i a} e^{i b} - e^{-i a} e^{-i b} } {4 i} | c = expanding }} {{eqn | o = | ro= + | r = \frac {e^{i a} e^{i b} - e^{i a} e^{-i b} + e^{-i a} e^{i b} - e^{-i a} e^{-i b} } {4 i} }} {{eqn | r = \frac {e^{i a} e^{i b} - e^{-i a} e^{-i b} } {2 i} | c = sinplifying }} {{eqn | r = \frac {e^{i \paren {a + b} } - e^{-i \paren {a + b} } } {2 i} | c = [[Exponential of Sum]] }} {{eqn | r = \map \sin {a + b} | c = [[Sine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
Follows directly from the definition of [[Definition:Modulo Addition|Modulo Addition]]: {{begin-eqn}} {{eqn | l = a | o = \equiv | r = b | rr= \pmod z | c = given }} {{eqn | l = c | o = \equiv | r = c | rr= \pmod z | c = [[Congruence Modulo Real Number is Equivalence Relation]] }} {{eqn | ll= \leadsto | l = a + c | o = \equiv | r = b + c | rr= \pmod z | c = {{Defof|Modulo Addition}} }} {{end-eqn}} {{qed}} [[Category:Modulo Addition]] mcnjhhb8e1wychegup4ic439u6s1c55	0
{{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x^3 \left({a x + b}\right)^2} | r = \int \left({\frac {3 a^2} {b^4 x} + \frac {-2 a} {b^3 x^2} + \frac 1 {b^2 x^3} + \frac {-3 a^3} {b^4 \left({a x + b}\right)} + \frac {-a^3} {b^3 \left({a x + b}\right)^2} }\right) \ \mathrm d x | c = [[Primitive of Reciprocal of x cubed by a x + b squared/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac {3 a^2} {b^4} \int \frac {\mathrm d x} x + \frac {-2 a} {b^3} \int \frac {\mathrm d x} {x^2} + \frac 1 {b^2} \int \frac {\mathrm d x} {x^3} + \frac {-3 a^3} {b^4} \int \frac {\mathrm d x} {a x + b} + \frac {-a^3} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {3 a^2} {b^4} \int \frac {\mathrm d x} x + \frac {-2 a} {b^3} \frac {-1} x + \frac 1 {b^2} \frac {-1} {2 x^2} + \frac {-3 a^3} {b^4} \int \frac {\mathrm d x} {a x + b} + \frac {-a^3} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {3 a^2} {b^4} \ln \left\vert{x}\right\vert + \frac {2 a} {b^3 x} - \frac 1 {2 b^2 x^2} + \frac {-3 a^3} {b^4} \int \frac {\mathrm d x} {a x + b} + \frac {-a^3} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac {3 a^2} {b^4} \ln \left\vert{x}\right\vert + \frac {2 a} {b^3 x} - \frac 1 {2 b^2 x^2} + \frac {-3 a^3} {b^4} \frac 1 a \ln \left\vert{a x + b}\right\vert + \frac {-a^3} {b^3} \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + C | c = [[Primitive of Reciprocal of a x + b]] }} {{eqn | r = \frac {3 a^2} {b^4} \ln \left\vert{x}\right\vert + \frac {2 a} {b^3 x} - \frac 1 {2 b^2 x^2} + \frac {-3 a^2} {b^4} \ln \left\vert{a x + b}\right\vert + \frac {-a^3} {b^3} \frac {-1} {a \left({a x + b}\right)} + C | c = [[Primitive of Reciprocal of a x + b squared]] }} {{eqn | r = \frac {2 a} {b^3 x} - \frac 1 {2 b^2 x^2} + \frac {a^2} {b^3 \left({a x + b}\right)} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = [[Difference of Logarithms]] }} {{eqn | r = \frac {2 a} {b^3 x} - \frac 1 {2 b^2 x^2} + \frac {a^2} {b^3 \left({a x + b}\right)} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + \frac {3 a^2} {2 b^4} + C | c = C is an arbitrary constant }} {{eqn | r = \frac {4 a b \left({a x + b}\right) x} {2 b^4 \left({a x + b}\right) x^2} - \frac {b^2 \left({a x + b}\right)} {2 b^4 \left({a x + b}\right) x^2} + \frac {2 a^2 b x^2} {2 b^4 \left({a x + b}\right) x^2} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + \frac {3 a^2 \left({a x + b}\right) x^2} {2 b^4 \left({a x + b}\right) x^2} + C | c = Represent with common denominators }} {{eqn | r = \frac {4 a b \left({a x + b}\right) x - b^2 \left({a x + b}\right) + 2 a^2 b x^2 + 3 a^2 \left({a x + b}\right) x^2} {2 b^4 \left({a x + b}\right) x^2} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = Combine fractions }} {{eqn | r = \frac {4 a^2 b x^2 + 4 a b^2 x - a b^2 x - b^3 + 2 a^2 b x^2 + 3 a^3 x^3 + 3 a^2 b x^2} {2 b^4 \left({a x + b}\right) x^2} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = Expansion }} {{eqn | r = \frac {6 a \left({a x + b}\right)^2 x - \left({a x + b}\right)^3 - 2 a^3 x^3} {2 b^4 \left({a x + b}\right) x^2} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = Factorisation }} {{eqn | r = \frac {6 a \left({a x + b}\right)^2 x} {2 b^4 \left({a x + b}\right) x^2} - \frac {\left({a x + b}\right)^3} {2 b^4 \left({a x + b}\right) x^2} - \frac {2 a^3 x^3} {2 b^4 \left({a x + b}\right) x^2} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = Split into separate fractions }} {{eqn | r = \frac {3 a \left({a x + b}\right)} {b^4 x} - \frac {\left({a x + b}\right)^2} {2 b^4 x^2} - \frac {a^3 x} {b^4 \left({a x + b}\right)} + \frac {3 a^2} {b^4} \ln \left\vert{\frac x {a x + b} }\right\vert + C | c = Simplify }} {{end-eqn}} {{qed}}	0
:$\map \phi 2 = 1$	0
Let $\sequence {x_n}$ be [[Definition:Decreasing Real Sequence|decreasing]] and [[Definition:Bounded Below Real Sequence|bounded below]]. Then $\sequence {x_n}$ [[Definition:Convergent Real Sequence|converges]] to its [[Definition:Infimum of Sequence|infimum]].	0
:$\dfrac 1 {1089} = 0 \cdotp \dot 00091 \, 82736 \, 45546 \, 37281 \, 9 \dot 1$	0
Let $m \in S$ and $m < p$. Then $\left\langle{x_n}\right\rangle$ eventually succeeds $m$. Thus by [[Extended Transitivity]], $\left\langle{y_n}\right\rangle$ eventually succeeds $m$. A similar argument using $\left\langle{z_n}\right\rangle$ proves the dual statement. Thus $\left\langle{y_n}\right\rangle$ is eventually in each ray containing $p$, so it converges to $p$. {{qed}} [[Category:Order Topology]] [[Category:Limits of Sequences]] skzmfnjs6c5wl2fah2fqawkhp58tt2k	0
For clarity the [[Quaternion Group/Cayley Table|Cayley table of $Q$]] is presented below: {{:Quaternion Group/Cayley Table}} By definition $Q$ is [[Definition:Hamiltonian Group|Hamiltonian]] {{iff}}: :$Q$ is [[Definition:Abelian Group|non-abelian]] and: :every [[Definition:Subgroup|subgroup]] of $Q$ is [[Definition:Normal Subgroup|normal]]. $Q$ is [[Definition:Abelian Group|non-abelian]] as demonstrated by the counter-example: :$a b \ne b a$ From [[Subgroups of Quaternion Group]]: {{:Subgroups of Quaternion Group}} From [[Trivial Subgroup and Group Itself are Normal]]: :$Q$ and $\set e$ are [[Definition:Normal Subgroup|normal subgroups]] of $Q$. From [[Center of Quaternion Group]], $\gen {a^2} = \set {e, a^2}$ is the [[Definition:Center of Group|center]] of $Q$. From [[Center of Group is Normal Subgroup]], $\set {e, a^2}$ is [[Definition:Normal Subgroup|normal]] in $Q$. The remaining [[Definition:Subgroup|subgroups]] of $Q$ are of [[Definition:Order of Group|order $4$]], and so have [[Definition:Index of Subgroup|index]] $2$. From [[Subgroup of Index 2 is Normal]] it follows that all of these [[Definition:Order of Group|order $4$]] [[Definition:Subgroup|subgroups]] of $Q$ are [[Definition:Normal Subgroup|normal]]. That accounts for all [[Definition:Subgroup|subgroups]] of $Q$. Hence the result. {{qed}}	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $\struct {\Q_p, \norm {\,\cdot\,}_p}$ be the [[Definition:P-adic Number|$p$-adic numbers]]. Let $\sequence {x_n} $ be a [[Definition:Sequence|sequence]] in $\Q_p$. {{TFAE}} === [[Definition:Convergent Sequence/P-adic Numbers/Definition 1|Definition 1]] === {{:Definition:Convergent Sequence/P-adic Numbers/Definition 1|Definition 1}} === [[Definition:Convergent Sequence/P-adic Numbers/Definition 2|Definition 2]] === {{:Definition:Convergent Sequence/P-adic Numbers/Definition 2}} === [[Definition:Convergent Sequence/P-adic Numbers/Definition 3|Definition 3]] === {{:Definition:Convergent Sequence/P-adic Numbers/Definition 3}} === [[Definition:Convergent Sequence/P-adic Numbers/Definition 4|Definition 4]] === {{:Definition:Convergent Sequence/P-adic Numbers/Definition 4}}	0
Follows from [[Real Multiplication Distributes over Addition|the distribution of multiplication over addition]]: {{begin-eqn}} {{eqn | l = \left({x + y}\right)^2 | r = \left({x + y}\right) \cdot \left({x + y}\right) }} {{eqn | r = x \cdot \left({x + y}\right) + y \cdot \left({x + y}\right) | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = x \cdot x + x \cdot y + y \cdot x + y \cdot y | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = x^2 + 2xy + y^2 | c = }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {x^2 \rd x} {\paren {a^2 - x^2}^2} = \frac x {2 \paren {a^2 - x^2} } - \frac 1 {4 a} \map \ln {\frac {a + x} {a - x} } + C$ for $x^2 < a^2$.	0
From [[Change of Base of Logarithm]]: :$\log_a x = \log_a b \, \log_b x$ Substituting $a = 10$ and $b = e$ gives: :$\log_{10} x = \paren {\log_{10} e} \paren {\ln x}$ The [[Common Logarithm of e]]: :$\log_{10} e = 0 \cdotp 43429 \, 44819 \, 03 \ldots$ can be calculated or looked up. {{qed}}	0
{{begin-eqn}} {{eqn |l = \zeta\left(s\right) - \eta\left(s\right) |r = \sum_{n \mathop = 1}^\infty \frac 1 {n^s} - \sum_{n\mathop = 1}^\infty \frac{(-1)^{n-1} }{n^s} |c = {{Defof|Riemann Zeta Function}}, {{Defof|Dirichlet Eta Function}} }} {{eqn |r = \sum_{n \mathop = 1}^\infty \left( \frac 1 {n^s} +\frac{(-1)^n}{n^s} \right) |c = [[Linearity of Series]] }} {{eqn |r = 2\sum_{n=1}^\infty \frac 1 {\left(2n\right)^s} }} {{eqn |r = 2^{1-s}\sum_{n=1}^\infty \frac 1 {n^s} }} {{eqn |r = 2^{1-s}\zeta\left(s\right) |c = {{Defof|Riemann Zeta Function}} }} {{end-eqn}} Rearranging, {{begin-eqn}} {{eqn |l=\left(2^{1-s}-1\right)\zeta\left(s\right) | r = -\eta\left(s\right) }} {{eqn |ll=\leadsto |l =\zeta\left(s\right) |r = \frac 1 {1-2^{1-s} } \eta\left(s\right) }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = x \, {}_2 \map {F_1} {\frac 1 2, 1; \frac 3 2; -x^2} | r = x \sum_{n \mathop = 0}^\infty \frac {\paren {\frac 1 2}^{\bar n} 1^{\bar n} } {\paren {\frac 3 2}^{\bar n} } \frac {\paren {-x^2}^n} {n!} | c = {{Defof|Gaussian Hypergeometric Function}} }} {{eqn | r = x \sum_{n \mathop = 0}^\infty \frac {\map \Gamma {\frac 1 2 + n} } {\map \Gamma {\frac 1 2} } \times \frac {\map \Gamma {\frac 3 2} } {\map \Gamma {\frac 3 2 + n} } n! \frac {\paren {-x^2}^n} {n!} | c = [[Rising Factorial as Quotient of Factorials]], [[One to Integer Rising is Integer Factorial]] }} {{eqn | r = x \sum_{n \mathop = 0}^\infty \frac {\map \Gamma {\frac 1 2 + n} } {\map \Gamma {\frac 1 2 + n} } \times \frac {\map \Gamma {\frac 1 2} } {\map \Gamma {\frac 1 2} } \times \frac 1 {2 \paren {n + \frac 1 2} } \paren {-1}^n x^{2 n} | c = [[Gamma Difference Equation]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {2 n + 1} }} {{eqn | r = \arctan x | c = [[Power Series Expansion for Real Arctangent Function]] }} {{end-eqn}} {{qed}}	0
:$\lcm \set {a, b} \times \gcd \set {a, b} = \size {a b}$ where: :$\lcm \set {a, b}$ denotes the [[Definition:Lowest Common Multiple of Integers|lowest common multiple]] of $a$ and $b$ :$\gcd \set {a, b}$ denotes the [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] of $a$ and $b$.	0
:$\map {\dfrac \d {\d x} } {\cosh^{-1} x} = \dfrac 1 {\sqrt {x^2 - 1} }$	0
Let $\struct {T_1, \odot}$ and $\struct {T_2, \oplus}$ be [[Definition:Monoid|monoids]]. Let $\phi: \struct {T_1, \odot} \to \struct {T_2, \oplus}$ be a [[Definition:Semigroup Homomorphism|(semigroup) homomorphism]]. Let $a$ be an [[Definition:Invertible Element|invertible element]] of $T_1$. Let $n \in \Z$. Let $\odot^n$ and $\oplus^n$ be as defined as in [[Index Laws for Monoids]]. Then: :$\forall n \in \Z: \map \phi {\map {\odot^n} a} = \map {\oplus^n} {\map \phi a}$	0
The operation of [[Definition:Real Multiplication|multiplication]] on the [[Definition:Set|set]] of [[Definition:Real Number|real numbers]] $\R$ is [[Definition:Associative|associative]]: :$\forall x, y, z \in \R: x \times \paren {y \times z} = \paren {x \times y} \times z$	0
From [[Sum of Arithmetic-Geometric Sequence]]: :$\displaystyle \sum_{j \mathop = 0}^n \paren {a + j d} r^j = \frac {a \paren {1 - r^{n + 1} } } {1 - r} + \frac {r d \paren {1 - \paren {n + 1} r^n + n r^{n + 1} } } {\paren {1 - r}^2}$ Hence: {{begin-eqn}} {{eqn | l = \displaystyle \sum_{j \mathop = 0}^n j \, 2^j | r = \frac {0 \paren {1 - 2^{n + 1} } } {1 - 2} + \frac {2 \times 1 \paren {1 - \paren {n + 1} 2^n + n 2^{n + 1} } } {\paren {1 - 2}^2} | c = putting $a = 0, d = 1, r = 2$ }} {{eqn | r = 2 \paren {1 - \paren {n + 1} 2^n + n 2^{n + 1} } | c = initial simplification }} {{eqn | r = 2 - \paren {n + 1} 2^{n + 1} + n 2^{n + 2} | c = further simplification }} {{end-eqn}} Hence the result. {{qed}}	0
Let $f$ be a [[Definition:Real Function|real function]] which is [[Definition:Convex Real Function|convex]] on the [[Definition:Open Real Interval|open interval]] $\left({a \,.\,.\, b}\right)$. Then $f$ is [[Definition:Continuous on Interval|continuous]] on $\left({a \,.\,.\, b}\right)$.	0
Let $\alpha = a + b i$ be a [[Definition:Unit of Ring|unit]] of $\struct {\Z \sqbrk i, +, \times}$. Then by definition of [[Definition:Unit of Ring|unit]]: :$\exists\beta = c + d i \in \Z \sqbrk i: \alpha \beta = 1$ Let $\cmod \alpha$ denote the [[Definition:Complex Modulus|modulus]] of $\alpha$. Then: {{begin-eqn}} {{eqn | l = \cmod \alpha^2 \cdot \cmod \beta^2 | r = \cmod {\alpha \beta}^2 | c = [[Modulus of Product]] }} {{eqn | r = \cmod 1^2 | c = }} {{eqn | r = 1 | c = }} {{end-eqn}} By [[Divisors of One]]: :$\cmod a^2 = 1$ or $-1$ Since $\cmod \alpha$ and $\cmod \beta$ are [[Definition:Positive Integer|positive integers]]: :$\cmod \alpha^2 = a^2 + b^2 = 1$ and so either: :$\cmod a = 1$ and $\cmod b = 0$ or: :$\cmod b = 1$ and $\cmod a = 0$. Hence the [[Definition:Set|set]] of [[Definition:Unit of Ring|units]] of $\struct {\Z \sqbrk i, +, \times}$ is $\set {\pm 1, \pm i}$. {{qed}}	0
[[Definition:Modulo Multiplication|Multiplication modulo $m$]] is [[Definition:Distributive Operation|distributive]] over [[Definition:Modulo Addition|addition modulo $m$]]: :$\forall \eqclass x m, \eqclass y m, \eqclass z m \in \Z_m$: :: $\eqclass x m \times_m \paren {\eqclass y m +_m \eqclass z m} = \paren {\eqclass x m \times_m \eqclass y m} +_m \paren {\eqclass x m \times_m \eqclass z m}$ :: $\paren {\eqclass x m +_m \eqclass y m} \times_m \eqclass z m = \paren {\eqclass x m \times_m \eqclass z m} +_m \paren {\eqclass y m \times_m \eqclass z m}$ where $\Z_m$ is the [[Definition:Integers Modulo m|set of integers modulo $m$]]. That is, $\forall x, y, z, m \in \Z$: : $x \paren {y + z} \equiv x y + x z \pmod m$ : $\paren {x + y} z \equiv x z + y z \pmod m$	0
We have: {{begin-eqn}} {{eqn | l = \int_0^{\frac \pi 2} \sin^2 x \rd x | r = \int_0^{\frac \pi 2} \sin^2 \paren {\frac \pi 2 - x} \rd x | c = [[Integral between Limits is Independent of Direction]] }} {{eqn | r = \int_0^{\frac \pi 2} \cos^2 x \rd x | c = [[Sine of Complement equals Cosine]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = 2 \int_0^{\frac \pi 2} \sin^2 x \rd x | r = \int_0^{\frac \pi 2} \paren {\sin^2 x + \cos^2 x} \rd x }} {{eqn | r = \int_0^{\frac \pi 2} \rd x | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \frac \pi 2 | c = [[Primitive of Constant]] }} {{end-eqn}} giving: :$\displaystyle \int_0^{\frac \pi 2} \sin^2 x \rd x = \frac \pi 4$ {{qed}}	0
:$\map {\dfrac \d {\d x} } {\tan x} = \sec^2 x = \dfrac 1 {\cos^2 x}$ when $\cos x \ne 0$.	0
By [[Existence of Laurent Series]], there exists a [[Definition:Laurent Series|Laurent series]]: :$\displaystyle f\left({z}\right) = \sum_{n \mathop = -\infty}^\infty c_n \left({z - a}\right)^n$ which is [[Definition:Convergent Series of Numbers|convergent]] in $D \setminus \left\{a\right\}$, where $\left({c_n}\right)$ is a doubly infinite sequence of complex coefficients. We are given that $f$ has only a [[Definition:Simple Pole|simple pole]] at $a$. Thus $c_n = 0$ for $n < -1$. So we can write: :$\displaystyle f\left({z}\right) = \sum_{n \mathop = 0}^\infty c_n \left({z - a}\right)^n + \frac {c_{-1} } {z - a}$ Then: {{begin-eqn}} {{eqn | l = \lim_{z \mathop \to a} \left({z - a}\right) f\left({z}\right) | r = \lim_{z \mathop \to a} \left({z - a}\right) \left({ \sum_{n \mathop = 0}^\infty c_n \left({z - a}\right)^n + \frac {c_{-1} } {z - a} } \right) }} {{eqn | r = \lim_{z \mathop \to a} \left({ \sum_{n \mathop = 0}^\infty c_n \left({z - a}\right)^{n + 1} + c_{-1} }\right) }} {{eqn | r = \sum_{n \mathop = 0}^\infty c_n \left({a - a}\right)^{n+1} + c_{-1} }} {{eqn | r = 0 \sum_{n \mathop = 0}^\infty c_n + c_{-1} }} {{eqn | r = c_{-1} }} {{eqn | r = \operatorname{Res} \left({f, a}\right) | c = {{Defof|Residue (Complex Analysis)|Residue}} }} {{end-eqn}} {{qed}} [[Category:Complex Analysis]] 0zekccwvg5l1wwblefgy8gxori8tg9q	0
:$\displaystyle \cos^{2n+1} \theta = \frac 1 {2^{2n}} \sum_{k \mathop = 0}^n \binom {2n+1} k \cos \left({2n - 2k + 1}\right) \theta$	0
Let $\struct {\R, \tau_d}$ be the [[Definition:Real Number Line with Euclidean Topology|real number line with the usual (Euclidean) topology]]. Let $a \in \R$ be a [[Definition:Real Number|real number]]. Then: :$\set a^\circ = \O$ where $\set a^\circ$ denotes the [[Definition:Interior (Topology)|interior]] of $\set a$ in $\R$.	0
{{AimForCont}} there exists a [[Definition:Endorelation|relation]] $\preceq$ on $\C$ which is [[Definition:Ordering Compatible with Ring Structure|ordering compatible with the ring structure of $\C$]]. That is: :$(1): \quad z \ne 0 \implies 0 \prec z \lor z \prec 0$, but not both :$(2): \quad 0 \prec z_1, z_2 \implies 0 \prec z_1 z_2 \land 0 \prec z_1 + z_2$ By [[Totally Ordered Ring Zero Precedes Element or its Inverse]], $(1)$ can be replaced with: :$(1'): \quad z \ne 0 \implies 0 \prec z \lor 0 \prec -z$, but not both. As $i \ne 0$, it follows that: :$0 \prec i$ or $0 \prec -i$ Suppose $0 \prec i$. Then: {{begin-eqn}} {{eqn | l = 0 | o = \prec | r = i \times i | c = from $(2)$ }} {{eqn | r = -1 | c = {{Defof|Complex Number|index = 1}} }} {{end-eqn}} Otherwise, suppose $0 \prec \paren {-i}$. Then: {{begin-eqn}} {{eqn | l = 0 | o = \prec | r = \paren {-i} \times \paren {-i} | c = from $(2)$ }} {{eqn | r = -1 | c = {{Defof|Complex Number|index = 1}} }} {{end-eqn}} Thus by [[Proof by Cases]]: :$0 \prec -1$ Thus it follows that: {{begin-eqn}} {{eqn | l = 0 | o = \prec | r = \paren {-1} \times \paren {-1} | c = from $(2)$ }} {{eqn | r = 1 | c = }} {{end-eqn}} Thus both: :$0 \prec -1$ and: :$0 \prec 1$ This [[Definition:Contradiction|contradicts]] hypothesis $(1')$: :$(1): \quad z \ne 0 \implies 0 \prec z \lor 0 \prec -z$, but not both Hence, by [[Proof by Contradiction]], there can be no such ordering. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\cot a x} | r = \int \tan a x \rd x | c = [[Cotangent is Reciprocal of Tangent]] }} {{eqn | r = \frac {-\ln \size {\cos a x} } a + C | c = [[Primitive of Tangent of a x/Cosine Form|Primitive of $\tan a x$]] }} {{end-eqn}} {{qed}}	0
Let $\sequence {x_n}_{n \mathop \in \N_{>0} }$ be the [[Definition:Real Sequence|sequence in $\R$]] defined as: :$x_n = \sqrt n$ Then, despite the fact that from [[Difference Between Adjacent Square Roots Converges]]: :$\size {\sqrt {n + 1} - \sqrt n} \to 0$ as $n \to \infty$ it is not the case that $\sequence {x_n}$ is a [[Definition:Real Cauchy Sequence|Cauchy sequence]].	0
For all $n \in \Z_{>0}$, let $\map {\phi_n} x$ be the [[Definition:Real Function|real function]] defined on the [[Definition:Real Interval|interval]] $\openint 0 \lambda$ as: :$\map {\phi_n} x = \sqrt {\dfrac 2 \lambda} \sin \dfrac {n \pi x} \lambda$ Let $S$ be the [[Definition:Set|set]]: :$S = \set {\phi_n: n \in \Z_{>0} }$ Then $S$ is an [[Definition:Orthonormal Set of Real Functions|orthonormal set]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {x \paren {a x + b} } | r = \int \paren {\dfrac 1 {b x} - \dfrac a {b \paren {a x + b} } } \rd x | c = [[Primitive of Reciprocal of x by a x + b/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac 1 b \int \frac {\d x} x - \frac a b \int \frac {\d x} {a x + b} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 b \ln \size x - \frac a b \int \frac {\d x} {a x + b} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 b \ln \size x - \frac 1 b \ln \size {a x + b} + C | c = [[Primitive of Reciprocal of a x + b|Primitive of $\dfrac 1 {a x + b}$]] }} {{eqn | r = \frac 1 b \ln \size {\frac x {a x + b} } + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
{{Proofread}} Let $C$ be any arbitrary closed curve which defines a region $R$ where the function $\map f z$ is [[Definition:Analytic Function|analytic]]. Let $z_0$ be any point in the region $R$ such that: :$\dfrac {\map f z} {z - z_0}$ is [[Definition:Analytic Function|analytic]] everywhere except at $z_0$. We draw a circle $C_1$ with center at $z_0$ and radius $r$ such that $r \to 0$. This makes $C$ and $C_1$ a multiply connected region. {{explain|A diagram at this point would be useful.}} According to Cauchy's Integral Theorem for a multiply connected region: {{begin-eqn}} {{eqn | l = I | o = := | r = \oint_C \frac {\map f z} {z - z_0} \rd z | c = }} {{eqn | r = \oint_{C_1} \frac {\map f z} {z - z_0} \rd z | c = }} {{eqn | r = \oint_{C_1} \frac {\map f {z_0} + \paren {\map f z - \map f {z_0} } } {z - z_0} \rd z | c = }} {{eqn | r = \map f {z_0} \oint_{C_1} \frac {\rd z} {z - z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z | c = }} {{end-eqn}} Let: {{begin-eqn}} {{eqn | l = z - z_0 | r = r e^{i \theta} | c = }} {{eqn | ll= \leadsto | l = \d z | r = i r e^{i \theta} \rd \theta | c = }} {{eqn | ll= \leadsto | l = \oint_{C_1} \frac {\rd z} {z - z_0} | r = \int_0^{2 \pi} \frac {i r e^{i \theta} } {r e^{i \theta} } \rd \theta | c = }} {{eqn | r = i \int_0^{2 \pi} \rd \theta | c = }} {{eqn | r = 2 \pi i | c = }} {{end-eqn}} Now: :$\displaystyle I = 2 \pi i \map f {z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z$ According to Epsilon-Delta definition of limit, for every $\left|{z - z_0}\right| < \delta$ there exists a $\epsilon \in \R_{>0}$ such that: :$\cmod {\map f z - \map f {z_0} } < \epsilon$ Hence: {{begin-eqn}} {{eqn | l = \cmod {\oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z} | o = \le | r = \oint_{C_1} \frac {\cmod {\map f z - \map f {z_0} } } {z - z_0} \cmod {\d z} | c = }} {{eqn | o = \le | r = \frac {\epsilon} {\delta} \oint_{C_1} \cmod {\d z} | c = }} {{eqn | r = 2 \pi \epsilon | c = }} {{end-eqn}} {{explain|The precise meaning of $\cmod {\d z}$}} As $\epsilon \to 0$: :$\displaystyle \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z = 0$ So: {{begin-eqn}} {{eqn | l = I | r = \map f {z_0} \oint_{C_1} \frac {\rd z} {z - z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z | c = }} {{eqn | r = 2 \pi i \map f {z_0} + 0 | c = }} {{eqn | ll= \leadsto | l = \oint_C \frac {\map f z} {z - z_0} \rd z | r = 2 \pi i \, \map f {z_0} | c = }} {{end-eqn}} {{qed}} {{Namedfor|Augustin Louis Cauchy|cat = Cauchy}}	0
The [[Definition:Characteristic of Ring|characteristic]] of any [[Definition:Subfield|subfield]] of the [[Definition:Field of Complex Numbers|field of complex numbers]] is $0$.	0
=== Proof of Existence === We have that: :$\forall a, b \in \Z: 1 \divides a \land 1 \divides b$ so $1$ is always a [[Definition:Common Divisor of Integers|common divisor]] of any two [[Definition:Integer|integers]]. {{qed|lemma}} === Proof of there being a Largest === As the definition of $\gcd$ shows that it is symmetric, we can assume without loss of generality that $a \ne 0$. First we note that from [[Absolute Value of Integer is not less than Divisors]]: :$\forall c \in \Z: \forall a \in \Z_{>0}: c \divides a \implies c \le \size c \le \size a$ The same applies for $c \divides b$. Now we have three different results depending on $a$ and $b$: {{begin-eqn}} {{eqn | l = a \ne 0 \land b \ne 0 | o = \implies | r = \gcd \set {a, b} \le \min \set {\size a, \size b} | c = }} {{eqn | l = a = 0 \lor b = 0 | o = \implies | r = \gcd \set {a, b} = \max \set {\size a, \size b} | c = }} {{eqn | l = a = b = 0 | o = \implies | r = \forall x \in \Z: x \divides a \land x \divides b | c = }} {{end-eqn}} So if $a$ and $b$ are ''both'' [[Definition:Zero (Number)|zero]], then ''any'' $n \in \Z$ divides both, and there is no [[Definition:Greatest Common Divisor|greatest common divisor]]. This is why the proviso that $a \ne 0 \lor b \ne 0$. So we have proved that [[Definition:Common Divisor|common divisors]] exist and are [[Definition:Bounded Above Set|bounded above]]. Therefore, from [[Set of Integers Bounded Above by Integer has Greatest Element]] there is always a [[Definition:Greatest Common Divisor|'''greatest''' common divisor]]. {{qed}}	0
Let $x$ and $y$ be elements of either the [[Definition:Real Number|real numbers]] $\R$ or the [[Definition:Complex Number|complex numbers]] $\C$. Then: :$\cmod {x - y} \ge \cmod x - \cmod y$ where $\cmod x$ denotes either the [[Definition:Absolute Value|absolute value]] of a [[Definition:Real Number|real number]] or the [[Definition:Complex Modulus|complex modulus]] of a [[Definition:Complex Number|complex number]].	0
Let $z = a x + \arctan \dfrac q p$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {p \sin a x + q \cos a x + r} | r = \int \frac {\d x} {r + \sqrt {p^2 + q^2} \map \sin {a x + \arctan \dfrac q p} } | c = [[Multiple of Sine plus Multiple of Cosine]] }} {{eqn | r = \frac 1 a \int \frac {\d z} {r + \sqrt {p^2 + q^2} \sin z} | c = [[Primitive of Function of a x + b|Primitive of Function of $a x + b$]] }} {{end-eqn}} Let $d = \sqrt {p^2 + q^2}$. Then: {{begin-eqn}} {{eqn | r = \int \frac {\d x} {p \sin a x + q \cos a x + r} | o = | c = }} {{eqn | r = \frac 1 a \int \frac {\d z} {r + d \sin z} | c = }} {{eqn | r = \frac 1 a \begin{cases} \ds \frac 2 {\sqrt {r^2 - d^2} } \map \arctan {\frac {r \tan \dfrac z 2 + d} {\sqrt {r^2 - d^2} } } + C & : d^2 - r^2 < 0 \\ \ds \frac 1 {\sqrt {d^2 - r^2} } \ln \size {\frac {r \tan \dfrac z 2 + d - \sqrt {r^2 - d^2} } {r \tan \dfrac z 2 + d + \sqrt {r^2 - d^2} } } + C & : r^2 - d^2 > 0 \\ \end{cases} | c = [[Primitive of Reciprocal of p plus q by Sine of a x|Primitive of $\dfrac 1 {p + q \sin a x}$]] }} {{eqn | r = \begin{cases} \ds \frac 2 {a \sqrt {r^2 - p^2 - q^2} } \map \arctan {\frac {r \tan \dfrac z 2 + \sqrt {p^2 + q^2} } {\sqrt {r^2 - p^2 - q^2} } } + C & : r^2 > p^2 + q^2 \\ \ds \frac 1 {a \sqrt {p^2 + q^2 - r^2} } \ln \size {\frac {r \tan \dfrac z 2 + \sqrt {p^2 + q^2} - \sqrt {r^2 - p^2 - q^2} } {r \tan \dfrac z 2 + \sqrt {p^2 + q^2} + \sqrt {r^2 - p^2 - q^2} } } + C & : r^2 < p^2 + q^2 \\ \end{cases} | c = substituting for $d$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \tan \frac z 2 | r = \map \tan {\frac {a x} 2 + \frac 1 2 \arctan \frac q p} | c = substituting for $z$ }} {{eqn | r = \frac {\tan \dfrac {a x} 2 + \map \tan {\frac 1 2 \arctan \frac q p} } {1 - \tan \dfrac {a x} 2 \map \tan {\frac 1 2 \arctan \frac q p} } | c = [[Tangent of Sum]] }} {{end-eqn}} Now let: {{begin-eqn}} {{eqn | l = y | r = \map \tan {\frac 1 2 \arctan \frac q p} | c = }} {{eqn | ll= \leadsto | l = \arctan y | r = \frac 1 2 \arctan \frac q p | c = }} {{eqn | ll= \leadsto | l = 2 \arctan y | r = \arctan \frac q p | c = }} {{eqn | ll= \leadsto | l = \map \tan {2 \arctan y} | r = \frac q p | c = }} {{eqn | ll= \leadsto | l = \frac {2 \map \tan {\arctan y} } {1 - \map {\tan^2} {\arctan y} } | r = \frac q p | c = [[Double Angle Formula for Tangent]] }} {{eqn | ll= \leadsto | l = \frac {2 y} {1 - y^2} | r = \frac q p | c = {{Defof|Arctangent}} }} {{eqn | ll= \leadsto | l = q y^2 + 2 p y - q | r = 0 | c = rearranging }} {{eqn | ll= \leadsto | l = y | r = \frac {-2 p \pm \sqrt{4 p^2 + 4 q ^2} } {2 q} | c = [[Quadratic Formula]] }} {{eqn | r = \frac {-p \pm \sqrt{p^2 + q^2} } q | c = simplifying }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \tan \frac z 2 | r = \frac {\tan \dfrac {a x} 2 + \dfrac {-p \pm \sqrt{p^2 + q^2} } q} {1 - \tan \dfrac {a x} 2 \paren {\dfrac {-p \pm \sqrt{p^2 + q^2} } q} } | c = [[Tangent of Sum]] }} {{eqn| r = \frac {q \tan \dfrac {a x} 2 - p \pm \sqrt{p^2 + q^2} } {q - \tan \dfrac {a x} 2 \paren {-p \pm \sqrt{p^2 + q^2} } } | c = }} {{end-eqn}}	0
Let $A$ be a [[Definition:Class (Class Theory)|class]] of [[Definition:Natural Number|natural numbers]]. Let $A$ have no [[Definition:Greatest Element|greatest element]]. Then $A$ is a [[Definition:Denumerable Class|denumerable class]].	0
Firstly, by definition of [[Definition:Natural Number Multiplication|multiplication]]: {{begin-eqn}} {{eqn | l = n \times 1 | r = \paren {n \times 0} + n }} {{eqn | r = n }} {{end-eqn}} Next, recall that [[Definition:Natural Number Multiplication|multiplication]] is [[Definition:Recursively Defined Mapping|recursively defined]] as: :$\forall m, n \in \N: \begin{cases} m \times 0 & = 0 \\ m \times \paren {n + 1} & = m \times n + m \end{cases}$ From the [[Principle of Recursive Definition]], there is only one [[Definition:Mapping|mapping]] $f$ satisfying this definition for $m = 1$; that is, such that: :$\forall n \in \N: \begin {cases} \map f 0 = 0 \\ \map f {n + 1} = \map f n + 1 \end{cases}$ Consider now $f'$ defined as $\map {f'} n = n$. Then evidently $\map {f'} 0 = 0$. Also: {{begin-eqn}} {{eqn | l = \map {f'} {n + 1} | r = n + 1 }} {{eqn | r = \map {f'} n + 1 }} {{end-eqn}} showing that $f'$ also satisfies the definition for $1 \times n$. Hence $n \times 1 = 1 \times n = n$ for all $n \in \N$. That is, $1$ is the [[Definition:Identity Element|identity element]] for [[Definition:Natural Number Multiplication|multiplication]]. {{qed}}	0
The [[Definition:Riemann Sphere|Riemann sphere]] is the only [[Definition:Elliptic Riemann Surface|elliptic Riemann surface]] (up to [[Definition:Conformal Isomorphism|conformal isomorphism]]).	0
Define a [[Definition:Topology|topology]] on the [[Definition:Integer|integers]] $\Z$ by declaring a [[Definition:Subset|subset]] $U \subseteq \Z$ to be an [[Definition:Open Set (Topology)|open set]] {{iff}} it is either: :the [[Definition:Empty Set|empty set]] $\O$ or: :a [[Definition:Set Union|union]] of [[Definition:Sequence|sequences]] $\map S {a, b}$, where: ::$\map S {a, b} = \set {a n + b: n \in \Z} = a \Z + b$ In other words, $U$ is [[Definition:Open Set (Topology)|open]] {{iff}} every $x \in U$ admits some non-[[Definition:Zero (Number)|zero]] [[Definition:Integer|integer]] $a$ such that $\map S {a, x} \subseteq U$. The [[Definition:Open Set Axioms|open set axioms]] are verified as follows: $(O1): \quad$ All [[Definition:Set Union|unions]] of [[Definition:Open Set (Topology)|open sets]] are [[Definition:Open Set (Topology)|open]]: For any [[Definition:Set|set]] of [[Definition:Open Set (Topology)|open sets]] $U_i$ and $x$ in their [[Definition:Set Union|union]] $U$, any of the numbers $a_i$ for which $\map S {a_i, x} \subseteq U_i$ also shows that $\map S {a_i, x} \subseteq U$. $(O2): \quad$ The [[Definition:Set Intersection|intersection]] of two (and hence [[Definition:Finite Set|finitely many]]) [[Definition:Open Set (Topology)|open sets]] is [[Definition:Open Set (Topology)|open]]: Let $U_1$ and $U_2$ be [[Definition:Open Set (Topology)|open sets]]. Let $x \in U_1 \cap U_2$ (with [[Definition:Integer|integers]] $a_1$ and $a_2$ establishing membership). Set $a$ to be the [[Definition:Lowest Common Multiple of Integers|lowest common multiple]] of $a_1$ and $a_2$. Then: :$\map S {a, x} \subseteq \map S {a_i, x} \subseteq U_1 \cap U_2$ $(O3): \quad$ By definition, $\O$ is [[Definition:Open Set (Topology)|open]]: $\Z$ is just the [[Definition:Sequence|sequence]] $\map S {1, 0}$, and so is [[Definition:Open Set (Topology)|open]] as well. The topology is quite different from the usual Euclidean one, and has two notable properties: :$(1): \quad$ Since any non-empty [[Definition:Open Set (Topology)|open set]] contains an infinite sequence, no finite set can be [[Definition:Open Set (Topology)|open]]. Put another way, the [[Definition:Set Complement|complement]] of a finite set cannot be a [[Definition:Closed Set (Topology)|closed set]]. :$(2): \quad$ The basis sets $\map S {a, b}$ are [[Definition:Clopen Set|both open and closed]]: they are [[Definition:Open Set (Topology)|open]] by definition, and we can write $\map S {a, b}$ as the [[Definition:Relative Complement|complement]] of an [[Definition:Open Set (Topology)|open set]] as follows: :$\displaystyle \map S {a, b} = \Z \setminus \bigcup_{j \mathop = 1}^{a - 1} \map S {a, b + j}$ The only [[Definition:Integer|integers]] that are not integer multiples of [[Definition:Prime Number|prime numbers]] are $-1$ and $+1$, that is: :$\displaystyle \Z \setminus \set {-1, + 1} = \bigcup_{\text {$p$ prime} } \map S {p, 0}$ By the first property, the [[Definition:Set|set]] on the {{LHS}} cannot be [[Definition:Closed Set (Topology)|closed]]. On the other hand, by the second property, the sets $\map S {p, 0}$ are [[Definition:Closed Set (Topology)|closed]]. So, if there were only [[Definition:Finite Set|finitely many]] [[Definition:Prime Number|prime numbers]], then the [[Definition:Set|set]] on the {{RHS}} would be a [[Definition:Finite Union|finite union]] of [[Definition:Closed Set (Topology)|closed sets]], and hence [[Definition:Closed Set (Topology)|closed]]. Therefore by [[Proof by Contradiction]], there must be infinitely many [[Definition:Prime Number|prime numbers]]. {{qed}}	0
:$\displaystyle \int \frac {x^3 \ \mathrm d x} {\left({x^2 - a^2}\right)^2} = \frac {-a^2} {2 \left({x^2 - a^2}\right)} + \frac 1 2 \ln \left({x^2 - a^2}\right) + C$ for $x^2 > a^2$.	0
{{begin-eqn}} {{eqn | l = \map \erfc 0 | r = 1 - \map \erf 0 | c = {{Defof|Complementary Error Function}} }} {{eqn | r = 1 | c = [[Error Function of Zero]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {x \rd x}{\cos a x} | r = \int x \sec a x \rd x | c = {{Defof|Secant Function}} }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {E_n \paren {a x}^{2 n + 2} } {\paren {2 n + 2} \paren {2 n}!} + C | c = [[Primitive of x by Secant of a x|Primitive of $x \sec a x$]] }} {{end-eqn}} {{qed}}	0
Let $M_1 = \left({X_1, d_1}\right), M_2 = \left({X_2, d_2}\right), M_3 = \left({X_3, d_3}\right)$ be [[Definition:Metric Space|metric spaces]]. Let $f: M_1 \to M_2$ be [[Definition:Continuous at Point of Metric Space|continuous at $a \in X_1$]]. Let $g: M_2 \to M_3$ be [[Definition:Continuous at Point of Metric Space|continuous at $f \left({a}\right) \in X_2$]]. Then their [[Definition:Composition of Mappings|composite]] $g \circ f: M_1 \to M_3$ is [[Definition:Continuous at Point of Metric Space|continuous at $a \in X_1$]].	0
{{begin-eqn}} {{eqn | l = \hav \theta | r = \dfrac 1 2 \paren {1 - \cos \theta} | c = {{Defof|Haversine}} }} {{eqn | ll= \leadsto | l = 2 \hav \theta | r = 1 - \cos \theta | c = }} {{eqn | ll= \leadsto | l = \cos \theta | r = 1 - 2 \hav \theta | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \tan^2 x \ \mathrm d x | r = \int \left({\sec^2 x - 1}\right) \ \mathrm d x | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = \int \sec^2 x \ \mathrm d x + \int \left({-1}\right) \ \mathrm d x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \tan x + C + \int \left({-1}\right) \ \mathrm d x | c = [[Primitive of Square of Secant Function]] }} {{eqn | r = \tan x - x + C | c = [[Primitive of Constant]] }} {{end-eqn}} {{Qed}}	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Then $z$ is [[Definition:Algebraic Number over Field|algebraic over $\R$]].	0
Recall that [[Rational Numbers form Field]]. The result then follows directly from [[Field is Integral Domain]]. {{qed}}	0
:$\sin 1^\circ = \sin \dfrac {\pi} {180} = $ where $\sin$ denotes the [[Definition:Sine Function|sine function]].	0
Assume that for $G \ge 4$ that $\displaystyle \sum_{n = G}^\infty \frac{1}{n} = L < \infty$. Namely, that for some number $G$, there is a tail of the harmonic series which converges. Then from [[Definition:Series/Sequence of Partial Sums]]: $s_N := \sum_{n = G}^N \frac{1}{n}$ is the partial sum of the above series. Which yields the sequence $\{ s_N \}$ of partial sums. And, from [[Definition:Convergent Series]] we have that $\displaystyle \sum_{n = G}^\infty \frac{1}{n}$ converges iff $\{ s_N\}$ converges. From [[Constant Sequence Converges to Constant in Normed Division Ring]]: The constant sequence $\{ G \}$ has limit $G$. Note: $\R$ is a normed division ring as it is a field. By [[Combination Theorem for Sequences/Real/Product Rule]]: The product of the sequences $\{ G \}$ and $\{ s_N\}$ has limit $GL$. Namely, the sequence $\{Gs_N\}$ has limit $GL$, by the opening assumption. $GL = \displaystyle \sum_{n = G}^\infty \frac{G}{n} = \underbrace {1}_{s_0} + \underbrace{ \frac{G}{G+1} + \ldots + \frac{G}{G+4}}_{s_1} + \underbrace { \frac{G}{G+5} + \ldots + \frac{G}{G+12} }_{s_2} + \underbrace { \frac{G}{G+13} + \ldots + \frac{G}{G+28} }_{s_3} + \ldots $ Where $s_0 = 1, s_1 = \frac{G}{G+1} + \ldots + \frac{G}{G+4}$ and for $k \ge 2, s_k = \displaystyle \sum_{i = 2^k + 2^{k-1} + \ldots 2^2 + 1}^{2^{k+1} + 2^{k} + \ldots 2^2} \frac{G}{G+i} $. From the above, $s_0 = 1, s_1 \ge \frac{4G}{G+4} \ge 1$ by inspection. And for $k \ge 2$ then $s_k \ge \frac{2^{k+1}G}{G+2^{k+2}}$ since $\frac{G}{G+2^{k+2}}$ is smaller than the smallest summand of $s_k$. If summed $2^{k+1}$ many times, $2^{k+1}$ being the number of summands in $s_k$, it yields a result less than $s_k$. Note: The smallest summand of $s_k$ is $\frac{G}{G+2^{k+1}+ \ldots + 2^2}$. Claim: For $k \ge 1, G \ge 4$ We have $\frac{2^{k+1}G}{G+2^{k+2}} \ge 1$ Proof: $\frac{2^{k+1}G}{G+2^{k+2}} \ge 1 \iff k + 1 \ge \log_2\frac{G}{G-2}$. If $G = 4$ then the righthand side of the second inequality is $\log_2(2) = 1$. If $G > 4$, then $ 1 < \frac{G}{G-2} < 2$. Namely as $G \uparrow$ we have $\frac{G}{G-2} \to 1$ meaning $\log_2\frac{G}{G-2} \to 0$. {{qed|lemma}} Now, $\displaystyle GL = s_0 + s_1 + s_2 + s_3 + \ldots \ge 1 + \frac{4G}{G+4} + \frac{8G}{G+16 } + \frac{16G}{G+32} + \ldots \ge 1 + 1 + 1 + 1 + \ldots \to \infty > GL$ Deriving a contradiction. Hence , the series does not converge which implies the sequence $\{s_N\}$ does not converge. Therefore by [[ Tail of Convergent Sequence ]] : A sequence $a_n$ converges iff the sequence $a_{n+N}, N \in \N$ converges. We have the tail of the harmonic series diverges for any $G$ thus the harmonic series will diverge. {{qed}}	0
{{:Graph of Arcsecant Function}} Let $y = \arcsec x$ where $\size x > 1$. Then: {{begin-eqn}} {{eqn | l = y | r = \arcsec x | c = }} {{eqn | ll= \leadsto | l = x | r = \sec y | c = where $y \in \closedint 0 \pi \land y \ne \dfrac \pi 2$ }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d y} | r = \sec y \ \tan y | c = [[Derivative of Secant Function]] }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \dfrac 1 {\sec y \tan y} | c = [[Derivative of Inverse Function]] }} {{eqn | ll= \leadsto | l = \paren {\frac {\d y} {\d x} }^2 | r = \frac 1 {\sec^2 y \ \tan^2 y} | c = squaring both sides }} {{eqn | r = \frac 1 {\sec^2 y \paren {\sec^2 y - 1} } | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = \frac 1 {x^2 \paren {x^2 - 1} } | c = Definition of $x$ }} {{eqn | ll= \leadsto | l = \size {\dfrac {\d y} {\d x} } | r = \dfrac 1 {\size x \sqrt {x^2 - 1} } | c = squaring both sides }} {{end-eqn}} Since $\dfrac {\d y} {\d x} = \dfrac 1 {\sec y \tan y}$, the sign of $\dfrac {\d y} {\d x}$ is the same as the sign of $\sec y \tan y$. Writing $\sec y \tan y$ as $\dfrac {\sin y} {\cos^2 y}$, it is evident that the sign of $\dfrac {\d y} {\d x}$ is the same as the sign of $\sin y$. From [[Sine and Cosine are Periodic on Reals]], $\sin y$ is never [[Definition:Negative Real Function|negative]] on its domain ($y \in \closedint 0 \pi \land y \ne \pi/2$). However, by definition of the [[Definition:Arcsecant|arcsecant]] of $x$: :$0 < \arcsec x < \dfrac \pi 2 \implies x > 1$ :$\dfrac \pi 2 < \arcsec x < \pi \implies x < -1$ Thus: :$\dfrac {\map \d {\arcsec x} } {\d x} = \dfrac 1 {\size x \sqrt {x^2 - 1} } = \begin{cases} \dfrac {+1} {x \sqrt {x^2 - 1} } & : 0 < \arcsec x < \dfrac \pi 2 \ (\text {that is: $x > 1$}) \\ \dfrac {-1} {x \sqrt {x^2 - 1} } & : \dfrac \pi 2 < \arcsec x < \pi \ (\text {that is: $x < -1$}) \\ \end{cases}$ Hence the result. {{qed}}	0
By [[Transfinite Induction/Schema 2|Transfinite Induction]] on $x$. The proof will use $<$, $\in$, and $\subset$ interchangeably. This is justified by [[Transitive Set is Proper Subset of Ordinal iff Element of Ordinal]]. === Base Case === By our hypothesis, $\omega \le x$, so $x \not < \omega$, so we may begin our induction at $\omega$. {{begin-eqn}} {{eqn | l = \left({n + \omega}\right) | r = \bigcup_{y \mathop \in \omega} \left({n + y}\right) | c = [[Definition:Ordinal Addition]] }} {{eqn | l = \forall y \in \omega: \left({n + y}\right) \le \omega | o = \implies | r = \bigcup_{y \mathop \in \omega} \left({n + y}\right) \le \omega | c = [[Natural Number Addition is Closed]] and [[Indexed Union Subset]] }} {{eqn | l = \forall y \in \omega: y \le \left({n + y}\right) | o = \implies | r = \bigcup_{y \mathop \in \omega} y \le \bigcup_{y \mathop \in \omega} \left({n + y}\right) | c = [[Ordinal is Less than Sum]] and [[Indexed Union Subset]] }} {{eqn | o = \implies | r = \omega \le \bigcup_{y \mathop \in \omega} \left({n + y}\right) | c = [[Union of Limit Ordinal]] }} {{end-eqn}} From these conclusions, we may deduce that: : $\displaystyle \omega = \bigcup_{y \mathop \in \omega} \left({n + y}\right)$ === Inductive Case === {{begin-eqn}} {{eqn | l = \left({n + x}\right) = x | o = \implies | r = \left({n + x}\right)^+ = x^+ | c = [[Equality of Successors]] }} {{eqn | o = \implies | r = \left({n + x^+}\right) = x^+ | c = [[Definition:Ordinal Addition]] }} {{end-eqn}} === Limit Case === {{begin-eqn}} {{eqn | l = \forall y \in x: \left({n + y}\right) = y | o = \implies | r = \bigcup_{y \mathop \in x} \left({n + y}\right) = \bigcup_{y \mathop \in x} y | c = [[Indexed Union Equality]] }} {{eqn | o = \implies | r = \left({n + x}\right) = \bigcup_{y \mathop \in x} y | c = [[Definition:Ordinal Addition]] }} {{eqn | o = \implies | r = \left({n + x}\right) = x | c = [[Union of Limit Ordinal]] }} {{end-eqn}} {{qed}}	0
Let $a, b, c \in R$ where $a < b < c$. Let $A$ be the [[Definition:Union of Adjacent Open Intervals|union of the two adjacent open intervals]]: :$A := \openint a b \cup \openint b c$ Then: :$A = A^\circ$ where $A^\circ$ is the [[Definition:Interior (Topology)|interior]] of $A$.	0
This page gathers together the [[Definition:Primitive (Calculus)|primitives]] of some expressions involving $a x^2 + b x + c$.	0
Let $\epsilon > 0$. Let $n_0 \in \N$ be such that $\displaystyle \prod_{n \mathop = n_0}^\infty a_n$ [[Definition:Convergence of Product|converges]] to some $a \in \mathbb K \setminus \set 0$. By [[Convergent Sequence is Cauchy Sequence]], there exists $N_0 \ge n_0$ such that: :$\displaystyle \norm {\prod_{n \mathop = n_0}^k a_n - \prod_{n \mathop = n_0}^l a_n} \le \epsilon$ for $k, l \ge N_0$. By [[Sequence Converges to Within Half Limit]], there exists $N_1 \ge n_0$ such that: :$\displaystyle \norm {\prod_{n \mathop = n_0}^M a_n} \ge \frac {\norm a}2$ for $M \ge N_1$. Let $N = \max \set {N_0, N_1}$. For $N + 1 \le k \le l$: {{begin-eqn}} {{eqn | l = \norm {\prod_{n \mathop = k}^l a_n - 1} | r = \norm {\frac {\prod_{n \mathop = n_0}^l a_n - \prod_{n \mathop = n_0}^{k - 1} a_n} {\prod_{n \mathop = n_0}^{k - 1} } } | c = }} {{eqn | o = \le | r = \frac {2 \epsilon} {\norm{a} } | c = $l, k - 1 \ge N_0$ and $k - 1 \ge N_1$ }} {{end-eqn}} Hence the result. {{qed}}	0
:[[File:Circular-Arc-in-Complex-Plane.png|420px]] By [[Geometrical Interpretation of Complex Subtraction]]: :$z - a$ represents the [[Definition:Line Segment|line]] from $A$ to $Z$ :$z - b$ represents the [[Definition:Line Segment|line]] from $B$ to $Z$ {{begin-eqn}} {{eqn | l = \arg \dfrac {z - b} {z - a} | r = \lambda | c = }} {{eqn | ll= \leadsto | l = \map \arg {z - b} - \map \arg {z - a} | r = \lambda | c = [[Argument of Quotient equals Difference of Arguments]] }} {{end-eqn}} Thus: :$\arg \dfrac {z - b} {z - a} = \lambda$ represents the statement that the [[Definition:Angle|angle]] between $AZ$ and $BZ$ is [[Definition:Constant|constant]]: :$\angle AZB = \lambda$ That is, the [[Definition:Angle|angle]] [[Definition:Subtend|subtended]] by $AB$ at $Z$ is $\lambda$. The result follows from the [[Inscribed Angle Theorem]]. {{qed}}	0
:$\displaystyle \csc x + \cot x = \cot {\frac x 2}$	0
From [[Reduction Formula for Primitive of Power of a x + b by Power of p x + q/Increment of Power|Reduction Formula for Primitive of Power of $a x + b$ by Power of $p x + q$: Increment of Power]]: :$\displaystyle \int \left({a x + b}\right)^m \left({p x + q}\right)^n \ \mathrm d x = \frac 1 {\left({n + 1}\right) \left({b p - a q}\right)} \left({\left({a x + b}\right)^{m+1} \left({p x + q}\right)^{n+1} - a \left({m + n + 2}\right) \int \left({a x + b}\right)^m \left({p x + q}\right)^{n+1} \ \mathrm d x}\right)$ Setting $m := -m$ and $n := -n$: {{begin-eqn}} {{eqn | o = | r = \int \frac {\mathrm d x} {\left({a x + b}\right)^m \left({p x + q}\right)^n} | c = }} {{eqn | r = \int \left({a x + b}\right)^{-m} \left({p x + q}\right)^{-n} \ \mathrm d x | c = }} {{eqn | r = \frac 1 {\left({-n + 1}\right) \left({b p - a q}\right)} \left({\left({a x + b}\right)^{-m+1} \left({p x + q}\right)^{-n+1} - a \left({-m - n + 2}\right) \int \left({a x + b}\right)^{-m} \left({p x + q}\right)^{-n+1} \ \mathrm d x}\right) | c = }} {{eqn | r = \frac {-1} {\left({n - 1}\right) \left({b p - a q}\right)} \left({\frac 1 {\left({a x + b}\right)^{m-1} \left({p x + q}\right)^{n-1} } + a \left({m + n - 2}\right) \int \frac {\mathrm d x} {\left({a x + b}\right)^m \left({p x + q}\right)^{n-1} } }\right) | c = }} {{end-eqn}} {{qed}}	0
By [[Definition:Continuous Complex Function#Epsilon-Delta Definition|definition of continuity]]: :$\forall \epsilon > 0: \exists \delta > 0: \left\vert{z - z_0}\right\vert < \delta \implies \left\vert{f \left({z}\right) - f \left({z_0}\right)}\right\vert < \epsilon$ Given $\epsilon > 0$, we find $\delta > 0$ so for all $z \in \C$ with $\left\vert{z - z_0}\right\vert < \delta$: {{begin-eqn}} {{eqn |l= \epsilon |o= > |r= \left\vert{f \left({z}\right) - f \left({z_0}\right) }\right\vert }} {{eqn |o= \ge |r= \left\vert{ \operatorname{Re} \left({f \left({z}\right) - f \left({z_0}\right) }\right) }\right\vert |c= by [[Modulus Larger than Real Part]] }} {{eqn |r= \left\vert{ \operatorname{Re} \left({f \left({z}\right) }\right) - \operatorname{Re} \left({f \left({z_0}\right) }\right) }\right\vert |c= by [[Addition of Real and Imaginary Parts]] }} {{end-eqn}} It follows that: :$\forall \epsilon > 0: \exists \delta > 0: \left\vert{z - z_0}\right\vert < \delta \implies \left\vert{ \operatorname{Re} \left({ f \left({z}\right) }\right) - \operatorname{Re} \left({f \left({z_0}\right)} \right)} \right\vert < \epsilon$ Then equation $(1)$ is proven by: {{begin-eqn}} {{eqn |l= \lim_{z \to z_o} \operatorname{Re} \left({ f \left({z}\right) }\right) |r= \operatorname{Re} \left({ f \left({z_0}\right) }\right) |c= by [[Definition:Limit of Complex Function|definition of limit]] }} {{eqn |r= \operatorname{Re} \left({ \lim_{z \to z_o} f \left({z}\right) }\right) |c= by [[Definition:Continuous Complex Function|definition of continuity]] }} {{end-eqn}} The proof for equation $(2)$ with [[Definition:Imaginary Part|imaginary parts]] follows when $\operatorname{Re}$ is replaced by $\operatorname{Im}$ in the equations above. {{qed}} [[Category:Complex Analysis]] pp0l9bv7yjwersw3h80slmzhqm50w6x	0
Let $\dfrac r s, \dfrac t u \in \Q$, where $r, t \in \Z, s, u \in \Z_{>0}$. Let $\dfrac r s < \dfrac t u$. Then: {{begin-eqn}} {{eqn | l = r u | o = < | r = t s | c = [[Real Number Ordering is Compatible with Multiplication]] }} {{eqn | ll= \leadsto | l = a^{r u} | o = > | r = a^{t s} | c = [[Power Function on Base between Zero and One is Strictly Decreasing/Integer|Power Function on Base between Zero and One is Strictly Decreasing: Integer]] }} {{eqn | ll= \leadsto | l = \paren {a^r}^u | o = > | r = \paren {a^t}^s | c = [[Product of Indices of Real Number/Integers|Product of Indices of Real Number: Integers]] }} {{eqn | ll= \leadsto | l = \sqrt [u] {\paren {a^r}^u} | o = > | r = \sqrt [u] {\paren {a^t}^s } | c = [[Root is Strictly Increasing]] }} {{eqn | ll= \leadsto | l = a^r | o = > | r = \sqrt [u] {\paren {a^t}^s } | c = {{Defof|Root (Analysis)|$u$th Root}} }} {{eqn | ll= \leadsto | l = \sqrt [s] {\paren {a^r} } | o = > | r = \sqrt [s] {\sqrt [u] {\paren {a^t}^s } } | c = [[Root is Strictly Increasing]] }} {{eqn | ll= \leadsto | l = \sqrt [s] {\paren {a^r} } | o = > | r = \sqrt [u] {\sqrt [s] {\paren {a^t}^s } } | c = [[Root is Commutative]] }} {{eqn | ll= \leadsto | l = \sqrt [s] {\paren {a^r} } | o = > | r = \sqrt [u] {\paren {a^t} } | c = {{Defof|Root (Analysis)|$s$th Root}} }} {{eqn | ll= \leadsto | l = a^{r / s} | o = > | r = a^{t / u} | c = {{Defof|Rational Power}} }} {{end-eqn}} {{qed}} [[Category:Power Function on Base between Zero and One is Strictly Decreasing]] byhapr90wzqu0vwr4t9p7jhfjdogro1	0
Consider the [[Definition:Second Order Ordinary Differential Equation|differential equation]]: :$(1): \quad D^2_x \map f x = -\map f x$ subject to the [[Definition:Initial Condition|initial conditions]]: :$(2): \quad \map f 0 = 1$ :$(3): \quad D_x \map f 0 = 0$ === Step 1 === We will prove that $y = \cos x$ is a [[Definition:Particular Solution|particular solution]] of $(1)$. {{begin-eqn}} {{eqn | l = y | r = \cos x | c = }} {{eqn | l = D^2_x y | r = D^2_x \cos x | c = taking [[Definition:Second Derivative|second derivative]] of both sides }} {{eqn | r = \map {D_x} {-\sin x} | c = [[Derivative of Cosine Function]] }} {{eqn | r = -\map {D_x} {\sin x} | c = [[Derivative of Constant Multiple]] }} {{eqn | r = -\cos x | c = [[Derivative of Sine Function]] }} {{eqn | r = -y | c = }} {{end-eqn}} Thus $y = \cos x$ fulfils $(1)$. Then from [[Cosine of Zero is One]]: :$\cos 0 = 1$ Thus $y = \cos x$ fulfils $(2)$. Then: {{begin-eqn}} {{eqn | l = D_x \cos 0 | r = -\sin 0 | c = [[Derivative of Cosine Function]] }} {{eqn | r = 0 | c = [[Sine of Zero is Zero]] }} {{end-eqn}} Thus $y = \cos x$ fulfils $(3)$. So $y = \cos x$ is a [[Definition:Particular Solution|particular solution]] of $(1)$. {{qed|lemma}} === Step 2 === We will prove that $z = \dfrac {e^{i x} + e^{-i x} } 2$ is a [[Definition:Particular Solution|particular solution]] of $(1)$. {{begin-eqn}} {{eqn | l = z | r = \frac {e^{i x} + e^{-i x} } 2 | c = }} {{eqn | l = D^2_x z | r = \map {D^2_x} {\frac {e^{i x} + e^{-i x} } 2} | c = taking [[Definition:Second Derivative|second derivative]] of both sides }} {{eqn | r = \frac 1 2 \paren {D^2_x e^{i x} + D^2_x e^{-i x} } | c = [[Linear Combination of Derivatives]] }} {{eqn | r = \frac 1 2 \paren {i D_x e^{i x} - i D_x e^{-i x} } | c = [[Derivative of Exponential Function]] }} {{eqn | r = \frac 1 2 \paren {i^2 e^{i x} - i \paren {-i} e^{-i x} } | c = [[Derivative of Exponential Function]] }} {{eqn | r = \frac 1 2 \paren {- e^{i x} - e^{-i x} } | c = $i^2 = -1$ }} {{eqn | r = -\frac {e^{i x} + e^{-i x} }2 | c = }} {{eqn | r = -z | c = }} {{end-eqn}} Thus $z = \dfrac {e^{i x} + e^{-i x} } 2$ fulfils $(1)$. Then: {{begin-eqn}} {{eqn | l = \frac {e^{i \times 0} + e^{-i \times 0} } 2 | r = \frac {1 + 1} 2 | c = [[Exponential of Zero]] }} {{eqn | r = 1 | c = }} {{end-eqn}} Thus $z = \dfrac {e^{i x} + e^{-i x} } 2$ fulfils $(2)$. Then: {{begin-eqn}} {{eqn | l = \intlimits {D_x \frac {e^{i x} + e^{-i x} } 2} {x \mathop = 0} {} | r = \intlimits {\frac {i e^{i x} - i e^{-i x} } 2} {x \mathop = 0} {} | c = [[Derivative of Exponential Function]] }} {{eqn | r = \frac {i - i} 2 | c = [[Exponential of Zero]] }} {{eqn | r = 0 | c = }} {{end-eqn}} Thus $z = \dfrac {e^{i x} + e^{-i x} } 2$ fulfils $(3)$. So $z = \dfrac {e^{i x} + e^{-i x} } 2$ is a [[Definition:Particular Solution|particular solution]] of $(1)$. {{qed|lemma}} We have shown that $y$ and $z$ are both [[Definition:Particular Solution|particular solutions]] of $(1)$. But a [[Definition:Particular Solution|particular solution]] to a [[Definition:Differential Equation|differential equation]] is [[Definition:Unique|unique]]. {{MissingLinks|This result definitely needs backing up with an appropriate link}} Therefore $y = z$. That is: :$\cos x = \dfrac {e^{i x} + e^{-i x} } 2$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\csc u} | r = \map {\frac \d {\d u} } {\csc u} \frac {\d u} {\d x} | c = [[Chain Rule for Derivatives]] }} {{eqn | r = -\csc u \cot u \frac {\d u} {\d x} | c = [[Derivative of Cosecant Function]] }} {{end-eqn}} {{qed}}	0
{{questionable|See below.}} It is demonstrated that the result is true for $n = 118$: :$\dfrac {4 \times 118} 3 = 157 \cdotp \dot 3$ The [[Definition:Prime Number|primes]] between $118$ and $157$ are: {{begin-eqn}} {{eqn | l = 127 | r = 4 \times 32 - 1 }} {{eqn | l = | r = 6 \times 21 + 1 }} {{eqn | l = 131 | r = 4 \times 33 - 1 }} {{eqn | l = | r = 6 \times 22 - 1 }} {{eqn | l = 137 | r = 4 \times 34 + 1 }} {{eqn | l = | r = 6 \times 23 - 1 }} {{eqn | l = 139 | r = 4 \times 35 - 1 }} {{eqn | l = | r = 6 \times 23 + 1 }} {{eqn | l = 149 | r = 4 \times 37 + 1 }} {{eqn | l = | r = 6 \times 25 - 1 }} {{eqn | l = 151 | r = 4 \times 38 - 1 }} {{eqn | l = | r = 6 \times 25 + 1 }} {{eqn | l = 157 | r = 4 \times 39 + 1 }} {{eqn | l = | r = 6 \times 26 + 1 }} {{end-eqn}} So it can be seen that: :the [[Definition:Prime Number|primes]] of the form $4 m - 1$ are: ::$127, 131, 139, 151$ :the [[Definition:Prime Number|primes]] of the form $4 m + 1$ are: ::$137, 139, 157$ :the [[Definition:Prime Number|primes]] of the form $6 m - 1$ are: ::$131, 137, 149$ :the [[Definition:Prime Number|primes]] of the form $6 m + 1$ are: ::$127, 139, 151, 157$ and so the conditions of the result are fulfilled for $118$. But then note we have: :$\dfrac {4 \times 117} 3 = 156$ and so the [[Definition:Prime Number|primes]] between $117$ and $157$ are the same ones as between $118$ and $157$, excluding $157$ itself. Thus the conditions also hold for $117$. It is puzzling as to why the condition $n \ge 118$ has been applied to the initial statement.	0
{{ProofWanted}} {{Namedfor|Magnus Gustaf Mittag-Leffler|cat = Mittag-Leffler}}	0
{{ProofWanted}} [[Category:Riemann Surfaces]] m5kylsa3axmq372rmkcc91uk2txeovp	0
For the first part, if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$: {{begin-eqn}} {{eqn | l = \cos x | r = +\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Cosine in terms of Tangent]] }} {{eqn | ll= \leadsto | l = \frac 1 {\paren {\frac 1 {\cos x} } } | r = +\frac 1 {\sqrt {1 + \tan^2 x} } }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\paren {\frac {\sin x} {\cos x} } } | r = +\frac 1 {\sqrt {1 + \tan^2 x} } | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\sin x$ }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\tan x} | r = + \frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | ll= \leadsto | l = \sin x | r = + \frac {\tan x} {\sqrt {1 + \tan^2 x} } }} {{end-eqn}} For the second part, if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$: {{begin-eqn}} {{eqn | l = \cos x | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Cosine in terms of Tangent]] }} {{eqn | ll= \leadsto | l = \frac 1 {\paren {\frac 1 {\cos x} } } | r = -\frac 1 {\sqrt {1 + \tan^2 x} } }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\paren {\frac {\sin x} {\cos x} } } | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\sin x$ }} {{eqn | ll= \leadsto | l = \frac {\sin x} {\tan x} | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | ll= \leadsto | l = \sin x | r = -\frac {\tan x} {\sqrt {1 + \tan^2 x} } }} {{end-eqn}} When $\cos x = 0$, $\tan x$ is undefined. {{qed}}	0
{{begin-eqn}} {{eqn | l = u | r = \sqrt {x^n - a^n} | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {n x^{n - 1} } {2 \sqrt {x^n - a^n} } | c = [[Derivative of Power]], [[Chain Rule for Derivatives]] }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {x \sqrt {x^n - a^n} } | r = \int \frac {2 \sqrt {x^n - a^n} \rd u} {n x^{n - 1} x \sqrt {x^n - a^n} } | c = [[Integration by Substitution]] }} {{eqn | r = \int \frac {2 \rd u} {n \paren {u^2 + a^n} } | c = completing substitution and simplifying }} {{eqn | r = \frac 2 n \int \frac {\d u} {\paren {u^2 + \paren {\sqrt {a^n} }^2} } | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 2 n \paren {\frac 1 {\sqrt {a^n} } \arctan \frac u {\sqrt {a^n} } } + C | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | n = 1 | r = \frac 2 {n \sqrt {a^n} } \arctan \frac {\sqrt {x^n - a^n} } {\sqrt {a^n} } + C | c = substituting for $u$ }} {{end-eqn}} Now: {{begin-eqn}} {{eqn | l = y | r = \arctan \frac {\sqrt {x^n - a^n} } {\sqrt {a^n} } | c = }} {{eqn | ll= \leadsto | l = \tan y | r = \frac {\sqrt {x^n - a^n} } {\sqrt {a^n} } | c = {{Defof|Arctangent}} }} {{eqn | ll= \leadsto | l = \tan^2 y | r = \frac {x^n - a^n} {a^n} | c = squaring both sides }} {{eqn | ll= \leadsto | l = \tan^2 y | r = \frac {x^n} {a^n} - 1 | c = }} {{eqn | ll= \leadsto | l = 1 + \tan^2 y | r = \frac {x^n} {a^n} | c = }} {{eqn | ll= \leadsto | l = \sec^2 y | r = \frac {x^n} {a^n} | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | ll= \leadsto | l = \cos^2 y | r = \frac {a^n} {x^n} | c = {{Defof|Secant Function}} }} {{eqn | ll= \leadsto | l = \cos y | r = \frac {\sqrt {a^n} } {\sqrt {x^n} } | c = }} {{eqn | ll= \leadsto | l = y | r = \arccos \frac {\sqrt {a^n} } {\sqrt {x^n} } | c = {{Defof|Arccosine}} }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {x \sqrt {x^n - a^n} } | r = \frac 2 {n \sqrt {a^n} } \arccos \sqrt {\frac {a^n} {x^n} } | c = substituting in $(1)$ }} {{end-eqn}} {{qed}}	0
{{ProofWanted|The proof will boil down to the fact that any real number can be expressed as the limit of a sequence of strictly increasing / decreasing rationals. In that way any interval whose endpoints are irrational can be expressed as the limit of the union of an infinite sequence of intervals with rational endpoints. // Perhaps. Another approach would be to show that each element of an open interval has an open interval around it with rational endpoints that lies within the given interval.}}	0
:$\csc 300^\circ = \csc \dfrac {5 \pi} 3 = -\dfrac {2 \sqrt 3} 3$	0
Consider the [[Definition:Real Number Line|real number line]] under the [[Definition:Euclidean Metric on Real Number Line|Euclidean metric]]: :$M = \struct {\R, d}$ where $d$ is the [[Definition:Distance Function|distance function]] given by: :$\map d {x, y} = \size {x - y}$ === Proof of $\text M 1$ === {{begin-eqn}} {{eqn | l = \map d {x, x} | r = \size {x - x} | c = Definition of $d$ }} {{eqn | r = \size 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $\text M 1$]] holds for $d$. {{qed|lemma}} === Proof of $\text M 2$ === {{begin-eqn}} {{eqn | l = \map d {x, y} + \map d {y, z} | r = \size {x - y} + \size {y - z} | c = Definition of $d$ }} {{eqn | o = \ge | r = \size {\paren {x - y} + \paren {y - z} } | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | r = \size {x - z} | c = }} {{eqn | r = \map d {x, z} | c = Definition of $d$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $M2$]] holds for $d$. {{qed|lemma}} === Proof of $\text M 3$ === {{begin-eqn}} {{eqn | l = \map d {x, y} | r = \size {x - y} | c = Definition of $d$ }} {{eqn | r = \size {y - x} | c = }} {{eqn | r = \map d {y, x} | c = Definition of $d$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $\text M 3$]] holds for $d$. {{qed|lemma}} === Proof of $\text M 4$ === {{begin-eqn}} {{eqn | l = x | o = \ne | r = y | c = }} {{eqn | ll= \leadsto | l = x - y | o = \ne | r = 0 | c = }} {{eqn | ll= \leadsto | l = \size {x - y} | o = > | r = 0 | c = }} {{eqn | ll= \leadsto | l = \map d {x, y} | o = > | r = 0 | c = Definition of $d$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $\text M 4$]] holds for $d$. {{qed}}	0
We have that [[Between two Real Numbers exists Rational Number]]: :$\forall a, b \in \R : a < b : \exists r \in \Q : a < r < b$ Let $a := x$ with $x \in \R$. Let $\epsilon \in \R_{\mathop > 0} : r - a < \epsilon$. Let $b := x + \epsilon$. Then: {{begin-eqn}} {{eqn | l = x - \epsilon | o = < | r = x }} {{eqn | o = < | r = r }} {{eqn | o = < | r = x + \epsilon }} {{end-eqn}} Since this holds for all $x$, we have that: :$\forall x \in \R : \exists \epsilon \in \R_{\mathop > 0} : \exists r \in \Q : \size {x - r} < \epsilon$ By [[Definition:Definition|definition]], $\Q$ is [[Definition:Everywhere Dense/Normed Vector Space|dense]] in $\R$.	0
{{begin-eqn}} {{eqn | l = n | o = \divides | r = \size n }} {{eqn | l = \size n | o = \divides | r = n }} {{end-eqn}} where: :$\size n$ is the [[Definition:Absolute Value|absolute value]] of $n$ :$\divides$ denotes [[Definition:Divisor of Integer|divisibility]].	0
{{begin-eqn}} {{eqn | l = \int \frac {\cos a x \rd x} x | r = \int \frac 1 x \paren {\sum_{k \mathop = 0}^\infty \paren {-1}^k \frac {\paren {a x}^{2 k} }{\paren {2 k}!} } \rd x | c = Definition of [[Definition:Real Cosine Function|Real Cosine Function]] }} {{eqn | r = \sum_{k \mathop = 0}^\infty \frac {\paren {-1}^k a^{2 k} } {\paren {2 k}!} \int \frac 1 x \paren {x^{2 k} } \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | n = 1 | r = \int \frac {\rd x} x + \sum_{k \mathop = 1}^\infty \frac {\paren {-1}^k a^{2 k} } {\paren {2 k}!} \int \paren {x^{2 k - 1} } \rd x | c = extracting case for $k = 0$ }} {{eqn | r = \int \frac {\d x} x + \sum_{k \mathop = 1}^\infty \frac {\paren {-1}^k a^{2 k} } {\paren {2 k}!} \frac {x^{2 k} } {2 k} + C | c = [[Primitive of Power]] }} {{eqn | r = \int \frac {\d x} x + \sum_{k \mathop \ge 1} \frac {\paren {-1}^k \paren {a x}^{2 k} } {\paren {2 k} \paren {2 k}!} + C | c = simplifying }} {{eqn | r = \ln \size x + \sum_{k \mathop \ge 1} \frac {\paren {-1}^k \paren {a x}^{2 k} } {\paren {2 k} \paren {2 k}!} + C | c = [[Primitive of Reciprocal]] }} {{end-eqn}} The validity of $(1)$ follows from [[Cosine Function is Absolutely Convergent]]. {{qed}}	0
{{MissingLinks|Each step of this proof needs to be backed up with evidence: a link to a result or a definition, preferably.}} Let $\map f z = \map \OO {e^{\cmod z^m} }$ and $\map g z = \map \OO {e^{\cmod z^k} }$. {{WLOG}}, let $m \ge k$. Then it follows that $\map f z + \map g z = \map \OO {e^{\cmod z^m} }$. Then the order of $f + g$ is at most $m$. Since $\alpha, \beta$ are the infima of $m, k$, it follows that the order of $f + g$ is at most $\alpha$. Let $\alpha > \beta$ and $\map f z = \map \OO {e^{\cmod z^m} }$. Then: :$\map g z = \map {\mathcal o} {e^{\cmod z^m} }$ So $\dfrac {\map g z} {e^{\cmod z^m} } \to 0$ as $\cmod z \to \infty$. In particular: :$\dfrac {\map f z + \map g z} {e^{\cmod z^m} } \sim \dfrac {\map f z} {e^{\cmod z^m} } > 0$ and minimizing $m$ we see that the order of $f + g$ is at least $\alpha$. {{ProofWanted}}	0
The [[Definition:Positive Real Number|set of non-negative real numbers]] $\R_{\ge 0}$ is not [[Definition:Well-Ordered Set|well-ordered]] under the [[Definition:Usual Ordering|usual ordering]] $\le$.	0
Proof by [[Principle of Mathematical Induction|induction]] on $n$: Let $x, y \in \R_{>0}$ be [[Definition:Strictly Positive Real Number|strictly positive real numbers]]. For all $n \in \Z_{>0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$x < y \implies x^n < y^n$ === Basis for the Induction === $P \left({1}\right)$ is true, as this just says: {{begin-eqn}} {{eqn | l = x^1 | r = x | c = Definition of [[Definition:Integer Power|integer power]] }} {{eqn | o = < | r = y | c = By [[Definition:Assumption|assumption]] }} {{eqn | r = y^1 | c = Definition of [[Definition:Integer Power|integer power]] }} {{end-eqn}} This is our [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $P \left({k}\right)$ is true, where $k \ge 1$, then it logically follows that $P \left({k+1}\right)$ is true. So this is our [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$x < y \implies x^k < y^k$ Then we need to show: :$x < y \implies x^{k+1} < y^{k+1}$ === Induction Step === This is our [[Principle of Mathematical Induction#Induction Step|induction step]]: First: {{begin-eqn}} {{eqn | l = x | o = < | r = y }} {{eqn | ll = \implies | l = x^k | o = < | r = y^k | c = [[Power Function is Strictly Increasing over Positive Reals/Natural Exponent#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | ll = \implies | l = x^{k+1} | o = < | r = x \times y^k | c = Multiply both sides by $x > 0$ }} {{eqn | o = < | r = y \times y^k | c = Multiply both sides of $x < y$ by $y^k > 0$ }} {{eqn | r = y^{k+1} | c = Definition of [[Definition:Integer Power|integer power]] }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{>0}: x < y \implies x^n < y^n$ {{qed}} [[Category:Powers]] dggzpwgfijhmelesjckhfrz3l7zc2u3	0
{{:Euclid:Proposition/II/5}} :[[File:Euclid-II-5.png|400px]] Let $AB$ be cut into equal segments at $C$ and unequal segments at $D$. Then the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$ together with the square on $CD$ equals the square on $BC$. (That is, let $x = AC, y = CD$. Then $\paren {x + y} \paren {x - y} + y^2 = x^2$.) This is proved as follows. [[Construction of Square on Given Straight Line|Construct the square $CBFE$]] on $CB$, and join $BE$. [[Construction of Parallel Line|Construct $DG$ parallel]] to $CE$ through $G$, and let $DG$ cross $BE$ at $H$. [[Construction of Parallel Line|Construct $KM$ parallel]] to $AB$ through $H$. [[Construction of Parallel Line|Construct $AK$ parallel]] to $BF$ through $A$. From [[Complements of Parallelograms are Equal]]: :$\Box CDHL = \Box FGHM$. Add the square $DBMH$ to each. Then $\Box CBML = \Box DBFG$. But as $AC = CB$, from [[Parallelograms with Equal Base and Same Height have Equal Area]] we have that: :$\Box ACLK = \Box CBML$ Add $\Box CDHL$ to each. Then $\Box ADHK$ is equal in area to the [[Definition:Gnomon|gnomon]] $CBFGHL$. But $\Box ADHK$ is the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$, because $DB = DH$. So the [[Definition:Gnomon|gnomon]] $CBFGHL$ is equal in area to the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$. Now $\Box LHGE$ is equal to the square on $CD$. Add $\Box LHGE$ to each of the [[Definition:Gnomon|gnomon]] $CBFGHL$ and $\Box ADHK$. Then the [[Definition:Gnomon|gnomon]] $CBFGHL$ together with $\Box LHGE$ equals the [[Definition:Containment of Rectangle|rectangle contained]] by $AD$ and $DB$ and the square on $CD$. But the [[Definition:Gnomon|gnomon]] $CBFGHL$ together with $\Box LHGE$ is the square $CBFE$. Hence the result. {{qed}}	0
This proof depends on the [[Definition:Exponential Function/Real/Sum of Series|definition of the exponential function as the function inverse]] of the [[Definition:Natural Logarithm|natural logarithm]]. From [[Logarithm is Strictly Increasing]], $\ln$ is [[Definition:Strictly Monotone Real Function|strictly monotone]] on $\R_{>0}$. From [[Natural Logarithm Function is Continuous]], $\ln$ is [[Definition:Continuous on Interval|continuous]] on $\R_{>0}$ Thus, from the [[Continuous Inverse Theorem]], $\exp := \ln^{-1}$ is continuous. {{qed}}	0
Let two [[Definition:Natural Numbers|(natural) numbers]] $A, B$ by [[Definition:Multiplication|multiplying]] any number $C$ make $D, E$. Then we need to show that: : $A : B = D : E$ :[[File:Euclid-VII-18.png|350px]] We have that: : $A \times C = D$ So from [[Natural Number Multiplication is Commutative]], also: : $C \times A = D$ For the same reason: : $C \times B = E$ Therefore from {{EuclidPropLink|book=VII|prop=17|title=Multiples of Ratios of Numbers}}: : $A : B = D : E$ {{qed}} {{Euclid Note|18|VII}}	0
Let $P:\C\to\C$ be a [[Definition:Complex Polynomial Function|polynomial function]]. Then $P$ has [[Definition:Order of Entire Function|order]] $0$.	0
A specific instance of [[Power of Prime is Deficient]]. {{qed}}	0
[[Definition:Integer Multiplication|Integer multiplication]] is [[Definition:Well-Defined Operation|well-defined]].	0
=== Sufficient Condition === From [[Holomorphic Function is Analytic]], if $f$ is [[Definition:Holomorphic Complex Function|holomorphic]] on some [[Definition:Open Ball|open ball]] $D$ in $\C$, then $f$ is [[Definition:Analytic Complex Function|complex analytic]] on $D$. However, as, by the definition of an [[Definition:Entire Function|entire function]], $f$ is [[Definition:Holomorphic Complex Function|holomorphic]] everywhere, the radius of $D$ can be made arbitrarily large, meaning that $f$ is [[Definition:Analytic Complex Function|complex analytic]] everywhere. This implies that an [[Definition:Entire Function|entire function]] is [[Definition:Analytic Complex Function|complex analytic]] everywhere. {{qed|lemma}} === Necessary Condition === It remains to prove that if $f$ is [[Definition:Analytic Complex Function|complex analytic]] everywhere, $f$ is [[Definition:Entire Function|entire]]. By [[Power Series is Termwise Differentiable within Radius of Convergence]], if $f$ is [[Definition:Analytic Complex Function|complex analytic]] everywhere, $f$ is [[Definition:Holomorphic Complex Function|holomorphic]] everywhere. That is, a function [[Definition:Analytic Complex Function|complex analytic]]everywhere is [[Definition:Entire Function|entire]]. Note that by the [[Cauchy-Hadamard Theorem/Complex Case|Cauchy-Hadamard theorem]]: :$\displaystyle \lim_{n \mathop \to \infty} \sqrt [n] {\size {a_n} } = 0$ {{iff}} this series has an infinite [[Definition:Radius of Convergence|radius of convergence]]. That is, {{iff}} the series converges for all $z \in \C$, satisfying our second demand. {{qed}} [[Category:Entire Functions]] dowwf37pjjl19876udtgduezr6puxvk	0
The operation of [[Definition:Rational Multiplication|multiplication]] on the [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] $\Q$ is [[Definition:Associative|associative]]: :$\forall x, y, z \in \Q: x \times \paren {y \times z} = \paren {x \times y} \times z$	0
Let $\N$ be the [[Definition:Natural Numbers|natural numbers]]. Let $+$ be [[Definition:Natural Number Addition|addition]] on $\N$. Then: :$\forall a, b, c \in \N: a + c = b + c \implies a = b$ :$\forall a, b, c \in \N: a + b = a + c \implies b = c$ That is, $+$ is [[Definition:Cancellable Operation|cancellable]] on $\N$.	0
Let $S_n$ be the $n$th [[Definition:Partial Sum|partial sum]] of $\displaystyle \sum_{n \mathop = 1}^\infty z_n$. Let $P_n$ be the $n$th [[Definition:Partial Product|partial product]] of $\displaystyle \prod_{n \mathop = 1}^\infty \exp \left({z_n}\right)$. By [[Exponential of Sum]], $\exp \left({S_n}\right) = P_n$ for all $n \in \N$. By [[Exponential Function is Continuous]], $\displaystyle \lim_{n \mathop \to \infty} \exp \left({S_n}\right) = \exp z$. By [[Exponential of Complex Number is Nonzero]], $\exp z \ne 0$. Thus $\displaystyle \lim_{n \mathop \to \infty} P_n = \exp z \ne 0$. {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan 195^\circ | r = \tan \left({360^\circ - 165^\circ}\right) | c = }} {{eqn | r = -\tan 165^\circ | c = [[Tangent of Conjugate Angle]] }} {{eqn | r = 2 - \sqrt 3 | c = [[Tangent of 165 Degrees]] }} {{end-eqn}} {{qed}}	0
The [[Definition:Sequence|sequence]] of best approximations to an [[Definition:Equilateral Triangle|equilateral triangle]] by a [[Definition:Heronian Triangle|Heronian triangle]] begins: :The [[Pythagorean Triangle/Examples/3-4-5|$\paren {3, 4, 5}$ triangle]], with [[Definition:Area|area]] $6$ :The $\paren {13, 14, 15}$ [[Definition:Triangle (Geometry)|triangle]], with [[Definition:Area|area]] $84$, where $14 = 4^2 - 2$ :The $\paren {193, 194, 195}$ [[Definition:Triangle (Geometry)|triangle]], where $194 = 14^2 - 2$ :The $\paren {37 \, 633, 37 \, 634, 37 \, 635}$ [[Definition:Triangle (Geometry)|triangle]], where $37 \, 634 = 194^2 - 2$ and so on. {{OEIS|A003010}}	0
Let $\sequence {a_n}$, $\sequence {b_n}$ and $\sequence {c_n}$ be [[Definition:Sequence|sequences]] of [[Definition:Real Number|real]] or [[Definition:Complex Number|complex numbers]]. Let $a_n = \map \OO {\sequence {b_n} }$ and $b_n = \map \OO {\sequence {c_n} }$, where $O$ denotes [[Definition:Big-O Notation for Sequences|big-O notation]]. Then $a_n = \map \OO {\sequence {c_n} }$.	0
From [[Sum of Arithmetic-Geometric Sequence]]: :$\ds \sum_{j \mathop = 0}^n \paren {a + j d} x^j = \frac {a \paren {1 - x^{n + 1} } } {1 - x} + \frac {x d \paren {1 - \paren {n + 1} x^n + n x^{n + 1} } } {\paren {1 - x}^2}$ Hence: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^n j x^j | r = \frac {0 \paren {1 - x^{n + 1} } } {1 - x} + \frac {x \times 1 \paren {1 - \paren {n + 1} x^n + n x^{n + 1} } } {\paren {1 - x}^2} | c = putting $a = 0, d = 1$ }} {{eqn | r = \frac {x \paren {1 - \paren {n + 1} x^n + n x^{n + 1} } } {\paren {1 - x}^2} | c = initial simplification }} {{eqn | r = \frac {x - \paren {n + 1} x^{n + 1} + n x^{n + 2} } {\paren {x - 1}^2} | c = further simplification }} {{end-eqn}} Hence the result. {{qed}}	0
We will show that: :$\forall \epsilon \in \R_{>0}: \size {a^{x y} - \paren {a^x}^y} < \epsilon$ {{WLOG}}, suppose that $x < y$. Consider $I := \closedint x y$. Let $I_\Q = I \cap \Q$. Let $M = \max \set {\size x, \size y}$ Fix $\epsilon \in \R_{>0}$. From [[Real Polynomial Function is Continuous]]: :$\exists \delta' \in \R_{>0}: \size {a^x - a^{x'} } < \delta' \leadsto \size {\paren {a^x}^{y'} - \paren {a^{x'} }^{y'} } < \dfrac \epsilon 4$ From [[Power Function on Strictly Positive Base is Continuous]]: {{begin-eqn}} {{eqn | lll=\exists \delta_1 \in \R_{>0} : | ll= \size {x x' - y y'} | lo= < | l = \delta_1 | o = \leadsto | m = \size {a^{x x'} - a^{x y'} } | mo= < | r = \dfrac \epsilon 4 }} {{eqn | lll=\exists \delta_2 \in \R_{>0} : | ll= \size {x y' - x' y'} | lo= < | l = \delta_2 | o = \leadsto | m = \size {a^{x y'} - a^{x' y'} } | mo= < | r = \dfrac \epsilon 4 }} {{eqn | lll=\exists \delta_3 \in \R_{>0} : | ll= \size {x' - x} | lo= < | l = \delta_3 | o = \leadsto | m = \size {a^{x'} - a^x} | mo= < | r = \delta' }} {{eqn | o = \leadsto | m = \size {\paren {a^x}^{y'} - \paren {a^{x'} }^{y'} } | mo = < | r = \dfrac \epsilon 4 }} {{eqn | lll=\exists \delta_4 \in \R_{>0} : | ll= \size {y' - y} | lo= < | l = \delta_4 | o = \leadsto | m = \size {\paren {a^x}^{y'} - \paren {a^x}^{y} } | mo= < | r = \dfrac \epsilon 4 }} {{end-eqn}} Further: {{begin-eqn}} {{eqn | l = \size {y - y'} | o = < | r = \frac {\delta_1} {\size x} }} {{eqn | ll= \leadsto | l = \size {x y - x y'} | r = \size x \size {y - y'} | c = [[Absolute Value Function is Completely Multiplicative]] }} {{eqn | o = < | r = \size x \frac {\delta_1} {\size x} | c = multiplying both sides by $\size x \ge 0$ }} {{eqn | r = \delta_1 }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \size {x - x'} | o = < | r = \frac {\delta_2} M }} {{eqn | ll= \leadsto | l = \size {x y' - x'y'} | r = \size {y'} \size {x - x'} | c = [[Absolute Value Function is Completely Multiplicative]] }} {{eqn | o = \le | r = M \size {x - x'} | c = [[Real Number Ordering is Compatible with Multiplication]] }} {{eqn | o = < | r = M \frac {\delta_1} M | c = multiplying both sides by $M \ge 0$ }} {{eqn | r = \delta_2 }} {{end-eqn}} Let $\delta = \max \set {\dfrac {\delta_1} {\size x}, \dfrac {\delta_2} M, \delta_3, \delta_4}$. From [[Closure of Rational Interval is Closed Real Interval]]: :$\exists r, s \in I_\Q: \size {x - r} < \delta \land \size {y - s} < \delta$ Thus: {{begin-eqn}} {{eqn | l = \size {a^{x y} - \paren {a^x}^y} | o = \le | r = \size {a^{x y} - a^{x s} } + \size {a^{x s} - a^{r s} } + \size {a^{r s} - \paren {a^r}^s} + \size {\paren {a^r}^s - \paren {a^x}^s} + \size {\paren {a^x}^s - \paren {a^x}^y} | c = [[Triangle Inequality for Real Numbers]] }} {{eqn | r = \size {a^{x y} - a^{x s} } + \size {a^{x s} - a^{r s} } + \size {\paren {a^r}^s - \paren {a^x}^s} + \size {\paren {a^x}^s - \paren {a^x}^y} | c = [[Product of Indices of Real Number/Rational Numbers|Product of Indices of Real Number: Rational Numbers]] }} {{eqn | o = < | r = \frac \epsilon 4 + \frac \epsilon 4 + \frac \epsilon 4 + \frac \epsilon 4 | c = Definition of $r$ and $s$ }} {{eqn | r = \epsilon }} {{end-eqn}} Hence the result, by [[Real Plus Epsilon]]. {{qed}}	0
Define a [[Definition:Mapping|mapping]] $f: \powerset {\N_{>0} } \to \R$ thus: :$\map f S = 0.d_1 d_2 \ldots$, interpreted as a [[Definition:Ternary Notation|ternary expansion]] where $\sequence {d_n}$ is the characteristic function of $S$. That is: :$\displaystyle \map f S = \sum_{i \mathop \in S} 3^{-i}$ By the [[Real Numbers are Uncountable/Proof 2 using Ternary Notation/Lemma|lemma]], $f$ is an [[Definition:Injection|injection]]. {{AimForCont}} that $\R$ is [[Definition:Countable Set|countable]]. Then there is an [[Definition:Injection|injection]] $g: \R \to \N$. By [[Composite of Injections is Injection]], $g \circ f: \powerset \N \to \N$ is an [[Definition:Injection|injection]]. But this [[Definition:Contradiction|contradicts]] [[No Injection from Power Set to Set]]. Hence, by [[Proof by Contradiction]], $\R$ is not [[Definition:Countable Set|countable]]. {{qed}}	0
:[[File:Spherical-Cosine-Formula-2.png|500px]] Let $A$, $B$ and $C$ be the [[Definition:Vertex of Polygon|vertices]] of a [[Definition:Spherical Triangle|spherical triangle]] on the surface of a [[Definition:Sphere (Geometry)|sphere]] $S$. By definition of a [[Definition:Spherical Triangle|spherical triangle]], $AB$, $BC$ and $AC$ are [[Definition:Arc of Circle|arcs]] of [[Definition:Great Circle|great circles]] on $S$. By definition of a [[Definition:Great Circle|great circle]], the [[Definition:Center of Circle|center]] of each of these [[Definition:Great Circle|great circles]] is $O$. Let $O$ be joined to each of $A$, $B$ and $C$. Let $P$ be an arbitrary [[Definition:Point|point]] on $OC$. Construct $PQ$ [[Definition:Perpendicular|perpendicular]] to $OA$ meeting $OA$ at $Q$. Construct $PR$ [[Definition:Perpendicular|perpendicular]] to $OB$ meeting $OB$ at $R$. In the [[Definition:Plane|plane]] $OAB$: :construct $QS$ [[Definition:Perpendicular|perpendicular]] to $OA$ :construct $RS$ [[Definition:Perpendicular|perpendicular]] to $OB$ where $S$ is the [[Definition:Point|point]] where $QS$ and $RS$ [[Definition:Intersection (Geometry)|intersect]]. Let $OS$ and $PS$ be joined. Let [[Definition:Tangent Line|tangents]] be constructed at $A$ to the [[Definition:Arc of Circle|arcs]] of the [[Definition:Great Circle|great circles]] $AC$ and $AB$. These [[Definition:Tangent Line|tangents]] [[Definition:Containment of Angle|contain]] the [[Definition:Spherical Angle|spherical angle]] $A$. But by construction, $QS$ and $QP$ are [[Definition:Parallel Lines|parallel]] to these [[Definition:Tangent Line|tangents]] Hence $\angle PQS = \sphericalangle A$. Similarly, $\angle PRS = \sphericalangle B$. Also we have: {{begin-eqn}} {{eqn | l = \angle COB | r = a }} {{eqn | l = \angle COA | r = b }} {{eqn | l = \angle AOB | r = c }} {{end-eqn}} It is to be proved that $PS$ is [[Definition:Line Perpendicular to Plane|perpendicular]] to the [[Definition:Plane|plane]] $AOB$. By construction, $OQ$ is [[Definition:Perpendicular|perpendicular]] to both $PQ$ and $QS$. Thus $OQ$ is [[Definition:Line Perpendicular to Plane|perpendicular]] to the [[Definition:Plane|plane]] $PQS$. Similarly, $OR$ is [[Definition:Line Perpendicular to Plane|perpendicular]] to the [[Definition:Plane|plane]] $PRS$. Thus $PS$ is [[Definition:Perpendicular|perpendicular]] to both $OQ$ and $OR$. Thus $PS$ is [[Definition:Perpendicular|perpendicular]] to every [[Definition:Straight Line|line]] in the [[Definition:Plane|plane]] of $OQ$ and $OR$. That is, $PS$ is [[Definition:Line Perpendicular to Plane|perpendicular]] to the [[Definition:Plane|plane]] $OAB$. In particular, $PS$ is [[Definition:Perpendicular|perpendicular]] to $OS$, $SQ$ and $SR$ It follows that $\triangle PQS$ and $\triangle PRS$ are [[Definition:Right Triangle|right triangles]]. From the [[Definition:Right Triangle|right triangles]] $\triangle OQP$ and $\triangle ORP$, we have: {{begin-eqn}} {{eqn | n = 1 | l = PQ | r = OP \sin b }} {{eqn | n = 2 | l = PR | r = OP \sin a }} {{eqn | n = 3 | l = OQ | r = OP \cos b }} {{eqn | n = 4 | l = OR | r = OP \cos a }} {{end-eqn}} From the [[Definition:Right Triangle|right triangles]] $\triangle PQS$ and $\triangle PRS$, we have: {{begin-eqn}} {{eqn | l = PS | r = PS \sin \angle PRS }} {{eqn | r = PQ \sin A }} {{eqn | l = PS | r = PR \sin \angle PRS }} {{eqn | r = PR \sin B }} {{eqn | ll= \leadsto | l = OP \sin b \sin A | r = OP \sin a \sin B | c = from $(1)$ and $(2)$ }} {{eqn | ll= \leadsto | l = \dfrac {\sin a} {\sin A} | r = \dfrac {\sin b} {\sin B} | c = }} {{end-eqn}} The result follows by applying this technique [[Definition:Mutatis Mutandis|mutatis mutandis]] to the other [[Definition:Spherical Angle|angles]] of $ABC$. {{qed}}	0
Let $\left({R, +, \circ}\right)$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\mathcal E$ be a [[Definition:Congruence Relation|congruence relation]] on $R$. Let $J = \left[\!\left[{0_R}\right]\!\right]_\mathcal E$ be the [[Definition:Equivalence Class|equivalence class]] of $0_R$ under $\mathcal E$. Then $J$ is an [[Definition:Ideal of Ring|ideal]] of $R$.	0
Let $\RR$ be a [[Definition:Congruence Relation|congruence relation]] on a [[Definition:Semigroup|semigroup]] $\struct {S, \circ}$. Then the [[Definition:Quotient Structure|quotient structure]] $\struct {S / \RR, \circ_\RR}$ is a [[Definition:Semigroup|semigroup]].	0
:$\displaystyle \int \frac {\cos^m a x} {\sin^n a x} \rd x = \frac {\cos^{m - 1} a x} {a \paren {m - n} \sin^{n - 1} a x} + \frac {m - 1} {m - n} \int \frac {\cos^{m - 2} a x} {\sin^n a x} \rd x + C$	0
We have: {{begin-eqn}} {{eqn | l = \cos 4 \theta + i \sin 4 \theta | r = \paren {\cos \theta + i \sin \theta}^4 | c = [[De Moivre's Formula]] }} {{eqn | r = \paren {\cos \theta}^4 + \binom 4 1 \paren {\cos \theta}^3 \paren {i \sin \theta} + \binom 4 2 \paren {\cos \theta}^2 \paren {i \sin \theta}^2 }} {{eqn | o = | ro= + | r = \binom 4 3 \paren {\cos \theta} \paren {i \sin \theta}^3 + \paren {i \sin \theta}^4 | c = [[Binomial Theorem]] }} {{eqn | r = \cos^4 \theta + 4 i \cos^3 \theta \sin \theta - 6 \cos^2 \theta \sin^2 \theta | c = substituting for [[Definition:Binomial Coefficient|binomial coefficients]] }} {{eqn | o = | ro= - | r = 4 i \cos \theta \sin^3 \theta + \sin^4 \theta | c = and using $i^2 = -1$ }} {{eqn | n = 1 | r = \cos^4 \theta - 6 \cos^2 \theta \sin^2 \theta + \sin^4 \theta }} {{eqn | o = | ro= + | r = 4 i \cos^3 \theta \sin \theta - 4 i \cos \theta \sin^3 \theta | c = rearranging }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \cos 4 \theta | r = \cos^4 \theta - 6 \cos^2 \theta \sin^2 \theta + \sin^4 \theta | c = equating [[Definition:Real Part|real parts]] in $(1)$ }} {{eqn | r = \cos^4 \theta - 6 \cos^2 \theta \paren {1 - \cos^2 \theta} + \paren {1 - \cos^2 \theta}^2 | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 8 \cos^4 \theta - 8 \cos^2 \theta + 1 | c = multiplying out and gathering terms }} {{end-eqn}} {{qed}}	0
The proof proceeds by [[Principle of Mathematical Induction|induction]] on $n$. For all $n \in \Z_{\ge 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_k \paren {-1}^k {n \brack k} = \delta_{n 0} - \delta_{n 1}$ $\map P 0$ is the case: {{begin-eqn}} {{eqn | l = \sum_k \paren {-1}^k {0 \brack k} | r = \sum_k \delta_{0 k} | c = [[Unsigned Stirling Number of the First Kind of 0]] }} {{eqn | r = 1 | c = all terms vanish but for $k = 0$ }} {{eqn | r = \delta_{0 0} - \delta_{0 1} | c = {{Defof|Kronecker Delta}} }} {{end-eqn}} Thus $\map P 0$ is seen to hold. $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \sum_k \paren {-1}^k {1 \brack k} | r = \sum_k \paren {-1}^k \delta_{1 k} | c = [[Unsigned Stirling Number of the First Kind of 1]] }} {{eqn | r = -1 | c = all terms vanish but for $k = 1$ }} {{eqn | r = \delta_{1 0} - \delta_{1 1} | c = {{Defof|Kronecker Delta}} }} {{end-eqn}} Thus $\map P 1$ is seen to hold. === Basis for the Induction === $\map P 2$ is the case: {{begin-eqn}} {{eqn | l = \sum_k \paren {-1}^k {2 \brack k} | r = - {2 \brack 1} + {2 \brack 2} | c = {{Defof|Summation}} }} {{eqn | r = - {2 \brack 1} + 1 | c = [[Unsigned Stirling Number of the First Kind of Number with Self]] }} {{eqn | r = -\binom 2 2 + 1 | c = [[Unsigned Stirling Number of the First Kind of n with n-1]] }} {{eqn | r = -1 + 1 | c = [[Binomial Coefficient with Self]] }} {{eqn | r = 0 | c = [[Binomial Coefficient with Self]] }} {{eqn | r = \delta_{2 0} - \delta_{2 1} | c = {{Defof|Kronecker Delta}} }} {{end-eqn}} Thus $\map P 2$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P m$ is true, where $m \ge 2$, then it logically follows that $\map P {m + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_k \paren {-1}^k {m \brack k} = \delta_{m 0} - \delta_{m 1} = 0$ from which it is to be shown that: :$\displaystyle \sum_k \paren {-1}^k {m + 1 \brack k} = \delta_{\paren {m + 1} 0} - \delta_{\paren {m + 1} 1} = 0$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_k \paren {-1}^k {m + 1 \brack k} | r = \sum_k \paren {-1}^k \paren {m {m \brack k} + {m \brack k - 1} } | c = {{Defof|Unsigned Stirling Numbers of the First Kind}} }} {{eqn | r = m \sum_k \paren {-1}^k {m \brack k} + \sum_k \paren {-1}^k {m \brack k - 1} | c = }} {{eqn | r = m \sum_k \paren {-1}^k {m \brack k} - \sum_k \paren {-1}^k {m \brack k} | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \paren {m - 1} \sum_k {m \brack k} | c = }} {{eqn | r = \paren {m + 1} \times 0 | c = [[Summation over Lower Index of Unsigned Stirling Numbers of the First Kind with Alternating Signs#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = 0 | c = }} {{end-eqn}} So $\map P m \implies \map P {m + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \Z_{\ge 0}: \sum_k \paren {-1}^k {n \brack k} = \delta_{n 0} - \delta_{n 1}$ {{qed}}	0
Apart from the general pattern, following directly from the definition of the [[Definition:Factorial|factorial]]: :$\paren {n!}! = n! \paren {n! - 1}!$ the only known [[Definition:Factorial|factorial]] which is the product of two [[Definition:Factorial|factorials]] is: :$10! = 6! \, 7!$	0
{{:Euclid:Proposition/X/17/Lemma}}	0
$65$ can be expressed as the [[Definition:Integer Addition|sum]] of two [[Definition:Square Number|square numbers]] in two [[Definition:Distinct|distinct]] ways: {{begin-eqn}} {{eqn | l = 65 | r = 8^2 + 1^2 }} {{eqn | r = 7^2 + 4^2 }} {{end-eqn}}	0
Let: {{begin-eqn}} {{eqn | l = z^2 | r = a^2 - x^2 | c = }} {{eqn | ll= \leadsto | l = 2 z \frac {\d z} {\d x} | r = -2 x | c = [[Chain Rule for Derivatives]], [[Power Rule for Derivatives]] }} {{eqn | ll= \leadsto | l = \int \frac {x \rd x} {\sqrt {a^2 - x^2} } | r = \int -\frac {z \rd z} z | c = [[Integration by Substitution]] }} {{eqn | r = -\int \rd z | c = }} {{eqn | r = -z + C | c = [[Primitive of Constant]] }} {{eqn | r = -\sqrt {a^2 - x^2} + C | c = substituting for $z$ }} {{end-eqn}} {{qed}}	0
=== 1 implies 2 === It suffices to show that: :$(1): \quad \displaystyle \sum_{n \mathop = 1}^\infty \map \Re {\ln f_n} = \map \Re {\ln f}$ [[Definition:Locally Uniform Convergence|locally uniformly]] :$(2): \quad \displaystyle \sum_{n \mathop = 1}^\infty \map \Im {\ln f_n} = \map \Im {\ln f} + 2 k \pi$ [[Definition:Locally Uniform Convergence|locally uniformly]] for some $k: K \to \Z$ ==== Real part ==== By [[Absolute Value of Uniformly Convergent Product]], $\displaystyle \prod_{n \mathop = 1}^\infty \cmod {f_n} = \cmod f$ [[Definition:Locally Uniform Convergence of Product|locally uniformly]]. By [[Logarithm of Infinite Product of Real Functions]] $\displaystyle \sum_{n \mathop = 1}^\infty \ln \cmod {f_n} = \ln \cmod f$ [[Definition:Locally Uniform Convergence|locally uniformly]]. Hence the result. ==== Imaginary Part ==== Let $K\subset X$ be [[Definition:Compact Space|compact]]. Let $P_n$ denote the $n$th [[Definition:Partial Product|partial product]]. Then $P_n \to f$ [[Definition:Uniform Convergence|uniformly]] on $K$. Let $\theta$ be the [[Definition:Argument of Complex Number|argument]] of $f$. Let $\theta_n = \arg P_n$ be the argument of $P_n$ in the [[Definition:Half-Open Real Interval|half-open interval]] $\hointl {\theta - \pi} {\theta + \pi}$. By [[Uniform Convergence of Complex Functions in Polar Form/Corollary|the corollary to Uniform Convergence of Complex Functions in Polar Form]]: :$\theta_n \to \theta$ [[Definition:Uniform Convergence|uniformly]] on $K$. Let $k_n: K \to \Z$ be such that: :$\displaystyle \sum_{j \mathop = 1}^n \map \Im {\ln \map {f_n} x} = \map {\theta_n} x + 2 \map {k_n} x \pi$ We show that $k_n$ is eventually equal to a fixed function $k: K \to \Z$. By the [[Triangle Inequality for Complex Numbers]]: :$2 \pi \sup_{x \mathop \in K} \size {\map {k_{n + 1} } x - \map {k_n} x} \le \sup_{x \mathop \in K} \cmod {\map {\theta_{n + 1} } x - \map {\theta_n} x} + \sup_{x \mathop \in K} \size {\map \Im {\ln \map {f_{n + 1} } x } }$ By [[Factors in Uniformly Convergent Product Converge Uniformly to One]], $f_n \to 1$ [[Definition:Uniform Convergence|uniformly]] on $K$. Let $n_0 \in \N$ be such that $\cmod {f_n - 1} \le \dfrac 1 2$ for $n \ge n_0$. By [[Complex Logarithm is Continuous Outside Branch]] and [[Heine-Cantor Theorem]], $\ln$ is [[Definition:Uniform Continuity|uniformly continuous]] on $\map {\overline B} {1, \dfrac 1 2}$. By [[Uniformly Continuous Function Preserves Uniform Convergence]], $\ln f_n \to 0$ [[Definition:Uniform Convergence|uniformly]] on $K$. Thus $\displaystyle 2 \pi \sup_{x \mathop \in K} \cmod {\map {k_{n + 1} } x - \map {k_n} x} \to 0$. So the sequence $k_n$ is eventually constant, say equal to $k: K \to \Z$. Then: :$\displaystyle \sum_{j \mathop = 1}^n \map \Im {\ln f_n} \to \theta + 2 k \pi$ [[Definition:Uniform Convergence|uniformly]] on $K$. {{qed|lemma}} === 2 implies 1 === Follows from [[Complex Exponential is Uniformly Continuous on Half-Planes]]. {{ProofWanted|Explain how.}}	0
:$\displaystyle \int \sinh a x \sin p x \ \mathrm d x = \frac {a \cosh a x \sin p x - p \sinh a x \cos p x} {a^2 + p^2} + C$	0
{{AimForCont}} $\map {\rho^k} i = \map {\rho^r} i$ for some $r > 0$. As $\rho$ has an [[Definition:Inverse Element|inverse]] in $S_n$: :$\map {\rho^{k - r} } i = i$ This [[Definition:Contradiction|contradicts]] the definition of $k$, because $k - r < k$ Thus: :$r = 0$ The result follows. {{qed}}	0
We have, by [[Laplace Transform of Power]]: :$\displaystyle \frac {\paren {-1}^n \map \Gamma s} {\paren {2 n + 1}^s} = \paren {-1}^n \int_0^\infty x^{s - 1} e^{-\paren {2 n + 1} x} \rd x$ for $\map \Re s > 0$. Summing, we have: {{begin-eqn}} {{eqn | l = \map \Gamma s \sum_{n \mathop = 0}^N \frac {\paren {-1}^n} {\paren {2 n + 1}^s} | r = \sum_{n \mathop = 0}^N \paren {-1}^n \int_0^\infty x^{s - 1} e^{-\paren {2 n + 1} x} \rd x }} {{eqn | r = \int_0^\infty x^{s - 1} \sum_{n \mathop = 0}^N \paren {-1}^n e^{-\paren {2 n + 1} x} \rd x | c = [[Linear Combination of Definite Integrals]] }} {{eqn | r = \int_0^\infty x^{s - 1} e^{-x} \paren {\sum_{n \mathop = 0}^N \paren {-e^{-2 x} }^n} \rd x }} {{end-eqn}} We have: {{begin-eqn}} {{eqn | l = \lim_{N \mathop \to \infty} \paren {\map \Gamma s \sum_{n \mathop = 0}^N \frac {\paren {-1}^n} {\paren {2 n + 1}^s} } | r = \map \Gamma s \lim_{N \mathop \to \infty} \paren {\sum_{n \mathop = 0}^N \frac {\paren {-1}^n} {\paren {2 n + 1}^s} } | c = [[Combination Theorem for Limits of Functions/Multiple Rule|Combination Theorem for Limits of Functions: Multiple Rule]] }} {{eqn | r = \map \Gamma s \map \beta s | c = {{Defof|Dirichlet Beta Function}} }} {{end-eqn}} Therefore: {{begin-eqn}} {{eqn | l = \map \Gamma s \map \beta s | r = \lim_{N \mathop \to \infty} \paren {\int_0^\infty x^{s - 1} e^{-x} \paren {\sum_{n \mathop = 0}^N \paren {-e^{-2 x} }^n} \rd x} }} {{eqn | r = \int_0^\infty x^{s - 1} e^{-x} \paren {\lim_{N \mathop \to \infty} \sum_{n \mathop = 0}^N \paren {-e^{-2 x} }^n} \rd x | c = [[Lebesgue's Dominated Convergence Theorem]] }} {{eqn | r = \int_0^\infty x^{s - 1} e^{-x} \paren {\sum_{n \mathop = 0}^\infty \paren {-e^{-2 x} }^n} \rd x }} {{eqn | r = \int_0^\infty \frac {x^{s - 1} e^{-x} } {1 - \paren {-e^{- 2x} } } \rd x | c = [[Sum of Infinite Geometric Sequence]] }} {{eqn | r = \int_0^\infty \frac {x^{s - 1} e^{-x} } {1 + e^{-2 x} } \rd x }} {{end-eqn}} giving: :$\displaystyle \map \beta s = \frac 1 {\map \Gamma s} \int_0^\infty \frac {x^{s - 1} e^{-x} } {1 + e^{-2 x} } \rd x$ {{qed}} [[Category:Dirichlet Beta Function]] [[Category:Gamma Function]] kpr57owmp33juk5buanht2wus09exkg	0
Follows directly from the identity: {{begin-eqn}} {{eqn | l = 8 \frac {k \paren {k + 1} } 2 + 1 | r = 4 k^2 + 4 k + 1 | c = }} {{eqn | r = \paren {2 k + 1}^2 | c = }} {{end-eqn}} as follows: Let $m$ be [[Definition:Triangular Number|triangular]]. Then from [[Closed Form for Triangular Numbers]]: :$\exists k \in \Z: m = \dfrac {k \paren {k + 1} } 2$ From the above identity: :$8 m + 1 = \paren {2 k + 1}^2$ which is an [[Definition:Odd Integer|odd]] [[Definition:Square Number|square]]. Let $n$ be an [[Definition:Odd Integer|odd]] [[Definition:Square Number|square]]. Then $n = r^2$ where $r$ is [[Definition:Odd Integer|odd]]. Let $r = 2 k + 1$, so that $n = \paren {2 k + 1}^2$. From the above identity: :$n = 8 \dfrac {k \paren {k + 1} } 2 + 1 = 8 m + 1$ where $m$ is [[Definition:Triangular Number|triangular]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = z_1 \times z_2 | r = \map \Im {\overline {z_1} z_2} | c = {{Defof|Vector Cross Product|subdef = Complex|index = 3|Complex Cross Product}} }} {{eqn | r = \map \Im {r_1 e^{-i \theta_1} r_2 e^{i \theta_2} } | c = [[Exponential Form of Complex Conjugate]] }} {{eqn | r = \map \Im {r_1 r_2 e^{i \paren {\theta_2 - \theta_1} } } | c = [[Product of Complex Numbers in Exponential Form]] }} {{eqn | r = \map \Im {r_1 r_2 \paren {\map \cos {\theta_2 - \theta_1} + i \, \map \sin {\theta_2 - \theta_1} } } | c = {{Defof|Polar Form of Complex Number}} }} {{eqn | r = r_1 r_2 \map \sin {\theta_2 - \theta_1} | c = {{Defof|Imaginary Part}} }} {{end-eqn}} {{qed}}	0
We have that: {{begin-eqn}} {{eqn | l = a \paren {r + s \sqrt 2}^2 + b \paren {r + s \sqrt 2} + c | r = 0 | c = }} {{eqn | ll= \leadsto | l = \paren {a r^2 + 2 a s + br + c} + \paren {2 a + b} s \sqrt 2 | r = 0 | c = }} {{end-eqn}} Because $a$, $b$, $c$, $r$ and $s$ are [[Definition:Rational Number|rational]], it must be that $\paren {2 a + b} s = 0$. Hence: {{begin-eqn}} {{eqn | l = a \paren {r - s \sqrt 2}^2 + b \paren {r - s \sqrt 2} + c | r = \paren {a r^2 + 2 a s + br + c} - \paren {2 a + b} s \sqrt 2 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} and so $\beta$ is also a [[Definition:Root of Mapping|root]] of $(1)$. {{qed}}	0
Let $\ds m_1 = \prod_{j \mathop \in J} X_j^{k_j}, m_2 = \prod_{j \mathop \in J} X_j^{k_j'}$ be two [[Definition:Monomial|monomials]]. Their product is: :$\ds m_1 \circ m_2 = \paren {\prod_{j \mathop \in J} X_j^{k_j + k_j'} }$ If $k_j + k_j' \ne 0$ then either $k_j \ne 0$ or $k_j' \ne 0$ (or both are nonzero). Therefore if $k_j + k_j' \ne 0$ for infinitely many $j$, then either $m_1$ or $m_2$ is not a [[Definition:Monomial|monomial]]. {{qed}} [[Category:Monomials]] h2lb3nxhe53agesh51ic1k8xmovh20p	0
Let $N \lhd G$ where $G$ is a [[Definition:Group|group]]. Let $a, a', b, b' \in G: a \circ N = a' \circ N, b \circ N = b' \circ N$. To show that the coset product is [[Definition:Well-Defined Operation|well-defined]], we need to demonstrate that $\paren {a \circ b} \circ N = \paren {a' \circ b'} \circ N$. So: {{begin-eqn}} {{eqn | l = a \circ N | r = a' \circ N | c = }} {{eqn | ll= \leadsto | l = a^{-1} \circ a' | o = \in | r = N | c = [[Cosets are Equal iff Product with Inverse in Subgroup]] }} {{eqn | ll= \leadsto | l = b^{-1} \circ a^{-1} \circ a' | o = \in | r = b^{-1} \circ N | c = {{Defof|Subset Product}} }} {{eqn | ll= \leadsto | l = b^{-1} \circ a^{-1} \circ a' | o = \in | r = N \circ b^{-1} | c = $N$ is a [[Definition:Normal Subgroup|normal subgroup]] }} {{eqn | ll= \leadsto | l = \exists n \in N: b^{-1} \circ a^{-1} \circ a' | r = n \circ b^{-1} | c = {{Defof|Subset Product}} }} {{eqn | ll= \leadsto | l = \paren {a \circ b}^{-1} \circ \paren {a' \circ b'} | r = n \circ b^{-1} \circ b' | c = [[Definition:Group|Group Properties]] }} {{eqn | ll= \leadsto | l = \paren {a \circ b}^{-1} \circ \paren {a' \circ b'} | o = \in | r = N | c = {{Defof|Subset Product}} }} {{end-eqn}} By [[Cosets are Equal iff Product with Inverse in Subgroup]]: :$\paren {a \circ b}^{-1} \circ \paren {a' \circ b'} \in N \implies \paren {a \circ b} \circ N = \paren {a' \circ b'} \circ N$ and the proof is complete. {{qed}}	0
:$\ceiling {-x} = -\floor x$	0
Let $n = p^3$ where $p$ is [[Definition:Prime Number|prime]]. The [[Definition:Positive Integer|positive]] [[Definition:Divisor of Integer|divisors]] of $n$ are: :$1, p, p^2, p^3$ This result follows from [[Divisors of Power of Prime]]. {{qed}}	0
From the [[Definition:Integer/Formal Definition|formal definition of integers]], $\eqclass {a, b} {}$ is an [[Definition:Equivalence Class|equivalence class]] of [[Definition:Ordered Pair|ordered pairs]] of [[Definition:Natural Numbers|natural numbers]]. Let $x = \eqclass {a, b} {}$ and $y = \eqclass {c, d} {}$ for some $x, y \in \Z$. We have: {{begin-eqn}} {{eqn | l = x | o = > | r = y | c = }} {{eqn | ll= \leadsto | l = \eqclass {a, b} {} | o = > | r = \eqclass {c, d} {} | c = {{Defof|Integer|subdef = Formal Definition}} }} {{eqn | ll= \leadsto | l = a + d | o = > | r = b + c | c = {{Defof|Strict Ordering on Integers}} }} {{eqn | ll= \leadsto | l = b + c | o = < | r = a + d | c = }} {{eqn | ll= \leadsto | l = \eqclass {b, a} {} | o = < | r = \eqclass {d, c} {} | c = }} {{eqn | ll= \leadsto | l = -\eqclass {a, b} {} | o = < | r = -\eqclass {c, d} {} | c = [[Negative of Integer]] }} {{eqn | ll= \leadsto | l = -x | o = < | r = -y | c = {{Defof|Integer|subdef = Formal Definition}} }} {{end-eqn}} {{qed}}	0
=== [[Zero Choose Zero|Binomial Coefficient $\dbinom 0 0$]] === {{:Zero Choose Zero}} === [[Zero Choose n|Binomial Coefficient $\dbinom 0 n$]] === {{:Zero Choose n}} === [[One Choose n|Binomial Coefficient $\dbinom 1 n$]] === {{:One Choose n}} === [[N Choose Negative Number is Zero]] === {{:N Choose Negative Number is Zero}} === [[Binomial Coefficient with Zero]] === {{:Binomial Coefficient with Zero}} === [[Binomial Coefficient with One]] === {{:Binomial Coefficient with One}} === [[Binomial Coefficient with Self]] === {{:Binomial Coefficient with Self}} === [[Binomial Coefficient with Self minus One]] === {{:Binomial Coefficient with Self minus One}} === [[Binomial Coefficient with Two]] === {{:Binomial Coefficient with Two}}	0
{{begin-eqn}} {{eqn | l = \sum_{k \mathop = 1}^n \dfrac {H_k} {k + 1} | r = \sum_{k \mathop = 1}^n \paren{\dfrac {H_{k + 1} } {k + 1} - \dfrac 1 {\left({k + 1}\right) \left({k + 1}\right)} } | c = {{Defof|Harmonic Number}} }} {{eqn | r = \sum_{k \mathop = 1}^n \dfrac {H_{k + 1} } {k + 1} - \sum_{k \mathop = 1}^n \dfrac 1 {\left({k + 1}\right)^2} | c = }} {{eqn | r = \sum_{k \mathop = 2}^{n + 1} \dfrac {H_k} k - \sum_{k \mathop = 2}^{n + 1} \dfrac 1 {k^2} | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \dfrac {H_k} k - \dfrac {H_1} 1 - \paren{\sum_{k \mathop = 1}^{n + 1} \dfrac 1 {k^2} - \dfrac 1 {1^2} } | c = }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \dfrac {H_k} k - 1 - \paren{\sum_{k \mathop = 1}^{n + 1} \dfrac 1 {k^2} - 1} | c = [[Harmonic Number H1|Harmonic Number $H_1$]] }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \dfrac {H_k} k - \sum_{k \mathop = 1}^{n + 1} \dfrac 1 {k^2} | c = simplifying }} {{eqn | r = \sum_{k \mathop = 1}^{n + 1} \dfrac {H_k} k - H_{n + 1}^{\left({2}\right)} | c = {{Defof|General Harmonic Numbers}} }} {{eqn | r = \dfrac { {H_{n + 1} }^2 + H_{n + 1}^{\left({2}\right)} } 2 - H_{n + 1}^{\left({2}\right)} | c = [[Summation to n of kth Harmonic Number over k]] }} {{eqn | r = \dfrac { {H_{n + 1} }^2 + H_{n + 1}^{\left({2}\right)} - 2 H_{n + 1}^{\left({2}\right)} } 2 | c = }} {{eqn | r = \dfrac { {H_{n + 1} }^2 - H_{n + 1}^{\left({2}\right)} } 2 | c = }} {{end-eqn}} Hence the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {t^2 \frac {\d^2 x} {\d t^2} + t \frac {\d x} {\d t} + (t^2-\alpha^2)x} s | r = 0 | c = [[Definition:Laplace Transform|Laplace Transform]] of [[Definition:Bessel's Equation|Bessel's Equation]] }} {{eqn | l = \frac {\d^2} {\d s^2}\mathcal{L}[x''] - \frac \d {\d s} \mathcal{L}[x'] + \frac {\d^2} {\d s^2} \mathcal{L}[x] - \alpha^2 \mathcal{L} [x] | r = 0 | c = setting the initial conditions as $x(0)=1,\ x'(0)=0$ }} {{eqn | l = (s^2+1) \mathcal{L}''[x] + 3 s \mathcal {L}'[x] + (1 - \alpha^2) \mathcal{L}[x] | r = 0 | c = Make the following change of variable $u = \sqrt {s^2 + 1} \mathcal {L} [x]$ }} {{eqn | l = u'' \sqrt {s^2 + 1} + \dfrac s {\sqrt {s^2 + 1} } u' | r = \frac {\alpha^2 u} {\sqrt {s^2 + 1} } | c = }} {{eqn | l = (u' \sqrt {s^2 + 1} )' | r = \frac {\alpha^2 u} {\sqrt {s^2 + 1} } | c = multiplying both sides by $u' \sqrt {s^2 + 1}$ }} {{eqn | l = \frac 1 2 ( (u' \sqrt {s^2 + 1} )^2)' | r = \frac 1 2 \alpha^2 (u^2)' | c = }} {{eqn | l = u' \sqrt {s^2 + 1} | r = -\alpha u | c = [[Definition:Arbitrary Constant|Constant of Integration]] removed by the [[Final Value Theorem of Laplace Transform]] }} {{eqn | l = \int \frac 1 u \rd u | r = \int -\frac \alpha {\sqrt {s^2 + 1} } \rd s | c = }} {{eqn | l = \map \ln u | r = \alpha \ln (\sqrt {s^2 + 1} - s) | c = }} {{eqn | l = u | r = (\sqrt {s^2 + 1} - s)^\alpha | c = reverting the substitution }} {{eqn | l = \mathcal{L} [x] | r = \dfrac {\paren {\sqrt {s^2 + 1} - s}^\alpha} {\sqrt {s^2 + 1} } | c = }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \mathcal{L}[J_\alpha (t)](s) | r = \dfrac {\paren {\sqrt {s^2 + 1} - s}^\alpha} {\sqrt {s^2 + 1} } | c = }} {{end-eqn}} {{qed}}	0
Consider $\N$, defined as a [[Definition:Naturally Ordered Semigroup|naturally ordered semigroup]] $\struct {S, \circ, \preceq}$. Let the [[Definition:Mapping|mapping]] $f$ be defined as: :$\map f x = \begin{cases} a & : x = 0 \\ \map s {\map f n} & : x = n \circ 1 \end{cases}$ if $\map f n$ is defined. Let $S' = \set {n \in S: \map f n \text{ is defined} }$. Then: :$0 \in S'$ and: :$n + 1 \in S'$ so by [[Principle of Mathematical Induction for Naturally Ordered Semigroup]]: :$S' = S$ Thus the [[Definition:Domain|domain]] of $f$ is $S$. Consequently, $f$ is a [[Definition:Mapping|mapping]] from $S$ into $T$ which satisfies: :$\map f 0 = a$ and: :$\map f {n \circ 1} = \map s {\map f n}$ for all $n \in S$. {{qed}} === Objections === $(1):$ In the above argument, $S'$ is not precisely defined. In order for a [[Definition:Set|set]] to be defined, that definition should be able to be expressed solely in terms of [[Definition:Logic|logic]] and definitions from [[Definition:Set Theory|set theory]]. In this case, the expression "is defined" does not meet that criterion. $(2):$ The [[Definition:Mapping|mapping]] $f$ is not defined properly. A [[Definition:Mapping|mapping]] needs to be defined as a [[Definition:Set|set]] of [[Definition:Ordered Pair|ordered pairs]] which needs to satisfy a [[Definition:Propositional Function|condition]]. In the above, it is indeed specified that :$\tuple {0, a} \in f$ and: :$\tuple {n \circ 1, \map s x} \in f$ whenever $\tuple {n, x} \in f$ Thus it appears either that: :$f$ itself is used to define $f$ or else that: :$f$ itself changes during the process in which it is being defined. Neither of these possibilities can be accepted. $(3):$ The only property of $\struct {S, \circ, \preceq}$ used in the argument is that it satisfies [[Principle of Mathematical Induction for Naturally Ordered Semigroup]]. However, consider the [[Definition:Commutative Semigroup|commutative semigroup]] $\struct {D, +}$ which has [[Definition:Element|elements]] $0$ and $1$, such that: :$D$ is the only [[Definition:Subset|subset]] of $D$: ::containing $0$ :and ::containing $x + 1$ whenever it contains $x$. Then: :for any set $T$ :any [[Definition:Mapping|mapping]] $g: T \to T$ :any [[Definition:Element|element]] $a \in T$ there exists a [[Definition:Mapping|mapping]] $f: D \to T$ such that: :$\map f y = \begin{cases} a & : y = 0 \\ \map s {\map f x} & : y = x + 1 \end{cases}$ But consider the [[Definition:Additive Group of Integers Modulo m|additive group of integers modulo $2$]]: :$\struct {\Z_2, +_2}$ which is indeed a [[Definition:Commutative Semigroup|commutative semigroup]] containing $0$ and $1$ which satisfies the hypothesis. But if $g: \N \to \N$ is the [[Definition:Mapping|mapping]] defined as: :$\map g n = n + 1$ there is no [[Definition:Mapping|mapping]] $f: \Z_2 \to \N$ which satisfies: :$\map f y = \begin{cases} 0 & : y = 0 \\ \map s {\map f x} & : y = x +_2 1 \end{cases}$ for all $x \in \Z_2$. Hence the argument is [[Definition:Invalid Argument|invalid]].	0
Follows directly from: :[[Subrings of Integers are Sets of Integer Multiples]] and: :[[Subgroup of Integers is Ideal]]. {{qed}}	0
Let $A \subseteq \R$ be the [[Definition:Set|set]] of all points on $\R$ defined as: :$A := \set 0 \cup \set {\dfrac 1 n : n \in \Z_{>0} }$ Let $\struct {A, \tau_d}$ be the [[Definition:Integer Reciprocal Space|integer reciprocal space]] with [[Definition:Zero (Number)|zero]] under the [[Definition:Euclidean Topology on Real Number Line|usual (Euclidean) topology]]. Then the [[Definition:Quasicomponent|quasicomponents]] of $A$ are [[Definition:Singleton|singletons]].	0
{{begin-eqn}} {{eqn | l = \cmod {\map \Gamma {\frac 1 2 + i t} }^2 | r = \map \Gamma {\frac 1 2 + i t} \overline {\map \Gamma {\frac 1 2 + i t} } | c = [[Modulus in Terms of Conjugate]] }} {{eqn | r = \map \Gamma {\frac 1 2 + i t} \map \Gamma {\frac 1 2 - i t} | c = [[Complex Conjugate of Gamma Function]] }} {{eqn | r = \map \Gamma {\frac 1 2 + i t} \map \Gamma {1 - \paren {\frac 1 2 + i t} } | c = applying some algebra }} {{eqn | r = \pi \map \csc {\pi \paren {\frac 1 2 + i t} } | c = [[Euler's Reflection Formula]] }} {{eqn | r = \pi \map \sec {\pi i t} | c = [[Sine of Complement equals Cosine]] }} {{eqn | r = \pi \map \sech {\pi t} | c = [[Hyperbolic Cosine in terms of Cosine]] }} {{end-eqn}} As $\cmod z \ge 0$ for all [[Definition:Complex Number|complex numbers]] $z$, we can take the non-negative [[Definition:Square Root|square root]] of both sides and write: :$\cmod {\map \Gamma {\dfrac 1 2 + i t} } = \sqrt {\pi \map \sech {\pi t} }$ {{qed}} [[Category:Gamma Function]] [[Category:Hyperbolic Secant Function]] 2livusfxejek9s219i2q3olje9w7s1s	0
First we need to show that $\theta_g$ is [[Definition:Well-Defined Mapping|well-defined]] and [[Definition:Injection|injective]]. {{begin-eqn}} {{eqn | l = x H | r = y H | c = }} {{eqn | ll= \leadstoandfrom | l = y^{-1} x | o = \in | r = H | c = }} {{eqn | ll= \leadstoandfrom | l = \paren {g y}^{-1} g x | r = y^{-1} x \in H | c = }} {{eqn | ll= \leadstoandfrom | l = \map {\theta_g} {y H} | r = \map {\theta_g} {x H} | c = }} {{end-eqn}} Thus $\theta_g$ is [[Definition:Well-Defined Mapping|well-defined]] and [[Definition:Injection|injective]]. Then we see that $\forall x H \in \mathbb S: \map {\theta_g} {g^{-1} x H} = x H$, so $\theta_g$ is [[Definition:Surjection|surjective]]. Thus $\theta_g$ is a [[Definition:Well-Defined Mapping|well-defined]] [[Definition:Bijection|bijection]] on $\mathbb S$, and therefore a [[Definition:Permutation|permutation]] on $\mathbb S$. Next we see: {{begin-eqn}} {{eqn | l = \map {\theta_{u v} } {x H} | r = u v x H | c = }} {{eqn | r = \map {\theta_u} {v x H} | c = }} {{eqn | r = \map {\theta_u} {\map {\theta_v }{x H} }= | c = }} {{end-eqn}} This shows that $\theta_{u v} = \theta_u \theta_v$, and thus: :$\map \theta {u v} = \map \theta u \, \map \theta v$ Thus $\theta$ is a [[Definition:Group Homomorphism|homomorphism]]. Now to calculate $\map \ker \theta$: {{begin-eqn}} {{eqn | l = \map \ker \theta | r = \set {g \in G: \theta_g = I_\mathbb S} | c = }} {{eqn | r = \set {g \in G: \forall x \in G: \map {\theta_g} {x H} = x H} | c = }} {{eqn | r = \set {g \in G: \forall x \in G: g x h = x H} | c = }} {{eqn | r = \set {g \in G: \forall x \in G: x^{-1} g x h = H} | c = }} {{eqn | r = \set {g \in G: \forall x \in G: x^{-1} g x \in H} | c = }} {{eqn | r = \set {g \in G: \forall x \in G: g \in x H x^{-1} } | c = }} {{eqn | r = \bigcap_{x \mathop \in G} x H x^{-1} | c = }} {{end-eqn}} as required. {{Qed}}	0
Let $a$ and $n$ be [[Definition:Integer|integers]]. Let the [[Definition:Multiplicative Order of Integer|multiplicative order of $a$ modulo $n$]] exist. Then $a \perp n$, that is, $a$ and $n$ are [[Definition:Coprime Integers|coprime]].	0
{{qed}} [[Category:Matroid Theory]] jh9v9x5cew5hy6sp49kofbr5je244sb	0
Add the terms of $H_{p - 1}$ using the definition of [[Definition:Rational Addition|rational addition]] to obtain $\dfrac m n$. Do not cancel common [[Definition:Prime Factor|prime factors]] from $m$ and $n$. It is seen that $n = \paren {p - 1}!$ Hence $p$ is not a [[Definition:Divisor of Integer|divisor]] of $n$. The [[Definition:Numerator|numerator]] $m$ is seen to be: :$m = \dfrac {\paren {p - 1}!} 1 + \dfrac {\paren {p - 1}!} 2 + \cdots + \dfrac {\paren {p - 1}!} {p - 1}$ Thus it is sufficient to show that $m$ is a [[Definition:Multiple of Integer|multiple]] of $p$. Each term in this sum is an integer of the form $\dfrac {\paren {p - 1}!} k$. For each $k \in \set {1, 2, \ldots, p - 1}$, define $k'= - \dfrac {\paren {p - 1}!} k \bmod p$. By [[Wilson's Theorem]] :$k k' \equiv -\paren {p - 1}! \equiv 1 \pmod p$ Therefore :$k' \equiv k^{-1} \pmod p$ From the [[Reduced Residue System under Multiplication forms Abelian Group/Corollary|corollary to Reduced Residue System under Multiplication forms Abelian Group]]: :$\struct {\Z'_p, \times}$ is an [[Definition:Abelian Group|abelian group]]. Since [[Inverse in Group is Unique]], the [[Definition:Set|set]]: :$\set {1', 2', \ldots, \paren {p - 1}'}$ is merely the [[Definition:Set|set]]: :$\set {1, 2, \ldots, p - 1}$ in a different order. Thus {{begin-eqn}} {{eqn | l = m | r = \dfrac {\paren {p - 1}!} 1 + \dfrac {\paren {p - 1}!} 2 + \cdots + \dfrac {\paren {p - 1}!} {p - 1} }} {{eqn | o = \equiv | r = 1 + 2 + \cdots + p - 1 | rr= \pmod p }} {{eqn | o = \equiv | r = \frac {p \paren {p - 1} } 2 | rr= \pmod p | c = [[Closed Form for Triangular Numbers]] }} {{eqn | o = \equiv | r = 0 | rr= \pmod p | c = }} {{end-eqn}} {{qed}}	0
Let $x \equiv y \pmod m$. Then by definition of [[Definition:Congruence Modulo Integer|congruence]]: :$\exists k \in Z: x - y = k m$ Hence: :$c x - c y = c k m$ and so by definition of [[Definition:Congruence Modulo Integer|congruence]]: :$c x \equiv c y \pmod m$ {{qed}} [[Category:Modulo Addition]] 5ro93s15864yankv4wl8fdte2utm0ix	0
:$\displaystyle \int \frac {x \rd x} {\sqrt {a^2 - x^2} } = -\sqrt {a^2 - x^2} + C$	0
Let $\map f t := \map \Si t = \displaystyle \int_0^t \dfrac {\sin u} u \rd u$. Then: :$\map f 0 = 0$ and: {{begin-eqn}} {{eqn | l = \map \Si t | r = \int_0^t \dfrac {\sin u} u \rd u | c = {{Defof|Sine Integral Function}} }} {{eqn | r = \int_0^1 \dfrac {\sin t v} v \rd v | c = [[Integration by Substitution]] $u = t v$ }} {{eqn | ll= \leadsto | l = \laptrans {\map \Si t} | r = \laptrans {\int_0^1 \dfrac {\sin t v} v \rd v} | c = }} {{eqn | r = \int_0^\infty e^{-s t} \paren {\int_0^1 \dfrac {\sin t v} v \rd v} \rd t | c = {{Defof|Laplace Transform}} }} {{eqn | r = \int_0^1 \dfrac 1 v \paren {\int_0^\infty e^{-s t} \sin t v \rd t} \rd v | c = exchanging order of integration }} {{eqn | r = \int_0^1 \dfrac {\laptrans {\sin t v} } v \rd v | c = {{Defof|Laplace Transform}} }} {{eqn | r = \int_0^1 \dfrac {\d v} {s^2 + v^2} | c = [[Laplace Transform of Sine]] }} {{eqn | r = \intlimits {\dfrac 1 s \arctan \dfrac v s} 0 1 | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{eqn | r = \dfrac 1 s \arctan \dfrac 1 s | c = }} {{end-eqn}} {{qed}}	0
{{AimForCont}} $S$ were [[Definition:Bounded Above Set|bounded above]]. Then $S$ has a [[Definition:Supremum of Set|supremum]] $B$. As $x > 1$, it follows that $\dfrac B x < B$ and so therefore $\dfrac B x$ can not be an [[Definition:Upper Bound of Set|upper bound]]. Therefore: :$\exists n \in \N: x^n > \dfrac B x \implies x^{n + 1} > B$ So $B$ can not be an [[Definition:Upper Bound of Set|upper bound]]. From that [[Definition:Contradiction|contradiction]] it can be concluded that $S$ can not have an [[Definition:Upper Bound of Set|upper bound]]. So by [[Proof by Contradiction]] it follows that $S$ is [[Definition:Bounded Above Set|unbounded above]]. {{qed}}	0
Let $U \in \tau_2$. By the definition of an [[Definition:Analytic Basis|analytic basis]], it follows that: :$\displaystyle \exists \mathcal A \subseteq \mathcal B: U = \bigcup \mathcal A$ Hence: {{begin-eqn}} {{eqn | l = f^{-1} \left({U}\right) | r = f^{-1} \left({\bigcup \mathcal A}\right) }} {{eqn | r = \bigcup_{B \mathop \in \mathcal A} f^{-1} \left({B}\right) | c = [[Preimage of Union under Mapping/General Result|Preimage of Union under Mapping: General Result]] }} {{eqn | o = \in | r = \tau_1 | c = [[Definition:By Hypothesis|by hypothesis]], and by the definition of a [[Definition:Topology|topology]] }} {{end-eqn}} The result follows from the definition of [[Definition:Everywhere Continuous Mapping (Topology)|continuity]]. {{qed}}	0
Suppose that $f$ [[Definition:Absolutely Convergent Series|converges absolutely]] at $\sigma_0 + i t_0$. If $\sigma \ge \sigma_0$, then: {{begin-eqn}} {{eqn | l = \size {\frac {a_n} {n^s} } | r = \frac {\size {a_n} } {n^\sigma} }} {{eqn | o = \le | r = \frac {\size {a_n} } {n^{\sigma_0} } }} {{eqn | r = \size {\frac {a_n} { n^{s_0} } } }} {{end-eqn}} Therefore [[Definition:Absolutely Convergent Series|absolute convergence]] of $\map f {s_0}$ directly implies [[Definition:Absolutely Convergent Series|absolute convergence]] of $\map f s$ for all $s = \sigma + i t$ with $\sigma > \sigma_0$. {{qed}}	0
:$\dfrac {\mathrm d \left({\operatorname{arcsec} x }\right)} {\mathrm d x} = \dfrac 1 {x^2 \sqrt {1 - \frac 1 {x^2}}}$	0
{{begin-eqn}} {{eqn | l = n^{\underline {n - 1} } | r = \dfrac {n!} {\left({n - \left({n - 1}\right)}\right)!} | c = [[Falling Factorial as Quotient of Factorials]] }} {{eqn | r = \dfrac {n!} {1!} | c = }} {{eqn | r = n! | c = [[Factorial/Examples/1|Factorial of $1$]] }} {{end-eqn}} {{qed}} [[Category:Falling Factorials]] bh4xpr99b4kgrrvf6xuijumw5kx1z4v	0
{{begin-eqn}} {{eqn | l = x^{\overline n} | r = \prod_{j \mathop = 0}^{n - 1} \paren {x + j} | c = {{Defof|Rising Factorial}} }} {{eqn | r = x \paren {x + 1} \paren {x + 2} \dotsm \paren {x + n - 1} | c = }} {{eqn | r = \dfrac {\paren {x + n - 1}!} {\paren {x - 1}!} | c = {{Defof|Factorial}} }} {{eqn | r = \dfrac {\map \Gamma {x + n} } {\map \Gamma x} | c = [[Gamma Function Extends Factorial]] }} {{end-eqn}} {{qed}}	0
:$\csc 345 \degrees = \csc \dfrac {23 \pi} {12} = -\paren {\sqrt 6 + \sqrt 2}$	0
We have that $1 \in \Z_m$ and that: :$1 \cdot_m 1 = 1$ We have that $m - 1 \in \Z_m$ and that: :$\left({m - 1}\right) \cdot_m \left({m - 1}\right) = 1$ Thus unless $m - 1 = 1$, that is, $m = 2$, there exist $2$ [[Definition:Residue (Modulo Arithmetic)|residues]] of $\Z_m$ whose [[Definition:Modulo Multiplication|product modulo $m$]] with itself equals $1$. There are $m - 2$ [[Definition:Residue (Modulo Arithmetic)|residues]] which, when [[Definition:Modulo Multiplication|multiplied modulo $m$]] with themselves have as a result a [[Definition:Residue (Modulo Arithmetic)|residue]]. Thus there can be at maximum $m - 2$ [[Definition:Residue (Modulo Arithmetic)|residues]] (excluding $1$) which can be the [[Definition:Modulo Multiplication|product modulo $m$]] of a [[Definition:Residue (Modulo Arithmetic)|residue]] with itself. But there are $m - 1$ [[Definition:Residue (Modulo Arithmetic)|residues]] (excluding $1$). So at least $1$ [[Definition:Residue (Modulo Arithmetic)|residues]] is not the [[Definition:Modulo Multiplication|product modulo $m$]] of a [[Definition:Residue (Modulo Arithmetic)|residue]] with itself. {{qed}}	0
This is an instance of [[Generating Function for Elementary Symmetric Function]]. {{qed}} [[Category:Newton-Girard Formulas]] dfvh7pgmd8wzkiufbotpjv2dbef24qm	0
Let $a \in \R_{\ne 0}$. Then: :$\displaystyle \int \frac {x^2 \rd x} {a x^2 + b x + c} = \frac x a - \frac b {2 a^2} \ln \size {a x^2 + b x + c} - \frac {b^2 - 2 a c} {2 a^2} \int \frac {\d x} {a x^2 + b x + c}$	0
{{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} \dfrac {\cmod {z^n} } {\cmod {z^{n - 1} } } | r = \lim_{n \mathop \to \infty} \cmod {\dfrac {z^n} {z^{n - 1} } } | c = }} {{eqn | r = \lim_{n \mathop \to \infty} \cmod z | c = }} {{eqn | r = \cmod z | c = }} {{end-eqn}} By the [[Ratio Test]], it follows that: :$S$ is [[Definition:Convergent Complex Series|convergent]] for $\cmod z < 1$ :$S$ is [[Definition:Divergent Series|divergent]] for $\cmod z > 1$. Hence the result by definition of [[Definition:Radius of Convergence of Complex Power Series|radius of convergence]]. {{Qed}}	0
{{ProofWanted}} {{Namedfor|Magnus Gustaf Mittag-Leffler|cat = Mittag-Leffler}}	0
{{AimForCont}} $\log_{10} 2$ is [[Definition:Rational Number|rational]]. Then: {{begin-eqn}} {{eqn | l = \log_{10} 2 | r = \frac p q | c = for some $p, q \in \Z_{\ne 0}$ }} {{eqn | ll= \leadsto | l = 2 | r = 10^{p / q} | c = {{Defof|General Logarithm}} }} {{eqn | ll= \leadsto | l = 2^q | r = 10^p | c = raising both sides to the [[Definition:Integer Power|power of $q$]] }} {{end-eqn}} Both $10^p$ and $2^q$ are [[Definition:Integer|integers]], by construction. But $10^p$ is [[Definition:Divisor of Integer|divisible]] by $5$, while $2^p$, which has only $2$ as a [[Definition:Prime Factor|prime factor]], is not. So $10^p \ne 2^q$. So, by [[Proof by Contradiction]], it follows that $\log_{10} 2$ is [[Definition:Irrational Number|irrational]]. {{qed}}	0
:$\displaystyle \int \frac {\d x} {x \paren {a^2 - x^2}^2} = \frac 1 {2 a^2 \paren {a^2 - x^2} } + \frac 1 {2 a^4} \map \ln {\frac {x^2} {a^2 - x^2} } + C$ for $x^2 < a^2$.	0
:[[File:SmallestPerfectSquareDissection.png|700px]] {{ProofWanted|That this is the smallest still needs to be proved.}}	0
:$\phi : \Z \to \Z_{\paren p} / p \Z_{\paren p}$ is a [[Definition:Surjective|surjection]].	0
The [[Definition:Integer|integers]] $\Z$ with the [[Definition:Mapping|mapping]] $\nu: \Z \to \Z$ defined as: :$\forall x \in \Z: \map \nu x = \size x$ form a [[Definition:Euclidean Domain|Euclidean domain]].	0
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]] or a [[Definition:Pseudometric Space|pseudometric space]]. Let $\sequence {x_k}$ be a [[Definition:Sequence|sequence in $A$]]. {{TFAE|def = Convergent Sequence|context = Metric Space|contextview = Metric Spaces}}	0
[[Definition:Congruence (Number Theory)|Congruence modulo zero]] is the [[Definition:Diagonal Relation|diagonal relation]]. That is: :$x \equiv y \pmod 0 \iff x = y$	0
:$\cos x - \sin x = \sqrt 2 \, \map \cos {x + \dfrac \pi 4}$	0
:[[File:Arcsinh.png|600px]]	0
The [[Definition:Set|set]] of [[Definition:Integer|integers]] under [[Definition:Integer Multiplication|multiplication]] $\struct {\Z, \times}$ is a [[Definition:Semigroup|semigroup]].	0
Let $\sequence {f_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Real Function|real functions]]. Let each of $\sequence {f_n}$ be [[Definition:Continuous on Interval|continuous]] on the [[Definition:Half-Open Real Interval|interval]] $\hointr a b$. {{explain|Investigation needed as to whether there is a mistake in {{BookReference|Special Functions of Mathematics for Engineers|1992|Larry C. Andrews|ed = 2nd|edpage = Second Edition}} -- should it actually be a closed interval?}} Let the [[Definition:Series|series]]: :$\displaystyle \map f x := \sum_{n \mathop = 1}^\infty \map {f_n} x$ be [[Definition:Uniform Convergence|uniformly convergent]] for all $x \in \closedint a b$. Then $f$ is [[Definition:Continuous on Interval|continuous]] on $\hointr a b$.	0
Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. By definition, $M$ is [[Definition:Totally Bounded Metric Space|totally bounded]] only if there exists a [[Definition:Finite Net|finite $\epsilon$-net]] for $M$. We use a [[Proof by Contradiction]]. That is, suppose that there exists no [[Definition:Finite Net|finite $\epsilon$-net]] for $M$. The aim is to construct an [[Definition:Infinite Sequence|infinite sequence]] $\sequence {x_n}_{n \ge 1}$ in $A$ that has no [[Definition:Convergent Sequence (Metric Space)|convergent]] [[Definition:Subsequence|subsequence]]. For all [[Definition:Natural Numbers|natural numbers]] $n \ge 1$, define the [[Definition:Set|set]]: :$\mathcal S_n = \set{F \subseteq A: \size F = n: \forall x, y \in F: x \ne y \implies \map d {x, y} \ge \epsilon}$ where $\size F$ denotes the [[Definition:Cardinality|cardinality]] of $F$. We use the [[Principle of Mathematical Induction]] to prove that $\mathcal S_n$ is [[Definition:Non-Empty Set|non-empty]]. It is [[Definition:Vacuous Truth|vacuously true]] that any [[Definition:Singleton|singleton]] $\set x \subseteq A$ is an [[Definition:Element|element]] of $\mathcal S_1$. Since $A$ is [[Definition:Non-Empty Set|non-empty]] by the definition of a [[Definition:Metric Space|metric space]], it follows from [[Existence of Singleton Set]] that $\mathcal S_1$ is [[Definition:Non-Empty Set|non-empty]]. Let $F \in \mathcal S_n$. By definition, $F$ is [[Definition:Finite Set|finite]]. So $F$ is not an [[Definition:Net (Metric Space)|$\epsilon$-net]] for $M$, [[Definition:By Hypothesis|by hypothesis]]. Hence, there exists an $x \in A$ such that: :$\forall y \in F: \map d {x, y} \ge \epsilon$ Note that, by [[Definition:Metric Space Axioms|axiom $\left({M1}\right)$ for a metric]]: :$x \notin F$ Consider the [[Definition:Set|set]]: :$F' := F \cup \set x$. Then: :$\left\vert{F'}\right\vert = n + 1$ and by [[Definition:Metric Space Axioms|axiom $\paren{M3}$ for a metric]]: :$F' \in \mathcal S_{n+1}$ Thus, we have proven that $\mathcal S_n$ is [[Definition:Non-Empty Set|non-empty]] for all [[Definition:Natural Numbers|natural numbers]] $n \ge 1$. Therefore, using the [[Axiom:Axiom of Countable Choice|axiom of countable choice]], we can obtain an [[Definition:Infinite Sequence|infinite sequence]] $\sequence {F_n}_{n \ge 1}$ such that: : $\forall n \in \N_{\ge 1}: F_n \in \mathcal S_n$ From [[Countable Union of Countable Sets is Countable]], there exists an [[Definition:Injection|injection]]: :$\displaystyle \phi: \bigcup_{n \mathop \ge 1} F_n \to \N$ We now construct an [[Definition:Infinite Sequence|infinite sequence]] $\sequence {x_n}_{n \ge 1}$ in $A$. {{explain|This seems a colloquial sense of PoRD, which needs to be formalised in some way}} To do this, we use the [[Principle of Recursive Definition]] to define the [[Definition:Sequence|sequence]] $\sequence {\tuple {x_1, x_2, \ldots, x_n} }_{n \ge 1}$ of [[Definition:Ordered Tuple|ordered $n$-tuples]]. Let $x_1 \in F_1$. Suppose that $x_1, x_2, \ldots, x_n$ have been defined, and let: :$T_n = \set {x_1, x_2, \ldots, x_n}$ Define: :$D_n = \set {x \in F_{n+1}: \forall y \in T_n: \map d {x, y} \ge \dfrac \epsilon 2}$ Using a [[Proof by Contradiction]], we show that $D_n$ is [[Definition:Non-Empty Set|non-empty]]. For all $x \in F_{n+1}$, define: :$\map {C_n} x = \set {y \in T_n: \map d {x, y} < \dfrac \epsilon 2}$ Let $x, x' \in F_{n+1}$ be [[Definition:Distinct Elements|distinct]]. Let $y \in \map {C_n} x$. Then it follows from: :the definition of $F_{n+1}$ :[[Definition:Metric Space Axioms|axioms $\paren {M2}$ and $\paren {M3}$ for a metric]] that: :$\map d {x', y} \ge \map d {x, x'} - \map d {x, y} > \dfrac \epsilon 2$ Hence, $y \notin \map {C_n} {x'}$. That is, the [[Definition:Indexed Family of Sets|indexed family of sets]]: :$\sequence {\map {C_n} x}_{x \in F_{n + 1}}$ is [[Definition:Pairwise Disjoint Family|pairwise disjoint]]. Suppose that $D_n$ is [[Definition:Empty Set|empty]]. That is: :$\forall x \in F_{n+1}: \map {C_n} x$ is [[Definition:Non-Empty Set|non-empty]] Then, from [[Cardinality is Additive Function]], [[Finite Union of Sets in Additive Function]], and [[Cardinality of Subset of Finite Set]], we have: :$\displaystyle \size {F_{n+1}} \le \sum_{x \mathop \in F_{n+1}} \size {\map {C_n} x} \le \size {T_n} < \size {F_{n+1}}$ which is a [[Definition:Contradiction|contradiction]]. {{explain|"Therefore ..." and explain what assumption was made so as to give rise to this contradiction.}} From the [[Well-Ordering Principle|well-ordering principle]], we have that $\struct {\N, \le}$ is a [[Definition:Well-Ordered Set|well-ordered set]]. Let $\le_{\phi}$ be the [[Definition:Ordering Induced by Injection|ordering induced by $\phi$]]. Then [[Injection Induces Well-Ordering|$\le_{\phi}$ is a well-ordering]]. We define $x_{n+1}$ as the ([[Smallest Element is Unique|unique]]) [[Definition:Smallest Element|$\le_{\phi}$-smallest element]] of $D_n$. By construction: :$\forall m, n \in \N_{>0}: m \le n \implies \map d {x_{n+1}, x_m} \ge \dfrac \epsilon 2$ Hence, by [[Principle of Mathematical Induction|induction]], it follows from [[Definition:Metric Space Axioms|axiom $\paren {M3}$ for a metric]] that: :$\forall m, n \in \N_{>0}: m \ne n \implies \map d {x_m, x_n} \ge \dfrac \epsilon 2$ Therefore, the [[Definition:Sequence|sequence]] $\sequence {x_n}$ has no [[Definition:Cauchy Sequence (Metric Space)|Cauchy]] [[Definition:Subsequence|subsequence]]. From [[Convergent Sequence in Metric Space is Cauchy Sequence]], $\sequence {x_n}$ has no [[Definition:Convergent Sequence (Metric Space)|convergent]] [[Definition:Subsequence|subsequence]] either. Thus, by definition, $M$ is not [[Definition:Sequentially Compact Space|sequentially compact]]. But this [[Definition:Contradiction|contradicts]] the original assumption that $M$ is [[Definition:Sequentially Compact Space|sequentially compact]]. Thus the assumption that there exists no [[Definition:Finite Net|finite $\epsilon$-net]] for $M$ was false. Therefore, by definition, $M$ is [[Definition:Totally Bounded Metric Space|totally bounded]]. Hence the result. {{qed}}	0
Let $z_0 \in \C$ be a [[Definition:Complex Number|complex number]]. Let $R \in \R_{>0}$ be a [[Definition:Real Number|real number]]. Let $\map {B'} {z_0, R}$ be the [[Definition:Open Punctured Complex Disk|open punctured disk]] at $z_0$ of radius $R$. Let $f: \map {B'} {z_0, R} \to \C$ be [[Definition:Holomorphic Function|holomorphic]]. Then there exists a [[Definition:Sequence|sequence]] $\sequence {a_n}_{n \mathop \in \Z}$ such that: :$\map f z = \displaystyle \sum_{n = -\infty}^\infty a_n \paren {z - z_0}^n$ for all $z \in B'(z_0, R)$.	0
{{proof wanted|An exercise for the student.}}	0
It is taken as a condition that $a \ne b \ne c \ne a$. We have that: :$\dfrac 1 a + \dfrac 1 b + \dfrac 1 c + \dfrac 1 n = 1$ and so we need to investigate the solutions to the above equations. From [[Sum of 4 Unit Fractions that equals 1]], we have that the only possible solutions are: {{begin-eqn}} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 7 + \dfrac 1 {42} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 8 + \dfrac 1 {24} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 9 + \dfrac 1 {18} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 {10} + \dfrac 1 {15} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 {12} + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 5 + \dfrac 1 {20} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 6 + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 8 + \dfrac 1 8 | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 5 + \dfrac 1 5 + \dfrac 1 {10} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 6 + \dfrac 1 6 + \dfrac 1 6 | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 3 + \dfrac 1 4 + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 3 + \dfrac 1 6 + \dfrac 1 {6} | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 6 | r = 1 }} {{eqn | l = \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 4 | r = 1 }} {{end-eqn}} From these, we can eliminate the following, because it is not the case that $a \ne b \ne c \ne a$: {{begin-eqn}} {{eqn | l = \dfrac 1 2 + \dfrac 1 5 + \dfrac 1 5 + \dfrac 1 {10} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 6 + \dfrac 1 6 + \dfrac 1 6 | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 3 + \dfrac 1 4 + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 3 + \dfrac 1 6 + \dfrac 1 {6} | r = 1 }} {{eqn | l = \dfrac 1 3 + \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 6 | r = 1 }} {{eqn | l = \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 4 + \dfrac 1 4 | r = 1 }} {{end-eqn}} Then we can see by inspection that the following are indeed solutions to the problem: {{begin-eqn}} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 7 + \dfrac 1 {42} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 8 + \dfrac 1 {24} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 9 + \dfrac 1 {18} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 3 + \dfrac 1 {12} + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 5 + \dfrac 1 {20} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 6 + \dfrac 1 {12} | r = 1 }} {{eqn | l = \dfrac 1 2 + \dfrac 1 4 + \dfrac 1 8 + \dfrac 1 8 | r = 1 }} {{end-eqn}} The remaining tuple we have is: :$\dfrac 1 2 + \dfrac 1 3 + \dfrac 1 {10} + \dfrac 1 {15} = 1$ But we note that: :$\dfrac 1 2 + \dfrac 1 3 + \dfrac 1 {10} = \dfrac {28} {30}$ which is not in the correct form. Hence the $7$ possible solutions given. {{qed}}	0
From [[Cross-Relation is Equivalence Relation]] we have that $\boxtimes$ is an [[Definition:Equivalence Relation|equivalence relation]]. We now need to show that: {{begin-eqn}} {{eqn | l = \tuple {x_1, y_1} | o = \boxtimes | r = \tuple {x_2, y_2} | c = }} {{eqn | lo= \land | l = \tuple {u_1, v_1} | o = \boxtimes | r = \tuple {u_2, v_2} | c = }} {{eqn | ll= \leadsto | l = \paren {\tuple {x_1, y_1} \oplus \tuple {u_1, v_1} } | o = \boxtimes | r = \paren {\tuple {x_2, y_2} \oplus \tuple {u_2, v_2} } | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \tuple {x_1, y_1} | o = \boxtimes | r = \tuple {x_2, y_2} | c = }} {{eqn | lo= \land | l = \tuple {u_1, v_1} | o = \boxtimes | r = \tuple {u_2, v_2} | c = By assumption }} {{eqn | l = \paren {x_1 \circ u_1} \circ \paren {y_2 \circ v_2} | r = \paren {x_1 \circ y_2} \circ \paren {u_1 \circ v_2} | c = [[Definition:Commutative Operation|Commutativity]] and [[Definition:Associative|associativity]] of $\circ$ }} {{eqn | r = \paren {x_2 \circ y_1} \circ \paren {u_2 \circ v_1} | c = By assumption }} {{eqn | r = \paren {x_2 \circ u_2} \circ \paren {y_1 \circ v_1} | c = [[Definition:Commutative Operation|Commutativity]] and [[Definition:Associative|associativity]] of $\circ$ }} {{eqn | ll= \leadsto | l = \tuple {x_1 \circ u_1, y_1 \circ v_1} | o = \boxtimes | r = \tuple {x_2 \circ u_2, y_2 \circ v_2} | c = Definition of $\boxtimes$ }} {{eqn | ll= \leadsto | l = \paren {\tuple {x_1, y_1} \oplus \tuple {u_1, v_1} } | o = \boxtimes | r = \paren {\tuple {x_2, y_2} \oplus \tuple {u_2, v_2} } | c = Definition of $\oplus$ }} {{end-eqn}} So $\boxtimes$ is a [[Definition:Congruence Relation|congruence relation]] on $\struct {S \times C, \oplus}$. {{Qed}}	0
By definition of [[Definition:Continuous Mapping (Metric Spaces)|continuous]], we have that :$\forall c \in S: \displaystyle \lim_{x \mathop \to c} \map f x = \map f c$ :$\forall c \in S: \displaystyle \lim_{x \mathop \to c} \map g x = \map g c$ Let $f$ and $g$ tend to the following [[Definition:Limit of a Function|limits]]: :$\displaystyle \lim_{x \mathop \to c} \map f x = l$ :$\displaystyle \lim_{x \mathop \to c} \map g x = m$ From the [[Sum Rule for Limits of Functions]], we have that: :$\displaystyle \lim_{x \mathop \to c} \paren {\map f x + \map g x} = l + m$ So, by definition of [[Definition:Continuous Mapping (Metric Spaces)|continuous]] again, we have that $f + g$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]] on $S$. {{qed}}	0
We have that $i = \sqrt {-1}$ is the [[Definition:Imaginary Unit|imaginary unit]]. Thus: {{begin-eqn}} {{eqn | l = 1^4 | o = | rr= = 1 }} {{eqn | l = i^4 | r = \paren {-1}^2 | rr= = 1 }} {{eqn | l = \paren {-1}^4 | r = 1^2 | rr= = 1 }} {{eqn | l = \paren {-i}^4 | r = \paren {-1}^2 \cdot \paren {-1}^2 | rr= = 1 }} {{end-eqn}} So $\set {1, i, -1, -i}$ constitutes the [[Definition:Set|set]] of the [[Definition:Complex Roots of Unity|$4$th roots of unity]]. {{qed}} [[Category:Gaussian Integers]] [[Category:Complex Roots of Unity]] pwj7xje8bvnpqyul3894062zu76x0j9	0
Let $A, B$ be two [[Definition:Natural Number|(natural) numbers]], and let $A$ by [[Definition:Natural Number Multiplication|multiplying]] $B$ make $C$, and $B$ by [[Definition:Natural Number Multiplication|multiplying]] $A$ make $D$. We need to show that $C = D$. :[[File:Euclid-VII-16.png|350px]] We have that $A \times B = C$. So $B$ [[Definition:Divisor of Integer|measures]] $C$ according to the [[Definition:Unit (One)|units]] of $A$. But the [[Definition:One|unit]] $E$ also [[Definition:Divisor of Integer|measures]] $A$ according to the [[Definition:Unit (One)|units]] in it. So $E$ [[Definition:Divisor of Integer|measures]] $A$ the same number of times that $B$ [[Definition:Divisor of Integer|measures]] $C$. Therefore from {{EuclidPropLink|book=VII|prop=15|title=Alternate Ratios of Multiples}} $E$ measures $B$ the same number of times that $A$ measures $C$. We also have that $A$ [[Definition:Divisor of Integer|measures]] $D$ according to the units of $B$. But the [[Definition:Unit (One)|unit]] $E$ also [[Definition:Divisor of Integer|measures]] $B$ according to the [[Definition:Unit (One)|units]] in it. Therefore from {{EuclidPropLink|book=VII|prop=15|title=Alternate Ratios of Multiples}} $E$ [[Definition:Divisor of Integer|measures]] $B$ the same number of times that $A$ [[Definition:Divisor of Integer|measures]] $D$. But we also have that $E$ [[Definition:Divisor of Integer|measures]] $B$ the same number of times that $A$ [[Definition:Divisor of Integer|measures]] $C$. So $A$ [[Definition:Divisor of Integer|measures]] $C$ and $D$ the same number of times. Therefore $C = D$. {{qed}}	0
From the equation $0_F = 0_F \circ d + 0_F$, the theorem is true for the trivial case $f = 0_F$. So, if there is a [[Definition:Counterexample|counterexample]] to be found, it will have a [[Definition:Degree of Polynomial over Field|degree]]. {{AimForCont}} there exists at least one [[Definition:Counterexample|counterexample]]. By a version of the [[Well-Ordering Principle]], we can assign a number $m$ to the lowest [[Definition:Degree of Polynomial over Field|degree]] possessed by any [[Definition:Counterexample|counterexample]]. So, let $f$ denote a [[Definition:Counterexample|counterexample]] which has that minimum [[Definition:Degree of Polynomial over Field|degree]] $m$. If $m < n$, the equation $f = 0_F \circ d + f$ would show that $f$ was not a [[Definition:Counterexample|counterexample]]. Therefore $m \ge n$. Suppose $d \divides f$ in $F \sqbrk X$. Then: :$\exists q \in F \sqbrk X: f = q \circ d + 0_F$ and $f$ would not be a [[Definition:Counterexample|counterexample]]. So $d \nmid f$ in $F \sqbrk X$. So, suppose that: {{begin-eqn}} {{eqn | l = f | r = \sum_{k \mathop = 0}^m {a_k \circ X^k} }} {{eqn | l = d | r = \sum_{k \mathop = 0}^n {b_k \circ X^k} }} {{eqn | l = m | o = \ge | r = n }} {{end-eqn}} We can create the [[Definition:Polynomial in Ring Element|polynomial]] $\paren {a_m \circ b_n^{-1} \circ X^{m - n} } \circ d$ which has the same [[Definition:Degree of Polynomial over Field|degree]] and [[Definition:Leading Coefficient of Polynomial|leading coefficient]] as $f$. Thus $f_1 = f - \paren {a_m \circ b_n^{-1} \circ X^{m - n} } \circ d$ is a [[Definition:Polynomial in Ring Element|polynomial]] of [[Definition:Degree of Polynomial over Field|degree]] less than $m$. Since $d \nmid f$, $f_1$ is a non-[[Definition:Null Polynomial over Ring|zero polynomial]]. There is no [[Definition:Counterexample|counterexample]] of [[Definition:Degree of Polynomial over Field|degree]] less than $m$. Therefore: :$f_1 = q_1 \circ d + r$ for some $q_1, r \in F \sqbrk X$, where either: :$r = 0_F$ or: :$r$ is non-[[Definition:Null Polynomial over Ring|zero]] with [[Definition:Degree of Polynomial over Field|degree]] strictly less than $n$. Hence: {{begin-eqn}} {{eqn | l = f | r = f_1 + \paren {a_m \circ b_n^{-1} \circ X^{m - n} } \circ d | c = }} {{eqn | r = \paren {q_1 + a_m \circ b_n^{-1} \circ X^{m - n} } \circ d + r | c = }} {{end-eqn}} Thus $f$ is not a [[Definition:Counterexample|counterexample]]. From this [[Proof by Contradiction|contradiction]] follows the result. {{qed}}	0
Let $m, n \in \Z_{>0}$ be [[Definition:Strictly Positive Integer|(strictly) positive integers]]. Let $\struct {\Z_m, +}$ denote the [[Definition:Additive Group of Integers Modulo m|additive group of integers modulo $m$]]. The number of distinct [[Definition:Group Homomorphism|homomorphisms]] $\phi: \struct {\Z_m, +} \to \struct {\Z_n, +}$ is $\gcd \set {m, n}$.	0
Let $\struct {\overline \R, \le}$ be the [[Definition:Extended Real Number Line|extended real numbers]] with the [[Definition:Ordering on Extended Real Numbers|usual ordering]]. Then $+\infty$ is a [[Definition:Maximal Element|maximal element]] of $\overline \R$.	0
Let $f$ be a [[Definition:Piecewise Continuous Function with One-Sided Limits|piecewise continuous function with one-sided limits]]. By definition, there exists a [[Definition:Finite Subdivision|finite subdivision]] $\set {x_0, x_1, \ldots, x_n}$ of $\closedint a b$, where $x_0 = a$ and $x_n = b$, such that $f$ is [[Definition:Continuous Real Function on Open Interval|continuous]] on $\openint {x_{i − 1} } {x_i}$ for every $i \in \set {1, 2, \ldots, n}$. For every $i \in \set {1, 2, \ldots, n}$, we define a [[Definition:Real Function|function]] $f_i$ with [[Definition:Domain of Mapping|domain]] $\closedint {x_{i − 1} } {x_i}$, as follows: :$\map {f_i} x := \begin{cases} \displaystyle \lim_{x \mathop \to x_{i − 1}^+} \map f x & : x = x_{i - 1} \\ \map f x & : x \in \openint {x_{i - 1} } {x_i} \\ \displaystyle \lim_{x \mathop \to x_i^-} \map f x & : x = x_i \end{cases}$ The [[Definition:One-Sided Limit of Real Function|one-sided limits]] in this definition exist because $f$ is [[Definition:Piecewise Continuous Function with One-Sided Limits|piecewise continuous with one-sided limits]]. We have that $f$ is [[Definition:Continuous Real Function on Open Interval|continuous]] on $\openint {x_{i - 1} } {x_i}$. We also have that $f_i = f$ on $\openint {x_{i - 1} } {x_i}$. Therefore $f_i$ is also [[Definition:Continuous Real Function on Open Interval|continuous]] on $\openint {x_{i - 1} } {x_i}$. By definition, $f_i$ is [[Definition:Right-Continuous at Point|right-continuous]] at $x_{i − 1}$ and [[Definition:Left-Continuous at Point|left-continuous]] at $x_i$. Therefore, $f_i$ is [[Definition:Continuous Real Function on Closed Interval|continuous]] throughout its [[Definition:Domain of Mapping|domain]] $\closedint {x_{i − 1} } {x_i}$. By [[Continuous Function on Compact Subspace of Euclidean Space is Bounded]] and [[Closed Real Interval is Compact in Metric Space]], $f_i$ is [[Definition:Bounded Real-Valued Function|bounded]]. Therefore, $f_i$ is [[Definition:Bounded Real-Valued Function|bounded]] on $\closedint {x_{i − 1} } {x_i}$. We have that $\openint {x_{i - 1} } {x_i}$ constitutes a [[Definition:Subset|subset]] of $\closedint {x_{i − 1} } {x_i}$. Thus $f_i$ is also bounded on $\openint {x_{i - 1} } {x_i}$. As $f_i = f$ on $\openint {x_{i - 1} } {x_i}$, $f$ is also [[Definition:Bounded Real-Valued Function|bounded]] on $\openint {x_{i - 1} } {x_i}$. We have that: :$f$ is [[Definition:Bounded Real-Valued Function|bounded]] on the [[Definition:Open Real Interval|intervals]] $\openint {x_{i - 1} } {x_i}$ and :the [[Definition:Set|set]] of these [[Definition:Open Real Interval|intervals]] is [[Definition:Finite Set|finite]]. Hence the [[Definition:Set|set]] of [[Definition:Bounded Real-Valued Function|bounds]] of $f$ on these [[Definition:Open Real Interval|intervals]] is itself [[Definition:Bounded Subset of Real Numbers|bounded]]. The [[Definition:Bound of Subset of Real Numbers|bound]] of this [[Definition:Set|set]] of [[Definition:Bound of Subset of Real Numbers|bounds]] serves as a [[Definition:Bound of Real-Valued Function|bound]] for $f$ on each of the [[Definition:Open Real Interval|intervals]] $\openint {x_{i - 1} } {x_i}$. Therefore, this [[Definition:Bound of Subset of Real Numbers|bound]] is a [[Definition:Bound of Real-Valued Function|bound]] for $f$ on the [[Definition:Set Union|union]] of these [[Definition:Open Real Interval|intervals]]. That is, $f$ is [[Definition:Bounded Real-Valued Function|bounded]] on the [[Definition:Set Union|union]] of $\openint {x_{i - 1} } {x_i}$ for all $i \in \set {1, 2, \ldots, n}$. The only points left to consider are the points in the set $\set {x_0, x_1, \ldots, x_n}$. Since this set is [[Definition:Finite Set|finite]], the [[Definition:Max Function|maximum]] $\map \max {\size {\map f {x_0} }, \size {\map f {x_1} }, \ldots, \size {\map f {x_n} } }$ is finite and serves as a [[Definition:Bounded Real-Valued Function|bound]] for $f$ on $\set {x_0, x_1, \ldots, x_n}$. Thus $f$ is [[Definition:Bounded Real-Valued Function|bounded]] on the whole of its [[Definition:Domain of Mapping|domain]] $\closedint a b$. Hence $f$ is a [[Definition:Bounded Piecewise Continuous Function|bounded piecewise continuous function]]. {{qed}}	0
Let $n \in \N$ be a [[Definition:Natural Number|natural number]]. Consider the [[Definition:Set|set]] $\N_n$ defined as: :$\N_n = \closedint 1 n = \set {1, 2, \ldots n}$ Let $Q_n$ be the largest [[Definition:Subset|subset]] of $\N_n$ such that no [[Definition:Element|element]] of $Q_n$ is the [[Definition:Divisor of Integer|divisor]] of another [[Definition:Element|element]] of $Q_n$. Let $\map f n$ be the [[Definition:Cardinality|cardinality]] of $Q_n$. Then for [[Definition:Sufficiently Large|sufficiently large]] $n$: :$0 \cdotp 6725 \ldots \le \dfrac {\map f n} n \le 0 \cdotp 6736 \ldots$	0
{{begin-eqn}} {{eqn | l = \map \cot {a + b i} | r = \dfrac {\cos a \cosh b - i \sin a \sinh b} {\sin a \cosh b + i \cos a \sinh b} | c = [[Cotangent of Complex Number/Formulation 1|Cotangent of Complex Number: Formulation 1]] }} {{eqn | r = \dfrac {\cot a \cosh b - i \sinh b} {\cosh b + i \cot a \sinh b} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\dfrac 1 {\sin a}$ }} {{eqn | r = \dfrac {\cot a \coth b - i} {\coth b + i \cot a} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\dfrac 1 {\sinh b}$ }} {{eqn | r = \dfrac {i \cot a \coth b - 1} {\cot a - i \coth b} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $-i$ }} {{end-eqn}} {{qed}}	0
Let $D$ be a [[Definition:Set|set]]. Let $\struct {V, \norm {\,\cdot\,} }$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $a_i, b_i$ be [[Definition:Mapping|mappings]] from $D \to M$. Let the following conditions be satisfied: :$(1): \quad$ The [[Definition:Sequence of Partial Sums|sequence of partial sums]] of $\displaystyle \sum_{n \mathop = 1}^\infty \map {a_n} x$ be [[Definition:Bounded Sequence|bounded]] on $D$ :$(2): \quad \sequence {\map {b_n} x}$ be [[Definition:Monotone Sequence|monotonic]] for each $x \in D$ :$(3): \quad \map {b_n} x \to 0$ [[Definition:Uniform Convergence|converge uniformly]] on $D$. Then: :$\displaystyle \sum_{n \mathop = 1}^\infty \map {a_n} x \, \map {b_n} x$ [[Definition:Uniform Convergence|converges uniformly]] on $D$.	0
We use the definition of the [[Definition:Natural Logarithm|natural logarithm]] as the [[Definition:Natural Logarithm/Positive Real/Definition 2|inverse of the exponential]]: :$\ln x = y \iff e^y = x$ Then: {{begin-eqn}} {{eqn | l = e^0 | r = 1 | c = [[Exponential of Zero]] }} {{eqn | ll= \leadstoandfrom | l = \ln 1 | r = 0 }} {{end-eqn}} {{qed}}	0
We have that: :$-2 < -1 \cdotp 1 \le -1$ Hence $-1$ is the [[Definition:Ceiling Function|ceiling]] of $-1 \cdotp 1$ by definition. {{qed}}	0
The [[Definition:Complex Inverse Tangent|inverse tangent]] of $i$ is not defined.	0
Let $\struct {\R \setminus \Q, \tau_d}$ be the [[Definition:Irrational Number Space|irrational number space]] under the [[Definition:Euclidean Topology on Real Number Line|Euclidean topology]] $\tau_d$. Then $\struct {\R \setminus \Q, \tau_d}$ is not a [[Definition:Locally Compact Hausdorff Space|locally compact Hausdorff Space]].	0
Let $G$ be a [[Definition:Finite Group|finite]] [[Definition:Abelian Group|abelian group]] with [[Definition:Identity Element|identity]] $e$. Let $G^*$ be the dual group of [[Definition:Character (Number Theory)|characters]] $\chi : G \to \C_{\ne 0}$. Let $\chi_0$ be the [[Definition:Trivial Character|trivial character]] on $G$. Let $\psi: G \to \C_{\ne 0}$ be any [[Definition:Character (Number Theory)|character]]. Let $y \in G$ be arbitrary. Then: :$\displaystyle \sum_{x \mathop \in G} \psi \left({x}\right) = \begin{cases} \left\lvert{G}\right\rvert & : \psi = \chi_0 \\ 0 & : \psi \ne \chi_0 \end{cases}$ and: :$\displaystyle \sum_{\chi \mathop \in G^*} \chi \left({y}\right) = \begin{cases} \left\lvert{G^*}\right\rvert & : y = e \\ 0 & : y \ne e \end{cases}$	0
:$\displaystyle \int F \left({\arcsin \frac x a}\right) \ \mathrm d x = a \int F \left({u}\right) \cos u \ \mathrm d u$ where $u = \arcsin \dfrac x a$.	0
:$\displaystyle \int \frac {\d x} {a^2 - x^2} = \frac 1 a \coth^{-1} \frac x a + C$ where $\size x > a$.	0
Let $a \in \R_{\ne 0}$. Then: :$\displaystyle \int \frac {\mathrm d x} {\left({a x^2 + b x + c}\right)^2} = \frac {2 a x + b} {\left({4 a c - b^2}\right) \left({a x^2 + b x + c}\right)} + \frac {2 a} {4 a c - b^2} \int \frac {\mathrm d x} {a x^2 + b x + c}$	0
From [[Primitive of Reciprocal of x by Power of x plus Power of a]]: :$\displaystyle \int \frac {\mathrm d x} {x \left({x^n + a^n}\right)} = \frac 1 {n a^n} \ln \left\vert{\frac {x^n} {x^n + a^n} }\right\vert + C$ So: {{begin-eqn}} {{eqn | l = \int \frac {\mathrm d x} {x \left({x^2 + a^2}\right)} | r = \frac 1 {2 a^2} \ln \left\vert{\frac {x^2} {x^2 + a^2} }\right\vert + C | c = [[Primitive of Reciprocal of x by Power of x plus Power of a|Primitive of $\dfrac 1 {x \left({x^n + a^n}\right)}$]] with $n = 2$ }} {{eqn | r = \frac 1 {2 a^2} \ln \left({\frac {x^2} {x^2 + a^2} }\right) + C | c = [[Absolute Value of Even Power]] }} {{end-eqn}} {{qed}}	0
:[[File:Euclid-XIII-5.png|400px]] Let the [[Definition:Line Segment|line]] $AB$ be cut in [[Definition:Golden Mean|extreme and mean ratio]] at the [[Definition:Point|point]] $C$. Let $AC$ be the greater [[Definition:Line Segment|segment]]. Let $AD = AC$. It is to be demonstrated that $DB$ is cut in [[Definition:Golden Mean|extreme and mean ratio]] at the [[Definition:Point|point]] $A$ where $BA > AD$. Let the [[Definition:Square (Geometry)|square]] $AE$ be described on $AB$. Let the figure be drawn as above. We have that $AB$ is cut in [[Definition:Golden Mean|extreme and mean ratio]] at $C$. Therefore from: :{{EuclidPropLink|book = VI|prop = 17|title = Rectangles Contained by Three Proportional Straight Lines}} and: :{{EuclidDefLink|VI|3|Extreme and Mean Ratio}} it follows that: :$AB \cdot BC = AC^2$ We have that: :$CE = AB \cdot BC$ and: :$CH = AC^2$ THerefore: :$CH = HC$ But: :$HE = CE$ and: :$DH = HC$ Therefore: :$DH = HE$ Therefore: :$DK = AE$ Thus: :$DK = BD \cdot DA$ That is: :$BD \cdot DA = AB^2$ Therefore: :$DB : BA = BA : AD$ and: :$DB > BA$ Therefore by {{EuclidPropLink|book = V|prop = 14|title = Relative Sizes of Components of Ratios}}: :$BA > AD$ Hence the result as stated. {{qed}} {{Euclid Note|5|XIII}}	0
Every [[Definition:Finite Group|finite]] [[Definition:Abelian Group|abelian group]] is an [[Definition:Internal Group Direct Product|internal group direct product]] of [[Definition:Cyclic Group|cyclic groups]] whose [[Definition:Order of Group|orders]] are [[Definition:Prime Power|prime powers]]. The number of terms in the [[Definition:Internal Group Direct Product|product]] and the [[Definition:Order of Group|orders]] of the [[Definition:Cyclic Group|cyclic groups]] are [[Definition:Unique|uniquely determined]] by the group.	0
:$\sec 120 \degrees = \sec \dfrac {2 \pi} 3 = -2$	0
Proof by [[Principle of Mathematical Induction|induction]]: For all $n \in \N$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\paren {m \cdot x} \circ \paren {n \cdot x} = \paren {m n} \cdot \paren {x \circ x}$ In what follows, we make extensive use of [[Powers of Ring Elements]]: :$\forall n \in \Z: \forall x \in R: \paren {m \cdot x} \circ x = m \cdot \paren {x \circ x} = x \circ \paren {m \cdot x}$ First we verify $\map P 0$. When $n = 0$, we have: {{begin-eqn}} {{eqn | l = \paren {m \cdot x} \circ \paren {0 \cdot x} | r = \paren {m \cdot x} \circ 0_R | c = }} {{eqn | r = 0_R | c = }} {{eqn | r = 0 \cdot \paren {x \circ x} | c = }} {{eqn | r = \paren {m 0} \cdot \paren {x \circ x} | c = }} {{end-eqn}} So $\map P 0$ holds. === Basis for the Induction === Next we verify $\map P 1$. When $n = 1$, we have: {{begin-eqn}} {{eqn | l = \paren {m \cdot x} \circ \paren {1 \cdot x} | r = \paren {m \cdot x} \circ x | c = }} {{eqn | r = m \cdot \paren {x \circ x} | c = }} {{eqn | r = \paren {m 1} \cdot \paren {x \circ x} | c = }} {{end-eqn}} So $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\paren {m \cdot x} \circ \paren {k \cdot x} = \paren {m k} \cdot \paren {x \circ x}$ Then we need to show: :$\paren {m \cdot x} \circ \paren {\paren {k + 1} \cdot x} = \paren {m \paren {k + 1} } \cdot \paren {x \circ x}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \paren {m \cdot x} \circ \paren {\paren {k + 1} \cdot x} | r = \paren {m \cdot x} \circ \paren {k \cdot x + x} | c = }} {{eqn | r = \paren {m \cdot x} \circ \paren {k \cdot x} + \paren {m \cdot x} \circ x | c = {{Ring-axiom|D}} }} {{eqn | r = \paren {m k} \cdot \paren {x \circ x} + m \cdot \paren {x \circ x} | c = [[Powers of Ring Elements/General Result#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {m k + k} \cdot \paren {x \circ x} | c = {{Ring-axiom|D}} }} {{eqn | r = \paren {m \paren {k + 1} } \cdot \paren {x \circ x} | c = }} {{end-eqn}} So $\map P K \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall m \in \Z: \forall n \in \N: \paren {m \cdot x} \circ \paren {n \cdot x} = \paren {m n} \cdot \paren {x \circ x}$ {{qed|lemma}} The result for $n < 0$ follows directly from [[Powers of Group Elements]]. {{qed}} [[Category:Ring Theory]] [[Category:Proofs by Induction]] nh2n0mqnbv15b7l1leixn5etc5jip76	0
Let $z = x + i y \in \C$ be a [[Definition:Complex Number|complex number]], where $x, y \in \R$. Let $\sin z$ denote the [[Definition:Complex Sine Function|complex sine function]]. Then: :$\map \Re {\sin z} = \sin x \cosh y$ where: :$\Re z$ denotes the [[Definition:Real Part|real part]] of a [[Definition:Complex Number|complex number]] $z$ :$\sin$ denotes the [[Definition:Sine Function|sine function]] ([[Definition:Real Sine Function|real]] and [[Definition:Complex Sine Function|complex]]) :$\cosh$ denotes the [[Definition:Hyperbolic Cosine|hyperbolic cosine function]].	0
:$\displaystyle \int x \left({\sqrt {x^2 + a^2} }\right)^3 \ \mathrm d x = \frac {\left({\sqrt {x^2 + a^2} }\right)^5} 5 + C$	0
{{begin-eqn}} {{eqn | l = \frac {a^x} {a^y} | r = a^x \paren {\frac 1 {a^y} } | c = }} {{eqn | r = \paren {a^x} \paren {a^{-y} } | c = [[Exponent Combination Laws/Negative Power|Exponent Combination Laws: Negative Power]] }} {{eqn | r = a^{x - y} | c = [[Product of Powers]] }} {{end-eqn}} {{qed}}	0
Running through the [[Definition:Positive Integer|positive integers]] in turn: {{begin-eqn}} {{eqn | l = 6 \times 1 - 1 | r = 5 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 1 + 1 | r = 7 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 2 - 1 | r = 11 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 2 + 1 | r = 13 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 3 - 1 | r = 17 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 3 + 1 | r = 19 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 4 - 1 | r = 23 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 4 + 1 | r = 25 = 5^2 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 5 - 1 | r = 29 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 5 + 1 | r = 31 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 6 - 1 | r = 35 = 5 \times 7 | c = and so is [[Definition:Composite Number|composite]] }} {{eqn | l = 6 \times 5 + 1 | r = 37 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 7 - 1 | r = 41 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 7 + 1 | r = 43 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 8 - 1 | r = 47 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 8 + 1 | r = 49 = 7^2 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 9 - 1 | r = 53 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 9 + 1 | r = 55 = 5 \times 11 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 10 - 1 | r = 59 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 10 + 1 | r = 61 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 11 - 1 | r = 65 = 5 \times 13 | c = and so is [[Definition:Composite Number|composite]] }} {{eqn | l = 6 \times 11 + 1 | r = 67 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 12 - 1 | r = 71 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 12 + 1 | r = 73 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 13 - 1 | r = 77 = 7 \times 11 | c = and so is [[Definition:Composite Number|composite]] }} {{eqn | l = 6 \times 13 + 1 | r = 79 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 14 - 1 | r = 83 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 14 + 1 | r = 85 = 5 \times 17 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 15 - 1 | r = 89 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 15 + 1 | r = 91 = 7 \times 13 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 16 - 1 | r = 95 = 5 \times 19 | c = and so is [[Definition:Composite Number|composite]] }} {{eqn | l = 6 \times 16 + 1 | r = 97 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 17 - 1 | r = 101 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 17 + 1 | r = 103 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 18 - 1 | r = 107 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 18 + 1 | r = 109 | c = which is [[Definition:Prime Number|prime]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 19 - 1 | r = 113 | c = which is [[Definition:Prime Number|prime]] }} {{eqn | l = 6 \times 19 + 1 | r = 115 = 5 \times 23 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{begin-eqn}} {{eqn | l = 6 \times 20 - 1 | r = 119 = 7 \times 17 | c = and so is [[Definition:Composite Number|composite]] }} {{eqn | l = 6 \times 20 + 1 | r = 121 = 11^2 | c = and so is [[Definition:Composite Number|composite]] }} {{end-eqn}} {{qed}}	0
Let $a$ and $b$ be [[Definition:Real Number|real numbers]]. Let $i$ be the [[Definition:Imaginary Unit|imaginary unit]]. Then:	0
Let $G$ be a [[Definition:Group|group]]. Let $N \lhd G$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$. Then every [[Definition:Subgroup|subgroup]] of the [[Definition:Quotient Group|quotient group]] $G / N$ is of the form $H / N = \set {h N: h \in H}$, where $N \le H \le G$. Conversely, if $N \le H \le G$ then $H / N \le G / N$. The correspondence between [[Definition:Subgroup|subgroups]] of $G / N$ and [[Definition:Subgroup|subgroups]] of $G$ containing $N$ is a [[Definition:Bijection|bijection]]. This [[Definition:Bijection|bijection]] maps [[Definition:Normal Subgroup|normal subgroups]] of $G / N$ onto [[Definition:Normal Subgroup|normal subgroups]] of $G$ which contain $N$.	0
{{begin-eqn}} {{eqn | l = \map \zeta 4 | r = \paren{\map \zeta 2 }^2 - 2 \dfrac { \pi^4} {5!} | c = Squaring Zeta of 2 produces Zeta of 4 plus two times the sum associated with the 4th power term in the sin(x)/x expansion }} {{eqn | r = \dfrac { \pi^4} {36} - \dfrac { \pi^4} {60} | c = simplifying }} {{eqn | r = \dfrac {\pi^4} {90} | c = simplifying }} {{end-eqn}} {{qed}}	0
We dismiss the cases where $k < 0$ by observing that in such cases $\dbinom n k = 0$ while $n^k > 0$. Similarly we dismiss $k = 0$: we have $\dbinom n 0 = 1 = n^0$. Let: : $N = \set {1, \ldots, n}$ : $K = \set {1, \ldots, k}$ From [[Cardinality of Set of Strictly Increasing Mappings]], $\dbinom n k$ is the number of [[Definition:Strictly Increasing Mapping|strictly increasing mappings]] from $K$ to $N$. From [[Cardinality of Set of All Mappings]], $n^k$ is the number of all [[Definition:Mapping|mappings]] from $K$ to $N$. For $k = 1$ there is exactly 1 [[Definition:Mapping|mapping]] from $K$ to $N$ which is trivially [[Definition:Strictly Increasing Mapping|strictly increasing]]. Otherwise we have that $K$ has more than one [[Definition:Element|element]], and so therefore does $n$. The [[Definition:Mapping|mapping]] $f: K \to N: \forall s \in K: \map f s = 1$ is clearly not a [[Definition:Strictly Increasing Mapping|strictly increasing mapping]]. So not all [[Definition:Mapping|mappings]] from $K$ to $N$ are [[Definition:Strictly Increasing Mapping|strictly increasing]]. Hence a [[Definition:Strict Inequality|strict inequality]] holds, and so $\dbinom n k < n^k$. {{qed}}	0
This proof assumes the [[Definition:Exponential Function/Real/Limit of Sequence|limit definition of $\exp$]]. So let: :$\forall n \in \N: \forall x \in \R: \map {f_n} x = \paren {1 + \dfrac x n}^n$ Let $x_0 \in \R$. Consider $I := \closedint {x_0 - 1} {x_0 + 1}$. Let: :$N = \ceiling {\max \set {\size {x_0 - 1}, \size {x_0 + 1} } }$ where $\ceiling {\, \cdot \,}$ denotes the [[Definition:Ceiling Function|ceiling function]]. From [[Closed Real Interval is Compact in Metric Space]], $I$ is [[Definition:Compact (Real Analysis)|compact]]. From [[Chain Rule for Derivatives]]: :$\dfrac \d {\d x} \map {f_n} x = \dfrac n {n + x} \map {f_n} x$ === [[Derivative of Exponential Function/Proof 5/Lemma|Lemma]] === {{:Derivative of Exponential Function/Proof 5/Lemma}}{{qed|lemma}} From the [[Derivative of Exponential Function/Proof 5/Lemma|lemma]]: :$\forall x \in I: \sequence {\dfrac \d {\d x} \map {f_{n + N} } x}$ is [[Definition:Increasing Real Sequence|increasing]] Hence, from [[Dini's Theorem]], $\sequence {\dfrac \d {\d x} f_{n + N} }$ is [[Definition:Uniform Convergence|uniformly convergent]] on $I$. Therefore, for $x \in I$: {{begin-eqn}} {{eqn | l = \frac \d {\d x} \exp x | r = \frac \d {\d x} \lim_{n \mathop \to \infty} \map {f_n} x }} {{eqn | r = \frac \d {\d x} \lim_{n \mathop \to \infty} \map {f_{n + N} } x | c = [[Tail of Convergent Sequence]] }} {{eqn | r = \lim_{n \mathop \to \infty} \frac \d {\d x} \map {f_{n + N} } x | c = [[Derivative of Uniformly Convergent Sequence of Differentiable Functions]] }} {{eqn | r = \lim_{n \mathop \to \infty} \frac n {n + x} \map {f_n} x | c = from above }} {{eqn | r = \lim_{n \mathop \to \infty} \map {f_n} x | c = [[Combination Theorem for Sequences]] }} {{eqn | r = \exp x }} {{end-eqn}} In particular: :$\dfrac \d {\d x} \exp x_0 = \exp x_0$ {{qed}}	0
Consider the [[Prime Number Race between 4n+1 and 4n-1|prime number race between $4 n + 1$ and $4 n - 1$]]. While the [[Definition:Prime Number|prime numbers]] of the form $4 n - 1$ appear usually to be in the majority, the lead changes from one to the other an [[Definition:Infinite Set|infinite number]] of times.	0
:[[File:Rotation-by-minus-1.png|600px]] By definition of the [[Definition:Imaginary Unit|imaginary unit]]: :$-1 = i^2$ and so: :$-1 \times z = i \paren {i z}$ From [[Multiplication by Imaginary Unit is Equivalent to Rotation through Right Angle]], [[Definition:Complex Multiplication|multiplication]] by $i$ is equivalent to [[Definition:Plane Rotation|rotation]] through a [[Definition:Right Angle|right angle]], in an [[Definition:Anticlockwise|anticlockwise]] direction. So multiplying by $i^2$ is equivalent to [[Definition:Plane Rotation|rotation]] through two [[Definition:Right Angle|right angles]] in an [[Definition:Anticlockwise|anticlockwise]] direction. {{qed}}	0
Let $p$ be an [[Definition:Odd Prime|odd prime]]. Then $p$ has $\dfrac {p-1} 2$ [[Definition:Quadratic Residue|quadratic residues]] and $\dfrac {p-1} 2$ [[Definition:Quadratic Non-Residue|quadratic non-residues]]. The [[Definition:Quadratic Residue|quadratic residues]] are [[Definition:Congruence Modulo Integer|congruent modulo $p$]] to the [[Definition:Integer|integers]] $1^2, 2^2, \ldots, \left({\dfrac {p-1} 2}\right)$.	0
Let $\N$ be the [[Definition:Natural Numbers|natural numbers]]. Let $m, n \in \N$. Then $m$ and $n$ are [[Definition:Comparable|comparable]] by the [[Definition:Ordering on Natural Numbers|ordering relation]] $\le$. That is, either: :$(1): \quad m \le n$ or: :$(2): \quad n \le m$ or possibly both.	0
There exists an [[Definition:Infinite Set|infinite number]] of [[Definition:Integer|integers]] which are simultaneously [[Definition:Sierpiński Number|Sierpiński]], [[Definition:Riesel Number|Riesel]] and [[Definition:Carmichael Number|Carmichael]].	0
We have the definition of the [[Definition:Power to Real Number|power to a real number]]: :$\displaystyle n^{1/n} = \map \exp {\frac 1 n \ln n}$ From [[Powers Drown Logarithms]], we have that: :$\displaystyle \lim_{n \mathop \to \infty} \frac 1 n \ln n = 0$ Hence: :$\displaystyle \lim_{n \mathop \to \infty} n^{1/n} = \exp 0 = 1$ and the result. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sqrt {x^2 + a^2} } | r = \map \ln {x + \sqrt {x^2 + a^2} } + C | c = [[Primitive of Reciprocal of Root of x squared plus a squared/Logarithm Form|Primitive of $\dfrac 1 {\sqrt {x^2 + a^2} }$ in Logarithm Form]] }} {{eqn | l = \int \frac {\d x} {-\sqrt {x^2 + a^2} } | r = -\map \ln {x + \sqrt {x^2 + a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \ln \size {x - \sqrt {x^2 + a^2} } + C + \map \ln {a^2} | c = [[Negative of Logarithm of x plus Root x squared plus a squared/Corollary|Negative of Logarithm of x plus Root x squared plus a squared: Corollary]] }} {{eqn | r = \ln \size {x - \sqrt {x^2 + a^2} } + C | c = subsuming $\map \ln {a^2}$ into the constant }} {{end-eqn}} {{qed}}	0
The [[Definition:Sequence|sequence]] of '''[[Definition:Ulam Number|Ulam numbers]]''' begins as follows: :$1, 2, 3, 4, 6, 8, 11, 13, 16, 18, 26, 28, 36, 38, 47, 48, 53, 57, 62, 69, \ldots{}$	0
From [[Countable Basis of Real Number Line]] we have that $\struct {\R, \tau_d}$ has a [[Definition:Countable Basis|countable basis]]. The result follows directly from the definition of a [[Definition:Second-Countable Space|second-countable space]]. {{qed}}	0
Let $\left({z_i}\right)_{i \in I}, \left({w_i}\right)_{i \in I}$ be [[Definition:Indexed Set|$I$-indexed families]] of [[Definition:Complex Number|complex numbers]]. That is, let $z_i, w_i \in \C$ for all $i \in I$. Suppose that $\displaystyle \sum \left\{{ z_i: i \in I }\right\}, \sum \left\{{ w_i: i \in I }\right\}$ [[Definition:Generalized Sum|converge]] to $z, w \in \C$, respectively. Then: :$(1): \quad \displaystyle \sum \left\{{ z_i + w_i: i \in I }\right\}$ [[Definition:Generalized Sum|converges]] to $z+w$ :$(2): \quad \forall \lambda \in \C: \displaystyle \sum \left\{{ \lambda z_i: i \in I }\right\}$ [[Definition:Generalized Sum|converges]] to $\lambda z$	0
Let $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ denote the [[Cyclotomic Ring/Examples/5th|$5$th cyclotomic ring]]. Then $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ is not a [[Definition:Unique Factorization Domain|unique factorization domain]]. The following [[Definition:Element|elements]] of $\struct {\Z \sqbrk {i \sqrt 5}, +, \times}$ are [[Definition:Irreducible Element of Ring|irreducible]]: :$2$ :$3$ :$1 + i \sqrt 5$ :$1 - i \sqrt 5$	0
By [[Intersection of Ring Ideals is Ideal]] we have that $\ideal k = \ideal m \cap \ideal n$ is an [[Definition:Ideal of Ring|ideal]] of $\Z$. By [[Ring of Integers is Principal Ideal Domain]] we have that $\ideal m$, $\ideal n$ and $\ideal k$ are all necessarily [[Definition:Principal Ideal of Ring|principal ideals]]. By [[Subrings of Integers are Sets of Integer Multiples]] we have that: :$\ideal m = m \Z, \ideal n = n \Z$ Thus: :$\ideal k = \set {x \in \Z: n \divides x \land m \divides x}$ The result follows by [[LCM iff Divides All Common Multiples]]. {{qed}}	0
Let $a$ and $b$ be [[Definition:Integer|integers]] such that $b \ne 0$ and $a \ne 0$ (that is, they are both non-zero). Let $\gcd \set {a, b}$ denote the [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] of $a$ and $b$. Then $a$ and $b$ are '''coprime''' {{iff}} $\gcd \set {a, b} = 1$.	0
It is clear that the [[Definition:Digit|digits]] are instances of $9$ except for the first $1$. It is also noted that it has $3020 + 1 = 3021$ [[Definition:Digit|digits]], making it [[Definition:Titanic Prime|titanic]]. {{ProofWanted|It remains to be shown that it is prime.}}	0
{{ProofWanted}} [[Category:Cardinality of Cartesian Product]] fockcv0indjj1astgczmkn0g1jhxaqz	0
{{begin-eqn}} {{eqn | l = \int_0^1 \frac {\ln x} {1 - x} \rd x | r = \int_0^1 \ln x \paren {\sum_{n \mathop = 0}^\infty x^n} \rd x | c = [[Sum of Geometric Sequence]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\int_0^1 x^n \ln x \rd x} | c = [[Fubini's Theorem]] }} {{eqn | r = -\sum_{n \mathop = 0}^\infty \frac {\map \Gamma 2} {\paren {n + 1}^2} | c = [[Definite Integral from 0 to 1 of Power of x by Power of Logarithm of x|Definite Integral from $0$ to $1$ of $x^m \paren {\ln x}^n$]] }} {{eqn | r = -\sum_{n \mathop = 1}^\infty \frac 1 {n^2} | c = shifting the index, [[Gamma Function Extends Factorial]] }} {{eqn | r = -\frac {\pi^2} 6 | c = [[Basel Problem]] }} {{end-eqn}} {{qed}}	0
{{:Euclid:Proposition/VII/8}} In modern algebraic language: :$a = \dfrac m n b, c = \dfrac m n d \implies a - c = \dfrac m n \paren {b - d}$	0
:$\dfrac 1 {1089} = 0 \cdotp \dot 00091 \, 82736 \, 45546 \, 37281 \, 9 \dot 1$	0
Let $p: \C \to \C$ be a [[Definition:Complex Number|complex]], non-[[Definition:Constant Polynomial|constant]] [[Definition:Polynomial|polynomial]]. {{AimForCont}} that $\map p z \ne 0$ for all $z \in \C$. Now consider the [[Definition:Closed Complex Contour Integral|closed contour integral]]: :$\displaystyle \oint \limits_{\gamma_R} \frac 1 {z \cdot \map p z} \rd z$ where $\gamma_R$ is a [[Definition:Circle|circle]] with [[Definition:Radius of Circle|radius]] $R$ around the [[Definition:Origin|origin]]. By [[Derivative of Complex Polynomial]], the polynomial $z \cdot \map p z$ is [[Definition:Holomorphic Function|holomorphic]]. Since $\map p z$ is assumed to have no [[Definition:Root of Function|zeros]], the only zero of $z \cdot \map p z$ is $0 \in \C$. Therefore by [[Reciprocal of Holomorphic Function]] $\dfrac 1 {z \cdot \map p z}$ is [[Definition:Holomorphic Function|holomorphic]] in $\C \setminus \set 0$. Hence the [[Cauchy-Goursat Theorem]] implies that the value of this integral is independent of $R > 0$. On the one hand, one can calculate the value of this integral in the [[Definition:Limit of Function|limit]] $R \to 0$ (or use the [[Residue Theorem]]), using the [[Definition:Parameterization (Curve)|parameterization]] $z = R e^{i \phi}$ of $\gamma_R$: {{begin-eqn}} {{eqn | l = \lim \limits_{R \mathop \to 0} \oint \limits_{\gamma_R} \frac 1 {z \cdot \map p z} \rd z | r = \frac 1 {\map p 0} \lim \limits_{R \mathop \to 0} \int \limits_0^{2 \pi} \frac 1 {R e^{i \phi} } \, i R e^{i \phi} \rd \phi | c = [[Real Polynomial Function is Continuous]] and [[Product Rule for Limits of Functions|Product Rule]] }} {{eqn | r = \lim_{R \mathop \to 0} \frac 1 {\map p 0} \int_0^{2 \pi} i \rd \phi }} {{eqn | r = \frac {2 \pi i} {\map p 0} | c = }} {{end-eqn}} which is non-zero. On the other hand, we have the following [[Definition:Upper Bound of Mapping|upper bound]] for the [[Definition:Absolute Value|absolute value]] of the integral: {{begin-eqn}} {{eqn | l = \size {\oint \limits_{\gamma_R} \frac 1 {z \cdot \map p z} \rd z} | o = \le | r = 2 \pi R \max \limits_{\size z \mathop = R} \paren {\frac 1 {\size {z \cdot \map p z} } } | c = [[Estimation Lemma]] }} {{eqn | r = 2 \pi \max \limits_{\size z \mathop = R} \paren {\frac 1 {\size {\map p z} } } }} {{end-eqn}} But this goes to zero for $R \to \infty$. We arrive at a contradiction. Hence the assumption that $\map p z \ne 0$ for all $z \in \C$ must be wrong. {{qed}}	0
:$\displaystyle \int \frac {\d x} {x^3 + a^3} = \frac 1 {6 a^2} \ln \size {\frac {\paren {x + a}^2} {x^2 - a x + a^2} } + \frac 1 {a^2 \sqrt 3} \arctan \frac {2 x - a} {a \sqrt 3}$	0
From {{EuclidPropLink|title = Between two Similar Solid Numbers exist two Mean Proportionals|book = VIII|prop = 19}}: :$\left({c^3, c^2 d, c d^2, d^3}\right)$ is a [[Definition:Geometric Sequence of Integers|geometric sequence]]. From {{EuclidPropLink|title = Geometric Sequences in Proportion have Same Number of Elements|book = VIII|prop = 8}}: :$\left({a, m_1, m_2, b}\right)$ is a [[Definition:Geometric Sequence of Integers|geometric sequence]] for some $m$. We have that $a$ is a [[Definition:Cube Number|cube number]]. From {{EuclidPropLink|title = If First of Four Numbers in Geometric Sequence is Cube then Fourth is Cube|book = VIII|prop = 23}}: :$b$ is a [[Definition:Cube Number|cube number]]. {{qed}} {{Euclid Note|25|VIII}}	0
Let $T$ be the set of all [[Definition:Labeled Graph|labeled]] [[Definition:Tree (Graph Theory)|trees]] of [[Definition:Order (Graph Theory)|order $n$]]. Let $P$ be the set of all [[Definition:Prüfer Sequence|Prüfer sequence]] of length $n-2$. Let $\phi: T \to P$ be the [[Definition:Mapping|mapping]] that maps each tree to its Prüfer sequence. * From [[Prüfer Sequence from Labeled Tree]], $\phi$ is clearly [[Definition:Well-Defined Mapping|well-defined]], as every element of $T$ maps uniquely to an element of $P$. * However, from [[Labeled Tree from Prüfer Sequence]], $\phi^{-1}: P \to T$ is also clearly [[Definition:Well-Defined Mapping|well-defined]], as every element of $P$ maps to a unique element of $T$. Hence the result. {{questionable|How is it immediate that the two constructions are mutually inverse?}} {{qed}} [[Category:Tree Theory]] [[Category:Combinatorics]] 1nztp06dejnykcd7hdywz33czkukpjk	0
Let $\struct {S, \vee, \wedge}$ be a [[Definition:Boolean Algebra/Definition 1|Boolean algebra, defined as in Definition 1]]. Then for all $a, b \in S$: :$a = a \vee \paren {a \wedge b}$ :$a = a \wedge \paren {a \vee b}$ That is, $\vee$ [[Definition:Absorb|absorbs]] $\wedge$, and $\wedge$ [[Definition:Absorb|absorbs]] $\vee$.	0
Let $C_n$ be the [[Definition:Cyclic Group|cyclic group]] of [[Definition:Order of a Structure|order $n$]]. Then: :$C_n \cong \dfrac {\struct {\Z, +} } {\struct {n \Z, +} } = \dfrac \Z {n \Z}$ where: :$\Z$ is the [[Definition:Additive Group of Integers|additive group of integers]] :$n \Z$ is the [[Definition:Additive Group of Integer Multiples|additive group of integer multiples]] :$\Z / n \Z$ is the [[Definition:Quotient Group|quotient group]] of $\Z$ by $n \Z$. Thus, every [[Definition:Cyclic Group|cyclic group]] is [[Definition:Group Isomorphism|isomorphic]] to one of: :$\Z, \dfrac \Z \Z, \dfrac \Z {2 \Z}, \dfrac \Z {3 \Z}, \dfrac \Z {4 \Z}, \ldots$	0
This proof assumes the [[Definition:Exponential Function/Real/Limit of Sequence|limit definition of $\exp$]]. That is, let: :$\displaystyle \exp x = \lim_{n \mathop \to \infty} \map {f_n} x$ where $\map {f_n} x = \paren {1 + \dfrac x n}^n$ First, fix $x \in \R$. Let $N = \ceiling {\size x}$, where $\ceiling {\, \cdot \,}$ denotes the [[Definition:Ceiling Function|ceiling function]]. Then: {{begin-eqn}} {{eqn | l = \exp x | r = \lim_{n \mathop \to \infty} \map {f_n} x }} {{eqn | r = \lim_{n \mathop \to \infty} \map {f_{n + N} } x | c = [[Tail of Convergent Sequence]] }} {{eqn | o = \ge | r = \map {f_{n + N} } x | c = [[Exponential Sequence is Eventually Increasing]] and [[Limit of Bounded Convergent Sequence is Bounded]] }} {{eqn | o = > | r = 0 | c = [[Exponential Sequence is Eventually Increasing|Corollary to Exponential Sequence is Eventually Increasing]] }} {{end-eqn}} {{MissingLinks|[[Exponential Sequence is Eventually Increasing|Corollary to Exponential Sequence is Eventually Increasing]] does not actually exist. The page it gets sent to does not give that result.}} {{qed}}	0
:$\tan \dfrac \theta 2 = \csc \theta - \cot \theta$	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|p-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]]. Let $\struct {\Q_p, \norm {\,\cdot\,}_p}$ be the [[Definition:P-adic Numbers as Quotient of Cauchy Sequences|$p$-adic numbers as a quotient of Cauchy sequences]]. That is, $\Q_p$ is the [[Definition:Quotient Ring|quotient ring]] $\CC \, \big / \NN$ where: :$\CC$ denotes the [[Definition:Ring of Cauchy Sequences|commutative ring of Cauchy sequences]] over $\struct {\Q, \norm {\,\cdot\,}_p}$ :$\NN$ denotes the [[Definition:Set|set]] of [[Definition:Null Sequence|null sequences]] in $\struct {\Q, \norm {\,\cdot\,}_p}$. Let $\phi: \Q \to \Q_p$ be the [[Definition:Mapping|mapping]] defined by: :$\map \phi r = \sequence {r, r, r, \dotsc} + \NN$ where $\sequence {r, r, r, \dotsc} + \NN$ is the [[Definition:Left Coset|left coset]] in $\CC \, \big / \NN$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {r, r, r, \dotsc}$. Then: :$\Q$ is [[Definition:Isometric Isomorphism|isometrically isomorphic]] to $\map \phi \Q$ which is a [[Definition:Everywhere Dense|dense]] [[Definition:Subfield|subfield]] of $\Q_p$.	0
:$\displaystyle \sum_{k \mathop = 0}^n \binom m k \left({k - \dfrac m 2}\right) = -\dfrac m 2 \binom {m - 1} n$ where $\dbinom m k$ etc. are [[Definition:Binomial Coefficient|binomial coefficients]].	0
Let $z_1 = r_1 e^{i \theta_1}$ and $z_2 = r_2 e^{i \theta_2}$ be [[Definition:Exponential Form of Complex Number|complex numbers expressed in exponential form]]. Let $z_3 = r_3 e^{i \theta_3} = z_1 + z_2$. Then: :$r_3 = \sqrt {r_1^2 + r_2^2 + 2 r_1 r_2 \cos \left({\theta_1 - \theta_2}\right)}$ :$\theta_3 = \map \arctan {\dfrac {r_1 \sin \theta_1 + r_2 \sin \theta_2} {r_1 \cos \theta_1 + r_2 \cos \theta_2} }$	0
:$\displaystyle \int \frac {\operatorname{csch}^{-1} \dfrac x a \ \mathrm d x} x = \begin{cases} \displaystyle \frac {\ln \left({\dfrac x a}\right) \ln \left({\dfrac {4 a} x}\right)} 2 - \sum_{k \mathop \ge 0} \frac {\left({2 k + 1}\right)!} {2^{2 k} \left({k!}\right)^2 \left({2 k + 1}\right)^3 \left({2 k}\right)^2} \left({\frac x a}\right)^{2 k} + C & : 0 < x < a \\ \displaystyle \frac {\ln \left({\dfrac {-x} a}\right) \ln \left({\dfrac {-x} {4 a} }\right)} 2 + \sum_{k \mathop \ge 0} \frac {\left({2 k + 1}\right)!} {2^{2 k} \left({k!}\right)^2 \left({2 k + 1}\right)^3 \left({2 k}\right)^2} \left({\frac x a}\right)^{2 k} + C & : -a < x < 0 \\ \displaystyle \sum_{k \mathop \ge 0} \frac {\left({-1}\right)^{k + 1} \left({2 k + 1}\right)!} {2^{2 k} \left({k!}\right)^2 \left({2 k + 1}\right)^3} \left({\frac a x}\right)^{2 k + 1} + C & : \left\vert{x}\right\vert > a \\ \end{cases}$	0
The smallest [[Definition:Doubleton|pair]] of consecutive [[Definition:Even Integer|even]] [[Definition:Nontotient|nontotients]] is $74$ and $76$.	0
Let $AB$ and $BC$ be [[Definition:Medial Line Segment|medial straight lines]] which are [[Definition:Commensurable in Square Only|commensurable in square only]]. Let $AB$ and $BC$ [[Definition:Containment of Rectangle|contain]] a [[Definition:Rational Area|rational]] [[Definition:Rectangle|rectangle]]. By definition, $AB$ and $BC$ are [[Definition:Incommensurable in Length|incommensurable in length]]. We have: :$AB : BC = AB \cdot BC : BC^2$ :$AB : BC = AB^2 : AB \cdot BC$ Therefore from {{EuclidPropLink|book=X|prop=11|title=Commensurability of Elements of Proportional Magnitudes}}: : $AB \cdot BC$ is [[Definition:Incommensurable|incommensurable]] with $AB^2$ and : $AB \cdot BC$ is [[Definition:Incommensurable|incommensurable]] with $BC^2$. But by {{EuclidPropLink|book=X|prop=6|title=Magnitudes with Rational Ratio are Commensurable}}: :$2 AB \cdot BC$ is [[Definition:Commensurable|commensurable]] with $AB \cdot BC$. We have that $AB$ and $BC$ are [[Definition:Commensurable in Square|commensurable in square]]. So from {{EuclidPropLink|book=X|prop=15|title=Commensurability of Sum of Commensurable Magnitudes}}: :$AB^2 + BC^2$ is [[Definition:Commensurable|commensurable]] with $BC^2$. So from {{EuclidPropLink|book=X|prop=13|title=Commensurable Magnitudes are Incommensurable with Same Magnitude}}: :$2 AB \cdot BC$ is [[Definition:Incommensurable|incommensurable]] with $AB^2 + BC^2$. Thus from {{EuclidPropLink|book=X|prop=16|title=Incommensurability of Sum of Incommensurable Magnitudes}}: :$2 AB \cdot BC + AB^2 + BC^2$ is [[Definition:Incommensurable|incommensurable]] with $AB \cdot BC$. We have that $AB$ and $BC$ [[Definition:Containment of Rectangle|contain]] a [[Definition:Rational Area|rational]] [[Definition:Rectangle|rectangle]]. Thus $AB \cdot BC$ is [[Definition:Rational Area|rational]]. From {{EuclidPropLink|book=II|prop=4|title=Square of Sum}}: :$AC^2 = \left({AB + BC}\right)^2 = 2 AB \cdot BC + AB^2 + BC^2$ Thus from {{EuclidDefLink|X|4|Rational Area}}: :$AC$ is [[Definition:Irrational Area|irrational]]. Such a [[Definition:Line Segment|straight line]] is called '''[[Definition:First Bimedial|first bimedial]]'''. {{qed}} {{Euclid Note|37|X}}	0
From [[Exponential of Sum]] we have: :$\forall x, y \in \R: \map \exp {x + y} = \exp x \cdot \exp y$ That is, $\exp$ is a [[Definition:Group Homomorphism|group homomorphism]]. Then we have that [[Exponential is Strictly Increasing]]. From [[Strictly Monotone Real Function is Bijective]], it follows that $\exp$ is a [[Definition:Bijection|bijection]]. So $\exp$ is a [[Definition:Bijection|bijective]] [[Definition:Group Homomorphism|group homomorphism]], and so a [[Definition:Group Isomorphism|group isomorphism]]. {{qed}}	0
Put $u = a x + b$. Then: {{begin-eqn}} {{eqn | l = x | r = \frac {u - b} a | c = }} {{eqn | l = \frac {\d u} {\d x} | r = \frac 1 a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x^2 \rd x} {\paren {a x + b}^3} | r = \int \frac 1 a \paren {\frac {u - b} a}^2 \frac 1 {u^2} \rd u | c = [[Integration by Substitution]] }} {{eqn | r = \int \frac 1 {a^3} \paren {\frac 1 u - \frac {2 b} {u^2} + \frac {b^2} {u^3} } \rd u | c = [[Square of Difference]], and simplification }} {{eqn | r = \frac 1 {a^3} \int \frac {\d u} u - \frac {2 b} {a^3} \int \frac {\d u} {u^2} + \frac {b^2} {a^3} \int \frac {\d u} {u^3} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {a^3} \ln \size u - \frac {2 b} {a^3} \int \frac {\d u} {u^2} + \frac {b^2} {a^3} \int \frac {\d u} {u^3} + C | c = [[Primitive of Reciprocal]] }} {{eqn | r = \frac 1 {a^3} \ln \size u - \frac {2 b} {a^3} \frac {-1} u + \frac {b^2} {a^3} \frac {-1} {2 u^2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {2 b} {a^3 \paren {a x + b} } - \frac {b^2} {2 a^3 \paren {a x + b}^2} + \frac 1 {a^3} \ln \size {a x + b} + C | c = substituting for $u$ and rearranging }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = A^{\circ -} | r = A^- | c = [[Interior of Union of Adjacent Open Intervals]]: $A^\circ = A$ }} {{eqn | r = \closedint a c | c = [[Closure of Union of Adjacent Open Intervals]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = A^{- \circ -} | r = \openint a c^- | c = [[Interior of Closure of Interior of Union of Adjacent Open Intervals]] }} {{eqn | r = \closedint a c | c = [[Closure of Open Ball in Metric Space]] }} {{end-eqn}} Hence the result. {{qed}}	0
Let $P$ be a [[Definition:Propositional Function|propositional function]] on the [[Definition:Natural Numbers|natural numbers]] $\N$. Suppose that: :$(1): \quad \forall n \in \N: \map P {2^n}$ holds. :$(2): \quad \map P n \implies \map P {n - 1}$. Then $\map P n$ holds for all $\forall n \in \N$. The proof technique based on this result is called '''backwards induction'''.	0
:$\map {\dfrac \d {\d x} } {\ln u} = \dfrac 1 u \dfrac {\d u} {\d x}$	0
Let $w_n$ denote an arbitrary [[Definition:Word (Abstract Algebra)|word]] of $n$ [[Definition:Element|elements]]. The number of [[Definition:Distinct|distinct]] [[Definition:Parenthesization|parenthesizations]] of $w_n$ is the [[Definition:Catalan Number|Catalan number]] $C_{n - 1}$: :$C_{n - 1} = \dfrac 1 n \dbinom {2 \paren {n - 1} } {n - 1}$	0
{{begin-eqn}} {{eqn | l = i \csc x | r = \frac i {\sin z} | c = {{Defof|Complex Cosecant Function}} }} {{eqn | r = \frac 1 {-i \sin z} | c = $i^2 = -1$ }} {{eqn | r = \frac 1 {-\sinh \paren {i z} } | c = [[Sine in terms of Hyperbolic Sine]] }} {{eqn | r = -\csch \paren {i z} | c = {{Defof|Hyperbolic Cosecant|index = 2}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \tan {2 \theta} | r = \frac {\map \sin {2 \theta} } {\map \cos {2 \theta} } | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac {2 \cos \theta \sin \theta} {\cos^2 \theta - \sin^2 \theta} | c = [[Double Angle Formula for Sine]] and [[Double Angle Formula for Cosine]] }} {{eqn | r = \frac {\frac {2 \cos \theta \sin \theta} {\cos^2 \theta} } {\frac {\cos^2 \theta - \sin^2 \theta} {\cos^2 \theta} } | c = dividing top and bottom by $\cos^2 \theta$ }} {{eqn | r = \frac {2 \tan \theta} {1 - \tan^2 \theta} | c = Simplifying: [[Tangent is Sine divided by Cosine]] }} {{end-eqn}} {{qed}}	0
The number $2^{58} + 1$ has the [[Definition:Prime Decomposition|prime decomposition]]: :$2^{58} + 1 = 5 \times 107 \, 367 \, 629 \times 536 \, 903 \, 681$	0
$210$ is the largest [[Definition:Integer|integer]] which can be represented as the [[Definition:Integer Addition|sum]] of two [[Definition:Prime Number|primes]] in the maximum number of ways. The full list of such numbers is as follows: :$1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 18, 24, 30, 36, 42, 48, 60, 90, 210$ {{OEIS|A141340}} The list contains: :$n \le 8$ :$n \le 18$ where $2 \divides n$ :$n \le 48$ where $2 \times 3 \divides n$ :$n \le 90$ where $2 \times 3 \times 5 \divides n$ :$210 = 2 \times 3 \times 5 \times 7$ {{WIP|Add this sequence to the number pages}}	0
Let $V$ be a [[Definition:Vector Space|vector space]]. Let $S$ be a [[Definition:Finite|finite]] [[Definition:Subset|subset]] of $V$. Let $\struct{S, \mathscr I}$ be the [[Definition:Matroid Induced by Linear Independence (Vector Space)|matroid induced on $S$ by linear independence in $V$]]. That is, $\mathscr I$ is the [[Definition:Set|set]] of [[Definition:Linearly Independent|linearly independent]] [[Definition:Subset|subsets]] of $S$. Then $\struct{S, \mathscr I}$ is a [[Definition:Matroid|matroid]].	0
Let $a, b, c \in \Z_{>0}$ be [[Definition:Strictly Positive Integer|strictly positive integers]]. Then: :$\lcm \set {a, b, c} = \dfrac {a b c \gcd \set {a, b, c} } {d_1 d_2 d_3}$ where: :$\gcd$ denotes [[Definition:Greatest Common Divisor of Integers|greatest common divisor]] :$\lcm$ denotes [[Definition:Lowest Common Multiple of Integers|lowest common multiple]] :$d_1 = \gcd \set {a, b}$ :$d_2 = \gcd \set {b, c}$ :$d_3 = \gcd \set {a, c}$	0
We have that the [[Definition:Real Exponential Function|exponential function]] is the [[Definition:Inverse Mapping|inverse]] of the [[Definition:Natural Logarithm|natural logarithm function]]: : $\ln e = 1$ Hence the result. {{qed}}	0
By [[Gauss's Lemma on Irreducible Polynomials]], it suffices to show that $f$ is [[Definition:Irreducible Polynomial|irreducible]] in $\Z \sqbrk x$. {{AimForCont}} that $f = g h$ where $g, h \in \Z \sqbrk x$ are both [[Definition:Constant Polynomial|non-constant]]. Let: :$\map g x = b_e x^e + b_{e - 1} x^{e - 1} + \dotsb + b_0$ :$\map h x = c_f x^f + c_{f - 1} x^{f - 1} + \dotsb + c_0$ Then we have for each $i$: :$\displaystyle a_i = \sum_{j + k \mathop = i} {b_j c_k}$ In particular, it follows that: :$a_0 = b_0 c_0$ Possibly after exchanging $g$ and $h$, we may arrange that: :$p \nmid c_0$ by condition $(2)$. From condition $(1)$, it follows that then necessarily: :$p \divides b_0$ We also have: :$a_d = b_e c_f$ and by condition $(1)$: :$p \nmid a_d$ and hence: :$p \nmid b_e$ It follows that there exists a smallest positive $i$ such that: :$p \nmid b_i$ Naturally, $i \le e$. By assumption, both $g$ and $h$ are [[Definition:Constant Polynomial|non-constant]]. Hence by [[Degree of Product of Polynomials over Integral Domain]]: :$i < d$ {{explain|Re-evaluate the above link - may need to be [[Degree of Product of Polynomials over Integral Domain not Less than Degree of Factors]]. Clarification needed.}} Consider: :$a_i = b_0 c_i + b_1 c_{i - 1} + \dotsb + b_i c_0$ with the convention that $c_j = 0$ if $j > f$. By the minimality of $i$, it follows that: :$p \divides b_k$ for $0 \le k < i$. Also, since neither $c_0$ nor $b_i$ is [[Definition:Divisor of Integer|divisible]] by $p$, the last term $b_i c_0$ is '''not''' [[Definition:Divisor of Integer|divisible]] by $p$. Thus, we conclude that: :$p \nmid a_i$ which contradicts condition $(1)$. Therefore, $f$ is [[Definition:Irreducible Polynomial|irreducible]]. {{qed}}	0
Let $a, b \in \Z_{>0}$ be [[Definition:Strictly Positive Integer|(strictly) positive integers]]. Let $c$ and $d$ be the number of [[Definition:Digit|digits]] in $a$ and $b$ respectively when expressed in [[Definition:Decimal Notation|decimal notation]]. Let the [[Euclidean Algorithm]] be employed to find the [[Definition:GCD of Integers|GCD]] of $a$ and $b$. Then it will take no more than $5 \times \min \set {c, d}$ cycles around the [[Euclidean Algorithm]] to find $\gcd \set {a, b}$.	0
Let $\RR$ be a [[Definition:One-to-One Relation|one-to-one relation]]. Let $\RR^{-1}$ denote its [[Definition:Inverse Relation|inverse]] By definition, $\RR$ is a [[Definition:Relation|relation]] which is both [[Definition:Many-to-One Relation|many-to-one]] and [[Definition:One-to-Many Relation|one-to-many]]. From [[Inverse of Many-to-One Relation is One-to-Many]]: :$\RR^{-1}$ is both [[Definition:One-to-Many Relation|one-to-many]] and [[Definition:Many-to-One Relation|many-to-one]]. Hence the result by definition of [[Definition:One-to-One Relation|one-to-one relation]]. {{qed}} [[Category:Inverse Relations]] 38i2x22s1p8077snqedjfr59djnaphc	0
{{TFAE|def = Complex Inverse Hyperbolic Tangent}} Let $S$ be the [[Definition:Subset|subset]] of the [[Definition:Complex Plane|complex plane]]: :$S = \C \setminus \left\{{-1 + 0 i, 1 + 0 i}\right\}$	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] which is [[Definition:Irreducible Space|irreducible]]. Then $T$ is [[Definition:Locally Connected Space|locally connected]].	0
Let $Q$ be the [[Definition:Mapping|mapping]] defined as: :$\forall H \le \mathbb H_1: Q \left({H}\right) = \left\{{\phi \left({h}\right): h \in H}\right\}$ Let $H$ be a [[Definition:Subgroup|subgroup]] of $G_1$ such that $K \subseteq H$. From [[Group Homomorphism Preserves Subgroups]], $\phi \left({H}\right)$ is a [[Definition:Subgroup|subgroup]] of $G_2$. This establishes that $Q$ is actually a [[Definition:Mapping|mapping]]. Let $N \lhd G_1$. From [[Group Epimorphism Preserves Normal Subgroups]], $\phi \left({N}\right)$ is a [[Definition:Normal Subgroup|normal subgroup]] of $G_2$. This establishes that: :$\forall N \lhd G_1: Q \left({N}\right) \lhd G_2$ Next it is shown that $Q$ is a [[Definition:Bijection|bijection]]. === Injective Nature of $Q$ === Let $H, J \in \mathbb H_1$. Let $Q \left({H}\right) = Q \left({J}\right)$. Let $h \in H$. {{begin-eqn}} {{eqn | l=\phi \left({h}\right) | o=\in | r=Q \left({H}\right) | c= }} {{eqn | ll=\implies | l=\phi \left({h}\right) | o=\in | r=Q \left({J}\right) | c= }} {{eqn | ll=\implies | lo=\exists j \in J: | l=\phi \left({j}\right) | r=\phi \left({h}\right) | c= }} {{eqn | ll=\implies | l=e_{G_2} | r=\left({\phi \left({j}\right)}\right)^{-1} \phi \left({h}\right) | c=definition of [[Definition:Inverse Element|inverse element]] }} {{eqn | r=\phi \left({j^{-1} }\right) \phi \left({h}\right) | c=[[Group Homomorphism Preserves Inverses]] }} {{eqn | r=\phi \left({j^{-1} h}\right) | c=[[Definition:Morphism Property|morphism property]] of $\phi$ }} {{eqn | ll=\implies | l=j^{-1} h | o=\in | r=K | c=definition of [[Definition:Kernel of Group Homomorphism|kernel]] }} {{eqn | ll=\implies | lo=\exists k \in K: | l=j^{-1} h | r=k | c= }} {{eqn | ll=\implies | l=h | r=j k | c= }} {{eqn | o=\in | r=J | c=as $K \subseteq J$ and so $k \in j$ }} {{eqn | ll=\implies | l=H | o=\subseteq | r=J | c= }} {{end-eqn}} A similar argument shows that $J \subseteq H$. So by definition of [[Definition:Set Equality/Definition 2|set equality]]: : $H = J$ Thus: : $Q \left({H}\right) = Q \left({J}\right) \implies H = J$ So by definition, $Q$ is [[Definition:Injection|injective]]. {{qed|lemma}} === Surjective Nature of $Q$ === Now let $N' \in \mathbb H_2$. By definition of $\mathbb H_2$, $N'$ is a [[Definition:Subgroup|subgroup]] of $G_2$. Let $N = \left\{{x: \phi \left({x}\right) = N'}\right\}$. We have from [[Identity of Subgroup]] that $e_{G_2} \in N'$. Thus by definition of [[Definition:Kernel of Group Homomorphism|kernel]], $K \subseteq N$. Now suppose $\phi \left({x}\right), \phi \left({y}\right) \in N'$. Then: {{begin-eqn}} {{eqn | l=\phi \left({x y^{-1} }\right) | r=\phi \left({x}\right) \phi \left({y^{-1} }\right) | c= }} {{eqn | r=\phi \left({x}\right) \phi \left({y}\right)^{-1} | c= }} {{eqn | o=\in | r=N' | c=[[One-Step Subgroup Test]] }} {{eqn | ll=\implies | l=x y^{-1} | o=\in | r=N }} {{end-eqn}} So by the [[One-Step Subgroup Test]], $N$ is a [[Definition:Subgroup|subgroup]] of $G_1$. It has been established that $K \subseteq N$, and so $N \in \mathbb H_1$. Thus it follows that for all $N' \in \mathbb H_2$ there exists $N \in H_1$ such that $Q \left({N}\right) = N'$. So $Q$ is a [[Definition:Surjection|surjection]]. {{qed|lemma}} So $Q$ has been shown to be both an [[Definition:Injection|injection]] and a [[Definition:Surjection|surjection]], and so by definition is a [[Definition:Bijection|bijection]]. Finally, it can then be shown that if $N'$ is [[Definition:Normal Subgroup|normal]] in $G_2$, it follows that $N = Q^{-1} \left({N'}\right)$ is [[Definition:Normal Subgroup|normal]] in $G_1$. This establishes that: :$\forall N \lhd G_2: Q^{-1} \left({N}\right) \lhd G_1$ {{finish|if $N'$ is [[Definition:Normal Subgroup|normal]] in $G_2$, it follows that $N {{=}} Q^{-1} \left({N'}\right)$ is [[Definition:Normal Subgroup|normal]] in $G_1$.}} {{qed}}	0
:$13^3 = 2197$	0
The number of elements in $\conjclass x$ is the number of [[Definition:Conjugate of Group Subset|conjugates]] of the set $\set x$. From [[Number of Distinct Conjugate Subsets is Index of Normalizer]], the number of distinct [[Definition:Subset|subsets]] of a $G$ which are [[Definition:Conjugate of Group Subset|conjugates]] of $S \subseteq G$ is $\index G {\map {N_G} S}$. The result follows. {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Scattered Space|scattered topological space]] which is also a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]]. Then $T$ is [[Definition:Totally Disconnected Space|totally disconnected]].	0
By definition, a [[Definition:Transcendental Number|transcendental number]] (in this context) is a [[Definition:Real Number|real number]] which is not an [[Definition:Algebraic Number|algebraic number]]. Recall that the [[Real Numbers are Uncountable]]. Also recall that the [[Algebraic Numbers are Countable]]. The result follows from [[Uncountable Set less Countable Set is Uncountable]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = 1 | r = \cos^2 x + \sin^2 x | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | ll= \leadsto | l = \map {D_x} 1 | r = \map {D_x} {\cos^2 x + \sin^2 x} | c = differentiating both sides }} {{eqn | ll= \leadsto | l = 0 | r = 2 \map {D_x} {\cos x} \cos x + 2 \sin x \cos x | c = [[Product Rule for Derivatives]] or [[Chain Rule for Derivatives]] and [[Derivative of Sine Function]] }} {{eqn | ll= \leadsto | l = \map {D_x} {\cos x} | r = -\sin x | c = provided $\cos x \ne 0$ }} {{end-eqn}} {{qed}}	0
Everywhere that the function is defined: :$\map {\tanh^{-1} } {\dfrac 1 x} = \coth^{-1} x$ where $\tanh^{-1}$ and $\coth^{-1}$ denote [[Definition:Inverse Hyperbolic Tangent|inverse hyperbolic tangent]] and [[Definition:Inverse Hyperbolic Cotangent|inverse hyperbolic cotangent]] respectively.	0
Let $G_n = \sequence {a_0, a_1, \ldots, a_n}$ be [[Definition:Natural Number|natural numbers]] in [[Definition:Geometric Sequence|geometric sequence]] such that $a_0 \perp a_n$. {{AimForCont}} $G\,'_n = \sequence {b_0, b_1, \cdots, b_n}$ be another set of [[Definition:Natural Number|natural numbers]] in [[Definition:Geometric Sequence|geometric sequence]] with the same [[Definition:Common Ratio of Geometric Sequence|common ratio]] where: :$\forall k \in \N_{\le n}: a_k > b_k$ By definition of [[Definition:Geometric Sequence|geometric sequence]]: :$a_0 = r^n a_n$ :$b_0 = r^n b_n$ Hence: :$\dfrac {a_0} {a_n} = \dfrac {b_0} {b_n}$ [[Definition:By Hypothesis|By hypothesis]]: : $a_0 \perp a_n$ Thus $\dfrac {a_0} {a_n}$ is in [[Definition:Canonical Form of Rational Number|canonical form]]. From [[Canonical Form of Rational Number is Unique]] it follows that $a_0$ and $a_n$ are the only two [[Definition:Integer|integers]] fulfilling these conditions. Thus: :$\forall p, q: \dfrac p q = \dfrac {a_0} {a_n}: a_0 \le p, a_n \le q$ But it was supposed that: :$b_0 < a_0$ :$b_n < a_n$ From this [[Proof by Contradiction|contradiction]] it follows that there can be no such $b_0, \ldots, b_n$. Hence the result. {{qed}}	0
{{Cross-Relation Context Definition}} Then $\boxtimes$ is an [[Definition:Equivalence Relation|equivalence relation]] on $\struct {S_1 \times S_2, \oplus}$.	0
{{begin-eqn}} {{eqn | l = \int \cot^2 x \rd x | r = \int \paren {\csc^2 x - 1} \rd x | c = [[Difference of Squares of Cosecant and Cotangent]] }} {{eqn | r = \int \csc^2 x \rd x + \int \paren {-1} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = -\cot x + C + \int \paren {-1} \rd x | c = [[Primitive of Square of Cosecant Function]] }} {{eqn | r = -\cot x - x + C | c = [[Primitive of Constant]] }} {{end-eqn}} {{Qed}}	0
We will prove the result by [[Principle of Mathematical Induction|induction]] on the [[Definition:Cardinality|number]] of [[Definition:Operand|operands]] $n$. For all $n \in \N_{>0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\exists i \in \closedint 1 n : x_i = \max \set {x_1, x_2, \dotsc, x_n}$ === Basis for the Induction === $\map P 1$ is the case: :$\exists i \in \closedint 1 1 : x_i = \max \set {x_1}$ By definition of the [[Definition:Max Operation|max operation]]: :$\max \set {x_1} = x_1$ Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\exists i \in \closedint 1 k : x_i = \max \set {x_1, x_2, \dotsc, x_k}$ from which it is to be shown that: :$\exists i \in \closedint 1 {k + 1} : x_i = \max \set {x_1, x_2, \dotsc, x_k, x_{k + 1} }$ === Induction Step === This is the [[Definition:Induction Step|induction step]]. By the definition of [[Definition: Max Operation (General Case)|Max Operation]]: :$\max \set {x_1, x_2, \dotsc, x_k, x_{k + 1} } = \max \set {\max \set {x_1, x_2, \dotsc, x_k}, x_{k + 1} }$ By the [[Max yields Supremum of Parameters/General Case#Induction Hypothesis|Induction hypothesis]]: :$\exists i \in \closedint 1 k : x_i = \max \set {x_1, x_2, \dotsc, x_k}$ So: :$\max \set {x_1, x_2, \dotsc, x_k, x_{k + 1} } = \max \set {x_i, x_{k + 1} }$ As $\struct {S, \preceq}$ is a [[Definition:Totally Ordered Set|totally ordered set]], all elements of $S$ are [[Definition:Comparable|comparable]] by $\preceq$. Therefore there are two cases to consider: ==== Case 1: $x_{k+1} \preceq x_i$ ==== By definition of the [[Definition:Max Operation|max operation]]: :$\max \set {x_i, x_{k + 1} } = x_i$ {{qed|lemma}} ==== Case 2: $x_i \preceq x_{k + 1}$ ==== By definition of the [[Definition:Max Operation|max operation]]: :$\max \set {x_i, x_{k + 1} } = x_{k + 1}$ {{qed|lemma}} In either case, the result holds. So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\exists i \in \closedint 1 n : x_i = \max \set {x_1, x_2, \dotsc, x_n}$ {{qed}} [[Category:Max and Min Operations]] jsficvqcwn813hmtxanb6d7e6u4q5n0	0
We use [[Principle of Mathematical Induction|mathematical induction]]. The [[Definition:Basis for the Induction|base case]] $n = 2$ is proved in [[Kuratowski Formalization of Ordered Pair]], and the [[Definition:Induction Step|induction step]] follows directly from the definition of an [[Definition:Ordered Tuple as Ordered Set|ordered tuple]]. {{qed}} {{finish}} [[Category:Cartesian Product]] 5aqtynhmlw7t2z7lkrvk4nv37eqx2c9	0
{{begin-eqn}} {{eqn | l = \int \frac {\d x} {\sin a x + \cos a x} | r = \frac 1 a \int \frac {\dfrac {2 \rd u} {1 + u^2} } {\dfrac {2 u} {1 + u^2} + \dfrac {1 - u^2} {1 + u^2} } | c = [[Weierstrass Substitution]]: $u = \tan \dfrac {a x} 2$ }} {{eqn | r = \frac 2 a \int \frac {\d u} {- u^2 + 2 u + 1} | c = simplifying }} {{eqn | r = \frac 2 a \paren {\frac 1 {\sqrt 8} \ln \size {\frac {-2 u + 2 - \sqrt 8} {-2 u + 2 + \sqrt 8} } } + C | c = [[Primitive of Reciprocal of a x squared plus b x plus c|Primitive of $\dfrac 1 {a x^2 + b x + c}$]] }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {u - 1 + \sqrt 2} {u - 1 - \sqrt 2} } + C | c = simplifying }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {\tan \dfrac {a x} 2 - \paren {1 - \sqrt 2} } {\tan \dfrac {a x} 2 - \paren {1 + \sqrt 2} } } + C | c = substituting for $u$ }} {{eqn | r = \frac 1 {a \sqrt 2} \ln \size {\frac {\tan \dfrac {a x} 2 - \tan \dfrac \pi 8} {\tan \dfrac {a x} 2 - \tan \dfrac {3 \pi} 8} } + C | c = [[Tangent of 22.5 Degrees|Tangent of $\dfrac \pi 8$]] and [[Tangent of 67.5 Degrees|Tangent of $\dfrac {3 \pi} 8$]] }} {{end-eqn}}	0
By definition, a [[Definition:Composition Series|composition series]] is a [[Definition:Normal Series|normal series]] whose [[Definition:Factor of Normal Series|factor groups]] are all [[Definition:Simple Group|simple]]. A [[Definition:Solvable Group|solvable group]], by definition, is one which has a [[Definition:Composition Series|composition series]] whose [[Definition:Factor of Normal Series|factor groups]] are all [[Definition:Cyclic Group|cyclic]]. From [[Cyclic Group is Simple iff Prime]], it follows that all the [[Definition:Factor of Normal Series|factor groups]] of a [[Definition:Composition Series|composition series]] of a [[Definition:Solvable Group|solvable group]] must all be [[Definition:Prime Group|prime]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \frac \d {\d x} \size x | r = \frac \d {\d x} \sqrt{x^2} | c = [[Square of Real Number is Non-Negative]] }} {{eqn | r = \frac \d {\d x} \paren {x^2}^{\frac 1 2} }} {{eqn | r = \frac 1 2 \paren {x^2}^{-\frac 1 2} \cdot 2 x | c = [[Chain Rule for Derivatives]] }} {{eqn | r = \frac x {\sqrt{x^2} } }} {{eqn | r = \frac x {\size x} }} {{end-eqn}} {{qed|lemma}} Now consider $x = 0$. From the definition of [[Definition:Derivative of Real Function at Point|derivative]]: {{begin-eqn}} {{eqn | l = \valueat {\dfrac {\d \size x} {\d x} } {x \mathop = 0} | r = \lim_{x \mathop \to 0}\frac {\size x - 0} {x - 0} }} {{eqn | r = \begin {cases} \lim_{x \mathop \to 0^+} \dfrac x x & : x > 0 \\ \lim_{x \mathop \to 0^-} \dfrac {-x} x & : x < 0 \end {cases} | c = {{Defof|Absolute Value}} }} {{eqn | r = \begin {cases} 1 & : x > 0 \\ -1 & : x < 0 \end{cases} }} {{end-eqn}} From [[Limit iff Limits from Left and Right]], the limit does not exist. {{qed}} [[Category:Derivatives]] [[Category:Absolute Value Function]] kwp3633hffzgpfq9nnhi7ctylso5m5w	0
By [[Form of Geometric Sequence of Integers from One]], the general [[Definition:Term of Geometric Sequence|term]] of $G_n$ can be expressed as: :$a_j = q^j$ for some $q \in \Z$. Let $k, m \in \set {1, 2, \ldots, n}$ such that $k \divides m$. By definition of [[Definition:Divisor of Integer|divisibility]]: :$\exists r \in \Z: m = r k$ Then: {{begin-eqn}} {{eqn | l = a_m | r = q^m | c = }} {{eqn | r = q^{r k} | c = }} {{eqn | r = \paren {q^r}^k | c = }} {{end-eqn}} That is, $a_m$ is a [[Definition:Integer Power|power]] of $k$. {{qed}} {{Euclid Note|8|IX}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Finite Complement Topology|finite complement topology]] on an [[Definition:Infinite Set|infinite]] set $S$. Then $T$ is not a [[Definition:T3 Space|$T_3$ space]], [[Definition:T4 Space|$T_4$ space]] or [[Definition:T5 Space|$T_5$ space]].	0
Let $m, n \in \Z$. Let $m \Z$ denote the [[Definition:Set of Integer Multiples|set of integer multiples of $m$]] Let $r \in \Z$ such that: :$m \Z \subseteq r \Z$ and: :$n \Z \subseteq r \Z$ Then: :$\gcd \set {m, n} \Z \subseteq r \Z$ where $\gcd$ denotes [[Definition:Greatest Common Divisor of Integers|greatest common divisor]].	0
From [[Group Homomorphism of Product with Inverse]], we have: :$\forall x, y \in G: \map \phi {x \circ y^{-1} } = \map \phi x * \paren {\map \phi y}^{-1}$ Putting $x = e_G$ and $y = x$ we have: {{begin-eqn}} {{eqn | l = \map \phi {x^{-1} } | r = \map \phi {e_G \circ x^{-1} } | c = }} {{eqn | r = \map \phi {e_G} * \paren {\map \phi x}^{-1} | c = }} {{eqn | r = e_H * \paren {\map \phi x}^{-1} | c = [[Group Homomorphism Preserves Identity]] }} {{eqn | r = \paren {\map \phi x}^{-1} | c = }} {{end-eqn}} {{qed}}	0
We have: {{begin-eqn}} {{eqn | n = 1 | l = \cos i + i \sin i | r = e^{i \times i} | c = [[Euler's Formula]] }} {{eqn | r = e^{-1} | c = Definition of [[Definition:Complex Number/Definition 1|Imaginary Unit]] }} {{eqn | r = \frac 1 e }} {{end-eqn}} Also: {{begin-eqn}} {{eqn | n = 2 | l = \cos i - i \sin i | r = \cos \left({-i}\right) + i \sin \left({-i}\right) | c = [[Cosine Function is Even]] and [[Sine Function is Odd]] }} {{eqn | r = e^{i \times \left({-i}\right)} | c = [[Euler's Formula]] }} {{eqn | r = e^1 | c = Definition of [[Definition:Complex Number/Definition 1|Imaginary Unit]] }} {{eqn | r = e }} {{end-eqn}} Then from $(1) + (2)$: {{begin-eqn}} {{eqn | l = 2 \cos i | r = \frac 1 e + e }} {{eqn | ll= \implies | l = \cos i | r = \frac 1 2 \left({\frac 1 e + e}\right) }} {{eqn | r = \frac e 2 + \frac 1 {2 e} }} {{end-eqn}} {{qed}}	0
Let $\mu$ be a [[Definition:Real Number|real number]]. Let $\sigma$ be a [[Definition:Positive Real Number|positive real number]]. Let $X \sim \Gaussian \mu {\sigma^2}$ where $\Gaussian \mu {\sigma^2}$ is the [[Definition:Gaussian Distribution|Gaussian distribution]] with parameters $\mu$ and $\sigma^2$. Then: :$\dfrac {X - \mu} \sigma \sim \Gaussian 0 1$ where $\Gaussian 0 1$ is the [[Definition:Standard Gaussian Distribution|standard Gaussian distribution]].	0
Let $A$ be [[Definition:Transitive Class|transitive]]. By [[Class is Transitive iff Union is Subset]]: :$\displaystyle \bigcup A \subseteq A$ By [[Union of Subclass is Subset of Union of Class]]: :$\displaystyle \map \bigcup {\bigcup A} \subseteq \bigcup A$ Then by [[Class is Transitive iff Union is Subset]]: :$\bigcup A$ is [[Definition:Transitive Class|transitive]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = y | r = \arctan x | c = }} {{eqn | ll= \leadsto | l = x | r = \tan y | c = {{Defof|Real Arctangent}} }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d y} | r = \sec^2 y | c = [[Derivative of Tangent Function]] }} {{eqn | r = 1 + \tan^2 y | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = 1 + x^2 | c = Definition of $x$ }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \frac 1 {1 + x^2} | c = [[Derivative of Inverse Function]] }} {{end-eqn}} {{qed}}	0
The following definitions of a [[Definition:Balanced Prime|balanced prime]] are [[Definition:Logical Equivalence|equivalent]]:	0
{{begin-eqn}} {{eqn | l = \dfrac 1 {\paren {1 - z}^{n + 1} } | r = \paren {1 + \paren {-z} }^{- n - 1} | c = }} {{eqn | r = \sum_{k \mathop \ge 0} \binom {- n - 1} k \paren {-z}^k | c = [[General Binomial Theorem]] }} {{eqn | r = \sum_{k \mathop \ge 0} \dbinom {n + 1 + k - 1} k \paren {-1}^k \paren {-z}^k | c = [[Negated Upper Index of Binomial Coefficient/Corollary 1|Negated Upper Index of Binomial Coefficient]] }} {{eqn | r = \sum_{k \mathop \ge 0} \dbinom {n + k} k z^k | c = }} {{end-eqn}} {{qed}}	0
Let $\struct {\R^2, \tau_d}$ be the [[Definition:Real Number Plane with Euclidean Topology|real number plane with the usual (Euclidean) topology]]. Let $A \subseteq R^2$ be the [[Definition:Set|set]] of all points defined as: :$A := \set {\tuple {x, y} \in \R^2: x y \ge 1}$ Then $A$ is a [[Definition:Closed Set (Topology)|closed set]] in $\struct {\R^2, d}$.	0
{{begin-eqn}} {{eqn | l = \int_0^1 \frac {\ln x} {1 + x} \rd x | r = \int_0^1 \frac {\ln x} {1 - \paren {-x} } \rd x }} {{eqn | r = \int_0^1 \ln x \paren {\sum_{n \mathop = 0}^\infty \paren {-x}^n} \rd x | c = [[Sum of Geometric Sequence]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \int_0^1 x^n \ln x \rd x | c = [[Fubini's Theorem]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n + 1} \map \Gamma 2} {\paren {n + 1}^2} | c = [[Definite Integral from 0 to 1 of Power of x by Power of Logarithm of x|Definite Integral from $0$ to $1$ of $x^m \paren {\ln x}^n$]] }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\paren {-1}^n 1!} {n^2} | c = [[Gamma Function Extends Factorial]], shifting the index }} {{eqn | r = -\sum_{n \mathop = 1}^\infty \frac {\paren {-1}^{n + 1} } {n^2} | c = writing $-\paren {-1}^{n + 1} = \paren {-1}^{n + 2} = \paren {-1}^2 \paren {-1}^n = \paren {-1}^n$ }} {{eqn | r = -\frac {\pi^2} {12} | c = [[Sum of Reciprocals of Squares Alternating in Sign]] }} {{end-eqn}} {{qed}}	0
The result follows from [[Ordinals under Multiplication form Monoid]] and [[Ordinals under Multiplication form Ordered Semigroup]]. {{qed}} [[Category:Ordinal Arithmetic]] 8xmk0bsyamoab02t2essh7tyv69ii0q	0
Let $T = \struct {S, \tau}$ be the [[Definition:Either-Or Space|either-or space]]. Then the [[Definition:Closed Set (Topology)|closed sets]] of $T$ are: :$\O$ :$S$ :$\set {-1}$ :$\set 1$ :$\set {-1, 1}$ :Any subset of $\closedint {-1} 1$ containing $\set 0$ as a [[Definition:Subset|subset]].	0
Let $G_1$ and $G_2$ be [[Definition:Group|groups]] whose [[Definition:Identity Element|identities]] are $e_{G_1}$ and $e_{G_2}$ respectively. Let $\phi: G_1 \to G_2$ be a [[Definition:Group Epimorphism|group epimorphism]]. Let $K := \ker \left({\phi}\right)$ be the [[Definition:Kernel of Group Homomorphism|kernel]] of $\phi$. Let $\mathbb H_1 = \left\{{H \subseteq G_1: H \le G_1, K \subseteq H}\right\}$ be the set of [[Definition:Subgroup|subgroups]] of $G_1$ which contain $K$. Let $\mathbb H_2 = \left\{{H \subseteq G_2: H \le G_2}\right\}$ be the set of [[Definition:Subgroup|subgroups]] of $G_2$. Then there exists a [[Definition:Bijection|bijection]] $Q: \mathbb H_1 \leftrightarrow \mathbb H_2$ such that: :$\forall N \lhd G_1: Q \left({N}\right) \lhd G_2$ :$\forall N \lhd G_2: Q^{-1} \left({N}\right) \lhd G_1$ where $N \lhd G_1$ denotes that $N$ is a [[Definition:Normal Subgroup|normal subgroup]] of $G_1$. That is, [[Definition:Normal Subgroup|normal subgroups]] map bijectively to [[Definition:Normal Subgroup|normal subgroups]] under $Q$.	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] such that there are no two [[Definition:Non-Empty Set|non-empty]] [[Definition:Separated Sets|separated sets]] whose [[Definition:Set Union|union]] is $S$. Let $D = \struct {\set {0, 1}, \tau}$ be the [[Definition:Discrete Space|discrete two-point space]] on $\set {0, 1}$. {{AimForCont}} $f: T \to \set {0, 1}$ is a [[Definition:Everywhere Continuous Mapping (Topology)|continuous]] [[Definition:Surjection|surjection]]. By definition of [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]: :$\map {f^{-1} } 0$ and $\map {f^{-1} } 1$ are [[Definition:Open Set (Topology)|open sets]] of $T$. From the definition of a [[Definition:Mapping|mapping]]: :$\map {f^{-1} } 0 \cup \map {f^{-1} } 1 = S$ and :$\map {f^{-1} } 0 \cap \map {f^{-1} } 1 = \O$ Then: :$\map {f^{-1} } 0 = S \setminus \map {f^{-1} } 1$ and: :$\map {f^{-1} } 1 = T \setminus \map {f^{-1} } 0$ are [[Definition:Clopen Set|clopen]]. From [[Closed Set equals its Closure]] they are their respective [[Definition:Closure (Topology)|closures]]. It follows from the definition that $\map {f^{-1} } 0$ and $\map {f^{-1} } 1$ are [[Definition:Separated Sets|separated subsets]] of $T$ whose [[Definition:Set Union|union]] is $S$. Hence, [[Definition:By Hypothesis|by hypothesis]], one of them must be [[Definition:Empty Set|empty]], and the other one must be $S$. Therefore $f$ is [[Definition:Constant Mapping|constant]], and so is not a [[Definition:Surjection|surjection]]. This [[Definition:Contradiction|contradicts]] the original hypothesis. That is, there exists no [[Definition:Everywhere Continuous Mapping (Topology)|continuous]] [[Definition:Surjection|surjection]] from $T$ onto a [[Definition:Discrete Topology|discrete two-point space]].	0
There are four cases to cover: :$(1): \quad$ Let $I = \left({a \,.\,.\, b}\right)$. From [[Closure of Open Real Interval is Closed Real Interval]]: :$I^- = \left[{a \,.\,.\, b}\right]$ {{qed|lemma}} :$(2): \quad$ Let $I = \left[{a \,.\,.\, b}\right)$. From [[Closure of Half-Open Real Interval is Closed Real Interval]]: :$I^- = \left[{a \,.\,.\, b}\right]$ {{qed|lemma}} :$(3): \quad$ Let $I = \left({a \,.\,.\, b}\right]$. From [[Closure of Half-Open Real Interval is Closed Real Interval]]: :$I^- = \left[{a \,.\,.\, b}\right]$ {{qed|lemma}} :$(4): \quad$ Let $I = \left[{a \,.\,.\, b}\right]$. From [[Closed Real Interval is Closed in Real Number Line]]: :$I$ is [[Definition:Closed Set (Topology)|closed]] in $\R$. From [[Set is Closed iff Equals Topological Closure]]: :$I^- = \left[{a \,.\,.\, b}\right]$ {{qed|lemma}} Thus all cases are covered. The result follows by [[Proof by Cases]]. {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arcsec} \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \begin{cases} \dfrac a {x \sqrt {x^2 - a^2} } & : 0 < \operatorname{arcsec} \dfrac x a < \dfrac \pi 2 \\ \dfrac {-a} {x \sqrt {x^2 - a^2} } & : \dfrac \pi 2 < \operatorname{arcsec} \dfrac x a < \pi \\ \end{cases} | c = [[Derivative of Arcsecant of x over a|Derivative of $\operatorname{arcsec} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x^2 | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^3} 3 | c = [[Primitive of Power]] }} {{end-eqn}} First let $\operatorname{arcsec} \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\left({0 \,.\,.\,\dfrac \pi 2}\right)$. Then: {{begin-eqn}} {{eqn | l = \int x^2 \operatorname{arcsec} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \int \frac {x^3} 3 \left({\frac a {x \sqrt {x^2 - a^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac a 3 \int \frac {x^2 \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac a 3 \left({\frac {x \sqrt {x^2 - a^2} } 2 + \frac {a^2} 2 \ln \left({x + \sqrt {x^2 - a^2} }\right)}\right) + C | c = [[Primitive of x squared over Root of x squared minus a squared|Primitive of $\dfrac {x^2} {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \frac {a x \sqrt{x^2 - a^2} } 6 - \frac {a^3} 6 \ln \left({x + \sqrt {x^2 - a^2} }\right) + C | c = simplifying }} {{end-eqn}} Similarly, let $\operatorname{arcsec} \dfrac x a$ be in the [[Definition:Open Real Interval|interval]] $\left({\dfrac \pi 2 \,.\,.\, \pi}\right)$. Then: {{begin-eqn}} {{eqn | l = \int x^2 \operatorname{arcsec} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a - \int \frac {x^3} 3 \left({\frac {-a} {x \sqrt {x^2 - a^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac a 3 \int \frac {x^2 \ \mathrm d x} {\sqrt {x^2 - a^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac a 3 \left({\frac {x \sqrt {x^2 - a^2} } 2 + \frac {a^2} 2 \ln \left({x + \sqrt {x^2 - a^2} }\right)}\right) + C | c = [[Primitive of x squared over Root of x squared minus a squared|Primitive of $\dfrac {x^2} {\sqrt {x^2 - a^2} }$]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arcsec} \frac x a + \frac {a x \sqrt{x^2 - a^2} } 6 + \frac {a^3} 6 \ln \left({x + \sqrt {x^2 - a^2} }\right) + C | c = simplifying }} {{end-eqn}} {{qed}}	0
The [[Definition:External Direct Product|external direct product]] of two [[Definition:Semigroup|semigroups]] is itself a [[Definition:Semigroup|semigroup]].	0
:$\paren {a b}^x = a^x b^x$	0
Recall from [[Real Number Line is Metric Space]] that the [[Definition:Real Number|set of real numbers]] $\R$ with the [[Definition:Distance between Element and Subset of Real Numbers|distance function]] $d$ is a [[Definition:Metric Space|metric space]]. The result is then seen to be an example of [[Distance from Subset to Supremum]]. {{Qed}}	0
Let $\left({S, \preceq}\right)$ be an [[Definition:Ordered Set|ordered set]]. Let $a \in S$ and $T \subseteq S$. The following are [[Definition:Dual Statement (Order Theory)|dual statements]]: :$a$ is an [[Definition:Upper Bound of Set|upper bound]] for $T$ :$a$ is a [[Definition:Lower Bound of Set|lower bound]] for $T$	0
==== Necessary Condition ==== Follows immediately from [[Leigh.Samphier/Sandbox/Definition:Base Axiom (Matroid)/Definition 4|Definition 4]] and [[Leigh.Samphier/Sandbox/Definition:Base Axiom (Matroid)/Definition 5|Definition 5]]. {{qed|lemma}} ==== Sufficient Condition ==== Let $\mathscr B$ satisfy the [[Leigh.Samphier/Sandbox/Definition:Base Axiom (Matroid)/Definition 5|base axiom]]: {{:Leigh.Samphier/Sandbox/Definition:Base Axiom (Matroid)/Definition 5}}	0
By [[If Every Element Pseudoprime is Prime then Way Below Relation is Multiplicative]]: :$\ll$ is a [[Definition:Multiplicative Relation|multiplicative relation]]. where $\ll$ denotes the [[Definition:Element is Way Below|way below relation]]. Thus by [[Arithmetic iff Way Below Relation is Multiplicative in Algebraic Lattice]]: :the result holds. {{qed}}	0
Let $\alpha$ and $\beta$ be [[Definition:Cut (Analysis)|cuts]]. Let $\alpha \beta$ denote the [[Definition:Multiplication of Cuts|product]] of [[Definition:Cut (Analysis)|cuts]]. Then $\alpha \beta$ is also a [[Definition:Cut (Analysis)|cut]]. Thus the operation of [[Definition:Multiplication of Cuts|multiplication]] on the [[Definition:Set|set]] of [[Definition:Cut (Analysis)|cuts]] is [[Definition:Closed Operation|closed]].	0
{{begin-eqn}} {{eqn|l=\left \langle {A \mathbf u,\mathbf v} \right \rangle |r=\left({A \mathbf u}\right)^T \mathbf v |c=Definition of [[Definition:Dot Product|Dot Product]] }} {{eqn|r=\mathbf u^T A^T \mathbf v |c=[[Transpose of Matrix Product]] }} {{eqn|r=\left \langle {\mathbf u, A^T \mathbf v} \right \rangle |c=Definition of [[Definition:Dot Product|Dot Product]] }} {{end-eqn}} {{Qed}} [[Category:Vector Algebra]] rrqdb9x5aqek4w4jjl5s25b9pbnyhsd	0
Let $x^t$ be the [[Definition:Transitive Closure (Set Theory)/Definition 2|transitive closure of $x$ by Definition 2]]. Let the [[Definition:mapping|mapping]] $G$ be defined as on that definition page. === $x \in x^t$ === $x \in \set x$ by the definition of [[Definition:singleton|singleton]]. Since $\map G 0 = \set 0$: :$\set x \in \map G \N$ Thus $x \in x^t$ by the definition of [[Definition:Union of Set of Sets|union]]. {{qed|lemma}} === $x^t$ is a Set === By [[Denumerable Class is Set]], the [[Definition:Image of Subset under Mapping|image]] of $G$ is a [[Definition:Set|set]]. Thus $x^t$ is a set by the [[Axiom:Axiom of Unions|Axiom of Unions]]. {{qed|lemma}} === $x^t$ is a Transitive Set === Let $y \in x^t$ and let $z \in y$. By the definition of $x^t$: :$\exists n \in \N: y \in \map G n$ Then by definition of [[Definition:Union of Set of Sets|union]]: :$\displaystyle z \in \bigcup \map G n$ But by the definition of $G$: :$z \in \map G {n^+}$ Thus by the definition of $x^t$: :$z \in x^t$ As this holds for all such $y$ and $z$, $x^t$ is [[Definition:Transitive Set|transitive]]. {{qed|lemma}} === $x^t$ is Smallest === Let $m$ be a [[Definition:Transitive Set|transitive set]] such that $x \in m$. We will show by [[Principle of Mathematical Induction|induction]] that $\map G n \subseteq m$ for each $n \in \N$. By [[Union is Smallest Superset]], that will show that $x^t \subseteq m$. Because $x \in m$: :$\map G 0 = \set x \subseteq m$ Suppose that $\map G n \subseteq m$. Then by [[Union is Increasing]]: :$\displaystyle \bigcup \map G n \subseteq \bigcup m$ {{explain|Transitive set includes its union.}} Thus: :$\displaystyle \bigcup \map G n \subseteq m$ {{qed|lemma}} By [[Smallest Element is Unique]], $x^t$ is the only set satisfying $(2)$. {{qed}} [[Category:Set Theory]] iqtibc9o9fsqisxkwxh0qhrkjxcvxyn	0
We will prove that $\left({x \vee y}\right)^\succeq \subseteq x^\succeq \cap y^\succeq$ Let $a \in \left({x \vee y}\right)^\succeq$ By definition of [[Definition:Upper Closure of Element|upper closure of element]]: :$x \vee y \preceq a$ By [[Join Succeeds Operands]]: :$x \preceq x \vee y$ and $y \preceq x \vee y$ By definition of [[Definition:Transitivity|transitivity]]: :$x \preceq a$ and $y \preceq a$ By definition of [[Definition:Upper Closure of Element|upper closure of element]]: :$a \in x^\succeq$ and $a \in y^\succeq$ Thus by definition of [[Definition:Set Intersection|intersection]]: :$a \in x^\succeq \cap y^\succeq$ {{qed|lemma}} We will prove that :$x^\succeq \cap y^\succeq \subseteq \left({x \vee y}\right)^\succeq$ Let $a \in x^\succeq \cap y^\succeq$ Thus bt definition of [[Definition:Set Intersection|intersection]]: :$a \in x^\succeq$ and $a \in y^\succeq$ By definition of [[Definition:Upper Closure of Element|upper closure of element]]: :$x \preceq a$ and $y \preceq a$ By definitions of [[Definition:Supremum of Set|supremum]] and [[Definition:Upper Bound of Set|Upper bound]]: :$x \vee y \preceq a$ Thus by definition of [[Definition:Upper Closure of Element|upper closure of element]]: :$a \in \left({x \vee y}\right)^\succeq$ {{qed|lemma}} Hence by definition of [[Definition:Set Equality|set equality]]: :$\left({x \vee y}\right)^\succeq = x^\succeq \cap y^\succeq$ {{qed}}	0
:$\map \erf 0 = 0$	0
Let $z_1 := r_1 e^{i \theta_1}$ and $z_2 := r_2 e^{i \theta_2}$ be [[Definition:Exponential Form of Complex Number|complex numbers expressed in exponential form]]. Then: :$z_1 z_2 = r_1 r_2 e^{i \paren {\theta_1 + \theta_2} }$	0
:$\displaystyle \int \frac {\mathrm d x} {\sin^3 a x} = \frac {-\cos a x} {2 a \sin^2 a x} + \frac 1 {2 a} \ln \left\vert{\tan \frac {a x} 2}\right\vert + C$	0
<onlyinclude> With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \arctan \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac a {x^2 + a^2} | c = [[Derivative of Arctangent of x over a|Derivative of $\arctan \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^2} 2 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \arctan \frac x a \ \mathrm d x | r = \frac {x^2} 2 \arctan \frac x a - \int \frac {x^2} 2 \left({\frac a {x^2 + a^2} }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arctan \frac x a - \frac a 2 \int \frac {x^2 \ \mathrm d x} {x^2 + a^2} + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arctan \frac x a - \frac a 2 \left({x - a \arctan{\frac x a} }\right) + C | c = [[Primitive of x squared over x squared plus a squared|Primitive of $\dfrac {x^2} {x^2 + a^2}$]] }} {{eqn | r = \frac {x^2 + a^2} 2 \arctan \frac x a - \frac {a x} 2 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $-1 < x, y, z < 1$. We check the [[Definition:Group Axioms|group axioms]] in turn: === $\text G 1$: Associativity === {{begin-eqn}} {{eqn | l = \paren {x \circ y} \circ z | r = \frac {\frac {x + y} {1 + x y} + z} {1 + \frac {x + y} {1 + xy} z} | c = }} {{eqn | r = \frac {x + y + z + x y z} {1 + x y + x z + y z} | c = }} {{eqn | l = x \circ \paren {y \circ z} | r = \frac {x + \frac {y + z} {1 + y z} } {1 + x \frac {y + z} {1 + y z} } | c = }} {{eqn | r = \frac {x + y + z + x y z} {1 + x y + x z + y z} | c = }} {{eqn | r = \paren {x \circ y} \circ z | c = }} {{end-eqn}} Thus $\circ$ has been shown to be [[Definition:Associative|associative]]. {{qed|lemma}} === $\text G 2$: Identity === {{begin-eqn}} {{eqn | l = x | r = 0 | c = }} {{eqn | ll= \leadsto | l = \frac {x + y} {1 + x y} | r = \frac {0 + y} {1 + 0 y} | c = }} {{eqn | r = \frac y 1 | c = }} {{eqn | r = y | c = }} {{end-eqn}} Similarly, putting $y = 0$ we find $x \circ y = x$. So $0$ is the [[Definition:Identity Element|identity]]. {{qed|lemma}} === $\text G 3$: Inverses === {{begin-eqn}} {{eqn | l = x \circ -x | r = \frac {x + \paren {-x} } {1 + x \paren {-x} } | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} Similarly, putting $x = -y$ gives us $\paren {-y} \circ y = 0$. So each $x$ has an [[Definition:Inverse Element|inverse]] $-x$. {{qed|lemma}} === $\text G 0$: Closure === First note that: :$-1 < x, y < 1 \implies x y > -1 \implies 1 + x y > 0$ Next: {{begin-eqn}} {{eqn | l = -1 | o = < | r = x, y < 1 | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \paren {1 - x} \paren {1 - y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = 1 + x y - \paren {x + y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \frac {1 + x y - \paren {x + y} } {1 + x y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \frac {1 + x y} {1 + x y} - \frac {x + y} {1 + x y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = 1 - \frac {x + y} {1 + x y} | c = }} {{eqn | ll= \leadsto | l = \frac {x + y} {1 + x y} | o = < | r = 1 | c = }} {{end-eqn}} Finally: {{begin-eqn}} {{eqn | l = -1 | o = < | r = x, y < 1 | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \paren {1 + x} \paren {1 + y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = 1 + x y + \paren {x + y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \frac {1 + x y + \paren {x + y} } {1 + x y} | c = }} {{eqn | ll= \leadsto | l = 0 | o = < | r = \frac {1 + x y} {1 + x y} + \frac {x + y} {1 + x y} | c = }} {{eqn | ll=\leadsto | l = 0 | o = < | r = 1 + \frac {x + y} {1 + x y} | c = }} {{eqn | ll= \leadsto | l = -1 | o = < | r = \frac {x + y} {1 + x y} | c = }} {{end-eqn}} Thus: :$-1 < x, y < 1 \implies -1 < x \circ y < 1$ and we see that in this range, $\circ$ is [[Definition:Closed Algebraic Structure|closed]]. {{qed|lemma}} Thus the given [[Definition:Set|set]] and [[Definition:Binary Operation|operation]] form a [[Definition:Group|group]]. {{Qed}}	0
=== Inflationary === $x$ is a [[Definition:Lower Bound of Set|lower bound]] of $x^\succeq$. Hence by [[Lower Bound for Subset]], $x$ is also a [[Definition:Lower Bound of Set|lower bound]] of $C \cap x^\succeq$. By the definition of [[Definition:Smallest Element|smallest element]], $x \preceq \map \cl x$. {{qed|lemma}} === Order-Preserving === Suppose that $x \preceq y$. Then: :$C \cap y^\succeq \subseteq C \cap x^\succeq$ By [[Smallest Element of Subset]]: :$\map \cl x \preceq \map \cl y$ === Idempotent === Let $x \in S$. For each $x \in S$: :$\map \cl x = \map \min {C \cap x^\succeq}$ Thus: :$\map \cl x \in \paren {C \cap x^\succeq} \subseteq C$ That is to say, $\map \cl x$ is its own [[Definition:Smallest Element|smallest]] [[Definition:Successor Element|successor]] in $C$. Thus: :$\map \cl x = \map \cl {\map \cl x}$ {{qed|lemma}} When $x \in C$, $x$ is the [[Definition:Smallest Element|minimum]] of $C \cap x^\succeq$ Hence, elements of $C$ are [[Definition:Closed Element|closed elements]] with respect to $\cl$. Suppose that $x$ is [[Definition:Closed Element|closed]] with respect to $\cl$. Then: :$x = \map \min {C \cap x^\succeq}$ so in particular: :$x \in C$ {{qed}} [[Category:Closure Operators]] [[Category:Order Theory]] hzff01klrhedb7dqje56pkzdwsi8ljk	0
: $m \mathrel \backslash n \iff -m \divides n \iff m \divides -n \iff -m \divides -n$	0
Let $P = \sequence {a_j}_{0 \mathop \le j \mathop \le n}$ be a [[Definition:Geometric Sequence of Integers|geometric sequence of integers]] of [[Definition:Length of Sequence|length]] $n$. Let $a_0$ not be a [[Definition:Divisor of Integer|divisor]] of $a_1$. Then: :$\forall j, k \in \set {0, 1, \ldots, n}, j \ne k: a_j \nmid a_k$ That is, if the [[Definition:Initial Term of Geometric Sequence|initial term]] of $P$ does not [[Definition:Divisor of Integer|divide]] the second, no [[Definition:Term of Geometric Sequence|term]] of $P$ [[Definition:Divisor of Integer|divides]] any other [[Definition:Term of Geometric Sequence|term]] of $P$. {{:Euclid:Proposition/VIII/6}}	0
Let $n \in \Z_{\ge 0}$ be a [[Definition:Positive Integer|positive integer]]. Let $x, y \in \R$ be [[Definition:Real Number|real numbers]] which satisfy: :$n \le y \le x \le y + 1$ Then there exists a [[Definition:Unique|unique]] [[Definition:Real Number|real number]] $z$ such that: :$\dbinom x {n + 1} = \dbinom y {n + 1} + \dbinom z n$ where $n - 1 \le z \le y$.	0
{{begin-eqn}} {{eqn | l = A \setminus S | r = A \cap \map \complement S | c = [[Set Difference as Intersection with Complement]] }} {{eqn | o = \subseteq | r = B \cap \map \complement S | c = [[Set Intersection Preserves Subsets/Corollary|Corollary to Set Intersection Preserves Subsets]] }} {{eqn | r = B \setminus S | c = [[Set Difference as Intersection with Complement]] }} {{end-eqn}} {{qed}} [[Category:Set Difference]] jn9zvcha8xojzr8k7s5voxtb7nljqts	0
=== Necessary Condition === Suppose $f$ is such that it is not the case that: :for all $A \subseteq H$: if $A$ is [[Definition:Bounded Ordered Set|bounded in $S$]], then $f \sqbrk A$ is [[Definition:Bounded Ordered Set|bounded in $T$]]. [[Proof by Counterexample]]: Let $S = \R_{>0}$ be the [[Definition:Set|set]] of all [[Definition:Strictly Positive Real Number|(strictly) positive real numbers]]. Let $H \subseteq S$ be the [[Definition:Open Real Interval|open real interval]] $H = \openint 0 1$. Let $T = H$. By the [[Definition:Real Number/Axiomatic Definition|axiomatic definition of real numbers]], $S$ and $T$ are [[Definition:Totally Ordered Set|totally ordered]]. From the [[Continuum Property]], $T$ is [[Definition:Order Complete Set|order complete]]. Let $f: H \to T$ be the [[Definition:Identity Mapping|identity mapping]]. Note that while $H$ has an [[Definition:Upper Bound of Set|upper bound]] in $S$, for example $1$ Let $y \in T$ be an [[Definition:Upper Bound of Set|upper bound]] of $f \sqbrk H$. By construction: :$y < 1$ and so: :$\exists \epsilon \in \R_{>0}: y + \epsilon = 1$ Thus: :$y < y + \dfrac \epsilon 2 < 1$ and so there exists $y' = y + \dfrac \epsilon 2 \in T$ such that: :$y < y'$ and such that: :$y' = \map f {y'} \in f \sqbrk H$ So $y$ is not an [[Definition:Upper Bound of Set|upper bound]] of $f \sqbrk H$. Thus $f \sqbrk H$ has no [[Definition:Upper Bound of Set|upper bound]] in $T$. Let $g: S \to T$ be an [[Definition:Extension of Mapping|extension]] of $f$ to $S$. Consider $\map g 1$. We have that $1$ is an [[Definition:Upper Bound of Set|upper bound]] of $H$. Let $\map g 1 = y$. As $y \in \openint 0 1$ it follows that: :$y > 1$ and so: :$\exists \epsilon \in \R_{>0}: y + \epsilon = 1$ But then: :$y < y + \dfrac \epsilon 2 < 1$ and so: :$\map g 1 < \map g {y + \dfrac \epsilon 2}$ and so $g$ is not [[Definition:Increasing Mapping|increasing]]. Thus it has been demonstrated that if it is not the case that: :for all $A \subseteq H$: if $A$ is [[Definition:Bounded Ordered Set|bounded in $S$]], then $f \sqbrk A$ is [[Definition:Bounded Ordered Set|bounded in $T$]] where: :$\struct {S, \preceq}$ and $\struct {T, \preccurlyeq}$ be [[Definition:Totally Ordered Set|tosets]] and: :$T$ is [[Definition:Order Complete Set|order complete]] and: :$f: H \to T$ is an [[Definition:Increasing Mapping|increasing mapping]] from $H$ to $T$. and: :$H \subseteq S$ is a [[Definition:Subset|subset]] of $S$ then it is not necessarily the case that $f$ always has an [[Definition:Extension of Mapping|extension]] to $S$ which is [[Definition:Increasing Mapping|increasing]]. {{qed|lemma}} === Sufficient Condition === Let $f$ be such that: :for all $A \subseteq H$: if $A$ is [[Definition:Bounded Ordered Set|bounded in $S$]], then $f \sqbrk A$ is [[Definition:Bounded Ordered Set|bounded in $T$]]. An [[Definition:Increasing Mapping|increasing mapping]] $g: S \to T$ is to be constructed such that $g$ is an [[Definition:Extension of Mapping|extension]] of $f$. So, let $A \subseteq H$ such that $A$ is [[Definition:Bounded Ordered Set|bounded in $S$]]. Let $x \in A$. Consider the [[Definition:Set|set]]: :$A' = \set {y \in A: y \preceq x}$ $A'$ is [[Definition:Bounded Ordered Set|bounded in $S$]]: :[[Definition:Bounded Below Set|bounded below]] by a [[Definition:Lower Bound of Set|lower bound]] of $A$ :[[Definition:Bounded Above Set|bounded above]] by $y$. Thus [[Definition:By Hypothesis|by hypothesis]] $f \sqbrk {A'}$ is [[Definition:Bounded Ordered Set|bounded in $T$]]. A [[Definition:Lower Bound of Set|lower bound]] of $f \sqbrk {A'}$ is also a [[Definition:Lower Bound of Set|lower bound]] of $f \sqbrk A$. Similarly for [[Definition:Upper Bound of Set|upper bounds]]. {{explain|{{BookReference|General Topology|1955|John L. Kelley}}, from which this has been taken, is unclear here exactly what is being demonstrated by the above.}} For each $x \in S$ let $L_x$ be defined as: :$L_x = \set {y \in H: y \le x}$ Note that $L_x$ is [[Definition:Bounded Above Set|bounded above]] by $x$ by definition of [[Definition:Bounded Above Set|bounded above]]. Suppose $L_x = \O$. Then $x$ is a [[Definition:Lower Bound of Set|lower bound]] for $H$. Thus $f \sqbrk H$ has an [[Definition:Infimum of Set|infimum]] $v$, and $\map g x$ can be defined as: :$\map g x = v$ Suppose $L_x \ne O$. We have that $x$ is an [[Definition:Upper Bound of Set|upper bound]] of $L_x$. Hence [[Definition:By Hypothesis|by hypothesis]] $f \sqbrk {L_x}$ has an [[Definition:Upper Bound of Set|upper bound]]. As $T$ is [[Definition:Order Complete Set|order complete]], $f \sqbrk {L_x}$ admits a [[Definition:Supremum of Set|supremum]]. Thus $\map g x$ can be defined as: :$\map g x = \sup \set {f \sqbrk {L_x} }$ It remains to be proved that $g$ is an [[Definition:Increasing Mapping|increasing mapping]]. {{finish}}	0
:$\displaystyle \int \frac {\d x} {x^3 \sqrt {a^2 - x^2} } = \frac {-\sqrt {a^2 - x^2} } {2 a^2 x^2} - \frac 1 {2 a^3} \map \ln {\frac {a + \sqrt {a^2 - x^2} } x} + C$	0
From [[Reduced Residue System under Multiplication forms Abelian Group]] it is noted that $\struct {\Z'_5, \times_5}$ is a [[Definition:Group|group]]. It remains to be shown that $\struct {\Z'_5, \times_5}$ is [[Definition:Cyclic Group|cyclic]]. It will be demonstrated that: :$\gen {\eqclass 2 5} = \struct {\Z'_5, \times_5}$ That is, that $\eqclass 2 5$ is a [[Definition:Generator of Cyclic Group|generator]] of $\struct {\Z'_5, \times_5}$. We note that $\eqclass 1 5$ is the [[Definition:Identity Element|identity element]] of $\struct {\Z'_5, \times_5}$. Thus successive [[Definition:Power of Group Element|powers]] of $\eqclass 2 5$ are taken, until $n \in \Z$ is found such that $\eqclass 2 5^n = \eqclass 1 5$: {{begin-eqn}} {{eqn | l = \eqclass 2 5^2 | r = \eqclass {2 \times 2} 5 | c = }} {{eqn | r = \eqclass 4 5 | c = }} {{eqn | l = \eqclass 2 5^3 | r = \eqclass 2 5^2 \times \eqclass 2 5 | c = }} {{eqn | r = \eqclass {2 \times 4} 5 | c = }} {{eqn | r = \eqclass 8 5 | c = }} {{eqn | r = \eqclass 3 5 | c = }} {{eqn | l = \eqclass 2 5^4 | r = \eqclass 2 5^3 \times \eqclass 2 5 | c = }} {{eqn | r = \eqclass {3 \times 2} 5 | c = }} {{eqn | r = \eqclass 6 5 | c = }} {{eqn | r = \eqclass 1 5 | c = }} {{end-eqn}} All [[Definition:Element|elements]] of $\struct {\Z'_5, \times_5}$ are seen to be in $\gen {\eqclass 2 5}$. Hence the result by definition of [[Definition:Cyclic Group|cyclic group]]. {{qed}} [[Category:Multiplicative Groups of Reduced Residues]] [[Category:Multiplicative Group of Reduced Residues Modulo 5]] [[Category:Examples of Cyclic Groups]] [[Category:Cyclic Group of Order 4]] 5n9m9g0fku8wbu7y1ytu42a6n0ncayy	0
=== Sufficient Condition === Let $x \in H$ be an [[Definition:Isolated Point (Topology)|isolated point]] in $H$. Then by definition of [[Definition:Isolated Point (Topology)|isolated point]]: :$\exists U \in \tau: H \cap U = \left\{ {x}\right\}$ That is, by definition of Definition of [[Definition:Unique|uniqueness]]: :$\lnot \forall U \in \tau: \left({x \in U \implies \exists y \in S: \left({y \in H \cap U \land x \ne y}\right)}\right)$ Hence by [[Characterization of Derivative by Open Sets]]: :$x \notin A'$ where $A'$ denotes the [[Definition:Set Derivative|derivative]] of $A$. Thus by definition of [[Definition:Set Derivative|derivative]]: :$x$ is not an [[Definition:Accumulation Point of Set|accumulation point]] of $H$. {{qed|lemma}} === Necessary Condition === Let $x \in H$ not be an [[Definition:Accumulation Point of Set|accumulation point]] of $H$. Thus by definition of [[Definition:Set Derivative|derivative]]: :$x \notin A'$ Hence: {{begin-eqn}} {{eqn | l = \lnot \forall U \in \tau | o = : | r = \left({x \in U \implies \exists y \in S: \left({y \in H \cap U \land x \ne y}\right)}\right) | c = [[Characterization of Derivative by Open Sets]] }} {{eqn | l = \exists U \in \tau | o = : | r = \lnot \left({x \in U \implies \exists y \in S: \left({y \in H \cap U \land x \ne y}\right)}\right) | c = [[Denial of Universality]] }} {{eqn | l = \exists U \in \tau | o = : | r = \left({x \in U \land \lnot \exists y \in S: \left({y \in H \cap U \land x \ne y}\right)}\right) | c = [[Conjunction with Negative Equivalent to Negation of Implication]] }} {{eqn | l = \exists U \in \tau | o = : | r = \left({x \in U \land \forall y \in S: \lnot \left({y \in H \cap U \land x \ne y}\right)}\right) | c = [[Denial of Existence]] }} {{eqn | l = \exists U \in \tau | o = : | r = \left({x \in U \land \forall y \in S: \left({y \in H \cap U \implies x = y}\right)}\right) | c = [[Implication Equivalent to Negation of Conjunction with Negative]] }} {{eqn | l = \exists U \in \tau | o = : | r = H \cap U = \left\{ {x}\right\} | c = Definition of [[Definition:Unique|Uniqueness]], and $x \in H$ }} {{end-eqn}} Thus by definition of [[Definition:Isolated Point (Topology)|isolated point]]: :$x$ is an isolated point in $H$. {{qed}} [[Category:Isolated Points]] [[Category:Accumulation Points]] grmf8cl1o8om1tbe6spc1mrqkf15c74	0
{{begin-eqn}} {{eqn | l = \map {\Pi_Z} s | r = \expect {s^Z} | c = {{Defof|Probability Generating Function}} }} {{eqn | r = \expect {s^{X + Y} } | c = Definition of $Z$ (see above) }} {{eqn | r = \expect {s^X s^Y} | c = [[Exponential of Sum]] }} {{eqn | r = \expect {s^X} \expect {s^Y} | c = [[Condition for Independence from Product of Expectations]] }} {{eqn | r = \map {\Pi_X} s \, \map {\Pi_Y} s | c = {{Defof|Probability Generating Function}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | lo= \forall x \in S: | l = \map {\paren {\paren {f + g} + h} } x | r = \paren {\map f x + \map g x} + \map h x | c = {{Defof|Pointwise Addition of Complex-Valued Functions}} }} {{eqn | r = \map f x + \paren {\map g x + \map h x} | c = [[Complex Addition is Associative]] }} {{eqn | r = \map {\paren {f + \paren {g + h} } } x | c = {{Defof|Pointwise Addition of Complex-Valued Functions}} }} {{end-eqn}} {{qed}} [[Category:Pointwise Addition]] [[Category:Complex Addition]] [[Category:Associativity]] 0af8d46jvxcja4hep7qyh7kib89npxf	0
By [[Set of All Self-Maps is Semigroup]], the [[Definition:Set|set]] of all [[Definition:Self-Map|self-maps]] on $S$ forms a [[Definition:Semigroup|semigroup]] under [[Definition:Composition of Mappings|composition]]. The result follows from [[Product of Commuting Idempotent Elements is Idempotent]]. {{qed}}	0
Let $\mathcal R$ be [[Definition:Symmetric Relation|symmetric]]. Then from [[Relation equals Inverse iff Symmetric]]: :$\mathcal R = \mathcal R^{-1}$ The result follows. {{qed}}	0
From [[Hartogs' Lemma (Set Theory)|Hartogs' lemma]] there exists an [[Definition:Ordinal|ordinal]] $\alpha$ such that there is no [[Definition:Injection|injection]] from $\alpha$ to $S$. We also have that [[Ordinals are Well-Ordered]]. It follows from the definition of [[Definition:Well-Ordering|well-ordering]] that there exists a [[Definition:Smallest Element|smallest]] such [[Definition:Ordinal|ordinal]]. Hence the result. {{qed}}	0
$f$ is a [[Definition:Surjection|surjection]] from ... {{link wanted|Needs a link to a result specifying that $f$ is surjective (may already exist).}} {{qed|lemma}} Then: {{begin-eqn}} {{eqn | l = \map f x \times \map f y | r = \paren {\cos x + i \sin x} \paren {\cos y + i \sin y} | c = }} {{eqn | r = \cos x \cos y + i \sin x \cos y + \cos x i \sin y + i \sin x i \sin y | c = }} {{eqn | r = \paren {\cos x \cos y - \sin x \sin y} + i \paren {\sin x \cos y + \cos x \sin y} | c = as $i^2 = -1$ }} {{eqn | r = \map \cos {x + y} + i \map \sin {x + y} | c = [[Cosine of Sum]] and [[Sine of Sum]] }} {{eqn | r = \map f {x + y} | c = }} {{end-eqn}} So $f$ is a [[Definition:Group Homomorphism|(group) homomorphism]]. {{qed|lemma}} Thus $f$ is seen to be a [[Definition:Surjection|surjective]] [[Definition:Group Homomorphism|homomorphism]]. Hence, by definition, it is a [[Definition:Group Epimorphism|(group) epimorphism]]. {{qed|lemma}} From [[Cosine of Multiple of Pi]]: :$\forall n \in \Z: \cos n \pi = \paren {-1}^n$ and from [[Sine of Multiple of Pi]]: :$\forall n \in \Z: \sin n \pi = 0$ From [[Sine and Cosine are Periodic on Reals]], it follows that these are the only values of $\Z$ for which this holds. For $\cos x + i \sin x = 1 + 0 i$ it is necessary that: :$\cos x = 1$ :$\sin x = 0$ and it can be seen that the only values of $x$ for this to happen is: :$x \in \set {2 \pi n: n \in \Z}$ Hence, by definition of [[Definition:Kernel of Group Homomorphism|kernel]]: :$\map \ker f = \set {2 \pi n: n \in \Z}$ {{qed}}	0
Let $a, b \in \Z$ such that $a$ and $b$ are not both [[Definition:Zero (Number)|zero]]. Let $J$ be the [[Definition:Set|set]] of all [[Definition:Integer Combination|integer combinations]] of $a$ and $b$: :$J = \set {x: x = m a + n b: m, n \in \Z}$ First we show that $J$ is an ideal of $\Z$ Let $\alpha = m_1 a + n_1 b$ and $\beta = m_2 a + n_2 b$, and let $c \in \Z$ Then $\alpha,\beta \in J$ and : {{begin-eqn}} {{eqn | l = \alpha + \beta | r = m_1 a + n_1 b + m_2 a + n_2 b }} {{eqn | r = \paren {m_1 + m_2} a + \paren {n_1 + n_2} b }} {{eqn | ll= \leadsto | l = \alpha + \beta | o = \in | r = J }} {{end-eqn}} {{begin-eqn}} {{eqn | l = c \alpha | r = c \paren {m_1 a + n_1 b} }} {{eqn | r = \paren {c m_1} a + \paren {c n_1} b }} {{eqn | ll= \leadsto | l = c \alpha | o = \in | r = J }} {{end-eqn}} Thus $J$ is an [[Definition:Integral Ideal|integral ideal]]. We have that: {{begin-eqn}} {{eqn | l = a | r = 1 a + 0 b | c = }} {{eqn | lo= \land | l = b | r = 0 a + 1 b | c = }} {{eqn | ll= \leadsto | l = a, b | o = \in | r = J | c = }} {{end-eqn}} $a$ and $b$ are not both [[Definition:Zero (Number)|zero]], thus: :$J \ne \set 0$ By the something {theorem about ideals}: :$\exists x_0 > 0 : J = x_0 \Z$ :$a \in J \land \set {J = x_0 \Z} \implies x_0 \divides a$ :$b \in J \land \set {J = x_0 \Z} \implies x_0 \divides b$ :$x_0 \divides a \land x_0 \divides b \implies x_0 \in \map D {a, b}$ {{explain|What is $\map D {a, b}$?}} Furthermore: :$x_0 \in J \implies \exists r, s \in \Z : x_0 = r a + s b$ Let $x_1 \in \map D {a, b}$. Then: {{begin-eqn}} {{eqn | l = x_1 \in \map D {a, b} | o = \leadsto | r = x_1 \divides a \land x_1 \divides b }} {{eqn | o = \leadsto | r = x_1 \divides \paren {r a + s b} }} {{eqn | o = \leadsto | r = x_1 \vert x_0 }} {{eqn | o = \leadsto | r = \size {x_1} \le \size {x_0} = x_0 }} {{end-eqn}} Thus: :$x_0 = \max \set {\map D {a, b} } = \gcd \set {a, b} = r a + s b$ {{qed}}	0
Let $\left({S, \preceq_1}\right)$ and $\left({T, \preceq_2}\right)$ be [[Definition:Ordered Set|ordered sets]]. Let $\phi: \left({S, \preceq_1}\right) \to \left({T, \preceq_2}\right)$ be [[Definition:Strictly Increasing Mapping|strictly increasing]]. From [[Strictly Precedes is Strict Ordering]]: :$x \preceq_1 y \implies x = y \lor x \prec_1 y$ So: {{begin-eqn}} {{eqn | l = x | r = y | c = }} {{eqn | ll= \implies | l = \phi \left({x}\right) | r = \phi \left({y}\right) | c = Definition of [[Definition:Mapping|Mapping]] }} {{eqn | ll= \implies | l = \phi \left({x}\right) | o = \preceq_2 | r = \phi \left({y}\right) | c = as $\preceq_2$, being an [[Definition:Ordering|ordering]], is [[Definition:Reflexive Relation|reflexive]] }} {{end-eqn}} This leaves us with: {{begin-eqn}} {{eqn | l = x | o = \prec_1 | r = y | c = }} {{eqn | ll= \implies | l = \phi \left({x}\right) | o = \prec_2 | r = \phi \left({y}\right) | c = Definition of [[Definition:Strictly Increasing Mapping|Strictly Increasing]] }} {{eqn | ll= \implies | l = \phi \left({x}\right) | o = \preceq_2 | r = \phi \left({y}\right) | c = [[Strictly Precedes is Strict Ordering]] }} {{end-eqn}} {{Qed}}	0
Let $\struct {R, +, \circ}$ be a [[Definition:Non-Null Ring|non-null]] [[Definition:Ring (Abstract Algebra)|ring]]. Then $R$ has no [[Definition:Zero Divisor of Ring|zero divisors]] {{iff}} $\struct {R^*, \circ}$ is a [[Definition:Semigroup|semigroup]].	0
Let $T = \struct {S, \tau}$ be a [[Definition:Hausdorff Space|Hausdorff space]] which is [[Definition:Compact Topological Space|compact]]. Then $\tau$ is [[Definition:Maximal Set|maximally]] [[Definition:Compact Topological Space|compact]].	0
From [[Number of Modified Perfect Faro Shuffles to return Deck of Cards to Original Order]], the [[Definition:Card|cards]] of $D$ will return to their original [[Definition:Order of Cards|order]] after $n$ such [[Definition:Shuffle|shuffles]], where: :$2^n \equiv 1 \pmod {13}$ From [[Fermat's Little Theorem]]: :$2^{12} \equiv 1 \pmod {13}$ so we know that $n$ is at most $12$. But $n$ may be smaller, so it is worth checking the values: Inspecting $2^n$ for $n$ from $1$: {{begin-eqn}} {{eqn | l = 2^1 | o = \equiv | r = 2 | rr= \pmod {13} | c = }} {{eqn | l = 2^2 | o = \equiv | r = 4 | rr= \pmod {13} | c = }} {{eqn | l = 2^3 | o = \equiv | r = 8 | rr= \pmod {13} | c = }} {{eqn | l = 2^4 | o = \equiv | r = 3 | rr= \pmod {13} | c = }} {{eqn | l = 2^6 | o = \equiv | r = 6 | rr= \pmod {13} | c = }} {{eqn | l = 2^6 | o = \equiv | r = 12 | rr= \pmod {13} | c = }} {{eqn | l = 2^7 | o = \equiv | r = 11 | rr= \pmod {13} | c = }} {{eqn | l = 2^8 | o = \equiv | r = 9 | rr= \pmod {13} | c = }} {{eqn | l = 2^9 | o = \equiv | r = 5 | rr= \pmod {13} | c = }} {{eqn | l = 2^{10} | o = \equiv | r = 10 | rr= \pmod {13} | c = }} {{eqn | l = 2^{11} | o = \equiv | r = 7 | rr= \pmod {13} | c = }} {{eqn | l = 2^{12} | o = \equiv | r = 1 | rr= \pmod {13} | c = }} {{end-eqn}} Hence the result. {{qed}}	0
Let $T_H = \struct {H, \tau_H}$ be a [[Definition:Topological Subspace|subspace]] of $T$. Let $\CC$ be an [[Definition:Open Cover|open cover]] of $T_H$. Let $U \in \CC$ be any [[Definition:Set|set]] in $C$. $U$ [[Definition:Cover of Set|covers]] all but a [[Definition:Finite Set|finite number]] of points of $T_H$. So for each of those points we pick an [[Definition:Element|element]] of $\CC$ which [[Definition:Cover of Set|covers]] each of those points. Hence we have a [[Definition:Finite Subcover|finite subcover]] of $T_H$. So by definition $T_H$ is a [[Definition:Compact Topological Space|compact space]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \tau 1 | r = 1 | c = {{TauLink|1}} }} {{eqn | l = \map \tau {625} | r = 5 | c = {{TauLink|625}} }} {{eqn | l = \map \tau {6561} | r = 9 | c = {{TauLink|6561}} }} {{eqn | l = \map \tau {4 \, 100 \, 625} | r = 45 | c = {{TauLink|4,100,625|4 \, 100 \, 625}} }} {{end-eqn}} Suppose $N = \map \tau {N^4}$. By [[Tau Function Odd Iff Argument is Square]], $N$ must be [[Definition:Odd Integer|odd]]. The case $N = 1$ is trivial. Suppose $N$ is a [[Definition:Prime Power|prime power]]. Write $N = p^n$. By [[Tau of Power of Prime]]: :$N = \map \tau {p^{4 n} } = 4 n + 1$ By [[Bernoulli's Inequality]]: :$N = p^n \ge 1 + n \paren {p - 1}$ This gives us the inequality: :$4 n + 1 \ge 1 + n \paren {p - 1}$ which can be simplified to: :$4 \ge p - 1$ The only [[Definition:Odd Prime|odd primes]] satisfying the inequality are $3$ and $5$. We have: :$\map \tau {3^4} = 5 > 3^1$ :$\map \tau {3^8} = 9 = 3^2$ :$\map \tau {3^{4 n} } = 4 n + 1 < 3^n$ for $n > 2$ :$\map \tau {5^4} = 5 = 5^1$ :$\map \tau {5^{4 n} } = 4 n + 1 < 5^n$ for $n > 1$ :$\map \tau {p^{4 n} } = 4 n + 1 < p^n$ for any $p > 5$ Hence $625$ and $6561$ are the only [[Definition:Prime Power|prime powers]] satisfying the property. Note that [[Tau Function is Multiplicative]]. To form an [[Definition:Integer|integer]] $N$ with our property, we must choose and multiply [[Definition:Prime Power|prime powers]] from the list above. The product $625 \times 6561 = 4 \, 100 \, 625$ gives equality. If we chose any $\tuple {p, n}$ with $\map \tau {p^{4 n} } < p^n$, we must choose $3^4$ in order for equality to possibly hold. Then $\map \tau {3^4} = 5 \divides N$, so a tuple $\tuple {5, n}$ must be chosen. If $\tuple {5, 1}$ was chosen, $5^2 \nmid N$. But $\map \tau {3^4 \times 5^4} = 25 \divides N$, which is a contradiction. Suppose $\tuple {5, n}$ with $n \ge 2$ was chosen. Then $\map \tau {3^4 \times 5^{4 n} } = 20 n + 1 < 3 \times 5^n$, a contradiction. Thus we have exhausted all cases. {{qed}}	0
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Then: {{begin-eqn}} {{eqn | l = x^{2 n + 1} + y^{2 n + 1} | r = \paren {x + y} \displaystyle \prod_{k \mathop = 1}^n \paren {x^2 + 2 x y \cos \dfrac {2 \pi k} {2 n + 1} + y^2} | c = }} {{eqn | r = \paren {x + y} \paren {x^2 + 2 x y \cos \dfrac {2 \pi} {2 n + 1} + y^2} \paren {x^2 + 2 x y \cos \dfrac {4 \pi} {2 n + 1} + y^2} \dotsm \paren {x^2 + 2 x y \cos \dfrac {2 n \pi} {2 n + 1} + y^2} | c = }} {{end-eqn}}	0
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let $H$ be a [[Definition:Subgroup|subgroup]] of $G$. Let $*: G \times G / H \to G / H$ be the [[Definition:Group Action on Coset Space|action on the (left) coset space]]: :$\forall g \in G, \forall g' H \in G / H: g * \paren {g' H} := \paren {g g'} H$ Then $G$ is a [[Definition:Transitive Group Action|transitive group action]].	0
Let $a \in \Z$. Let $p$ be an [[Definition:Odd Prime|odd prime]]. Let $b = a^{\frac {\paren {p - 1} } 2}$. Then one of the following cases holds: :$b \bmod p = 0$ which happens exactly when $a \equiv 0 \pmod p$, or: :$b \bmod p = 1$ or: :$b \bmod p = p - 1$ where: :$b \bmod p$ denotes the [[Definition:Modulo Operation|modulo operation]] :$x \equiv y \pmod p$ denotes that $x$ is [[Definition:Congruence (Number Theory)|congruent modulo $p$]] to $y$.	0
$b$ is the [[Definition:Smallest Set by Set Inclusion (Class Theory)|smallest element]] of $N$.	0
We use a [[Proof by Contraposition]]. Thus, we show that if a [[Definition:Basis Expansion|basis expansion]] of a [[Definition:Real Number|(real) number]] [[Definition:Termination of Basis Expansion|terminates]] or [[Definition:Recurrence of Basis Expansion|recurs]], then that number is [[Definition:Rational Number|rational]]. Suppose $x \in \R$ were to [[Definition:Termination of Basis Expansion|terminate]] in some [[Definition:Number Base|number base]] $b$. Then (using the notation of that definition): :$\exists k \in \N: f_k = 0$ and so we can express $x$ precisely as: :$x = \left[{s . d_1 d_2 d_3 \ldots d_{k - 1} }\right]_b$ This means: :$x = s + \dfrac {d_1} b + \dfrac {d_2} {b^2} + \dfrac {d_3} {b^3} + \cdots + \dfrac {d_{k - 1} } {b^{k - 1} }$ This is the same as: :$x = \dfrac {s b^{k - 1} + d_1 b^{k - 2} + d_2 b^{k - 3} + \cdots + d_{k - 1} } {b^{k - 1} }$ Both [[Definition:Numerator|numerator]] and [[Definition:Denominator|denominator]] are [[Definition:Integer|integers]] and so $x$ is a [[Definition:Rational Number|rational number]]. Now suppose $x \in \R$ were to [[Definition:Recurring Basis Expansion|recur]] in some [[Definition:Number Base|number base]] $b$. Let $y$ be the [[Definition:Recurring Part of Recurring Basis Expansion|recurring part]] of $x$. That is, $y$ is obtained from $x$ by first subtracting $\floor x$ and then subtracting the [[Definition:Non-Recurring Part of Recurring Basis Expansion|non-recurring part]] of the [[Definition:Recurring Basis Expansion|basis expansion]] (if any). Then we can express $y$ precisely as: :$y = \sqbrk {0 . 0 0 \ldots 0 d_1 d_2 \ldots d_{k - 1} d_1 d_2 \ldots d_{k - 1} \ldots}_b$ for some number $m$ (possibly $m = 0$) of [[Definition:Zero Digit|zeroes]] following the [[Definition:Radix Point|point]]. We define: :$z:= \sqbrk {0 . d_1 d_2 \ldots d_{k - 1} }_b = \dfrac {d_1} b + \dfrac {d_2} {b^2} + \dfrac {d_3} {b^3} + \cdots + \dfrac {d_{k - 1} } {b^{k - 1} }$ Then $z$ is a [[Definition:Rational Number|rational number]]. Furthermore: :$y = \dfrac z {b^m} + \dfrac z {b^{m + k} } + \dfrac z {b^{m + 2 k} } + \cdots = \dfrac z {b^m} \paren {1 + \dfrac 1 {b^k} + \dfrac 1 {b^{2 k } } + \cdots} = \dfrac z {b^m} \cdot \dfrac 1 {1 - \dfrac 1 {b^k} }$ so $y$ is a [[Definition:Rational Number|rational number]] as well. Thus as $x - y$ is [[Definition:Rational Number|rational]], $x$ is also [[Definition:Rational Number|rational]]. {{Qed}} [[Category:Arithmetic]] [[Category:Proofs by Contraposition]] [[Category:Basis Expansions]] 16apnzaix4hlgyx904es6ug7zb69rbo	0
The [[Definition:Quadratic Residue|quadratic residues]] of $p$ are the [[Definition:Integer|integers]] which result from the evaluation of the [[Definition:Square Number|squares]]: : $1^2, 2^2, \ldots, \left({p - 1}\right)^2$ modulo $p$ But: :$r^2 = \left({-r}\right)^2$ and so these $p - 1$ [[Definition:Integer|integers]] fall into [[Definition:Congruence Modulo Integer|congruent]] pairs modulo $p$, namely: {{begin-eqn}} {{eqn | l = 1^2 | o = \equiv | r = \left({p - 1}\right)^2 | rr= \pmod p }} {{eqn | l = 2^2 | o = \equiv | r = \left({p - 2}\right)^2 | rr= \pmod p }} {{eqn | o = \ldots }} {{eqn | l = \left({\frac {p - 1} 2}\right)^2 | o = \equiv | r = \left({\frac {p + 1} 2}\right)^2 | rr= \pmod p | c = Note: we require $p$ to be [[Definition:Odd Integer|odd]] here. }} {{end-eqn}} Therefore each [[Definition:Quadratic Residue|quadratic residue]] of $p$ is [[Definition:Congruence Modulo Integer|congruent modulo $p$]] to one of the $\dfrac {p-1} 2$ [[Definition:Integer|integers]] $1^2, 2^2, \ldots, \left({\dfrac {p-1} 2}\right)^2$. Note that as $r^2 \not \equiv 0 \pmod p$ for $1 \le r < p$, the [[Definition:Integer|integer]] $0$ is not among these. All we need to do now is show that no two of these [[Definition:Integer|integers]] are [[Definition:Congruence Modulo Integer|congruent modulo $p$]]. So, suppose that $r^2 \equiv s^2 \pmod p$ for some $1 \le r \le s \le \dfrac {p-1} 2$. What we are going to do is prove that $r = s$. Now $r^2 \equiv s^2 \pmod p$ means that $p$ is a [[Definition:Divisor of Integer|divisor]] of $r^2 - s^2 = \left({r + s}\right) \left({r - s}\right)$. From [[Euclid's Lemma]] either: :$p \mathrel \backslash \left({r + s}\right)$ or: :$p \mathrel \backslash \left({r - s}\right)$ $p \mathrel \backslash \left({r + s}\right)$ is impossible as $2 \le r + s \le p - 1$. Take $p \mathrel \backslash \left({r - s}\right)$. As $0 \le r - s < \dfrac {p-1} 2$, that can happen only when: :$r - s = 0$ or: :$r = s$ So there must be exactly $\dfrac {p-1} 2$ [[Definition:Quadratic Residue|quadratic residues]]. That means there must also be exactly $\dfrac {p-1} 2$ [[Definition:Quadratic Non-Residue|quadratic non-residues]]. {{qed}} [[Category:Prime Numbers]] [[Category:Quadratic Residues]] gu8df47d8cj4ooq8wlpmxutqwv31zsi	0
The case where $n = 1$ can be taken separately. From [[Binomial Coefficient with Zero]]: :$\dbinom 1 0 = 1$ demonstrating that the result holds for $n = 1$. Let $n \in \N: n > 1$. From the [[Definition:Binomial Coefficient|definition of binomial coefficients]]: :$\dbinom n {n - 1} = \dfrac {n!} {\left({n - 1}\right)! \left({n - \left({n - 1}\right)}\right)!} = \dfrac {n!} {\left({n - 1}\right)! \ 1!}$ the result following directly from the definition of the [[Definition:Factorial|factorial]]. {{qed}}	0
Let $m \in \Z$ and $a \equiv b \pmod z$. Suppose $m = 0$. Then the {{RHS}} of the assertion degenerates to $0 \equiv 0 \pmod z$ which is trivially true. Otherwise, from [[Congruence by Product of Moduli]], we have: :$a \equiv b \iff m a \equiv m b \pmod z$ As $m \in \Z$, it follows that $m z$ is an [[Definition:Integer Multiple|integer multiple]] of $z$. Hence from [[Congruence by Divisor of Modulus]], it follows that: :$m a \equiv m b \implies m a \equiv m b \pmod z$ {{qed}}	0
Let $b \in \R_{>0}$ such that $b \ne 1$. Then: :$\log_b b = 1$ where $\log_b$ denotes the [[Definition:General Logarithm|logarithm]] to [[Definition:Base of Logarithm|base $b$]].	0
From [[Closure of Open Set of Closed Extension Space]] we have that: :$\forall U \in \tau^*_p: U \ne \O \implies U^- = S$ where $U^-$ is the [[Definition:Closure (Topology)|closure]] of $U$. The result then follows by definition of [[Definition:Irreducible Space/Definition 6|irreducible space]]. {{qed}}	0
We are given that $f$ is an [[Definition:Injection|injective mapping]]. Hence by definition $f$ is a [[Definition:One-to-One Relation|one-to-one relation]]. The result follows from from [[Inverse of One-to-One Relation is One-to-One]]. {{Qed}} [[Category:Inverse Relations]] [[Category:Injections]] [[Category:Inverse Mappings]] jujitmkas2e8cfyh5sv1yrpnynt98g1	0
Let $\struct {\R, \tau}$ be the [[Definition:Real Number Line with Euclidean Topology|real number line with the usual (Euclidean) topology]]. Let $\Q$ be the [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]]. Then: :$\Q^- \cap \paren {\R \setminus \Q}^- = \R$ where: :$\R \setminus \Q$ denotes the [[Definition:Set|set]] of [[Definition:Irrational Number|irrational numbers]] :$\Q^-$ denotes the [[Definition:Closure (Topology)|closure]] of $\Q$.	0
For $k \ge 1$, let $f_c^k$ be the [[Definition:Constant Mapping|constant function]] of $k$ variables with value $c$. We know from [[Constant Function is Primitive Recursive]] that $f_c^1$ is [[Definition:Primitive Recursive Function|primitive recursive]]. Now: :$\map {f_c^k} {n_1, n_2, \ldots, n_k} = \map {f_c^1} {n_1} = \map {f_c^1} {\map {\pr_1^k} {n_1, n_2, \ldots, n_k} }$ where $\pr_1^k$ is a [[Definition:Projection Function|projection function]] which is [[Definition:Basic Primitive Recursive Function|basic primitive recursive]]. So $f_c^k$ is obtained from the [[Definition:Primitive Recursive Function|primitive recursive function]] $f_c^1$ and the [[Definition:Basic Primitive Recursive Function|basic primitive recursive function]] $\pr_1^k$ by [[Definition:Substitution (Mathematical Logic)|substitution]]. Hence by definition, $f_c^k$ is [[Definition:Primitive Recursive Function|primitive recursive]]. {{qed}} [[Category:Primitive Recursive Functions]] [[Category:Constant Mappings]] p7103pmh89wjmd8e0mfrg551rg2r1jz	0
=== Existence === First, we prove that such a [[Definition:Subgroup|subgroup]] exists. Let $\mathbb S$ be the [[Definition:Set|set]] of all [[Definition:Subgroup|subgroups]] of $G$ which contain $S$. $\mathbb S \ne \O$ because [[Group is Subgroup of Itself|$G$ is itself a subgroup]] of $G$, and thus $G \in \mathbb S$. Let $H$ be the [[Definition:Set Intersection|intersection]] of all the [[Definition:Element|elements]] of $\mathbb S$. By [[Intersection of Subgroups is Subgroup]], $H$ is the largest element of $\mathbb S$ contained in each element of $\mathbb S$. Thus $H$ is a [[Definition:Subgroup|subgroup]] of $G$. Since $\forall x \in \mathbb S: S \subseteq x$, we see that $S \subseteq H$, so $H \in \mathbb S$. === Smallest === Now to show that $H$ is the smallest such [[Definition:Subgroup|subgroup]]. If any $K \le G: S \subseteq K$, then $K \in \mathbb S$ and therefore $H \subseteq K$. So $H$ is the smallest [[Definition:Subgroup|subgroup]] of $G$ containing $S$. === Uniqueness === Now we show that $H$ is unique. Suppose $\exists H_1, H_2 \in \mathbb S$ such that $H_1$ and $H_2$ were two such smallest [[Definition:Subgroup|subgroups]] containing $S$. Then, by the definition of "smallest", each would be equal in size. If one is not a subset of the other, then their intersection (by definition containing $S$) would be a smaller subgroup and hence neither $H_1$ nor $H_2$ would be the smallest. Hence one must be a [[Definition:Subset|subset]] of the other. By definition of [[Definition:Set Equality/Definition 2|set equality]], that means they must be the same set. So the smallest subgroup, whose existence we have proved above, is unique. {{Qed}}	0
There exists a [[Definition:Subset|subset]] of the [[Definition:Real Number|real numbers]] which is not [[Definition:Measurable Set|measurable]].	0
There exist [[Definition:Irrational Number|irrational numbers]] $a$ and $b$ such that $a^b$ is [[Definition:Rational Number|rational]].	0
By [[Set is Subset of Itself]], the result follows by setting $K = H^-$ in [[Set between Connected Set and Closure is Connected]]. {{qed}}	0
:$\displaystyle \int \frac {\d x} {\paren {p + q \cos a x}^2} = \frac {q \sin a x} {a \paren {q^2 - p^2} \paren {p + q \cos a x} } - \frac p {q^2 - p^2} \int \frac {\mathrm d x} {p + q \cos a x}$	0
We aim to demonstrate that the "Sum of Cubes" is the "Square of the Sum" using simple Multiplication Tables. On the right hand side of the equation, the "Square of the Sum" will be represented by a two dimensional array. The sum of all numbers contained inside the two dimensional (n x n) array is the "Square of the Sum". For example: :$ \paren {1 + 2}^2 = 1 + 2 + 2 + 4$ :$\begin{array}{r|cccccccccc} \paren {1 + 2}^2 & 1 & 2 \\ \hline 1 & 1 & 2 \\ 2 & 2 & 4 \\ \end{array}$ :$ \paren {1 + 2 + 3}^2 = 1 + 2 + 3 + 2 + 4 + 6 + 3 + 6 + 9$ :$\begin{array}{r|cccccccccc} \paren {1 + 2 + 3}^2 & 1 & 2 & 3 \\ \hline 1 & 1 & 2 & 3 \\ 2 & 2 & 4 & 6 \\ 3 & 3 & 6 & 9 \\ \end{array}$ :$ \cdots $ :$ \paren {1 + 2 + 3 + \cdots + N}^2 = $ :$\begin{array}{r|cccccccccc} \paren {\sum_{i \mathop = 1}^n i}^2 & 1 & 2 & 3 & 4 & 5 & 6 & \cdots & N \\ \hline 1 & 1 & 2 & 3 & 4 & 5 & 6 & \cdots & N \\ 2 & 2 & 4 & 6 & 8 & 10 & 12 & \cdots & 2N \\ 3 & 3 & 6 & 9 & 12 & 15 & 18 & \cdots & 3N \\ 4 & 4 & 8 & 12 & 16 & 20 & 24 & \cdots & 4N \\ 5 & 5 & 10 & 15 & 20 & 25 & 30 & \cdots & 5N \\ 6 & 6 & 12 & 18 & 24 & 30 & 36 & \cdots & 6N \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ N & N & 2N & 3N & 4N & 5N & 6N & \cdots & N^2 \\ \end{array}$ From [[Closed Form for Triangular Numbers]]: :$\displaystyle \sum_{i \mathop = 1}^n i = \frac {n \paren {n + 1} } 2$ So the sum of the numbers in the two dimensional n x n array (i.e. "Square of the Sum") is: :$\displaystyle \paren {\sum_{i \mathop = 1}^n i}^2 = \dfrac {n^2 \paren {n + 1}^2} 4$ So where is the "Sum of the Cubes"? Consider the sum of the set of all numbers in each outermost square edge above of length $1$, $2$, $3$, $\ldots$. We have: {{begin-eqn}} {{eqn | l = 1 | r = \paren 2 \paren 1 \paren 1 - 1^2 = 1^3 | c = }} {{eqn | l = 2 + 4 + 2 | r = \paren 2 \paren 2 \paren {1 + 2} - 2^2 = 2^3 | c = }} {{eqn | l = 3 + 6 + 9 + 6 + 3 | r = \paren 2 \paren 3 \paren {1 + 2 + 3} - 3^2 = 3^3 | c = }} {{eqn | o = \cdots }} {{eqn | l = n + 2 n + 3 n + \cdots + \paren {n - 1} \paren n + n^2 + \paren {n - 1} \paren n + \cdots + 2 n + n | r = \paren 2 \paren n \paren {1 + 2 + \cdots + n} - n^2 | c = }} {{eqn | r = 2 n \sum_{i \mathop = 1}^n i - n^2 | c = }} {{eqn | r = 2 n \frac {n \paren {n + 1} } 2 - n^2 | c = [[Closed Form for Triangular Numbers]] }} {{eqn | r = n^3 + n^2 - n^2 | c = }} {{eqn | r = n^3 | c = }} {{end-eqn}} Therefore, The "Square of the Sum" (i.e. the sum of all of the numbers in an n x n multiplication table) is identical to The "Sum of the Cubes" (i.e. the sum of the numbers on each incremental outer square-edge is a cube, sum up the outer edges), or: :$\forall n \in \Z_{>0}: \displaystyle \sum_{i \mathop = 1}^n i^3 = \dfrac {n^2 \paren {n + 1}^2} 4$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \csc 150 \degrees | r = \map \csc {180 \degrees - 30 \degrees} | c = }} {{eqn | r = \csc 30 \degrees | c = [[Cosecant of Supplementary Angle]] }} {{eqn | r = 2 | c = [[Cosecant of 30 Degrees|Cosecant of $30 \degrees$]] }} {{end-eqn}} {{qed}}	0
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $A \in \map B {H, K}$ be a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. Then the [[Definition:Norm on Bounded Linear Transformation|norm]] of $A$ satisfies: :$\norm A^2 = \norm {A^*}^2 = \norm {A^* A}$ where $A^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$.	0
For a [[Definition:Set|set]] $S$, the following conditions are [[Definition:Logical Equivalence|equivalent]]: :$(1): \quad$ $S$ is [[Definition:Dedekind-Infinite|Dedekind-infinite]]. :$(2): \quad$ $S$ has a [[Definition:Countably Infinite|countably infinite]] [[Definition:Subset|subset]]. The above [[Definition:Logical Equivalence|equivalence]] can be proven in [[Definition:Zermelo-Fraenkel Set Theory|Zermelo-Fraenkel set theory]]. If the [[Axiom:Axiom of Countable Choice|axiom of countable choice]] is accepted, then it can be proven that the following condition is also [[Definition:Logical Equivalence|equivalent]] to the above two: :$(3): \quad$ $S$ is [[Definition:Infinite|infinite]].	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $L = \left({\tau, \preceq}\right)$ be an [[Definition:Ordered Set|ordered set]] where $\preceq \mathop = \subseteq\restriction_{\tau \times \tau}$ Let $x \in \tau$. Then :$x$ is [[Definition:Compact Element|compact]] in $L$ (it means: $x \ll x$) {{iff}} :$T_x$ is [[Definition:Compact Topological Subspace|compact (topologically)]] where $T_x = \left({x, \tau_x}\right)$ denotes the [[Definition:Topological Subspace|topological subspace]] of $x$.	0
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $\struct {R, +, *}$ be the [[Definition:Opposite Ring|opposite ring]] of $\struct {R, +, \times}$. Let $\struct {R, +, \circ}$ be the [[Definition:Opposite Ring|opposite ring]] of $\struct {R, +, *}$. Then $\struct {R, +, \circ} = \struct {R, +, \times}$.	0
We have [[Definition:Excluded Set Space|by definition]] that $S \in \tau_{\bar H}$. Also, as $H \cap \O = \O$, we have that $\O \in \tau_{\bar H}$. Now let $U_1, U_2 \in \tau_{\bar H}$. By definition: :$H \cap U_1 = \O$ and: :$H \cap U_2 = \O$ and so by definition of [[Definition:Set Intersection|set intersection]]: :$H \cap \paren {U_1 \cap U_2} = \O$ So: :$U_1 \cap U_2 \in \tau_{\bar H}$ Now let $\UU \subseteq \tau_{\bar H}$. We have that: :$\forall U \in \UU: H \cap U = \O$ Hence from [[Subset of Union]]: :$H \cap \bigcup \UU = \O$ So all the conditions are fulfilled for $\tau_{\bar H}$ to be a [[Definition:Topology|topology]] on $S$. {{qed}}	0
{{begin-eqn}} {{eqn | l = 0 | o = \divides | r = n | c = }} {{eqn | ll= \leadsto | l = \exists q \in \Z: n | r = q \times 0 | c = {{Defof|Divisor of Integer}} }} {{eqn | ll= \leadsto | l = n | r = 0 | c = Integers have no [[Definition:Zero Divisor of Ring|zero divisors]], as [[Integers form Integral Domain]]. }} {{end-eqn}} {{Qed}}	0
=== Necessary Condition === {{begin-eqn}} {{eqn | l = x | o = \prec | r = y | c = }} {{eqn | ll= \implies | l = e | r = x^{-1} \circ x \prec x^{-1} \circ y | c = [[Strict Ordering Preserved under Product with Cancellable Element]] }} {{eqn | ll= \implies | l = y^{-1} | r = e \circ y^{-1} \prec x^{-1} \circ y \circ y^{-1} = x^{-1} | c = }} {{eqn | ll= \implies | l = y^{-1} | o = \prec | r = x^{-1} | c = }} {{end-eqn}} {{qed|lemma}} === Sufficient Condition === {{begin-eqn}} {{eqn | l = y^{-1} | o = \prec | r = x^{-1} | c = }} {{eqn | ll= \implies | l = x | r = \left({x^{-1} }\right)^{-1} \prec \left({y^{-1} }\right)^{-1} = y | c = }} {{eqn | ll= \implies | l = x | o = \prec | r = y | c = }} {{end-eqn}} {{qed}}	0
Let $I = \left[{a \,.\,.\, b}\right]$. Let $S \subseteq I$. If $S = \varnothing$, then it has a [[Definition:Supremum of Set|supremum]] in $I$ of $a$ and an [[Definition:Infimum of Set|infimum]] in $I$ of $b$. Let $S \ne \varnothing$. Since $S \subseteq I$, $a$ is a [[Definition:Lower Bound of Set|lower bound]] of $S$ and $b$ is an [[Definition:Upper Bound of Set|upper bound]] of $S$. Since $L$ is a [[Definition:Complete Lattice|complete lattice]], $S$ has an [[Definition:Infimum of Set|infimum]], $p$, and a [[Definition:Supremum of Set|supremum]], $q$, in $L$. Thus by the definitions of [[Definition:Infimum of Set|infimum]] and [[Definition:Supremum of Set|supremum]]: : $a \preceq p$ and $q \preceq b$ Let $x \in S$. Since an [[Definition:Infimum of Set|infimum]] is a [[Definition:Lower Bound of Set|lower bound]]: : $p \preceq x$ Since a [[Definition:Supremum of Set|supremum]] is an [[Definition:Upper Bound of Set|upper bound]]: : $x \preceq q$ Thus $a \preceq p \preceq x \preceq q \preceq b$. Since $\preceq$ is an [[Definition:Ordering|ordering]], it is [[Definition:Transitive Relation|transitive]], so by [[Transitive Chaining]]: :$a \preceq p \preceq b$ and $a \preceq q \preceq b$. That is, $p, q \in I$. Thus $p$ and $q$ are the [[Definition:Infimum of Set|infimum]] and [[Definition:Supremum of Set|supremum]] of $S$ in $I$. As every subset of $I$ has a [[Definition:Supremum of Set|supremum]] and [[Definition:Infimum of Set|infimum]] in $I$, $I$ is a [[Definition:Complete Lattice|complete lattice]]. {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Uncountable Finite Complement Topology|finite complement topology]] on an [[Definition:Uncountable Set|uncountable set]] $S$. Let $U$ be an [[Definition:Open Set (Topology)|open set]] of $T$. From [[Open Set of Uncountable Finite Complement Topology is not F-Sigma|Closed Set of Uncountable Finite Complement Topology is not $F_\sigma$]]: :$U$ is not an [[Definition:F-Sigma Set|$F_\sigma$ set]] of $T$. Hence the result. {{qed}}	0
Let $a, b \in S$. Then: {{begin-eqn}} {{eqn | l = a \vee \paren {a \wedge b} | r = \paren {a \wedge \top} \vee \paren {a \wedge b} | c = [[Definition:Boolean Algebra/Axioms/Definition 1|Boolean Algebra: Axiom $(BA_1 \ 3)$]]: $\top$ is the [[Definition:Identity Element|identity]] for $\wedge$ }} {{eqn | r = a \wedge \paren {\top \vee b} | c = [[Definition:Boolean Algebra/Axioms/Definition 2|Boolean Algebra: Axiom $(BA_1 \ 2)$]]: $\wedge$ [[Definition:Distributive Operation|distributes]] over $\vee$ }} {{eqn | r = a \wedge \top | c = [[Identities of Boolean Algebra also Zeroes]] }} {{eqn | r = a | c = [[Definition:Boolean Algebra/Axioms/Definition 1|Boolean Algebra: Axiom $(BA_1 \ 3)$]] $\top$ is the [[Definition:Identity Element|identity]] for $\wedge$ }} {{end-eqn}} as desired. {{qed|lemma}} The result: :$a = a \wedge \paren {a \vee b}$ follows from the [[Duality Principle (Boolean Algebras)|Duality Principle]]. {{qed}}	0
=== $(2)$ implies $(1)$ === Follows from [[Upper Set is Convex]], [[Lower Set is Convex]], and [[Intersection of Convex Sets is Convex Set (Order Theory)]]. {{qed|lemma}} === $(1)$ implies $(3)$ === {{MissingLinks}} Let $C$ be a convex set in $S$. Let $U$ and $L$ be the upper and lower closures of $C$, respectively. Since $C \subseteq U$ and $C \subseteq L$: :$C \subseteq U \cap L$. Let $p \in U \cap L$. Then $a \preceq p \preceq b$ for some $a, b \in C$. Since $C$ is convex, $p \in C$. Since this holds for all $p \in U \cap L$: :$U \cap L \subseteq C$. Since we know that $C \subseteq U \cap L$, $C = U \cap L$. {{qed|lemma}} === $(3)$ implies $(2)$ === Follows from [[Upper Closure is Upper Set]] and [[Lower Closure is Lower Set]]. {{qed}} [[Category:Convex Sets]] 2rudyxkfvqqz8vkpr8bjamysr5o11lu	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. We have that: :$\forall a \in G: e \circ a = a \circ e \implies e \in \map {C_G} a$ Thus $\map {C_G} a \ne \O$. Let $x, y \in \map {C_G} a$. Then from [[Commutation with Group Elements implies Commuation with Product with Inverse]]: :$a \circ x \circ y^{-1} = x \circ y^{-1} \circ a$ so: :$x \circ y^{-1} \in\map {C_G} a$ The result follows by the [[One-Step Subgroup Test]], the result follows. {{qed}}	0
Let $S$ be a [[Definition:Finite Set|finite set]]. Then the [[Definition:Power Set|power set]] of $S$ is likewise [[Definition:Finite Set|finite]].	0
Let $G$ be a [[Definition:Finite Group|finite group]] of [[Definition:Order of Group|order]] $p^n m$, where $p \nmid m$ and $n > 0$. Let $H$ be a [[Definition:Sylow p-Subgroup|Sylow $p$-subgroup]] of $G$. We have that: :$\order H = p^n$ :$\index G H = m$ Let $S_1, S_2, \ldots, S_m$ denote the [[Definition:Left Coset|left cosets]] of $G \pmod H$. We have that $G$ [[Definition:Group Action|acts on]] $G / H$ by the rule: :$g * S_i = g S_i$. Let $H_i$ denote the [[Definition:Stabilizer|stabilizer]] of $S_i$. By the [[Orbit-Stabilizer Theorem]]: :$\order {H_i} = p^n$ while: :$S_i = g H \implies g H g^{-1} \subseteq H_i$ Because $\order {g H g^{-1} } = \order H = \order {H_i}$, we have: :$g H g^{-1} \subseteq H_i$ Let $H'$ be a second [[Definition:Sylow p-Subgroup|Sylow $p$-subgroup]] of $G$. Then $H'$ acts on $G / H$ by the same rule as $G$. Since $p \nmid m$, there exists at least one [[Definition:Orbit (Group Theory)|orbit]] under $H'$ whose [[Definition:Cardinality|cardinality]] is not [[Definition:Divisor of Integer|divisible]] by $p$. Suppose that $S_1, S_2, \ldots, S_r$ are the [[Definition:Element|elements]] of an [[Definition:Orbit (Group Theory)|orbit]] where $p \nmid r$. Let $K = H' \cap H_1$. Then $K$ is the [[Definition:Stabilizer|stabilizer]] of $S_1$ under the action of $H'$. Therefore: :$\index {H'} K = r$ However: :$\order {H'} = p^n$ and: :$p \nmid r$ from which it follows that: :$r = 1$ and: :$K = H'$ Therefore: :$\order K = \order {H'} = \order {H_1} = p^n$ and: :$H' = K = H_1$ Thus $H'$ and $H$ are [[Definition:Conjugate of Group Subset|conjugates]].	0
Let $a, b \in \Z, m \in \N$. Let $a$ be [[Definition:Congruence Modulo Integer|congruent]] to $b$ modulo $m$. Then the [[Definition:Greatest Common Divisor of Integers|GCD]] of $a$ and $m$ is equal to the [[Definition:Greatest Common Divisor of Integers|GCD]] of $b$ and $m$. That is: :$a \equiv b \pmod m \implies \gcd \set {a, m} = \gcd \set {b, m}$	0
{{begin-eqn}} {{eqn | l = y | o = \in | r = f \sqbrk {S_1} \setminus f \sqbrk {S_2} | c = }} {{eqn | ll= \leadsto | l = \exists x \in {S_1}: x \notin {S_2}: \tuple {x, y} | o = \in | r = f | c = {{Defof|Image of Subset under Mapping}} }} {{eqn | ll= \leadsto | l = \exists x \in {S_1} \setminus {S_2}: \tuple {x, y} | o = \in | r = f | c = {{Defof|Set Difference}} }} {{eqn | ll= \leadsto | l = y | o = \in | r = f \sqbrk {S_1 \setminus S_2} | c = {{Defof|Image of Subset under Mapping}} }} {{end-eqn}} {{qed}}	0
:$\sin 225 \degrees = \sin \dfrac {5 \pi} 4 = -\dfrac {\sqrt 2} 2$	0
Let $\gen x$ be the [[Definition:Generated Subgroup|subgroup of $G$ generated]] by $x$. By definition, $\gen x \le G$. All the [[Definition:Element|elements]] of $\gen x$ are [[Definition:Power of Group Element|powers]] of $x$. As $x \in H$ it follows by [[Definition:Group Axioms|group axiom $G0$: closure]] that all the [[Definition:Power of Group Element|powers]] of $x$ are [[Definition:Element|elements]] of $H$. That is: :$\gen x \le H$ By [[Order of Cyclic Group equals Order of Generator]]: :$\order x = \order {\gen x}$ But $\gen x$ is the same [[Definition:Cyclic Group|cyclic group]] whether it is considered as embedded in $H$ or $G$. Hence the result. {{qed}}	0
=== [[Definition:Ordering/Definition 1|Definition 1]] implies [[Definition:Ordering/Definition 2|Definition 2]] === Let $\RR$ be a [[Definition:Relation|relation]] on $S$ satisfying: {{begin-axiom}} {{axiom | n = 1 | lc= $\RR$ is [[Definition:Reflexive Relation|reflexive]] | q = \forall a \in S | m = a \mathrel \RR a }} {{axiom | n = 2 | lc= $\RR$ is [[Definition:Transitive Relation|transitive]] | q = \forall a, b, c \in S | m = a \mathrel \RR b \land b \mathrel \RR c \implies a \mathrel \RR c }} {{axiom | n = 3 | lc= $\RR$ is [[Definition:Antisymmetric Relation|antisymmetric]] | q = \forall a \in S | m = a \mathrel \RR b \land b \mathrel \RR a \implies a = b }} {{end-axiom}} ==== Condition $(1)$ ==== Let $\tuple {x, y} \in \RR \circ \RR$. Then there exists a $z \in \RR$ such that: :$\tuple {x, z}, \tuple {z, y} \in \RR$ By $\RR$ being [[Definition:Transitive Relation|transitive]]: :$\tuple {x, y} \in \RR$ Hence: :$\RR \circ \RR \subseteq \RR$ Now let $\tuple {x, y} \in \RR$. By $\RR$ being [[Definition:Reflexivity|reflexive]]: :$\tuple {y, y} \in \RR$ Hence by the definition of [[Definition:Composition of Relations|relation composition]]: :$\tuple {x, y} \in \RR \circ \RR$ Hence: :$\RR \subseteq \RR \circ \RR$ ==== Condition $(2)$ ==== Follows immediately from [[Relation is Antisymmetric iff Intersection with Inverse is Coreflexive]] and $\RR$ being [[Definition:Reflexive Relation|reflexive]]. Thus $\RR$ is an [[Definition:Ordering/Definition 2|ordering by definition 2]]. {{qed|lemma}} === [[Definition:Ordering/Definition 2|Definition 2]] implies [[Definition:Ordering/Definition 1|Definition 1]] === Let $\RR$ be a [[Definition:Relation|relation]] which fulfils the conditions: :$(1): \quad \RR \circ \RR = \RR$ :$(2): \quad \RR \cap \RR^{-1} = \Delta_S$ ==== Reflexivity ==== By [[Intersection is Subset]] the condition: :$\RR \cap \RR^{-1} = \Delta_S$ implies: :$\Delta_S \subseteq \RR$ Thus $\RR$ is [[Definition:Reflexive Relation/Definition 2|reflexive]] by definition. ==== Antisymmetry ==== By [[Relation is Antisymmetric iff Intersection with Inverse is Coreflexive]] the condition: :$\RR \cap \RR^{-1} = \Delta_S$ implies that $\RR$ is [[Definition:Antisymmetric Relation|antisymmetric]]. ==== Transitivity ==== Let $\tuple {x, y}, \tuple {y, z} \in \RR$. Then by the definition of [[Definition:Composition of Relations|relation composition]]: :$\tuple {x, z} \in \RR \circ \RR$ But by the condition: :$\RR \circ \RR = \RR$ It follows that: :$\tuple {x, z} \in \RR$ Hence $\RR$ is [[Definition:Transitive Relation|transitive]]. Thus $\RR$ is an [[Definition:Ordering/Definition 1|ordering by definition 1]]. {{qed}}	0
Let $\R$ be the [[Definition:Real Number|set of real numbers]]. Let $d: \R \times \R \to \R$ be the [[Definition:Euclidean Plus Metric|Euclidean plus metric]]: :$\map d {x, y} := \size {x - y} + \displaystyle \sum_{i \mathop = 1}^\infty 2^{-i} \map \inf {1, \size {\max_{j \mathop \le i} \frac 1 {\size {x - r_j} } - \max_{j \mathop \le i} \frac 1 {\size {y - r_j} } } }$ Let $d': \R \times \R \to \R$ be the [[Definition:Euclidean Metric on Real Number Plane|Euclidean metric]]. Let $\epsilon \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|(strictly) positive real number]]. Let $\map {B_\epsilon} {p; d}$ be an [[Definition:Open Ball of Metric Space|open $\epsilon$-ball]] of $p$ in $\R$ on $d$. Let $\map {B'_\epsilon} {p; d'}$ be an [[Definition:Open Ball of Metric Space|open $\epsilon$-ball]] of $p$ in $\R$ on $d'$. Then: :$\map {B_\epsilon} {p; d} \subseteq \map {B'_\epsilon} {p; d'}$	0
Let $m \ne n$. {{begin-eqn}} {{eqn | l = \int \sin m x \sin n x \rd x | r = \frac {\sin \paren {m - n} x} {2 \paren {m - n} } - \frac {\sin \paren {m + n} x} {2 \paren {m + n} } + C | c = [[Primitive of Sine of p x by Sine of q x|Primitive of $\sin m x \sin n x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \sin n x \rd x | r = \intlimits {\frac {\sin \paren {m - n} x} {2 \paren {m - n} } - \frac {\sin \paren {m + n} x} {2 \paren {m + n} } } 0 \pi | c = }} {{eqn | r = \paren {\frac {\sin \paren {m - n} \pi} {2 \paren {m - n} } - \frac {\sin \paren {m + n} \pi} {2 \paren {m + n} } } | c = }} {{eqn | o = | ro= - | r = \paren {\frac {\sin 0} {2 \paren {m - n} } - \frac {\sin 0} {2 \paren {m + n} } } | c = }} {{eqn | r = \frac {\sin \paren {m - n} \pi} {2 \paren {m - n} } - \frac {\sin \paren {m + n} \pi} {2 \paren {m + n} } | c = [[Sine of Zero is Zero]] }} {{eqn | r = 0 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} {{qed|lemma}} When $m = n$ we have: {{begin-eqn}} {{eqn | l = \int \sin m x \sin m x \rd x | r = \int \sin^2 m x \rd x | c = }} {{eqn | r = \frac x 2 - \frac {\sin 2 m x} {4 m} + C | c = [[Primitive of Square of Sine of a x|Primitive of $\sin^2 m x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \sin m x \rd x | r = \intlimits {\frac x 2 - \frac {\sin 2 m x} {4 m} } 0 \pi | c = }} {{eqn | r = \paren {\frac \pi 2 - \frac {\sin \paren {2 m \pi} } {4 m} } - \paren {\frac 0 2 - \frac {\sin 0} {4 m} } | c = }} {{eqn | r = \frac \pi 2 - \frac {\sin \paren {2 m \pi} } {4 m} | c = [[Sine of Zero is Zero]] }} {{eqn | r = \frac \pi 2 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} {{qed}}	0
Let $X$ be a [[Definition:Topological Space|topological space]]. Let $V$ be a [[Definition:Normed Vector Space|normed vector space]] over $\R$ or $\C$ with [[Definition:Norm on Vector Space|norm]] $\norm {\,\cdot\,}$. Let $f, g, h: X \to V$ be functions. Let $x_0 \in X$. Let $f = \map \OO g$ and $g = \map \OO h$ as $x \to x_0$, where $\OO$ denotes [[Definition:Big-O Notation|big-O notation]]. Then $f = \map \OO h$ as $x \to x_0$.	0
As $\struct {F, +, \circ}$ is a [[Definition:Field (Abstract Algebra)|field]], it is also by definition a [[Definition:Ring (Abstract Algebra)|ring]]. Thus from [[Polynomial Ring of Sequences is Ring]] we have that $\struct {\GF, \oplus, \otimes}$ is a [[Definition:Ring (Abstract Algebra)|ring]]. {{explain|Use an analogous result to [[Ring of Polynomial Forms is Commutative Ring with Unity]] to get the CRU bit done}} From [[Field is Integral Domain]], a [[Definition:Field (Abstract Algebra)|field]] is also by definition an [[Definition:Integral Domain|integral domain]]. Let $f, g \in \GF$ such that neither $f$ nor $g$ are the [[Definition:Null Polynomial over Sequence|null polynomial]]. Let: :$\deg f = m, \deg g = n$ where $\deg$ denotes the [[Definition:Degree of Polynomial over Field as Sequence|degree]] of $f$ and $g$ respectively. By [[Degree of Product of Polynomials over Integral Domain]], the [[Definition:Degree of Polynomial over Field as Sequence|degree]] of $f \times g$ is $m + n$. Then by definition of [[Definition:Multiplication of Polynomials over Field as Sequence|polynomial multiplication]], its [[Definition:Leading Coefficient of Polynomial|leading coefficient]] is $a_m \circ b_n$. As by definition an [[Definition:Integral Domain|integral domain]] has no [[Definition:Proper Zero Divisor|proper zero divisors]]: :$a_m \circ b_n \ne 0_F$. So, by definition, $f \otimes g$ has a [[Definition:Leading Coefficient of Polynomial|leading coefficient]] which is not $0_F$. That is, $f \otimes g$ is not the [[Definition:Null Polynomial over Sequence|null polynomial]] The result follows by definition of [[Definition:Integral Domain|integral domain]].	0
Let $\mathbf{Set}$ be the [[Definition:Category of Sets|category of sets]]. Let $S = \left\{{x}\right\}$ be any [[Definition:Singleton|singleton set]]. Then $S$ is a [[Definition:Terminal Object|terminal object]] of $\mathbf{Set}$.	0
Denote with $\le$ the [[Definition:Ordering on Extended Real Numbers|usual ordering]] on the [[Definition:Extended Real Number Line|extended real numbers]] $\overline \R$. Then $\le$ is an [[Definition:Ordering|ordering]], and so $\overline \R$ is an [[Definition:Ordered Set|ordered set]].	0
Let $\phi$ be an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]]. Then by definition $\phi$ is a [[Definition:Bijection|bijection]]. Thus $\exists \phi^{-1}$ such that $\phi^{-1}$ is also a [[Definition:Bijection|bijection]] from [[Bijection iff Inverse is Bijection]]. That is: :$\exists \phi^{-1}: \left({T, *}\right) \to \left({S, \circ}\right)$ It follows that: {{begin-eqn}} {{eqn | ll=\forall s \in S, t \in T: | o= | r=\phi \left({s}\right) = t \iff \phi^{-1} \left({t}\right) = s | c=[[Inverse Element of Bijection]] }} {{eqn | o=\implies | r=\phi \left({s_1 \circ s_2}\right) = t_1 * t_2 | c=[[Definition:Morphism Property|Morphism Property]] }} {{eqn | o=\implies | r=\phi^{-1} \left({t_1 * t_2}\right) = s_1 \circ s_2 = \phi^{-1} \left({t_1}\right) \circ \phi^{-1} \left({t_2}\right) | c=[[Inverse Element of Bijection]] }} {{end-eqn}} So $\phi^{-1}: \left({T, *}\right) \to \left({S, \circ}\right)$ is a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]]. $\phi^{-1}$ is also (from above) a [[Definition:Bijection|bijection]]. Thus, by definition, $\phi^{-1}$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]]. Let $\phi^{-1}: \left({T, *}\right) \to \left({S, \circ}\right)$ be an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]]. Applying the same result as above in reverse, we have that $\left({\phi^{-1}}\right)^{-1}: \left({S, \circ}\right) \to \left({T, *}\right)$ is also an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]]. But by [[Inverse of Inverse of Bijection]]: : $\left({\phi^{-1}}\right)^{-1} = \phi$ and hence the result. {{qed}}	0
By [[Subset Relation is Compatible with Subset Product]], the [[Definition:Subset|subset]] relation $\subseteq$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ_{\mathcal P}$. From [[Inverse of Subset Relation is Superset]], the [[Definition:Inverse Relation|inverse]] of $\subseteq$ is $\supseteq$. The result follows from [[Inverse of Relation Compatible with Operation is Compatible]]. {{qed}} [[Category:Compatible Relations]] 0ld0w22afe9z6m2t6oiestk41kajtld	0
Define [[Definition:Relation|relation]] $\mathcal R$ on $S$: :$\forall x, y \in S: \left({x, y}\right) \in \mathcal R \iff x \in f\left({y}\right)$ By definition of [[Definition:Relation Segment|$\mathcal R$-segment]]: :$\forall x \in S: f\left({x}\right) = x^{\mathcal R}$ We will prove that :$\mathcal R$ is an [[Definition:Approximating Relation|approximating relation]]. Let $x \in S$. Suppose the case holds that: :$x \preceq \sup I$ Thus: {{begin-eqn}} {{eqn | l = \sup \left({x^{\mathcal R} }\right) | r = \sup f \left({x}\right) }} {{eqn | r = \sup \left\{ {x \wedge d: d \in I}\right\} | c = definition of $f$ }} {{eqn | r = x \wedge \sup I | c = {{Defof|Meet-Continuous Lattice}} }} {{eqn | r = x | c = [[Preceding iff Meet equals Less Operand]] }} {{end-eqn}} Suppose the case holds that :$x \npreceq \sup I$ Thus {{begin-eqn}} {{eqn | l = \sup \left({x^{\mathcal R} }\right) | r = \sup f \left({x}\right) }} {{eqn | r = \sup \left({x^\preceq}\right) | c = definition of $f$ }} {{eqn | r = x | c = [[Supremum of Lower Closure of Element]] }} {{end-eqn}} {{qed}}	0
Let $F / K$ be a [[Definition:Field Extension|field extension]]. Let $\alpha \in F$ be [[Definition:Algebraic Field Extension|algebraic]] over $K$. Let $\mu_\alpha$ be the [[Definition:Minimal Polynomial|minimal polynomial]] of $\alpha$ over $K$. Let $K \left[{\alpha}\right]$ (resp. $K \left({\alpha}\right)$) be the [[Definition:Subring|subring]] (resp. [[Definition:Subfield|subfield]]) of $F$ [[Definition:Generator of Subfield|generated]] by $K \cup \left\{ {\alpha}\right\}$. Then: :$K \left[{\alpha}\right] = K \left({\alpha}\right) \simeq K \left[{X}\right] / \left\langle{\mu_\alpha}\right\rangle$ where $\left\langle{\mu_\alpha}\right\rangle$ is the [[Definition:Ideal of Ring|ideal]] of the [[Definition:Ring of Polynomial Functions|ring of polynomial functions]] [[Definition:Generator|generated]] by $\mu_\alpha$. {{explain|Link to Generator needs refining}} Moreover: :$n := \left[{K \left({\alpha}\right) : K}\right] = \deg \mu_\alpha$ and: :$1, \alpha, \dotsc, \alpha^{n - 1}$ is a [[Definition:Basis (Linear Algebra)|basis]] of $K \left({\alpha}\right)$ over $K$.	0
{{begin-eqn}} {{eqn | l=\frac 1 {1 - \sin x} - \frac 1 {1 + \sin x} | r=\frac {1 + \sin x} {1 - \sin^2 x} - \frac {1 - \sin x} {1 - \sin^2 x} | c=[[Difference of Two Squares]] }} {{eqn | r=\frac {2 \sin x} {\cos^2 x} |c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r=\frac {2 \tan x} {\cos x} | c=[[Tangent is Sine divided by Cosine]] }} {{eqn | r=2 \tan x \ \sec x | c=[[Secant is Reciprocal of Cosine]] }} {{end-eqn}} {{qed}} [[Category:Trigonometric Identities]] [[Category:Sine Function]] kr8f565ecjdh42ma8fnbv7d6yd46ffh	0
Let $A$ be a [[Definition:Class (Class Theory)|class]] satisfying the following conditions: * $\varnothing \in A$ * $\forall x \in A: x^+ \in A$ * If $y$ is a [[Definition:Limit Ordinal|limit ordinal]], then $\left({ \forall x < y: x \in A }\right) \implies y \in A$ where $x^+$ denotes the [[Definition:Successor Set|successor]] of $x$. Then $\operatorname{On} \subseteq A$.	0
Let $G$ be a [[Definition:Group|group]] of [[Definition:Order of Structure|order]] $p^3$, where $p$ is a [[Definition:Prime Number|prime]]. Let $\map Z G$ be the [[Definition:Center of Group|center]] of $G$. Then $\order {\map Z G} \ne p^2$.	0
Let $\struct {S, \circ}$ be a [[Definition:Monoid|monoid]] whose [[Definition:Identity Element|identity element]] is $e$. For $a \in S$, let $\circ^n a = a^n$ be the [[Definition:Power of Element of Magma with Identity|$n$th power of $a$]]. Then: :$\forall m, n \in \N: a^{n m} = \paren {a^n}^m = \paren {a^m}^n$	0
:$\xymatrix { X \ar[d]_\iota \ar[rd]^{\forall f} & \\ F_X \ar[d]_\pi \ar[r]^{\exists ! g} & H \\ F_X^{\mathrm {ab} } \ar[ru]_{\exists ! h} }$ Lef $H$ be any [[Definition:Abelian Group|abelian group]] and $f:X \to H$ be any mapping. By [[Definition:Universal Property of Free Group on Set|Universal Property of Free Group on Set]], there exists a [[Definition:Unique|unique]] [[Definition:Group Homomorphism|group homomorphism]] $g:F_X \to H$ such that $g \circ \iota = f$. By [[Universal Property of Abelianization of Group]], there exists a [[Definition:Unique|unique]] [[Definition:Group Homomorphism|group homomorphism]] $h:F_X^{\mathrm {ab} } \to H$ such that $h \circ \pi = g$. Therefore, $h \circ (\pi \circ \iota) = f$. {{Handwaving|Actually we have to show that $\pi \circ \iota$ is the natural inclusion from $X$ to $F_X^{\mathrm {ab} }$...}} Hence, $F_X^{\mathrm {ab} }$ satisfies the [[Universal Property of Free Abelian Group on Set]]. Hence $F_X^{\mathrm {ab} }$ is a Free Abelian Group on $X$. {{qed}} [[Category:Group Theory]] hq65lur9bypni916vpjsxgl3gmdoo43	0
:$\tan 195^\circ = \tan \dfrac {13 \pi} {12} = 2 - \sqrt 3$	0
A [[Definition:Subset|subset]] $X \subseteq S \times D$ is [[Definition:Closed Set (Topology)|closed]] in $\tau$ {{iff}} for some [[Definition:Closed Set (Topology)|closed set]] $C$ of $\tau$: :$X = C \times D$	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $x \in S$. Then $x$ is an [[Definition:Isolated Point (Topology)|isolated point]] of the [[Definition:Singleton|singleton set]] $\left\{{x}\right\}$, but not necessarily an [[Definition:Isolated Point (Topology)|isolated point of $T$]].	0
Suppose $\mathbf a + \mathbf 1 = \mathbf b + \mathbf 1$. Then from [[Condition for Existence of Cardinal Sum]] there exists some [[Definition:Cardinal|cardinal]] $\mathbf c$ such that :$\mathbf a + \mathbf 1 = \mathbf b + \mathbf 1 = \mathbf c$ By definition of [[Definition:Cardinal|cardinal]] there exists a [[Definition:Set|set]] $C$ such that $\mathbf c = \map \Card C$. $C$ also has [[Definition:Subset|subsets]] $A$ and $B$ such that there also exist [[Definition:Element|elements]] of $\alpha, \beta \in C$ such that: :$\relcomp C A = \set \alpha$ :$\relcomp C B = \set \beta$ If $\alpha = \beta$ then $A = B$ and so $\mathbf a = \mathbf b$. On the other hand, if $\alpha \ne \beta$ then $\alpha \in B$ and $\beta \in A$ and so: :$\set \beta \cup \paren {A \cap B} = A$ :$\set \alpha \cup \paren {A \cap B} = B$ Since: :$\set \beta \cap A \cap B = \O = \set \alpha \cap A \cap B$ it follows that: :$\mathbf a = \map \Card A = \mathbf 1 + \map \Card {A \cap B} = \map \Card B = \mathbf b$ {{qed}}	0
Follows directly from the [[Definition:Group Axioms|group axioms]]. {{Qed}}	0
Let $m \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\Z_m$ be the [[Definition:Set of Residue Classes|set of residue classes modulo $m$]]. Then: :$\card {Z_m} = m$ where $\card { \, \cdot \,}$ denotes [[Definition:Cardinality|cardinality]].	0
Proof by [[Principle of Mathematical Induction|induction]]: Let $n \in \Z$. For all $m \in \N$, let $\map P m$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{j \mathop = 0}^m \binom {n + j} n = \binom {n + m + 1} {n + 1}$ $\map P 0$ is true, as this just says: :$\dbinom n n = \dbinom {n + 1} {n + 1}$ But $\dbinom n n = \dbinom {n + 1} {n + 1} = 1$ from the {{Defof|Binomial Coefficient}}. === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^1 \binom {n + j} n | r = \binom n n + \binom {n + 1} n | c = }} {{eqn | r = 1 + \paren {n + 1} | c = {{Defof|Binomial Coefficient}} }} {{eqn | r = n + 2 | c = }} {{eqn | r = \binom {n + 2} {n + 1} | c = {{Defof|Binomial Coefficient}} }} {{end-eqn}} So: :$\displaystyle \sum_{j \mathop = 0}^1 \binom {n + j} n = \binom {n + 2} {n + 1}$ and $\map P 1$ is seen to hold. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 0}^k \binom {n + j} n = \binom {n + k + 1} {n + 1}$ Then we need to show: :$\displaystyle \sum_{j \mathop = 0}^{k+1} \binom {n + j} n = \binom {n + k + 2} {n + 1}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^{k + 1} \binom {n + j} n | r = \sum_{j \mathop = 0}^k \binom {n + j} n + \binom {n + k + 1} n | c = }} {{eqn | r = \binom {n + k + 1} {n + 1} + \binom {n + k + 1} n | c = [[Rising Sum of Binomial Coefficients#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \binom {n + k + 2} {n + 1} | c = [[Pascal's Rule]] }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \sum_{j \mathop = 0}^m \binom {n + j} n = \binom {n + m + 1} {n + 1}$ Finally, from [[Symmetry Rule for Binomial Coefficients]]: :$\dbinom {n + m + 1} {n + 1} = \dbinom {n + m + 1} m$ {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {\cos^2 a x} = \frac {\tan a x} a + C$	0
:$\tan 105^\circ = \tan \dfrac {7 \pi} {12} = - \left({2 + \sqrt 3}\right)$	0
From the definition of a [[Definition:Surjection|surjection]]: :$\forall t \in T: \exists s \in S: \map f s = t$ Thus there are as many [[Definition:Equivalence Class|$\mathcal R_f$-classes]] of $f$ as there are [[Definition:Element|elements]] of $T$. Hence the result. {{qed}}	0
By [[Group of Order 15 is Cyclic Group]], any [[Definition:Subgroup|subgroup]] of $G$ of [[Definition:Order of Group|order]] $15$ is [[Definition:Cyclic Group|cyclic]]. It remains to be proved that a [[Definition:Subgroup|subgroup]] of $G$ of [[Definition:Order of Group|order]] $15$ exists, and that it is [[Definition:Normal Subgroup|normal]]. Let $n_3$ denote the number of [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]] of $G$. From the [[Fourth Sylow Theorem]]: :$n_3 \equiv 1 \pmod 3$ and from the [[Fifth Sylow Theorem]]: :$n_3 \divides 10$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. It follows that $n_3 \in \set {1, 10}$. Let $n_5$ denote the number of [[Definition:Sylow p-Subgroup|Sylow $5$-subgroups]] of $G$. From the [[Fourth Sylow Theorem]]: :$n_5 \equiv 1 \pmod 5$ and from the [[Fifth Sylow Theorem]]: :$n_5 \divides 30$ It follows that $n_5 \in \set {1, 6}$. Suppose $n_3 = 1$. Let $P$ denote the [[Definition:Unique|unique]] [[Definition:Sylow p-Subgroup|Sylow $3$-subgroup]] of $G$. From [[Sylow p-Subgroup is Unique iff Normal|Sylow $p$-Subgroup is Unique iff Normal]], $P$ is [[Definition:Normal Subgroup|normal]]. Thus we may form the [[Definition:Quotient Group|quotient group]] $G / P$, which is of [[Definition:Order of Group|order $10$]]. From [[Groups of Order 2p]], $G / P$ has a [[Definition:Unique|unique]] [[Definition:Sylow p-Subgroup|Sylow $5$-subgroup]], which we will denote $N / P$. By the [[Correspondence Theorem (Group Theory)|Correspondence Theorem]], $N$ is a [[Definition:Normal Subgroup|normal subgroup]] of $G$ with $15$ [[Definition:Element|elements]]. As noted, $N$ is [[Definition:Cyclic Group|cyclic]]. {{qed|lemma}} Suppose $n_3 = 10$. Let the [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]] of $G$ be denoted $P_1, P_2, \ldots, P_{10}$. Each [[Definition:Set Intersection|intersection]] $P_i \cap P_j$ for $i, j \in \set {1, 2, \ldots, 10}, i \ne j$ is the [[Definition:Trivial Subgroup|trivial subgroup]] of $G$: :$P_i \cap P_j = \set e$ Thus $G$ contains: :The [[Definition:Identity Element|identity element]] $e$ :$20$ [[Definition:Element|elements]] of [[Definition:Order of Group Element|order]] $3$, of which $2$ each are in $P_1, P_2, \ldots, P_{10}$ :$9$ more [[Definition:Element|elements]] of $G$ which are in [[Definition:Subgroup|subgroups]] of $G$ different from the [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]], and so have [[Definition:Order of Group Element|orders]] different from $3$. Suppose $n_5 = 6$. By a similar argument to the above, these $6$ [[Definition:Sylow p-Subgroup|Sylow $5$-subgroups]] contribute $24$ [[Definition:Element|elements]] of [[Definition:Order of Group Element|order]] $5$ to $G$. This is not possible because there are only $9$ [[Definition:Element|elements]] of $G$ so far unaccounted for. So if $G$ has $10$ [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]], it can have only $1$ [[Definition:Sylow p-Subgroup|Sylow $5$-subgroup]], which we will denote $Q$. From [[Sylow p-Subgroup is Unique iff Normal|Sylow $p$-Subgroup is Unique iff Normal]], $Q$ is [[Definition:Normal Subgroup|normal]]. Thus we may form the [[Definition:Quotient Group|quotient group]] $G / Q$, which is of [[Definition:Order of Group|order $16$]]. From [[Groups of Order 2p]], $G / Q$ has a [[Definition:Unique|unique]] [[Definition:Sylow p-Subgroup|Sylow $3$-subgroup]], which we will denote $N / Q$. Again by the [[Correspondence Theorem (Group Theory)|Correspondence Theorem]], $N$ is a [[Definition:Normal Subgroup|normal subgroup]] of $G$ with $15$ [[Definition:Element|elements]]. As noted, $N$ is [[Definition:Cyclic Group|cyclic]]. {{qed|lemma}} Both cases have been accounted for: :that $G$ has $1$ [[Definition:Sylow p-Subgroup|Sylow $3$-subgroup]] :that $G$ has $10$ [[Definition:Sylow p-Subgroup|Sylow $3$-subgroups]]. Both cases result in $G$ having a [[Definition:Normal Subgroup|normal subgroup]] of [[Definition:Order of Group|order]] $15$ which is [[Definition:Cyclic Group|cyclic]]. {{qed}}	0
Let $A^-$ denote the [[Definition:Closure (Topology)|closure]] of $A$. It is required to be shown that $x \in A^-$ {{iff}}, for every [[Definition:Open Neighborhood|open neighborhood]] $U$ of $x$, the [[Definition:Set Intersection|intersection]] $A \cap U$ is [[Definition:Non-Empty Set|non-empty]]. For a [[Definition:Subset|subset]] $H \subseteq S$, let $H^{\complement}$ denote the [[Definition:Relative Complement|relative complement]] of $H$ in $S$. We have that: {{begin-eqn}} {{eqn | l = A \cap U | r = \O }} {{eqn | ll= \leadstoandfrom | l = A | o = \subseteq | r = U^{\complement} | c = Note that $U^{\complement}$ is [[Definition:Closed Set (Topology)|closed]] }} {{eqn | ll= \leadstoandfrom | l = A^- | o = \subseteq | r = U^{\complement} | c = [[Set Closure is Smallest Closed Set in Topological Space]] }} {{eqn | ll= \leadstoandfrom | l = U | o = \subseteq | r = \paren {A^-}^{\complement} | c = [[Relative Complement inverts Subsets]] and [[Relative Complement of Relative Complement]] }} {{eqn | ll= \leadstoandfrom | l = A^- \cap U | o = = | r = \O | c = }} {{end-eqn}} Thus: : $x \in U \iff x \notin A^-$ The result follows from the [[Rule of Transposition]].	0
{{begin-eqn}} {{eqn | l = \map {\laptrans {e^{b t} \sin a t} } s | r = \map {\laptrans {\sin a t} } {s - b} | c = [[Laplace Transform of Exponential times Function]] }} {{eqn | r = \frac a {\paren {s - b}^2 + a^2} | c = [[Laplace Transform of Sine]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sum_{i \mathop = 0}^{m - 1} \paren {a + i d} | r = m \paren {a + \frac {m - 1} 2 d} | c = [[Sum of Arithmetic Sequence]] }} {{eqn | l = \sum_{i \mathop = 0}^n \paren {a + i d} | r = \paren {n + 1} \paren {a + \frac n 2 d} | c = Let $n = m - 1$ }} {{eqn | l = \sum_{i \mathop = 0}^n i | r = \paren {n + 1} \paren {\frac n 2} | c = Let $a = 0$ and $d = 1$ }} {{eqn | l = 0 + \sum_{i \mathop = 1}^n i | r = \frac {n \paren {n + 1} } 2 | c = }} {{eqn | l = \sum_{i \mathop = 1}^n i | r = \frac {n \paren {n + 1} } 2 | c = }} {{end-eqn}} {{qed}}	0
Let $x$ be an [[Definition:Object|object]]. Then the [[Definition:Power Set|power set]] of the [[Definition:Singleton|singleton]] $\set x$ is: :$\powerset {\set x} = \set {\O, \set x}$	0
{{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\cos x} | r = \lim_{h \mathop \to 0} \frac {\map \cos {x + h} - \cos x} h | c = {{Defof|Derivative of Real Function at Point}} }} {{eqn | r = \lim_{h \mathop \to 0} \frac {\cos x \cos h - \sin x \sin h - \cos x} h | c = [[Cosine of Sum]] }} {{eqn | r = \lim_{h \mathop \to 0} \frac {\cos x \cos h - \cos x} h + \lim_{h \mathop \to 0} \frac {-\sin x \sin h} h | c = [[Sum Rule for Limits of Functions]] }} {{eqn | r = \cos x \lim_{h \mathop \to 0} \frac {\cos h - 1} h - \sin x \lim_{h \mathop \to 0} \frac {\sin h} h | c = [[Multiple Rule for Limits of Functions]] }} {{eqn | r = \cos x \times 0 - \sin x \times 1 | c = [[Limit of (Cosine (X) - 1) over X]] and [[Limit of Sine of X over X]] }} {{eqn | r = -\sin x }} {{end-eqn}} {{qed}}	0
Follows directly from the [[Orbit-Stabilizer Theorem]] applied to [[Group Action on Coset Space]]. {{Qed}}	0
Let $\left({S_1, \preceq_1}\right)$, $\left({S_2, \preceq_2}\right)$ be [[Definition:Meet Semilattice|meet semilattices]]. Let $f: S_1 \to S_2$ be a [[Definition:Mapping|mapping]]. Let $f$ [[Definition:Mapping Preserves Infimum/Finite|preserve finite infima]] and [[Definition:Mapping Preserves Infimum/Filtered|preserve filtered infima]]. Then $f$ also [[Definition:Mapping Preserves Infimum/All|preserves all infima]]	0
From [[Integer Reciprocal Space with Zero is Totally Separated]], $A$ is [[Definition:Totally Separated Space|totally separated]]. The result follows from [[Totally Separated iff Quasicomponents are Singletons]]. {{qed}}	0
Recall the definition of a [[Definition:Limit of Real Function|limit of a real function]]: :$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in \R: 0 < \size {x - c} < \delta \implies \size {\map f x - L} < \epsilon$ Denote by $\map P {\epsilon,\delta}$ the [[Definition:Proposition|proposition]] that the above statement holds. Suppose that $\map P {\epsilon,\delta}$ holds for all rational $\epsilon$ in the interval of interest: :$\forall \epsilon \in \Q_{>0}: \exists \delta \in \Q_{>0}: \forall x \in \R: 0 < \size {x - c} < \delta \implies \size {\map f x - L} < \epsilon$ Let $\epsilon_0 > 0$ be a real number. From [[Between two Real Numbers exists Rational Number]], there is some rational $\epsilon_0'$ such that $0 < \epsilon_0' < \epsilon_0$. Then $\size {\map f x - L} < \epsilon_0'$ implies $\size {\map f x - L} < \epsilon_0$. Thus $\map P {\epsilon,\delta}$ holding for all rational $\epsilon$ implies $\map P {\epsilon,\delta}$ for all real $\epsilon$. Now suppose $\map P {\epsilon,\delta}$ has been established for some rational $\epsilon$ and some real $\delta_0$. From [[Between two Real Numbers exists Rational Number]], there is some rational $\delta_0'$ such that $\size {x-c} < \delta_0' < \delta_0$. Then $\size {x-c} < \delta_0$ is implied by $\size {x-c} < \delta_0'$ This means $\map P {\epsilon,\delta_0'}$ implies $\map P {\epsilon,\delta_0}$ holds. We conclude: :$\map P {\epsilon,\delta}$ holds for all rational $\epsilon$ and $\delta$ in the interval of interest is a [[Definition:Stronger Statement|stronger statement]] than: :$\map P {\epsilon,\delta}$ holds for all real $\epsilon$ and $\delta$ in the interval of interest. {{qed}}	0
Consider the [[Definition:Farey Sequence|Farey sequence]]: :$\sequence {a_n} = \dfrac 1 2, \dfrac 1 3, \dfrac 2 3, \dfrac 1 4, \dfrac 2 4, \dfrac 3 4, \dfrac 1 5, \dfrac 2 5, \dfrac 3 5, \dfrac 4 5, \dfrac 1 6, \ldots$ Every [[Definition:Element|element]] of the [[Definition:Closed Real Interval|closed real interval]] $\closedint 0 1$ is the [[Definition:Limit of Real Sequence|limit]] of a [[Definition:Subsequence|subsequence]] of $\sequence {a_n}$.	0
Let $T$ be a [[Definition:Topological Space|topological space]]. Let $\H$ be a [[Definition:Set|set]] of [[Definition:Subset|subsets]] of $T$. That is, let $\H \subseteq \powerset T$ where $\powerset T$ is the [[Definition:Power Set|power set]] of $T$. Then the [[Definition:Set Union|union]] of the [[Definition:Interior (Topology)|interiors]] of the [[Definition:Element|elements]] of $\H$ is a [[Definition:Subset|subset]] of the [[Definition:Interior (Topology)|interior]] of the [[Definition:Set Union|union]] of $\H$. :$\displaystyle \bigcup_{H \mathop \in \H} H^\circ \subseteq \paren {\bigcup_{H \mathop \in \H} H}^\circ $	0
=== [[Complex Sequence is Cauchy iff Convergent/Lemma 1|Lemma]] === {{:Complex Sequence is Cauchy iff Convergent/Lemma 1}} Let $\sequence {x_n}$ be a [[Definition:Real Sequence|real sequence]] where: :$x_n = \Re \paren {z_n}$ for every $n$ :$\Re \paren {z_n}$ is the [[Definition:Real Part|real part]] of $z_n$ Let $\sequence {y_n}$ be a [[Definition:Real Sequence|real sequence]] where :$y_n = \Im \paren {z_n}$ for every $n$ :$\Im \paren {z_n}$ is the [[Definition:Imaginary Part|imaginary part]] of $z_n$ We find: :$\sequence {z_n}$ is a [[Definition:Complex Cauchy Sequence|Cauchy sequence]] :$\iff \sequence {x_n}$ and $\sequence {y_n}$ are [[Definition:Real Cauchy Sequence|Cauchy sequences]] by [[Complex Sequence is Cauchy iff Convergent/Lemma 1|Lemma]] :$\iff \sequence {x_n}$ and $\sequence {y_n}$ are [[Definition:Convergent Real Sequence|convergent]] by [[Real Sequence is Cauchy iff Convergent]] :$\iff \sequence {z_n}$ is [[Definition:Convergent Complex Sequence|convergent]] by definition of [[Definition:Convergent Complex Sequence|convergent complex sequence]] {{qed}}	0
We have [[Definition:Excluded Point Space|by definition]] that $S \in \tau_{\bar p}$, and as $p \notin \varnothing$ we have that $\varnothing \in \tau_{\bar p}$. Now let $U_1, U_2 \in \tau_{\bar p}$. By definition $p \notin U_1$ and $p \notin U_2$, and so $p \notin U_1 \cap U_2$ by definition of [[Definition:Set Intersection|set intersection]]. So $U_1 \cap U_2 \in \tau_{\bar p}$. Now let $\mathcal U \subseteq \tau_{\bar p}$. We have that $\forall U \in \mathcal U: p \notin U$. Hence from [[Subset of Union]] $p \notin \bigcup \mathcal U$. So all the properties are fulfilled for $\tau_{\bar p}$ to be a [[Definition:Topology|topology]] on $S$. {{qed}}	0
Let $T = \struct {S, \tau_p}$ be a [[Definition:Fort Space|Fort space]]. Let $x \in S$ such that $x \ne p$. Then $p$ is the only [[Definition:Limit Point of Point|limit point]] of $x$.	0
Let $T = \struct {S, \set {\O, S} }$ be an [[Definition:Indiscrete Space|indiscrete topological space]] such that $S$ is [[Definition:Uncountable Set|uncountable]]. Let $a, b \in S$. Consider an [[Definition:Injection|injection]] $f: \closedint 0 1 \to S$ such that $\map f 0 = a$ and $\map f 1 = b$. This can always be found because $S$ is itself [[Definition:Uncountable Set|uncountable]]. From [[Mapping to Indiscrete Space is Continuous]], we have that $f$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. Thus $T$ is [[Definition:Arc-Connected Space|arc-connected]]. Now suppose $S$ is an [[Definition:Indiscrete Space|indiscrete topological space]] which is [[Definition:Arc-Connected Space|arc-connected]]. Then there exists an [[Definition:Injection|injection]] $f: \closedint 0 1 \to S$ such that $\map f 0 = a$ and $\map f 1 = b$. This can only exist if $S$ is [[Definition:Uncountable Set|uncountable]]. {{qed}}	0
The [[Definition:Closure (Topology)|closure]] of the [[Definition:Interior (Topology)|interior]] of the [[Definition:Closure (Topology)|closure of $A$ in $\R$]] is given by: {{begin-eqn}} {{eqn | l = A^{- \, \circ \, -} | r = \left[{0 \,.\,.\, 2}\right] | c = {{Defof|Closed Real Interval}} }} {{eqn | o = | ro= \cup | r = \left[{4 \,.\,.\, 5}\right] | c = {{Defof|Closed Real Interval}} }} {{end-eqn}} :[[File:Kuratowski-Closure-Complement-Theorem-ClosIntClos.png|500px]]	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $\mathcal B \subseteq \tau$ be a [[Definition:Basis (Topology)|basis]] for $T$. Let $U \in \mathcal B$. Then $U$ is a '''basic open set''' of $T$. That is, a '''basic open set''' of a [[Definition:Topology|topology]] is an [[Definition:Open Set (Topology)|open set]] of that topology which is an [[Definition:Element|element]] of a [[Definition:Basis (Topology)|basis]] for that [[Definition:Topology|topology]]. The [[Definition:Basis (Topology)|basis]] itself needs to be specified for this definition to make sense. [[Category:Definitions/Topology]] [[Category:Definitions/Topological Bases]] p16ojsxmpoxw5t9h7f3eos4sdcjv7po	0
[[Proof by Counterexample]]: :[[File:Same-degree-non-isomorphic-graphs.png|400px]] Consider a [[Definition:Bijection|bijection]] $\phi: \map V {G_1} \to \map V {G_2}$, where $G_1$ is the [[Definition:Graph (Graph Theory)|graph]] on the left and $G_2$ is the [[Definition:Graph (Graph Theory)|graph]] on the right. The [[Definition:Vertex of Graph|vertices]] $v_1$, $v_2$ and $v_5$ of $G_2$ are each [[Definition:Adjacent Vertices (Graph Theory)|adjacent]] to both of the others. Because $\phi$ is a [[Definition:Bijection|bijection]], it must map $3$ [[Definition:Vertex of Graph|vertices]] of $G_1$ to $v_1$, $v_2$ and $v_5$. For $\phi$ to be an [[Definition:Isomorphism (Graph Theory)|isomorphism]], two of the [[Definition:Vertex of Graph|vertices]] of $G_1$ are [[Definition:Adjacent Vertices (Graph Theory)|adjacent]] {{iff}} the two [[Definition:Image of Element under Mapping|image]] [[Definition:Vertex of Graph|vertices]] in $G_2$ udner $\phi$ are also [[Definition:Adjacent Vertices (Graph Theory)|adjacent]]. So the $3$ [[Definition:Vertex of Graph|vertices]] of $G_1$ which map to $v_1$, $v_2$ and $v_5$ of $G_2$ must also each be [[Definition:Adjacent Vertices (Graph Theory)|adjacent]] to both of the others. But $G_1$ does not contain $3$ such [[Definition:Vertex of Graph|vertices]]. It follows that there is no such [[Definition:Isomorphism (Graph Theory)|isomorphism]] from $\map V {G_1}$ to $\map V {G_2}$. That is, $G_1$ and $G_2$ are not [[Definition:Isomorphism (Graph Theory)|isomorphic]]. {{qed}}	0
Let $\mathbb K$ be the [[Definition:Set|set]] of all [[Definition:Closed Set (Metric Space)|closed sets]] $K$ of $M$ such that $H \subseteq K$. From [[Closure of Subset of Closed Set of Metric Space is Subset]]: :$\forall K \in \mathbb K: H^- \subseteq K$ From [[Intersection is Largest Subset]]: :$\displaystyle H^- \subseteq \bigcap \mathbb K$ {{qed|lemma}} From [[Closure of Subset of Metric Space is Closed]], $H^-$ is [[Definition:Closed Set (Metric Space)|closed]]. From [[Subset of Metric Space is Subset of its Closure]], $H \subseteq H^-$. So $H^-$ is, by definition, a [[Definition:Closed Set (Metric Space)|closed set]] of $M$ which contains $H$. But $\displaystyle \bigcap \mathbb K$ is the [[Definition:Set Intersection|intersection]] of '''all''' [[Definition:Closed Set (Metric Space)|closed sets]] of $M$ which contain $H$. So from [[Intersection is Subset]] it follows that: : $\displaystyle \bigcap \mathbb K \subseteq H^-$ {{qed|lemma}} Finally, we have that: : $\displaystyle H^- \subseteq \bigcap \mathbb K$ : $\displaystyle \bigcap \mathbb K \subseteq H^-$ So by definition of [[Definition:Set Equality/Definition 2|set equality]]: : $\displaystyle H^- = \bigcap \mathbb K$ which is what we needed to prove. {{qed}}	0
=== Reflexivity === :$\forall \tuple {x_1, y_1} \in S_1 \times S_2: x_1 \circ y_1 = x_1 \circ y_1 \implies \tuple {x_1, y_1} \boxtimes \tuple {x_1, y_1}$ So $\boxtimes$ is a [[Definition:Reflexive Relation|reflexive relation]]. {{qed|lemma}} === Symmetry === {{begin-eqn}} {{eqn | l = \tuple {x_1, y_1} | o = \boxtimes | r = \tuple {x_2, y_2} | c = }} {{eqn | ll= \leadsto | l = x_1 \circ y_2 | r = x_2 \circ y_1 | c = }} {{eqn | ll= \leadsto | l = x_2 \circ y_1 | r = x_1 \circ y_2 | c = $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | ll= \leadsto | l = \tuple {x_2, y_2} | o = \boxtimes | r = \tuple {x_1, y_1} | c = }} {{end-eqn}} So $\boxtimes$ is a [[Definition:Symmetric Relation|symmetric relation]]. {{qed|lemma}} === Transitivity === {{begin-eqn}} {{eqn | l = \tuple {x_1, y_1} | o = \boxtimes | r = \tuple {x_2, y_2} | c = }} {{eqn | lo= \land | l = \tuple {x_2, y_2} | o = \boxtimes | r = \tuple {x_3, y_3} | c = }} {{eqn | ll= \leadsto | l = x_1 \circ y_2 | r = x_2 \circ y_1 | c = }} {{eqn | lo= \land | l = x_2 \circ y_3 | r = x_3 \circ y_2 | c = }} {{eqn | ll= \leadsto | l = x_1 \circ y_3 \circ y_2 | r = x_1 \circ y_2 \circ y_3 | c = $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | r = x_2 \circ y_1 \circ y_3 | c = substituting $x_2 \circ y_1$ for $x_1 \circ y_2$ }} {{eqn | r = x_2 \circ y_3 \circ y_1 | c = $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | r = x_3 \circ y_2 \circ y_1 | c = substituting $x_3 \circ y_2$ for $x_2 \circ y_3$ }} {{eqn | r = x_3 \circ y_1 \circ y_2 | c = $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | ll= \leadsto | l = x_1 \circ y_3 | r = x_3 \circ y_1 | c = as $y_2 \in S_2$, therefore $y_2 \in C$, and so is [[Definition:Cancellable Element|cancellable]] for $\circ$ }} {{eqn | ll= \leadsto | l = \tuple {x_1, y_1} | o = \boxtimes | r = \tuple {x_3, y_3} | c = }} {{end-eqn}} So $\boxtimes$ is a [[Definition:Transitive Relation|transitive relation]]. {{qed|lemma}} All the criteria are therefore seen to hold for $\boxtimes$ to be an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Discrete Topology|discrete topological space]]. Then $T$ is [[Definition:Fully T4 Space|fully $T_4$]].	0
We have that the [[Countable Complement Topology is Expansion of Finite Complement Topology]]. We also have that a [[Finite Complement Space is T1|Finite Complement Space is $T_1$]]. Then from [[Separation Properties Preserved by Expansion]], we have that $T$ is a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]]. {{qed}}	0
We have for each $k \in \left\{{1, \dots, m}\right\}$ a [[Definition:Decomposition (Topology)|decomposition]] $\left\{{A_{k, 1}, \cdots, A_{k, l_k}}\right\}$ and set of [[Definition:Isometry (Metric Spaces)|isometries]] $\phi_{i, j}: \R^n \to \R^n$ such that: :$\displaystyle S_k = \bigcup_{a \mathop = 1}^{l_k} \phi_{k, a} \left({A_{k, a} }\right)$ and similarly for $T_k$ and some [[Definition:Isometry (Metric Spaces)|isometries]] $\theta_{i, j}: \R^n \to \R^n$: :$\displaystyle T_k = \bigcup_{a \mathop = 1}^{l_k} \theta_{k, a} \left({A_{k, a} }\right)$ Thus: :$\displaystyle S = \bigcup_{k \mathop = 1}^m \bigcup_{i \mathop = 1}^{l_k} \phi_{k, i} \left({A_{k, i} }\right)$ and: :$\displaystyle T = \bigcup_{k \mathop = 1}^m \bigcup_{i \mathop = 1}^{l_k} \theta_{k, i} \left({A_{k, i} }\right)$. This yields equivalent [[Definition:Decomposition (Topology)|decompositions]] of $S$ and $T$. {{qed}} [[Category:Topology]] khsd67uh7ztk2vsb1ngqax6ihrywa2e	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. {{TFAE|def = Hereditarily Compact Space}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] which is [[Definition:Regular Space|regular]] and [[Definition:Paracompact Space|paracompact]]. Then it is not necessarily the case that $T$ is [[Definition:Metrizable Space|metrizable]].	0
=== Topological Spaces === Let $\struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] The '''Borel sigma-algebra''' $\map {\mathcal B} {S, \tau}$ of $\struct {S, \tau}$ is the [[Definition:Sigma-Algebra Generated by Collection of Subsets|$\sigma$-algebra generated by $\tau$]]. That is, it is the $\sigma$-algebra generated by the set of [[Definition:Open Set (Topology)|open sets]] in $S$. === Metric Spaces === Let $\struct {X, \norm {\,\cdot\,} }$ be a [[Definition:Metric Space|metric space]]. The '''Borel sigma-algebra''' (or '''$\sigma$-algebra''') on $\struct {X, \norm {\,\cdot\,} }$ is the [[Definition:Sigma-Algebra Generated by Collection of Subsets|$\sigma$-algebra generated by]] the [[Definition:Open Set of Metric Space|open sets]] in $\powerset X$. By the definition of [[Definition:Topology Induced by Metric|a topology induced by a metric]], this definition is a particular instance of the definition of a Borel $\sigma$-algebra on topological spaces. === [[Definition:Borel Sigma-Algebra/Borel Set|Borel Set]] === {{:Definition:Borel Sigma-Algebra/Borel Set}}	0
Suppose $A$ and $B$ are [[Definition:Separated Sets|separated subsets]] of $T$ such that $A \cup B = S$. By definition of [[Definition:Separated Sets|separated sets]]: :$A \cap B^- = \O$ Then: {{begin-eqn}} {{eqn | l = S | r = A \cup B | c = }} {{eqn | o = \subseteq | r = A \cup B^- | c = [[Set is Subset of its Topological Closure]] }} {{eqn | o = \subseteq | r = S | c = by definition of $S$ }} {{end-eqn}} Hence $A = S \setminus B^-$. From [[Topological Closure is Closed]], $B^-$ is [[Definition:Closed Set (Topology)|closed]] in $T$. Thus $A$ is [[Definition:Open Set (Topology)|open]] in $T$. Also by definition of [[Definition:Separated Sets|separated sets]]: :$A^- \cap B = \O$ Hence, by the same reasoning, $B$ must also be [[Definition:Open Set (Topology)|open]]. But: :$A \cap B \subseteq A \cap B^- = \O$ and $A \cup B = S$, by assumption. So: :$A = S \setminus B$ and $B = S \setminus A$ and we conclude that both $A$ and $B$ are [[Definition:Clopen Set|clopen]]. Therefore, [[Definition:By Hypothesis|by hypothesis]], one of them must be $S$ and the other must be $\O$. That is, there are no two [[Definition:Non-Empty Set|non-empty]] [[Definition:Separated Sets|separated sets]] of $T$ whose [[Definition:Set Union|union]] is $S$.	0
Let $N$ be a [[Definition:Neighborhood (Metric Space)|neighborhood]] of $a$ in $M$. Then by definition: :$\exists \epsilon' \in \R_{>0}: \map {B_{\epsilon'} } a \subseteq N$ where $\map {B_{\epsilon'} } a$ is the [[Definition:Open Ball|open $\epsilon'$-ball at $a$]]. From [[Open Ball in Real Number Line is Open Interval]]: :$\map {B_{\epsilon'} } a = \openint {a - \epsilon'} {a + \epsilon'}$ From [[Between two Real Numbers exists Rational Number]]: :$\exists \epsilon'' \in \Q: 0 < \epsilon'' < \epsilon'$ Let $\epsilon''$ be expressed in [[Definition:Canonical Form of Rational Number|canonical form]]: :$\epsilon'' = \dfrac p q$ Let $\epsilon = \dfrac 1 q$ Then $\epsilon \le \epsilon'' < \epsilon'$ and so: :$\openint {a - \epsilon} {a + \epsilon} \subseteq \openint {a - \epsilon'} {a + \epsilon'}$ From [[Open Real Interval is Open Ball]] :$\map {B_\epsilon} a = \openint {a - \epsilon} {a + \epsilon}$ is the [[Definition:Open Ball|open $\epsilon$-ball at $a$]]. From [[Subset Relation is Transitive]]: :$\openint {a - \epsilon} {a + \epsilon} \subseteq N$ From [[Open Ball is Neighborhood of all Points Inside]], $\openint {a - \epsilon} {a + \epsilon}$ is a [[Definition:Neighborhood (Metric Space)|neighborhood]] of $a$ in $M$. Hence there exists a [[Definition:Neighborhood (Metric Space)|neighborhood]] $\openint {a - \epsilon} {a + \epsilon}$ of $a$ which is a [[Definition:Subset|subset]] of $N$. Hence the result by definition of [[Definition:Basis for Neighborhood System|basis for the neighborhood system]] of $a$. {{qed}}	0
Let $\struct {S, \preceq}$ be a [[Definition:Toset|toset]]. Let $\tau$ be the [[Definition:Order Topology|order topology]] on $S$. Then $\struct {S, \tau}$ is [[Definition:Normal Space|normal]].	0
Let $M = \struct {S, d}$ be a [[Definition:Metric Space|metric space]]. Let $A \subseteq S$ be a [[Definition:Everywhere Dense|dense]] [[Definition:Subset|subset]]. Suppose that every [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] in $A$ [[Definition:Convergent Sequence (Metric Space)|converges]] in $M$. Then $M$ is [[Definition:Complete Metric Space|complete]].	0
Let $M = \struct {A, d}$ be a [[Definition:Totally Bounded Metric Space|totally bounded metric space]]. Let $M' = \struct {A', d'}$ be a [[Definition:Metric Space|metric space]]. Let $M$ be [[Definition:Homeomorphic Metric Spaces|homeomorphic]] to $M'$. Then it is not necessarily the case that $M'$ is [[Definition:Totally Bounded Metric Space|totally bounded]].	0
:$Y_i = \prod_{j \mathop \in I} Z_j$	0
Let $H$ be a [[Definition:Closed Set (Topology)|closed set]] of $T$. Then $H$ is [[Definition:Nowhere Dense|nowhere dense]] in $T$ {{iff}} $S \setminus H$ is [[Definition:Everywhere Dense|everywhere dense]] in $T$.	0
Let $T = \struct {S, \tau}$ be the [[Definition:Arens-Fort Space|Arens-Fort space]]. Then $T$ is a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]].	0
Let $\phi: \N \to S$ be the [[Definition:Mapping|mapping]] defined as: :$\forall n \in \N: \map \phi n = \dfrac i n$ $\phi$ is a [[Definition:Bijection|bijection]]. Hence the result by definition of [[Definition:Countably Infinite|countably infinite]]. {{qed}}	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Then :$T$ is a [[Definition:T0 Space|$T_0$ space]] {{iff}} ::$\forall x, y \in S: x \ne y \implies x \notin \left\{{y}\right\}^- \lor y \notin \left\{{x}\right\}^-$ where $\left\{{y}\right\}^-$ denotes the [[Definition:Closure (Topology)|closure]] of $\left\{{y}\right\}$.	0
Let $A := \displaystyle \bigcup \AA$. Let $D = \set {0, 1}$, with the [[Definition:Discrete Space|discrete topology]]. Let $f: A \to D$ be [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. To show that $A$ is [[Definition:Connected Set (Topology)|connected]], we need to show that $f$ is not a [[Definition:Surjection|surjection]]. Since each $C \in \AA$ is [[Definition:Connected Set (Topology)|connected]] and the [[Definition:Restriction of Mapping|restriction]] $f \restriction_C$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]: :$\map f C = \set {\map \epsilon C}$ where $\map \epsilon C = 0$ or $1$. But, for all $B, C \in \AA$: :$B \cap C \ne \O$ Hence $\map \epsilon B = \map \epsilon C$. Thus $f$ is [[Definition:Constant Mapping|constant]] on $A$ as required. {{qed}}	0
Let $\sequence {x_n}$ be [[Definition:Increasing Real Sequence|increasing]] and [[Definition:Unbounded Above Real Sequence|unbounded above]]. Then $x_n \to +\infty$ as $n \to \infty$.	0
=== Sufficient Condition === Let $T$ be [[Definition:Locally Compact Space|locally compact]]. Let $U \in \tau, x \in U$. Since $T$ is [[Definition:Locally Compact Space|locally compact]] there exists a [[Definition:Neighborhood Basis|neighborhood basis]] $\mathcal B$ of $x$ consisting of [[Definition:Compact Topological Subspace|compact sets]]. From [[Set is Open iff Neighborhood of all its Points]], $U$ is a [[Definition:Neighborhood (Topology)|neighborhood]] of $x$. By the definition of a [[Definition:Neighborhood Basis|neighborhood basis]]: :$\exists K \in \mathcal B : K \subseteq U$. By the definition of a [[Definition:Neighborhood (Topology)|neighborhood]] of $x$: :$\exists V \in \tau : x \in V \subseteq K$ By the definition of the [[Definition:Interior (Topology)|interior]] of a [[Definition:Subset|subset]] then: :$x \in V \subseteq K^\circ$ Thus :$\exists K \subseteq S: x \in K^\circ \land K \subseteq U \land K$ is [[Definition:Compact Topological Subspace|compact]] {{qed|lemma}} === Necessary Condition === Let :$\forall U \in \tau, x \in U: \exists K \subseteq S: x \in K^\circ \land K \subseteq U \land K$ is [[Definition:Compact Topological Subspace|compact]] Let $\mathcal B$ be the [[Definition:Set|set]] of all [[Definition:Compact Topological Subspace|compact]] [[Definition:Neighborhood (Topology)|neighborhoods]] of $x$. It is shown that $\mathcal B$ is a [[Definition:Neighborhood Basis|neighborhood basis]] of $x$. Let $N$ be a [[Definition:Neighborhood (Topology)|neighborhood]] of $x$. By the definition of a [[Definition:Neighborhood (Topology)|neighborhood]] of $x$: :$\exists U \in \tau : x \in U \subseteq N$ By assumption: :$\exists K \subseteq S: x \in K^\circ \land K \subseteq U \land K$ is [[Definition:Compact Topological Subspace|compact]] By the definition of the [[Definition:Interior (Topology)|interior]] of a [[Definition:Subset|subset]] $K^\circ$ is [[Definition:Open Set/Topology|open]]. By the definition of a [[Definition:Neighborhood (Topology)|neighborhood]] then $K$ is a [[Definition:Neighborhood (Topology)|neighborhood]] of $x$. Hence $K \in \mathcal B$ and $K \subseteq U \subseteq N$. As $N$ was arbitrary, then $B$ is a [[Definition:Neighborhood Basis|neighborhood basis]] of $x$ consisting of [[Definition:Compact Topological Subspace|compact sets]].	0
:$\displaystyle \lim_{n \mathop \to \infty} \paren {x_n - y_n} = l - m$	0
Let $x, y \in \R$ be [[Definition:Real Number|real numbers]]. Let $\map d {x, y}$ be the [[Definition:Distance between Real Numbers|distance]] between $x$ and $y$: :$\map d {x, y} = \size {x - y}$ Then $\map d {x, y}$ is a [[Definition:Metric|metric]] on $\R$. Thus it follows that $\tuple {\R, d}$ is a [[Definition:Metric Space|metric space]].	0
Let $T = \struct {S, \tau_p}$ be a [[Definition:Fort Space|Fort space]] on an [[Definition:Infinite Set|infinite set]] $S$. Then $T$ is not an [[Definition:Extremally Disconnected Space|extremally disconnected space]].	0
Recall from [[Real Number Line is Metric Space]] that the [[Definition:Real Number|set of real numbers]] $\R$ with the [[Definition:Distance between Element and Subset of Real Numbers|distance function]] $d$ is a [[Definition:Metric Space|metric space]]. The result is then seen to be an example of [[Distance from Subset to Infimum]]. {{Qed}}	0
Let $x, y \in S$ such that: :$x \preceq y$ By [[Infimum of Singleton]]: :$\set x$ and $\set y$ admit [[Definition:Infimum of Set|infima]] in $\struct {S, \preceq}$ By [[Infimum of Upper Closure of Set]]: :$\set x^\succeq$ and $\set y^\succeq$ admit [[Definition:Infimum of Set|infima]] in $\struct {S, \preceq}$ where $\set x^\succeq$ denotes the [[Definition:Upper Closure of Subset|upper closure]] of $\set x$. By [[Upper Closure of Singleton]] :$x^\succeq$ and $y^\succeq$ admit [[Definition:Infimum of Set|infima]] in $\struct {S, \preceq}$ By [[Upper Closure of Element is Filter]]: :$x^\succeq$ and $y^\succeq$ are [[Definition:Filter in Ordered Set|filter]] in $\struct {S, \preceq}$ By assumption and definition of [[Definition:Mapping Preserves Infimum/Subset|mapping preserves the infimum on subset]]: :$\map {f^\to} {x^\succeq}$ and $\map {f^\to} {y^\succeq}$ admit [[Definition:Infimum of Set|infima]] in $\struct {T, \precsim}$ and :$\inf \set {\map {f^\to} {x^\succeq} } = \map f {\inf \set {x^\succeq} }$ and $\inf \set {\map {f^\to} {y^\succeq} } = \map f {\inf \set {y^\succeq} }$ By [[Infimum of Upper Closure of Element]]: :$\inf \set {x^\succeq} = x$ and $\inf \set {y^\succeq} = y$ By [[Upper Closure is Decreasing]]: :$y^\succeq \subseteq x^\succeq$ By [[Image of Subset under Relation is Subset of Image/Corollary 2]]: :$\map {f^\to} {y^\succeq} \subseteq \map {f^\to} {x^\succeq}$ Thus by [[Infimum of Subset]]: :$\map f x \precsim \map f y$ Thus by definition: :$f$ is [[Definition:Increasing Mapping|increasing]]. {{qed}}	0
Let $T = \left({S, \tau_p}\right)$ be a [[Definition:Particular Point Topology|particular point space]], whose [[Definition:Particular Point Topology|particular point]] is $p$. Let $H = S \setminus \left\{ {p}\right\}$ where $\setminus$ denotes [[Definition:Set Difference|set difference]]. Then the [[Definition:Topological Subspace|topological subspace]] $T_H = \left({H, \tau_H}\right)$ induced on $H$ by $\tau_p$ is a [[Definition:Discrete Space|discrete space]].	0
{{begin-eqn}} {{eqn | lo= \forall n \ge 1: | l = \sum_{j \mathop = 1}^n F_{2 j} | r = F_2 + F_4 + F_6 + \cdots + F_{2 n} | c = }} {{eqn | r = F_{2 n + 1} - 1 | c = }} {{end-eqn}}	0
:$\displaystyle \forall n \in \N: \sum_{i \mathop = 1}^n i^2 = \frac {n \paren {n + 1} \paren {2 n + 1} } 6$	0
First it is to be shown that $j_t$ is [[Definition:Left-Total Relation|left-total]]. This follows from the fact that $j_t$ is defined for all $s$: :$\map {j_t} s = \tuple {s, t}$ {{qed|lemma}} Next it is to be shown that $j_t$ is [[Definition:Many-to-One Relation|many-to-one]], that is: :$\forall s_1, s_2 \in S: \map {j_t} {s_1} \ne \map {j_t} {s_2} \implies s_1 \ne s_2$ We have that: {{begin-eqn}} {{eqn | l = \map {j_t} {s_1} | o = \ne | r = \map {j_t} {s_2} | c = }} {{eqn | ll= \leadsto | l = \tuple {s_1, t} | o = \ne | r = \tuple {s_2, t} | c = Definition of $j_t$ }} {{eqn | ll= \leadsto | l = s_1 | o = \ne | r = s_2 | c = {{Defof|Ordered Pair}} }} {{end-eqn}} Hence the result. {{qed}}	0
:$\displaystyle \sum_{i \mathop = 1}^n i^5 + \sum_{i \mathop = 1}^n i^7 = 2 \paren {\sum_{i \mathop = 1}^n i}^4$	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]]. Let $H \subseteq S$. === [[Definition:Closure (Topology)/Definition 1|Definition 1]] === {{:Definition:Closure (Topology)/Definition 1}} === [[Definition:Closure (Topology)/Definition 2|Definition 2]] === {{:Definition:Closure (Topology)/Definition 2}} === [[Definition:Closure (Topology)/Definition 3|Definition 3]] === {{:Definition:Closure (Topology)/Definition 3}} === [[Definition:Closure (Topology)/Definition 4|Definition 4]] === {{:Definition:Closure (Topology)/Definition 4}} === [[Definition:Closure (Topology)/Definition 5|Definition 5]] === {{:Definition:Closure (Topology)/Definition 5}} === [[Definition:Closure (Topology)/Definition 6|Definition 6]] === {{:Definition:Closure (Topology)/Definition 6}}	0
From [[Kuratowski's Closure-Complement Problem/Closure of Interior|Kuratowski's Closure-Complement Problem: Closure of Interior]]: {{begin-eqn}} {{eqn | l = A^{\circ \, -} | r = \left[{0 \,.\,.\, 2}\right] | c = {{Defof|Closed Real Interval}} }} {{end-eqn}} From [[Interior of Closed Real Interval is Open Real Interval]]: :$\left[{0 \,.\,.\, 2}\right]^\circ = \left({0 \,.\,.\, 2}\right)$ Hence the result. {{qed}} [[Category:Kuratowski's Closure-Complement Problem]] [[Category:Set Interiors]] [[Category:Set Closures]] 46s5m5z3ka0ul2mo5rjbcwvhbilhqig	0
Let $T_1, T_2, T_3$ be [[Definition:Topological Space|topological spaces]]. Let $f: T_1 \to T_2$ and $g: T_2 \to T_3$ be [[Definition:Homeomorphism (Topological Spaces)|homeomorphisms]]. Then $g \circ f: T_1 \to T_3$ is also a [[Definition:Homeomorphism (Topological Spaces)|homeomorphism]].	0
{{ProofWanted}} [[Category:Uniform Convergence]] [[Category:Infinite Products]] ep2a1h07fdbi7lmd3a8xz9h8x05dyuk	0
By [[Open Balls form Basis for Open Sets of Metric Space]], the [[Definition:Set|set]] of [[Definition:Open Ball|open balls]] are an [[Definition:Analytic Basis|analytic basis]] for the [[Definition:Topology|topology]] $\tau_d$. By [[Analytic Basis Characterization of Denseness|Analytic Basis Characterization of Denseness]] then: :$S$ is [[Definition:Everywhere Dense|(everywhere) dense]] in $\struct{X, \tau_d}$ {{iff}} every [[Definition:Open Ball|open ball]] contains an [[Definition:Element|element]] of $S$. {{qed}} [[Category:Denseness]] 5c156wij1viyrf4bkv3dd0c590r9jv0	0
Let $\tau_K$ be the [[Definition:Subspace Topology|subspace topology]] on $K$ induced by $\tau$. Let $\tau’_K$ be the [[Definition:Subspace Topology|subspace topology]] on $K$ induced by $\tau_H$. Then {{begin-eqn}} {{eqn | l = V \in \tau’_K | o = \leadstoandfrom | r = \exists U’ \in \tau_H : V = U’ \cap K | c = {{Defof|Subspace Topology}} $\tau’_K$ }} {{eqn | o = \leadstoandfrom | r = \exists U \in \tau : V = \paren {U \cap H} \cap K | c = {{Defof|Subspace Topology}} $\tau_H$ }} {{eqn | o = \leadstoandfrom | r = \exists U \in \tau : V = U \cap \paren {H \cap K} | c = [[Intersection is Associative]] }} {{eqn | o = \leadstoandfrom | r = \exists U \in \tau : V = U \cap K | c = [[Intersection with Subset is Subset]] }} {{eqn | o = \leadstoandfrom | r = V \in \tau_K | c = {{Defof|Subspace Topology}} $\tau_K$ }} {{end-eqn}} [[Category:Topological Subspaces]] mrfeh7eskyccopbrjwm7cr98zmshv6h	0
{{begin-eqn}} {{eqn | l = \sin x | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Real Sine Function}} }} {{eqn | r = \left({-1}\right)^0 \frac{x^{2 \cdot 0 + 1} } { \left({2 \cdot 0 + 1}\right)!} + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} }} {{eqn | r = x + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \lim_{x \mathop \to 0} \frac {\sin x} x | r = \lim_{x \mathop \to 0} \frac {x + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} } x }} {{eqn | r = \lim_{x \mathop \to 0} \frac x x + \lim_{x \mathop \to 0} \frac{\sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} } x }} {{eqn | r = 1 + \lim_{x \mathop \to 0} \frac {\sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!} } 1 | c = [[Power Series is Differentiable on Interval of Convergence]] and [[L'Hôpital's Rule]] }} {{eqn | r = 1 + \lim_{x \mathop \to 0} \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!} }} {{eqn | r = 1 + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {0^{2 n} } {\paren {2 n}!} | c = [[Real Polynomial Function is Continuous]] }} {{eqn | r = 1 }} {{end-eqn}} {{qed}}	0
Let $S = \bigcup \set {U \subseteq S : U \in \tau \text { and } U \text { is connected} }$. Let $C$ be a [[Definition:Component (Topology)|component]] of $T$. ==== [[Components are Open iff Union of Open Connected Sets/Lemma 1|Lemma]] ==== :For any [[Definition:Connected Set (Topology)|connected set]] $U$ then: {{:Components are Open iff Union of Open Connected Sets/Lemma 1}} Then: {{begin-eqn}} {{eqn | l = C | r = C \cap S | c = [[Intersection with Subset is Subset]] }} {{eqn | r = C \cap \bigcup \set { U \subseteq S : U \in \tau \text { and } U \text { is connected} } }} {{eqn | r = \bigcup \set {C \cap U : U \in \tau \text { and } U \text { is connected} } | c = [[Intersection Distributes over Union]] }} {{eqn | r = \bigcup \set {C \cap U : U \in \tau, U \cap C \ne \empty \text { and } U \text { is connected} } | c = [[Union with Empty Set]] }} {{eqn | r = \bigcup \set {U \subseteq C : U \in \tau \text { and } U \text { is connected} } | c = [[Components are Open iff Union of Open Connected Sets/Lemma 1|Lemma]] }} {{end-eqn}} Hence $C$ is the [[Definition:Set Union|union]] of [[Definition:Open Set (Topology)|open sets]]. By definition of a [[Definition:Topological Space|topology]] then $C$ is an [[Definition:Open Set (Topology)|open set]]. The result follows.	0
Let $P = p_1 p_2 \ldots p_k$ be the longest [[Definition:Path (Graph Theory)|path]] in $G$. If $p_1$ is [[Definition:Adjacent (Graph Theory)|adjacent]] to some vertex $v$ not in $P$, then the path $v p_1 p_2 \ldots p_k$ would be longer than $P$, contradicting the choice of $P$. The same argument can be made for $p_k$. So both $p_1$ and $p_k$ are adjacent only to vertices in $P$. Since $\deg(p_1) \ge \dfrac n 2$ and $p_1$ cannot be adjacent to itself, $k \ge \dfrac n 2 +1$. '''Claim:''' There is some value of $j$ ($1 \le j \le k$) such that: * $p_j$ is [[Definition:Adjacent (Graph Theory)|adjacent]] to $p_k$, and * $p_{j+1}$ is adjacent to $p_1$. Suppose that the claim is not true. Then since all vertices adjacent to $p_1$ or $p_k$ lie on $P$, there must be at least $\deg(p_1)$ vertices on $P$ not adjacent to $p_k$. Since all the vertices adjacent to $p_k$ and $p_k$ itself also lie on $P$, the path must have at least $\deg(p_1) + \deg(p_k) + 1 \ge n+1$ vertices. But $G$ has only $n$ vertices: a contradiction. This gives a [[Definition:Cycle (Graph Theory)|cycle]] $C = p_{j+1} p_{j+2} \ldots p_{k} p_j p_{j-1} \ldots p_2 p_1 p_{j+1}$. Suppose $G - C$ is nonempty. Then since $G$ is connected, there must be a vertex $v \in G-C$ adjacent to some $p_i$. So the path from $v$ to $p_i$ and then around $C$ to the vertex adjacent to $p_i$ is longer than $P$, contradicting the definition of $P$. Therefore all vertices in $G$ are contained in $C$, making $C$ a [[Definition:Hamilton Cycle|Hamilton cycle]]. {{Qed}}	0
Consider the [[Definition:Set|set]] $A \subseteq R^2$ of all points defined as: :$A := \set {\tuple {x, y} \in \R^2: x y \ge 1}$ By [[Subset of Euclidean Plane whose Product of Coordinates are Greater Than or Equal to 1 is Closed]]: :$A$ is a [[Definition:Closed Set (Topology)|closed set]] in $\struct {\R^2, d}$. By inspection, it can be seen that the [[Definition:Image of Subset under Mapping|image of $A$ under $\rho$]] is: :$\rho \sqbrk A = \openint \gets 0 \cup \openint 0 \to$ which by [[Union of Open Sets of Metric Space is Open]] is [[Definition:Open Set (Topology)|open]]. Hence the result by definition of [[Definition:Closed Mapping|closed mapping]]. {{qed}}	0
Let $T$ be the set of all [[Definition:Labeled Graph|labeled]] [[Definition:Tree (Graph Theory)|trees]] of [[Definition:Order (Graph Theory)|order $n$]]. Let $P$ be the set of all [[Definition:Prüfer Sequence|Prüfer sequence]] of length $n-2$. Let $\phi: T \to P$ be the [[Definition:Mapping|mapping]] that maps each tree to its Prüfer sequence. * From [[Prüfer Sequence from Labeled Tree]], $\phi$ is clearly [[Definition:Well-Defined Mapping|well-defined]], as every element of $T$ maps uniquely to an element of $P$. * However, from [[Labeled Tree from Prüfer Sequence]], $\phi^{-1}: P \to T$ is also clearly [[Definition:Well-Defined Mapping|well-defined]], as every element of $P$ maps to a unique element of $T$. Hence the result. {{questionable|How is it immediate that the two constructions are mutually inverse?}} {{qed}} [[Category:Tree Theory]] [[Category:Combinatorics]] 1nztp06dejnykcd7hdywz33czkukpjk	0
Let $A \subseteq S$. Then from [[Set in Discrete Topology is Clopen]] it follows that $A$ is both [[Definition:Open Set (Topology)|open]] and [[Definition:Closed Set (Topology)|closed]] in $T$. From [[Closed Set Equals its Closure]] we have that $A = A^-$. From [[Set Interior is Largest Open Set]], we have that $A^\circ = A$. {{qed}}	0
First we show the following. Let $W \in T_2$. We note that $f^{-1} \sqbrk T_2 = T_1$. Hence, from [[Preimage of Set Difference under Mapping]], we have: :$f^{-1} \sqbrk {T_2 \setminus W} = T_1 \setminus f^{-1} \sqbrk W$ === Necessary Condition === Suppose the condition on [[Definition:Closed Set (Topology)|closed sets]] holds. Let $U$ be [[Definition:Open Set (Topology)|open]] in $T_2$. By [[Relative Complement of Relative Complement]], $T_2 \setminus \paren { T_2 \setminus U } = U$. Therefore $T_2 \setminus \paren { T_2 \setminus U }$ is [[Definition:Open Set (Topology)|open]] in $T_2$. Then $T_2 \setminus U$ is [[Definition:Closed Set (Topology)|closed]] in $T_2$. By hypothesis, $f^{-1} \sqbrk {T_2 \setminus U} = T_1 \setminus f^{-1} \sqbrk U$ is [[Definition:Closed Set (Topology)|closed]] in $T_1$. So $f^{-1} \sqbrk U$ is [[Definition:Open Set (Topology)|open]] in $T_1$. This is true for any $U \in T_2$. Hence $f$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. {{qed|lemma}} === Sufficient Condition === Now let $f$ be [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. Let $V$ be [[Definition:Closed Set (Topology)|closed]] in $T_2$. Then $T_2 \setminus V$ is [[Definition:Open Set (Topology)|open]] in $T_2$. As $f$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]], $f^{-1} \sqbrk {T_2 \setminus V} = T_1 \setminus f^{-1} \sqbrk V$ is [[Definition:Open Set (Topology)|open]] in $T_1$. So $f^{-1} \sqbrk V$ is [[Definition:Closed Set (Topology)|closed]] in $T_1$. {{qed}}	0
Let $\left({Y, \preceq, \tau}\right)$ be a [[Definition:Generalized Ordered Space/Definition 3|generalized ordered space (GO-space) by Definition 3]]. That is: : let $\left({Y, \tau}\right)$ be a [[Definition:Hausdorff Space|Hausdorff space]] and: : let $\tau$ have a [[Definition:Sub-Basis|sub-basis]] consisting of [[Definition:Upper Set|upper sets]] and [[Definition:Lower Set|lower sets]] relative to $\preceq$. Then $\left({Y, \preceq, \tau}\right)$ is a [[Definition:Generalized Ordered Space/Definition 2|GO-space by Definition 2]]. That is, there is a [[Definition:Linearly Ordered Space|linearly ordered space]] $\left({X, \preceq', \tau'}\right)$ and a [[Definition:Mapping|mapping]] from $Y$ to $X$ which is a [[Definition:Order Embedding|order embedding]] and a [[Definition:Topological Embedding|topological embedding]].	0
Since $a \preceq a$, $a$ is an [[Definition:Upper Bound of Set|upper bound]] of $\set a$. Let $b$ be another [[Definition:Upper Bound of Set|upper bound]] of $\set a$. Then necessarily $a \preceq b$. It follows that indeed: :$\sup \set a = a$ as desired. {{qed}}	0
Let $x \in \R$. Let $\sequence {x_n}$ be the [[Definition:Real Sequence|sequence in $\R$]] defined as $x_n = x^n$. Then: :$\size x < 1$ {{iff}} $\sequence {x_n}$ is a [[Definition:Real Null Sequence|null sequence]].	0
Let $\struct {X, \preceq, \tau}$ be a [[Definition:Linearly Ordered Space|linearly ordered space]]. Let $C$ be a [[Definition:Subset|subset]] of $X$. Then $C$ is [[Definition:Closed Set (Topology)|closed]] in $X$ {{iff}} for all [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subsets]] $S$ of $C$: :If $s \in X$ is a [[Definition:Supremum of Set|supremum]] or [[Definition:Infimum of Set|infimum]] of $S$ in $X$, then $s \in C$.	0
Let $\left({R_1, +_1, \circ_1}\right), \left({R_2, +_2, \circ_2}\right), \ldots, \left({R_n, +_n, \circ_n}\right)$ be [[Definition:Ring (Abstract Algebra)|rings]]. Let $\displaystyle \left({R, +, \circ}\right) = \prod_{k \mathop = 1}^n \left({R_k, +_k, \circ_k}\right)$ be their [[Definition:Ring Direct Product|direct product]]. For each $k \in \left[{1 \,.\,.\, n}\right]$, let: :$R'_k = \left\{{\left({x_1, \ldots, x_n}\right) \in R: \forall j \ne k: x_j = 0}\right\}$ Let $\operatorname{pr}_k: R \to R'_k$ be the [[Definition:Projection (Mapping Theory)|projection on the $k$th coordinate]] of $\left({R, +, \circ}\right)$ onto $R'_k$. Then $\operatorname{pr}_k$ is an [[Definition:Ring Epimorphism|epimorphism]].	0
Let $T_1$ and $T_2$ be [[Definition:Topological Space|topological spaces]]. Let $S_1 \subseteq T_1$ be [[Definition:Connected Set (Topology)|connected]]. Let $f: T_1 \to T_2$ be a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]. Then the [[Definition:Image of Mapping|image]] $f \left({S_1}\right)$ is [[Definition:Connected Set (Topology)|connected]].	0
Let $q \in S$ such that $q < p$. We will show that $F_2$ has an element which is a subset of $q^\ge$. {{explain|$q^\ge$ is the [[Definition:Upper Closure of Element|upper closure]] in what set?}} Since $F_1$ converges to $p$, it has an element: :$A \subseteq q^\ge$. Thus there is an element $k$ in $A$ and an element $M$ in $F_2$ such that all elements of $M$ succeed $k$. Then by [[Extended Transitivity]]: :$M \subseteq q^\ge$ A similar argument using $F_3$ proves the dual statement. Thus $F_2$ converges to $p$. {{explain}} {{qed}} [[Category:Filter Theory]] [[Category:Order Topology]] [[Category:Convergence]] ia5fwo0pwzub1pc2obn184j9ojlmykf	0
Let $\sequence {a_n}$ be the [[Definition:Real Sequence|sequence]] defined as: :$a_n = \paren {1 + \dfrac x n}^n$ Let $\sequence {b_n}$ be the [[Definition:Real Sequence|sequence]] defined as: :$b_n = \paren {1 - \dfrac x n}^n$ Then the [[Definition:Real Multiplication|product]] of the [[Definition:Limit of Real Sequence|limits]] of $\sequence {a_n}$ and $\sequence {b_n}$ equals $1$	0
Let $\family{X_i}_{i \mathop \in I}$ be an [[Definition:Indexed Family|indexed family]] of [[Definition:Non-Empty Set|non-empty]] [[Definition:Topological Space|topological spaces]] where $I$ is an arbitrary [[Definition:Indexing Set|index set]]. Let $\displaystyle X := \prod_{i \mathop \in I} X_i$ be the corresponding [[Definition:Product Space of Topological Spaces|product space]]. Let $\pr_i: X \to X_i$ denote the [[Definition:Projection (Mapping Theory)|projection]] from $X$ onto $X_i$. Let $\FF \subset \powerset X$ be a [[Definition:Filter on Set|filter]] on $X$. Then $\FF$ [[Definition:Convergent Filter|converges]] {{iff}} for all $i \in I$ the [[Definition:Image Filter|image filter]] $\map {\pr_i} \FF$ converges.	0
Let $\left({\tau_i}\right)_{i \in I}$ be an arbitrary [[Definition:Indexing Set|indexed set]] of [[Definition:Topology|topologies]] for a [[Definition:Set|set]] $S$. Then $\tau := \displaystyle \bigcap_{i \mathop \in I} {\tau_i}$ is also a topology for $S$.	0
From [[Basis for Open Ordinal Topology]], the [[Definition:Set of Sets|set]] $\BB$ of [[Definition:Subset|subsets]] of $\hointr 0 \Gamma$ of the form: :$\openint \alpha {\beta + 1} = \hointl \alpha \beta = \set {x \in \hointr 0 \Gamma: \alpha < x < \beta + 1}$ for $\alpha, \beta \in \hointr 0 \Gamma$, forms a [[Definition:Basis (Topology)|basis]] for $\hointr 0 \Gamma$. As $\Gamma$ [[Definition:Strictly Precede|strictly precedes]] $\Omega$, there are a [[Definition:Countable Set|countable number]] of points of $\hointr 0 \Gamma$. Each point in $\hointr 0 \Gamma$ has a [[Definition:Countable Set|countable]] [[Definition:Basis (Topology)|basis]]. From [[Countable Union of Countable Sets is Countable]], it follows that $\hointr 0 \Gamma$ has a [[Definition:Countable Set|countable]] [[Definition:Basis (Topology)|basis]]. {{qed}}	0
By definition, $T$ is [[Definition:Weakly Countably Compact Space|weakly countably compact]] {{iff}} every [[Definition:Infinite Set|infinite]] [[Definition:Subset|subset]] of $S$ has a [[Definition:Limit Point of Set|limit point]] in $S$. Let $H \subseteq S$ be an [[Definition:Infinite Set|infinite]] [[Definition:Subset|subset]] of $S$ where $p \notin H$. $H$ is not [[Definition:Open Set (Topology)|open]] in $T$ by definition. So from [[Subset of Particular Point Space is either Open or Closed]], $H$ is [[Definition:Closed Set (Topology)|closed]] in $T$. Then we have that a [[Closed Set in Particular Point Space has no Limit Points]]. The result follows from definition of [[Definition:Weakly Countably Compact Space|weakly countably compact]]. {{qed}} If $T = \struct {S, \tau_p}$ is a [[Definition:Finite Particular Point Topology|finite particular point space]], then [[Finite Space Satisfies All Compactness Properties]] applies.	0
If $T$ has one of the following properties then $T_H$ has the same property: :[[Definition:Regular Space|Regular Property]] :[[Definition:Tychonoff Space|Tychonoff (Completely Regular) Property]] :[[Definition:Completely Normal Space|Completely Normal Property]] That is, the above properties are all [[Definition:Hereditary Property (Topology)|hereditary]].	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]]. Let $H \subseteq S$. === [[Definition:Interior (Topology)/Definition 1|Definition 1]] === {{:Definition:Interior (Topology)/Definition 1}} === [[Definition:Interior (Topology)/Definition 2|Definition 2]] === {{:Definition:Interior (Topology)/Definition 2}} === [[Definition:Interior (Topology)/Definition 3|Definition 3]] === {{:Definition:Interior (Topology)/Definition 3}}	0
Every non-empty [[Definition:Open Set (Real Analysis)|open set]] $I \subseteq \R$ can be expressed as a [[Definition:Countable|countable]] [[Definition:Set Union|union]] of [[Definition:Pairwise Disjoint|pairwise disjoint]] [[Definition:Open Real Interval|open intervals]]. If: :$\displaystyle I = \bigcup_{n \mathop \in \N} J_n$ :$\displaystyle I = \bigcup_{n \mathop \in \N} K_n$ are two such expressions, then there exists a [[Definition:Permutation|permutation]] $\sigma$ of $\N$ such that: :$\forall n \in \N: J_n = K_{\map \sigma n}$	0
Let $\struct {\mathbb K, \norm{\,\cdot\,} }$ be a [[Definition:Valued Field|valued field]]. Let $\displaystyle \prod_{n \mathop = 1}^\infty a_n$ [[Definition:Convergent Product|converge]] to $a$. Let $\displaystyle \prod_{n \mathop = 1}^\infty b_n$ [[Definition:Convergent Product|converge]] to $b$. Then $\displaystyle \prod_{n \mathop = 1}^\infty a_nb_n$ [[Definition:Convergent Product|converges]] to $ab$.	0
The [[Definition:Cross-Relation|cross-relation]] $\boxtimes$ is a [[Definition:Congruence Relation|congruence relation]] on $\left({S \times C, \oplus}\right)$. === [[Construction of Inverse Completion/Equivalence Relation/Members of Equivalence Classes|Members of Equivalence Classes]] === {{:Construction of Inverse Completion/Equivalence Relation/Members of Equivalence Classes}} === [[Construction of Inverse Completion/Equivalence Relation/Equivalence Class of Equal Elements|Equivalence Class of Equal Elements]] === {{:Construction of Inverse Completion/Equivalence Relation/Equivalence Class of Equal Elements}}	0
Directly apparent from the definition of [[Definition:Cartesian Product|cartesian product]]. {{qed}}	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Let $L = \left({\tau, \preceq}\right)$ be an [[Definition:Inclusion Ordered Set|inclusion ordered set]] of $\tau$. Let $X, Y \in \tau$. Then $X \vee Y = X \cup Y$ and $X \wedge Y = X \cap Y$	0
Let $T = \struct {S, \tau}$ be a [[Definition:T3 Space|$T_3$ space]]. Then $T$ is [[Definition:Fully T4 Space|fully $T_4$]] {{iff}} $T$ is [[Definition:Paracompact Space|paracompact]].	0
Let $T = \struct {S, \tau}$ be a [[Definition:Finite Complement Topology|finite complement topology]] on an [[Definition:Infinite Set|infinite set]] $S$. Then $T$ is a [[Definition:Connected Topological Space|connected space]].	0
From the definition of [[Definition:Closure (Topology)|closure]]: :$\map \cl H$ is the [[Definition:Set Union|union]] of $H$ and its [[Definition:Limit Point of Set|limit points]]. Let $x \in \map \cl H$. If $x \in H$ then $x \in K \implies x \in \map \cl K$. Otherwise $x$ is a [[Definition:Limit Point of Set|limit point]] of $H$. That is, every [[Definition:Open Set (Topology)|open set]] $U$ of $T$ such that $x \in U$ contains $y \in H$ such that $y \ne x$. But as $y \in H$ it follows that $y \in K$. So every [[Definition:Open Set (Topology)|open set]] $U$ of $T$ such that $x \in U$ contains $y \in K$ such that $y \ne x$. This is the definition for a [[Definition:Limit Point of Set|limit point]] of $K$. Thus $x \in \map \cl K$. {{ExtractTheorem|Extract the above paragraph into something like "Limit Point of Subset is Limit Point" (which proves that $H \subseteq K \implies H' \subseteq K'$). Then use [[Set Union Preserves Subsets]] to prove this result.}} {{qed}}	0
We will show that we can choose an [[Definition:Infinite Sequence|infinite sequence]] of [[Definition:Node (Graph Theory)|nodes]] $t_0, t_1, t_2, \ldots$ of $T$ such that: * $t_0$ is the [[Definition:Root Node|root node]]; * $t_{n+1}$ is a [[Definition:Child Node|child]] of $t_n$; * Each $t_n$ has [[Definition:Infinite Set|infinitely]] many [[Definition:Descendant Node|descendants]]. Then the sequence $t_0, t_1, t_2, \ldots$ is such a [[Definition:Branch (Graph Theory)|branch]] of [[Definition:Infinite Branch|infinite]] [[Definition:Length of Branch|length]]. Take the [[Definition:Root Node|root node]] $t_0$. By definition, it has a [[Definition:Finite Set|finite number]] of [[Definition:Child Node|children]]. Suppose that all of these childen had a [[Definition:Finite Set|finite number]] of [[Definition:Descendant Node|descendants]]. Then that would mean that $t_0$ had a finite number of descendants, and that would mean $T$ was [[Definition:Finite Branch|finite]]. So $t_0$ has at least one child with [[Definition:Infinite Set|infinitely many]] [[Definition:Descendant Node|descendants]]. Thus, we may pick $t_1$ as any one of those [[Definition:Child Node|children]]. Now, suppose node $t_k$ has [[Definition:Infinite Set|infinitely many]] [[Definition:Descendant Node|descendants]]. As $t_k$ has a finite number of [[Definition:Child Node|children]], by the same argument as above, $t_k$ has at least one child with [[Definition:Infinite Set|infinitely many]] [[Definition:Descendant Node|descendants]]. Thus we may pick $t_{k+1}$ which has [[Definition:Infinite Set|infinitely many]] [[Definition:Descendant Node|descendants]]. The assertion follows by the [[Axiom:Axiom of Dependent Choice|Axiom of Dependent Choice]]. {{qed}}	0
Checking in turn each of the criteria for [[Definition:Equivalence Relation|equivalence]]: === Reflexivity === From [[Point is Path-Connected to Itself]], we have that $a \sim a$. So $\sim$ is [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetry === If $a \sim b$ then $a$ is is [[Definition:Path-Connected Points|path-connected]] to $b$ by definition. We form the mapping $g: \closedint 0 1 \to \closedint 0 1$: :$\map g x = 1 - x$ which is trivially [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. By [[Composite of Continuous Mappings is Continuous]] $f \circ g$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. Putting it together we see that $f \circ g$ maps $0$ to $b$ and $1$ to $a$. So $b \sim a$ and $\sim$ has been shown to be [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitivity === Follows directly from [[Joining Paths makes Another Path]]. {{qed|lemma}} $\sim$ has been shown to be [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. Hence by definition it is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	0
Let $\mathbf x, \mathbf y \in \R^n$ be arbitrary points of $\R^n$. Define $l: \closedint 0 1 \to \R^n$ by: :$\map l t = \paren {1 - t} \mathbf x + t \mathbf y$ Then $\map l 0 = 1 \mathbf x + 0 \mathbf y = \mathbf x$, whereas $\map l 1 = 0 \mathbf x + 1 \mathbf y = \mathbf y$. Finally, it remains to show that $l$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]]. Fix $\epsilon > 0$ and suppose that $t, t' \in \closedint 0 1$ are such that $\size {t - t'} < \dfrac {\epsilon} {1 + \norm {\mathbf x} + \norm {\mathbf y} }$. Then: {{begin-eqn}} {{eqn | l = \map l t - \map l {t'} | r = \paren {1 - t} \mathbf x + t \mathbf y - \paren {\paren {1 - t'} \mathbf x + t' \mathbf y} }} {{eqn | r = \paren {\paren {1 - t} - \paren {1 - t'} } \mathbf x + \paren {t - t'} \mathbf y }} {{eqn | r = \paren {t' - t} \mathbf x + \paren {t - t'} \mathbf y }} {{end-eqn}} We can now estimate the [[Definition:Euclidean Norm|norm]] of this last expression: {{begin-eqn}} {{eqn | l = \norm {\paren {t' - t} \mathbf x + \paren {t - t'} \mathbf y} | o = \le | r = \norm {\paren {t' - t} \mathbf x} + \norm {\paren {t - t'} \mathbf y} | c = [[Triangle Inequality for Vectors in Euclidean Space]] }} {{eqn | r = \size {t' - t} \norm {\mathbf x} + \size {t - t'} \norm {\mathbf y} | c = Axiom $(N2)$ for [[Definition:Norm on Vector Space|norms]] }} {{eqn | r = \size {t' - t} \paren {\norm {\mathbf x} + \norm {\mathbf y} } }} {{eqn | o = < | r = \frac \epsilon {1 + \norm {\mathbf x} + \norm {\mathbf y} } \paren {\norm {\mathbf x} + \norm {\mathbf y} } }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Since $\epsilon$ was arbitrary, we conclude that $l$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]]. Therefore, it forms a [[Definition:Path (Topology)|path]] from $\mathbf x$ to $\mathbf y$. Since $\mathbf x$ and $\mathbf y$ were arbitrary, it follows that $\R^n$ is [[Definition:Path-Connected Metric Space|path-connected]]. {{qed}}	0
Let $S' = S \times \set {a, b}$. Let $F' \subseteq S'$ such that $F'$ is [[Definition:Closed Set (Topology)|closed]] in $T \times D$. Then $F' = F \times \set {a, b}$ or $F' = F \times \O$ by definition of the [[Definition:Double Pointed Topology|double pointed topology]]. If $F' = F \times \O$ then $F' = \O$ from [[Cartesian Product is Empty iff Factor is Empty]], and the result is trivial. So suppose $F' = F \times \set {a, b}$. From [[Open and Closed Sets in Multiple Pointed Topology]] it follows that $F$ is [[Definition:Closed Set (Topology)|closed]] in $T$. Let $y' = \tuple {y, q} \in \relcomp {S'} {F'}$. Then $y \notin F$. Suppose that $T$ is a [[Definition:T3 Space|$T_3$ space]]. Then by definition: :For any [[Definition:Closed Set (Topology)|closed set]] $F$ of $T$ and any point $y \in S$ such that $y \notin F$ there exist [[Definition:Disjoint Sets|disjoint]] [[Definition:Open Set (Topology)|open sets]] $U, V \in \tau$ such that $F \subseteq U$, $y \in V$. Then $y' \in V \times \set {a, b}$ and $F' \subseteq U \times \set {a, b}$ and: :$U \times \set {a, b} \cap V \times \set {a, b} = \O$ demonstrating that $T \times D$ is a [[Definition:T3 Space|$T_3$ space]]. Now suppose that $T \times D$ is a [[Definition:T3 Space|$T_3$ space]]. Then $\exists U', V' \in S': y' \in V'$ and $F' \subseteq U'$ such that $U' \cap V' = \O$. As $D$ is the [[Definition:Indiscrete Topology|indiscrete topology]] it follows that: :$U' = U \times \set {a, b}$ :$V' = V \times \set {a, b}$ for some $U, V \subseteq T$. From [[Open and Closed Sets in Multiple Pointed Topology]] it follows that $U$ and $V$ are [[Definition:Open Set (Topology)|open]] in $T$. As $U' \cap V' = \O$ it follows that $U \cap V = \O$. It follows that $F$ and $y$ fulfil the conditions that make $T$ a [[Definition:T3 Space|$T_3$ space]]. Hence the result.	0
Recall from [[Real Number Line is Metric Space]] that the [[Definition:Real Number|set of real numbers]] $\R$ with the [[Definition:Distance between Element and Subset of Real Numbers|distance function]] $d$ is a [[Definition:Metric Space|metric space]]. The result is then seen to be an example of [[Distance from Subset to Element]]. {{Qed}}	0
We use the [[Method of Infinite Descent]]. Suppose $T$ is a [[Definition:Tree (Graph Theory)|tree]] which has no [[Definition:Leaf Node|leaf nodes]]. First note that no tree has all [[Definition:Even Vertex (Graph Theory)|even nodes]]. That is because by [[Characteristics of Eulerian Graph]], it would then be an [[Definition:Eulerian Graph|Eulerian graph]], and by definition, trees do not have [[Definition:Circuit|circuits]]. From the [[Handshake Lemma]], we know that $T$ must therefore have at least two [[Definition:Odd Vertex (Graph Theory)|odd nodes]] whose [[Definition:Degree of Vertex|degree]] is at least $3$. As $T$ is [[Definition:Finite Tree|finite]], this number must itself be [[Definition:Finite|finite]]. Of those nodes, there must be two (call them $u$ and $v$) which can be joined by a path $P$ containing no odd nodes apart from $u$ and $v$. (Otherwise you can pick as one of the two nodes one of those in the interior of $P$.) Consider that [[Definition:Path (Graph Theory)|path]] $P$ from $u$ to $v$. As a [[Definition:Tree (Graph Theory)|tree]] has no [[Definition:Circuit|circuits]], all [[Definition:Node of Tree|nodes]] of $P$ are distinct, or at least part of $P$ will describe a [[Definition:Cycle (Graph Theory)|cycle]]. Now consider the [[Definition:Subgraph|subgraph]] $S$ formed by removing the [[Definition:Edge of Graph|edges]] comprising $P$ from $T$, but leaving the [[Definition:Node of Tree|nodes]] where they are. The nodes $u$ and $v$ at either end of $P$ will no longer be [[Definition:Odd Vertex (Graph Theory)|odd]], as they have both had one edge removed from them. All the nodes on $P$ other than $u$ and $v$ will stay [[Definition:Even Vertex (Graph Theory)|even]]. The graph $S$ may become [[Definition:Disconnected Graph|disconnected]], and may even contain [[Definition:Isolated Vertex|isolated nodes]]. However, except for these isolated nodes (which became that way because of being nodes of degree $2$ on $P$), and however many [[Definition:Component (Graph Theory)|components]] $S$ is now in, all the nodes of $S$ are still either even or odd with degree of $3$ or higher. That is because by removing $P$, the only odd nodes we have affected are $u$ and $v$, which are now even. Now, if the nodes in any component of $S$ are all even, that component is [[Characteristics of Eulerian Graph|Eulerian]]. Hence $S$ contains a [[Definition:Circuit|circuit]], and is therefore not a [[Definition:Tree (Graph Theory)|tree]]. From [[Connected Subgraph of Tree is Tree]], it follows that $T$ can not be a [[Definition:Tree (Graph Theory)|tree]] after all. However, if the nodes in any component $T'$ of $S$ are not all even, then there can't be as many [[Definition:Odd Vertex (Graph Theory)|odd nodes]] in it as there are in $T$ (because we have reduced the number by $2$). Also, because of the method of construction of $T'$, all of its [[Definition:Odd Vertex (Graph Theory)|odd nodes]] are of [[Definition:Degree of Vertex|degree]] of at least $3$. By applying the [[Method of Infinite Descent]], it follows that $T$ must contain a [[Definition:Circuit|circuit]], and is therefore not a [[Definition:Tree (Graph Theory)|tree]]. So every tree must have at least one [[Definition:Node of Tree|node]] of [[Definition:Degree of Vertex|degree]] $1$. Now, suppose that $T$ is a [[Definition:Tree (Graph Theory)|tree]] with exactly $1$ [[Definition:Node of Tree|node]] of [[Definition:Degree of Vertex|degree]] $1$. Call this node $u$. From the [[Handshake Lemma]], we know that $T$ must therefore have at least one [[Definition:Odd Vertex (Graph Theory)|odd node]] whose [[Definition:Degree of Vertex|degree]] is at least $3$. Let $P$ be a [[Definition:Path (Graph Theory)|path]] from $u$ to any such odd node such that $P$ passes only through even nodes, as we did above. Again, let us remove all the edges of $P$. By a similar argument to the one above, we will once again have reduced the tree to one in which any remaining odd nodes all have [[Definition:Degree of Vertex|degree]] of at least $3$. Then we are in a position to apply the argument above. Hence $T$ must have at least two [[Definition:Node of Tree|nodes]] of [[Definition:Degree of Vertex|degree]] $1$. {{qed}} === Note === It is instructive to see what happens to the above argument for a tree with exactly two [[Definition:Node of Tree|nodes]] of [[Definition:Degree of Vertex|degree]] $1$. There may be no other [[Definition:Odd Vertex (Graph Theory)|odd nodes]] but these two (call them $u$ and $v$). Such a graph is a [[Definition:Path Graph|path graph]] (which is itself a [[Definition:Tree (Graph Theory)|tree]]), and removing all the edges from the path from $u$ to $v$ leaves an [[Definition:Edgeless Graph|edgeless graph]], which of course has no [[Definition:Circuit|circuits]].	0
{{begin-eqn}} {{eqn | l = \closedint a b | r = \set {x \in \R: x \ge a \land x \le b} | c = {{Defof|Closed Real Interval}} }} {{eqn | ll= \leadsto | l = \R \setminus \closedint a b | r = \R \setminus \set {x \in \R: x \ge a \land x \le b} | c = }} {{eqn | r = \set {x \in \R: x < a \lor x > b} | c = [[De Morgan's Laws (Logic)/Disjunction of Negations|De Morgan's Laws: Disjunction of Negations]] }} {{eqn | r = \openint {-\infty} a \cup \openint b \infty | c = {{Defof|Open Real Interval}} }} {{end-eqn}} From the [[Open Real Interval is Open Set/Corollary|corollary to Open Real Interval is Open Set]], both $\openint {-\infty} a$ and $\openint b \infty$ are [[Definition:Open Set (Metric Space)|open sets]] in $M$. From [[Union of Open Sets of Metric Space is Open]] it follows that $\openint {-\infty} a \cup \openint b \infty$ is [[Definition:Open Set (Metric Space)|open]] in $\R$. But $\openint {-\infty} a \cup \openint b \infty$ is the [[Definition:Relative Complement|relative complement]] of $\closedint a b$ in $\R$. The result follows by definition of [[Definition:Closed Set (Metric Space)|closed set]]. {{qed}}	0
Let $T_A$ be a [[Definition:Sequentially Compact Space|sequentially compact]] space. Take an [[Definition:Infinite Sequence|infinite sequence]] $\sequence {x_n} \subseteq S_B$. From the [[Definition:Surjection|surjectivity]] of $\phi$, there exists another [[Definition:Infinite Sequence|infinite sequence]] $\sequence {y_n} \subseteq S_A$ such that $\map \phi {y_n} = x_n$. By the definition of [[Definition:Sequentially Compact Space|sequential compactness]], there exists a [[Definition:Subsequence|subsequence]] of $\sequence {y_n}$ Let this [[Definition:Subsequence|subsequence]] be named $\sequence {y_{n_k} }$. Let $\sequence {y_{n_k} }$ [[Definition:Convergent Sequence (Topology)|converge]] to $y \in S_A$ with respect to $T_A$. From the continuity of $\phi$, it is concluded that $\map \phi {y_{n_k} } = x_{n_k}$ [[Definition:Convergent Sequence (Topology)|converges]] to $\map \phi y \in S_B$. Thus, $\sequence {x_n}$ has a [[Definition:Subsequence|subsequence]] that converges. By definition, $T_B$ is [[Definition:Sequentially Compact Space|sequentially compact]]. {{qed}}	0
Let $S$ be a [[Definition:Singleton|singleton]]. The only possible [[Definition:Topology|topology]] on $S$ is the [[Definition:Indiscrete Topology|indiscrete topology]].	0
=== $(1)$ implies $(2)$ === Suppose $(1)$ holds. Pick $\varnothing \ne S \subseteq P$. Let $x_1 \in S$ be arbitrary. Given $x_k \in S$, pick $x_{k+1} \in S$ strictly bigger than $x_k$. By hypothesis the process must eventually terminate, say $x_n$ is the last element. Then by construction there are no larger elements than $x_n$, i.e. $x_n$ is a maximal element of $S$. {{qed|lemma}} {{improve|Last three sentences can be made more explicit and less handwaving}} === $(2)$ implies $(1)$ === Suppose $(2)$ holds. Let $\left({x_k}\right)_{k \in \N}$ be an [[Definition:Increasing Sequence|increasing sequence]] of elements of $P$. By hypothesis, the set $\left\{{x_k: k \in \N}\right\}$ has a [[Definition:Maximal Element|maximal element]], say $x_n$. Then since $x_m \ge x_n$ for all $m \geq n$, we must have $x_n = x_{n+1} = \cdots$. {{qed}} [[Category:Increasing Mappings]] [[Category:Sequences]] 26tn5iv2h0yqpm30p23o2v3z3lvjqoz	0
Let each point $x$ of $T$ have a [[Definition:Local Basis|local basis]] $\mathcal D_x$ consisting entirely of [[Definition:Connected Set (Topology)|connected sets]] in $T$.	0
Let $T = \struct {S, \tau_{\bar p} }$ be an [[Definition:Uncountable Excluded Point Topology|uncountable excluded point space]]. Then $T$ is not [[Definition:Second-Countable Space|second-countable]].	0
Let $\struct {A, d}$ be a [[Definition:Sequentially Compact Space|sequentially compact]] [[Definition:Metric Space|metric space]]. From [[Sequentially Compact Metric Space is Lindelöf]], every [[Definition:Open Cover|open cover]] of $A$ has a [[Definition:Countable Subcover|countable subcover]]. Let $C$ be an [[Definition:Open Cover|open cover]] $A$. Extract from it a [[Definition:Countable Subcover|countable subcover]] $\set {U_1, U_2, \ldots}$. {{AimForCont}} there exists no [[Definition:Finite Subcover|finite subcover]] of $C$. Then, for all $n \in \N_{\ge 1}$, the [[Definition:Set|set]] $\set {U_1, \ldots, U_n}$ does ''not'' [[Definition:Cover of Set|cover]] $A$. Hence it is possible to choose $x_n \in A$ such that: :$x_n \notin U_1 \cup \cdots \cup U_n$. Thus we construct an [[Definition:Infinite Sequence|infinite sequence]] $\sequence {x_n}_{n \mathop \ge 1}$ of points of $A$. By assumption $A$ is [[Definition:Sequentially Compact Space|sequentially compact]]. Thus $\sequence {x_n}_{n \mathop \ge 1}$ has a [[Definition:Subsequence|subsequence]] which [[Definition:Convergent Sequence (Metric Space)|converges]] to some $x \in A$. But because the $U_i: i \ge 1$ forms a [[Definition:Cover of Set|cover]] for $A$, there exists some $U_m$ such that $x \in U_m$. Hence by one of the characterizations of convergence, there is an [[Definition:Infinite Set|infinite number]] of [[Definition:Term of Sequence|terms]] in the [[Definition:Sequence|sequence]] $\sequence {x_i}$ which are contained in $U_m$. {{explain|Determine which of those characterizations of convergence.}} But from the method of construction of $\sequence {x_i}$, each $U_n$ can contain only points $x_i$ with $i < n$. That is, each $U_n$ can only contain a [[Definition:Finite Set|finite number]] of the [[Definition:Term of Sequence|terms]] of $\sequence {x_i}$. This is a [[Definition:Contradiction|contradiction]]. Thus the supposition that there exists no [[Definition:Finite Subcover|finite subcover]] of $C$ was false. Hence the result. {{qed}}	0
Let $z_n = x_n + i y_n$. Let $w_n = u_n + i v_n$. Let $c = a + i b$ Let $d = e + i f$. By definition of [[Definition:Convergent Complex Sequence|convergent complex sequence]]: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} z_n | r = c | c = }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} x_n + i \lim_{n \mathop \to \infty} y_n | r = a + i b | c = {{Defof|Convergent Complex Sequence}} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} w_n | r = d | c = }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} u_n + i \lim_{n \mathop \to \infty} v_n | r = e + i f | c = {{Defof|Convergent Complex Sequence}} }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \lim_{n \mathop \to \infty} z_n w_n | r = \lim_{n \mathop \to \infty} \paren {\paren {x_n u_n - y_n v_n} + i \paren {y_n u_n + x_n v_n} } | c = {{Defof|Complex Multiplication}} }} {{eqn | r = \lim_{n \mathop \to \infty} \paren {x_n u_n - y_n v_n} + i \lim_{n \mathop \to \infty} \paren {y_n u_n + x_n v_n} | c = {{Defof|Convergent Complex Sequence}} }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {x_n u_n} - \lim_{n \mathop \to \infty} \paren {y_n v_n} } + i \paren {\lim_{n \mathop \to \infty} \paren {y_n u_n} + \lim_{n \mathop \to \infty} \paren {x_n v_n} } | c = [[Sum Rule for Real Sequences]] }} {{eqn | r = \paren {\lim_{n \mathop \to \infty} \paren {x_n} \lim_{n \mathop \to \infty} \paren {u_n} - \lim_{n \mathop \to \infty} \paren {y_n} \lim_{n \mathop \to \infty} \paren {v_n} } + i \paren {\lim_{n \mathop \to \infty} \paren {y_n} \lim_{n \mathop \to \infty} \paren {u_n} + \lim_{n \mathop \to \infty} \paren {x_n} \lim_{n \mathop \to \infty} \paren {v_n} } | c = [[Product Rule for Real Sequences]] }} {{eqn | r = \paren {a e - b f} + i \paren {b e + a f} | c = }} {{eqn | r = \paren {a + i b} \paren {e + i f} | c = {{Defof|Complex Multiplication}} }} {{eqn | r = c d | c = }} {{end-eqn}} {{qed}}	0
From the definition of [[Definition:Distance between Real Numbers|distance]]: :$\forall x, y \in \R: \map d {x, y} = \size {x - y}$ Thus: :$\displaystyle \map d {x, S} = \map {\inf_{y \mathop \in S} } {\size {x - y} }$ Consider the set $T = \set {\size {x - y}: y \in S}$. This has $0$ as a [[Definition:Lower Bound of Set|lower bound]] as [[Absolute Value is Bounded Below by Zero]]. So: :$\displaystyle \map d {x, S} = \map {\inf_{y \mathop \in S} } {\size {x - y} } \ge 0$ If $x \in S$ then: :$\size {x - x} = 0 \in T$ and so: :$\displaystyle 0 \le \map {\inf_{y \mathop \in S} } {\map d {x, y} }$ Thus: :$\displaystyle \map d {x, S} = \map {\inf_{y \mathop \in S} } {\map d {x, y} } = 0$ {{Qed}}	0
From [[Closed Form for Triangular Numbers]], for $n = 1, 3, 6, 10, \ldots$: :$n = \dfrac {a_n \left({a_n + 1}\right)} 2$ Thus by the [[Quadratic Formula]]: $a_n = \dfrac {-1 \pm \sqrt {1 + 8 n} } 2$ In this context it is the [[Definition:Positive Real Number|positive]] root that is required. The result follows by definition of [[Definition:Ceiling Function|ceiling function]]. {{qed}}	0
From [[Topology induced by Usual Metric on Positive Integers is Discrete]]: :$\struct {\Z_{>0}, \tau_d}$ is a [[Definition:Discrete Space|discrete space]]. From [[Topology induced by Scaled Euclidean Metric on Positive Integers is Discrete]]: :$\struct {\Z_{>0}, \tau_\delta}$ is a [[Definition:Discrete Space|discrete space]]. Let $I_{\Z_{>0} }$ be the [[Definition:Identity Mapping|identity mapping]] from $\Z_{>0}$ to itself. From [[Mapping from Discrete Space is Continuous]]: :$I_{\Z_{>0} }: \struct {\Z_{>0}, \tau_d} \to \struct {\Z_{>0}, \tau_\delta}$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]] and: :$I_{\Z_{>0} }: \struct {\Z_{>0}, \tau_\delta} \to \struct {\Z_{>0}, \tau_d}$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. Hence the result by definition of [[Definition:Homeomorphic Topological Spaces|homeomorphic]]. {{qed}}	0
Let $T$ be a [[Definition:Topological Space|topological space]], and let $H \subseteq T$. Then $H$ is [[Definition:Clopen Set|both closed and open]] in $T$ {{iff}}: :$\partial H = \O$ where $\partial H$ is the [[Definition:Boundary (Topology)|boundary]] of $H$.	0
Let $T = \left({A, \tau}\right)$ be a [[Definition:Hausdorff Space|Hausdorff space]]. Let $\left \langle {x_n} \right \rangle$ be a [[Definition:Sequence|sequence]] in $T$. Suppose $\left \langle {x_n} \right \rangle$ has two [[Definition:Convergent Sequence (Topology)|convergent]] [[Definition:Subsequence|subsequences]] with different [[Definition:Limit Point of Sequence|limit]]. Then $\left \langle {x_n} \right \rangle$ is [[Definition:Divergent Sequence|divergent]].	0
Let $\struct {S, \tau}$ be a [[Definition:Compact Space|compact]] [[Definition:Connected Topological Space|connected]] [[Definition:Hausdorff Space|Hausdorff space]]. Let $A$ be a [[Definition:Closed Set (Topology)|closed]], [[Definition:Non-Empty Set|non-empty]] [[Definition:Proper Subset|proper subset]] of $S$. Let $C$ be a [[Definition:Component (Topology)|component]] of $A$. Then: :$C \cap \partial A \ne \O$ where $\partial A$ denotes the [[Definition:Boundary (Topology)|boundary]] of $A$.	0
It is possible to find a [[Definition:Geometric Sequence of Integers|geometric sequence of integers]] $G_n$ of [[Definition:Length of Sequence|length]] $n + 1$ with a given [[Definition:Common Ratio of Geometric Sequence|common ratio]] such that $G_n$ is in its [[Definition:Geometric Sequence of Integers in Lowest Terms|lowest terms]]. {{:Euclid:Proposition/VIII/2}}	0
Let $\struct {S, \tau_S}$ be a [[Definition:Topological Space|topological space]]. Let $D$ be a [[Definition:Doubleton|doubleton]] endowed with the [[Definition:Indiscrete Topology|indiscrete topology]]. Let $\struct {S \times D, \tau}$ be the [[Definition:Double Pointed Topology|double pointed topology]] on $S$. Let $X \subseteq S \times D$ be a [[Definition:Subset|subset]] of $S \times D$. Then the [[Definition:Closure (Topology)|closure]] of $X$ in $\tau$ is: :$\map \cl X = \map \cl {\map {\pr_1} X} \times D$ where $\pr_1$ denotes the [[Definition:First Projection|first projection]] on $S \times D$.	0
By definition of [[Definition:Particular Point Topology|particular point space]], any [[Definition:Subset|subset]] of $S$ which contains $p$ is [[Definition:Open Set (Topology)|open]] in $T$. {{AimForCont}} $T$ is [[Definition:Meager Space|meager]]. By definition, $T$ is [[Definition:Meager Space|meager]] {{iff}} it is a [[Definition:Countable Union|countable union]] of [[Definition:Subset|subsets]] of $S$ which are [[Definition:Nowhere Dense|nowhere dense]] in $T$. At least one such [[Definition:Nowhere Dense|nowhere dense]] [[Definition:Subset|subset]] $U$ of $S$ must contain $p$. By definition, $U$ is [[Definition:Nowhere Dense|nowhere dense]] in $T$ {{iff}}: :$U^-$ contains no [[Definition:Open Set (Topology)|open set]] of $T$ which is [[Definition:Non-Empty Set|non-empty]] where $U^-$ denotes the [[Definition:Closure (Topology)|closure]] of $U$. By definition of [[Definition:Particular Point Topology|particular point space]], $U$ is [[Definition:Open Set (Topology)|open]] in $T$. By [[Closure of Open Set of Particular Point Space]], $U^- = S$. But $S$ is itself [[Definition:Open Set (Topology)|open]] in $T$ and [[Definition:Non-Empty Set|non-empty]], and so $U$ is not [[Definition:Nowhere Dense|nowhere dense]]. From this [[Definition:Contradiction|contradiction]] it follows that $T$ is [[Definition:Non-Meager Space|non-meager]] {{qed}}	0
Let $S$ be a [[Definition:Set|set]]. Let $\mathcal C$ be a [[Definition:Cover of Set|cover]] for $S$. Let $\mathcal V$ be a [[Definition:Subcover|subcover]] of $\mathcal C$. Then $\mathcal V$ is a [[Definition:Refinement of Cover|refinement]] of $\mathcal C$.	0
Let $T = \struct {S, \tau_S}$ be a [[Definition:Topological Space|topological space]]. Let $D = \struct {\set {a, b}, \tau_D}$ be the [[Definition:Indiscrete Topology|indiscrete topology]] on two points. Let $T \times D$ be the [[Definition:Double Pointed Topology|double pointed topology]] on $T$. Then: :$T \times D$ is not a [[Definition:Kolmogorov Space|$T_0$ (Kolmogorov) space]], a [[Definition:Fréchet Space (Topology)|$T_1$ (Fréchet) space]], a [[Definition:Hausdorff Space|$T_2$ (Hausdorff) space]] or a [[Definition:Completely Hausdorff Space|$T_{2 \frac 1 2}$ (completely Hausdorff) space]]. :$T \times D$ is a [[Definition:T3 Space|$T_3$ space]], a [[Definition:T3 1/2 Space|$T_{3 \frac 1 2}$ space]], a [[Definition:T4 Space|$T_4$ space]] or a [[Definition:T5 Space|$T_5$ space]] {{iff}} $T$ is.	0
[[Proof by Counterexample]]: Let $T = \struct {\R, \tau}$ denote the [[Definition:Right Order Topology on Real Numbers|right order topology on $\R$]]. Let $H \subseteq \R$ be a [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] of $\R$. Let $\CC$ denote the [[Definition:Set|set]] of [[Definition:Condensation Point|condensation points]] of $H$. From [[Finite Set of Right Order Topology with Condensation Points is not Closed]], $H \cup \CC$ is not a [[Definition:Closed Set (Topology)|closed set]] of $T$. {{qed}}	0
Let $T = \struct {S, \set {\O, S} }$ be an [[Definition:Indiscrete Space|indiscrete topological space]]. Then $T$ is [[Definition:Path-Connected Space|path-connected]].	0
Let $\BB_\alpha = \set {N^\alpha_i : i \in \N}$ be a [[Definition:Countable Basis|countable basis]] for $\tau_\alpha$ for each $\alpha \in I$. Let $\pr_\alpha$ denote the [[Definition:Projection|projection]] of $S$ onto $S_\alpha$. Let $\LL_\alpha = \set {\map {\pr_\alpha^{-1} } {N^\alpha_i}: N^\alpha_i \in B_\alpha}$. Let $\KK_J = \displaystyle \set {\bigcap_{\alpha \mathop \in J} L_\alpha : \forall \alpha \in J: L_\alpha \in \LL_\alpha}$ for $J \subset I$, $\size J < \infty$. From [[Product Space Basis Induced from Factor Space Bases]]: :$\displaystyle \BB = \bigcup_{J \mathop \subset I \mathop , \size J \mathop < \infty} \KK_J$ forms a [[Definition:Basis (Topology)|basis]] of the [[Definition:Product Space (Topology)|product space]]. Now, each of the $\LL_\alpha$'s is [[Definition:Countable Set|countable]]. Since the $\KK_J$'s can be identified with a [[Definition:Finite Set|finite]] [[Definition:Cartesian Product|product]] of [[Definition:Countable Set|countable sets]], they are each [[Definition:Countable Set|countable]] From [[Countable Union of Countable Sets is Countable]], $\BB$ forms a [[Definition:Countable Basis|countable basis]] of $S$. {{qed}}	0
By [[Galois Connection implies Upper Adjoint is Surjection iff Lower Adjoint is Injection]]: :$d$ is an [[Definition:Injection|injection]]. Define $d' = d:T \to g\left[{T}\right]$ By definition: :$d'$ is an [[Definition:Injection|injection]]. By definition of [[Definition:Galois Connection|Galois connection]]: :$d$ is an [[Definition:Increasing Mapping|increasing mapping]]. We will prove that :$d$ is [[Definition:Order Embedding|order embedding]]. Let $x, y \in T$. By definition of [[Definition:Increasing Mapping|increasing mapping]]: :$x \precsim y \implies d\left({x}\right) \preceq d\left({y}\right)$ Thus by definition of $d'$ and [[Definition:Ordered Subset|ordered subset]]: :$x \precsim y \implies d'\left({x}\right) \preceq' d'\left({y}\right)$ Assume that :$d'\left({x}\right) \preceq' d'\left({y}\right)$ By [[Upper Adjoint of Galois Connection is Surjection implies Lower Adjoint at Element is Minimum of Preimage of Singleton of Element]] :$\forall t \in T: d\left({t}\right) = \min\left({g^{-1}\left[{\left\{ {t}\right\} }\right]}\right)$ By [[Lower Adjoint at Element is Minimum of Preimage of Singleton of Element implies Composition is Identity]]: :$g \circ d = I_T$ By definition of [[Definition:Galois Connection|Galois connection]]: :$g$ is an [[Definition:Increasing Mapping|increasing mapping]]. Thus by definition of $d'$ and [[Definition:Ordered Subset|ordered subset]]: :$d\left({x}\right) \preceq d\left({y}\right)$ By definition of [[Definition:Increasing Mapping|increasing mapping]]: :$g\left({d\left({x}\right)}\right) \precsim g\left({d\left({y}\right)}\right)$ Thus by definitions of [[Definition:Composition of Mappings|composition of mappings]] and [[Definition:Identity Mapping|identity mapping]]: :$x \precsim y$ {{qed|lemma}} By definition: :$d'$ is a [[Definition:Surjection|surjection]]. Hence $d'$ is [[Definition:Order Isomorphism|order isomorphism]] between $R$ and $N$. {{qed}}	0
Let $S$ be a [[Definition:Set|set]]. Let $\tau_1$ and $\tau_2$ be [[Definition:Topology|topologies]] on $S$. {{TFAE|def = Finer Topology}}	0
From [[Closure of Subset of Indiscrete Space]], $H^- = S$, where $H^-$ denotes [[Definition:Closure (Topology)|set closure]]. From [[Interior of Subset of Indiscrete Space]], $H^\circ = \O$, where $H^\circ$ denotes [[Definition:Interior (Topology)|set interior]]. By definition: :$\partial H = H^- \setminus H^\circ = S \setminus \O = S$ From [[Open and Closed Sets in Topological Space]], $\O$ and $S$ are both [[Definition:Closed Set (Topology)|closed]] and [[Definition:Open Set (Topology)|open]] in $T$. So from [[Set Clopen iff Boundary is Empty]]: :$\partial H = \O$ {{qed}}	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]]. Let $\tau^*_{\bar p}$ be the [[Definition:Open Extension Topology|open extension topology]] of $\tau$. Then $\tau^*_{\bar p}$ is a [[Definition:Topology|topology]] on $S^*_p = S \cup \set p$.	0
By definition of [[Definition:Isomorphism (Abstract Algebra)|isomorphism]], each $\phi_k$ is a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]] which is a [[Definition:Bijection|bijection]]. From [[Cartesian Product of Bijections is Bijection/General Result|Cartesian Product of Bijections is Bijection: General Result]], $\phi$ is a [[Definition:Bijection|bijection]]. From [[Homomorphism of External Direct Products/General Result|Homomorphism of External Direct Products: General Result]], $\phi$ is a [[Definition:Homomorphism (Abstract Algebra)|homomorphism]]. Hence the result. {{qed}}	0
Consider the definition of $\mathcal C$ [[Definition:Cantor Set/Limit of Decreasing Sequence|as a limit of a decreasing sequence]]. In the notation as introduced there, we see that each $S_n$ is a collection of [[Definition:Disjoint Sets|disjoint]] [[Definition:Closed Real Interval|closed intervals]]. From [[Closed Set Measurable in Borel Sigma-Algebra]], these are [[Definition:Measurable Set|measurable sets]]. Furthermore, each $S_n$ is [[Definition:Finite Set|finite]]. Hence by [[Sigma-Algebra Closed under Union]], it follows that $C_n := \displaystyle \bigcup S_n$ is [[Definition:Measurable Set|measurable]] as well. Then, as we have: :$\mathcal C = \displaystyle \bigcap_{n \mathop \in \N} C_n$ it follows from [[Sigma-Algebra Closed under Countable Intersection]] that $\mathcal C$ is [[Definition:Measurable Set|measurable]]. The $C_n$ also form a [[Definition:Decreasing Sequence of Sets|decreasing sequence of sets]] with [[Definition:Limit of Decreasing Sequence of Sets|limit]] $\mathcal C$. Thus, from [[Characterization of Measures|Characterization of Measures: $(3')$]], it follows that: :$\lambda \left({\mathcal C}\right) = \displaystyle \lim_{n \to \infty} \lambda \left({C_n}\right)$ It is not too hard to show that, for all $n \in \N$: :$\lambda \left({C_n}\right) = \left({\dfrac 2 3}\right)^n$ {{finish|yes, I know}} Now we have by [[Sequence of Powers of Number less than One]] that: :$\displaystyle \lim_{n \mathop \to \infty} \left({\frac 2 3}\right)^n = 0$ and the result follows. {{qed}}	0
Let $T = \struct {X, \tau}$ and $S = \struct {Y,\sigma}$ be [[Definition:Topological Space|topological spaces]]. For all $i \in \set {1, 2, \ldots, n}$, let $C_i$ be [[Definition:Closed Set (Topology)|closed]] in $T$. Let $f: X \to Y$ be a [[Definition:Mapping|mapping]] such that the [[Definition:Restriction of Mapping|restriction]] $f \restriction_{C_i}$ is [[Definition:Continuous Mapping (Topology)|continuous]] for all $i$. Then $f$ is [[Definition:Continuous Mapping (Topology)|continuous]] on $C = \displaystyle \bigcup_{i \mathop = 1}^n C_i$, that is, $f \restriction_C$ is [[Definition:Continuous Mapping (Topology)|continuous]]. If $\family {C_i}$ is [[Definition:Infinite Set|infinite]], the result does not necessarily hold.	0
By [[Set with Relative Complement forms Partition]], $H$ and $\relcomp S H$ form a [[Definition:Partition (Set Theory)|partition]] of $S$. By [[Complement of Clopen Set is Clopen]], $\relcomp S H$ is also a [[Definition:Clopen Set|clopen set]] of $T$. By definition of [[Definition:Clopen Set|clopen set]], both $H$ and $\relcomp S H$ are [[Definition:Open Set (Topology)|open]] in $T$. Thus $H$ and $\relcomp S H$ are a pair of [[Definition:Open Set (Topology)|open sets]] in $T$ forming a [[Definition:Partition (Set Theory)|partition]] of $S$. Hence the result, by definition of [[Definition:Separation (Topology)|separation]]. {{qed}} [[Category:Clopen Sets]] [[Category:Separations]] mtqtkd0juvgm6ca51szmsyuesx1qhac	0
Let $\struct {S, \tau}$ be an [[Definition:Urysohn Space|Urysohn space]]. Then $\struct {S, \tau}$ is also a [[Definition:Completely Hausdorff Space|$T_{2 \frac 1 2}$ (completely Hausdorff) space]].	0
Let $\sequence {x_n}, \sequence {y_n}$ be [[Definition:Real Sequence|sequences in $\R$]]. Let $x_n \to l, y_n \to m$ as $n \to \infty$. Suppose that for all $n \in \N$, $x_n \le y_n$. Then: :$l \le m$ that is: :$\displaystyle \lim_{n \mathop \to \infty} x_n \le \lim_{n \mathop \to \infty} y_n$ This is often phrased as: ''limits preserve inequalities''.	0
{{AimForCont}} $T$ is a [[Definition:Completely Hausdorff Space|$T_{2 \frac 1 2}$ (completely Hausdorff) space]]. Then from [[Completely Hausdorff Space is Hausdorff Space]], $T$ is a [[Definition:T2 Space|$T_2$ (Hausdorff) space]]. This [[Definition:Contradiction|contradicts]] the result [[Partition Topology is not Hausdorff|Partition Topology is not Hausdorff]]. Hence the result, by [[Proof by Contradiction]]. {{qed}}	0
Let $\mathbf 2$ denote [[Definition:Two (Boolean Algebra)|two]]. Then $\mathbf 2$ is a [[Definition:Boolean Algebra|Boolean algebra]].	0
:$x_n \to \infty$ as $n \to \infty$ {{iff}} $\size {\dfrac 1 {x_n} } \to 0$ as $n \to \infty$	0
Let $S$ be a [[Definition:Set|set]] containing either exactly one or exactly two [[Definition:Element|elements]]. Let $\family {\tau_i}_{i \mathop \in I}$ be an arbitrary [[Definition:Non-Empty Set|non-empty]] [[Definition:Indexing Set|indexed set]] of [[Definition:Topology|topologies]] for a [[Definition:Set|set]] $S$. Then $\tau := \displaystyle \bigcup_{i \mathop \in I} {\tau_i}$ is also a [[Definition:Topology|topology]] for $S$.	0
Let $T = \struct {S, \tau_p}$ be a [[Definition:Particular Point Topology|particular point space]]. Then $T$ is [[Definition:First-Countable Space|first-countable]].	0
Let $F \subseteq S$ be [[Definition:Closed Set (Topology)|closed]], and let $x \in S$, $x \notin F$. Denote by $S \setminus F$ the [[Definition:Relative Complement|relative complement]] of $F$ in $S$. Define a [[Definition:Mapping|mapping]] $f: S \to \closedint 0 1$ as: :$\map f s := \begin{cases} 1 & : \text { if $s \in F$} \\ 0 & : \text { if $s \in S \setminus F$} \end{cases}$ Then $f$ is identically $1$ on $F$, and identically $0$ on $\set x$. Now if $f$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]], it will be a [[Definition:Urysohn Function|Urysohn function]] for $F$ and $\set y$, and $T$ will be a [[Definition:T3 1/2 Space|$T_{3 \frac 1 2}$ space]]. Now for any $V \subseteq \closedint 0 1$, we have: :$f^{-1} \sqbrk V = \begin{cases} \O & : \text{ if $0,1 \notin V$} \\ F & : \text{ if $0 \notin V$ and $1 \in V$} \\ S \setminus F & : \text{ if $0 \in V$ and $1 \notin V$} \\ S & : \text{ if $0,1 \in V$} \end{cases}$ By definition of $\tau$, $F$ is [[Definition:Open Set (Topology)|open]] in $T$. By [[Open Set in Partition Topology is also Closed]], $F$ is also [[Definition:Closed Set (Topology)|closed]], and so $S \setminus F$ is [[Definition:Open Set (Topology)|open]] in $T$. Thus, the [[Definition:Preimage of Subset under Mapping|preimage]] of any [[Definition:Subset|subset]] $V$ of $\closedint 0 1$ is [[Definition:Open Set (Topology)|open]] in $T$. In particular, this holds for the [[Definition:Open Set (Topology)|open sets]] of $\closedint 0 1$. It follows that $f$ is a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]], and so a [[Definition:Urysohn Function|Urysohn function]]. Hence $T$ is [[Definition:T3 1/2 Space|$T_{3 \frac 1 2}$ space]]. {{qed}}	0
Let $S = \R_{\ge 0} \setminus \Z$. Let $\tau$ be the [[Definition:Deleted Integer Topology|deleted integer topology]] on $S$. Then the [[Definition:Topological Space|topological space]] $T = \struct {S, \tau}$ is [[Definition:Second-Countable Space|second-countable]].	0
Let $\struct {S, \preceq}$ and $\struct {T, \preceq'}$ be [[Definition:Ordered Set|ordered sets]]. Let $\phi: S \to T$ be an [[Definition:Order Embedding|order embedding]] of $\struct {S, \preceq}$ into $\struct {T, \preceq'}$ Let $p \in S$. Then: :$\map {\phi^{-1} } {\map \phi p^{\succ'} } = p^\succ$ where $\cdot^\succ$ and $\cdot^{\succ'}$ represent [[Definition:Strict Upper Closure of Element|strict upper closure]] with respect to $\preceq$ and $\preceq'$, respectively.	0
We observe that: {{begin-eqn}} {{eqn | l = \frac 1 j - \frac 1 {j + 2} | r = \frac {\left({j + 2}\right) - j} {j \left({j + 2}\right)} | c = }} {{eqn | r = \frac 2 {\left({j \left({j + 2}\right)}\right)} | c = }} {{end-eqn}} Therefore: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 1}^n \frac 1 {j \left({j + 2}\right)} | r = \frac 1 2 \sum_{j \mathop = 1}^n \left({\frac 1 j - \frac 1 {j + 2} }\right) | c = }} {{eqn | r = \frac 1 2 \left({\left({\frac 1 1 - \frac 1 3}\right) + \left({\frac 1 2 - \frac 1 4}\right) + \left({\frac 1 3 - \frac 1 5}\right) + \cdots + \left({\frac 1 {n - 1} - \frac 1 {n + 1} }\right) + \left({\frac 1 n - \frac 1 {n + 2} }\right)}\right) | c = }} {{eqn | r = \frac 1 2 \left({1 + \frac 1 2 - \frac 1 {n + 1} - \frac 1 {n + 2} }\right) | c = [[Definition:Telescoping Series|Telescoping Series]] }} {{eqn | r = \frac 3 4 - \frac {\left({n + 2}\right) + \left({n + 1}\right)} {2 \left({n + 1}\right) \left({n + 2}\right)} | c = }} {{eqn | r = \frac 3 4 - \frac {2 n + 3} {2 \left({n + 1}\right) \left({n + 2}\right)} | c = }} {{end-eqn}} {{qed}} [[Category:Sums of Sequences]] [[Category:Reciprocals]] od0yoj6ihkbagwbz6odwmwc4tyb6knk	0
We have that the [[Definition:Euclidean Metric|Euclidean metric]] on $\mathcal A$ is defined as: :$\displaystyle \map {d_2} {x, y} = \paren {\sum_{i \mathop = 1}^n \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2}$ where $x = \tuple {x_1, x_2, \ldots, x_n}, y = \tuple {y_1, y_2, \ldots, y_n} \in \mathcal A$. === Proof of $M1$ === {{begin-eqn}} {{eqn | l = \map {d_2} {x, x} | r = \paren {\sum_{i \mathop = 1}^n \paren {\map {d_{i'} } {x_i, x_i} }^2}^{\frac 1 2} | c = Definition of $d_2$ }} {{eqn | r = \paren {\sum_{i \mathop = 1}^n 0^2}^{\frac 1 2} | c = as $d_{i'}$ fulfils [[Definition:Metric Space Axioms|axiom $M1$]] }} {{eqn | r = 0 | c = }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $M1$]] holds for $d_2$. {{qed|lemma}} === Proof of $M2$ === Let: :$(1): \quad z = \tuple {z_1, z_2, \ldots, z_n}$ :$(2): \quad$ all summations be over $i = 1, 2, \ldots, n$ :$(3): \quad \map {d_{i'} } {x_i, y_i} = r_i$ :$(4): \quad \map {d_{i'} } {y_i, z_i} = s_i$. Thus we need to show that: :$\displaystyle \paren {\sum \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2} + \paren {\sum \paren {\map {d_{i'} } {y_i, z_i} }^2}^{\frac 1 2} \ge \paren {\sum \paren {\map {d_{i'} } {x_i, z_i} }^2}^{\frac 1 2}$ We have: {{begin-eqn}} {{eqn | l = \map {d_2} {x, y} + \map {d_2} {y, z} | r = \paren {\sum \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2} + \paren {\sum \paren {\map {d_{i'} } {y_i, z_i} }^2}^{\frac 1 2} | c = Definition of $d_2$ }} {{eqn | r = \paren {\sum r_i^2}^{\frac 1 2} + \paren {\sum s_i^2}^{\frac 1 2} | c = }} {{eqn | o = \ge | r = \paren {\sum \paren {r_i + s_i}^2}^{\frac 1 2} | c = [[Minkowski's Inequality for Sums/Index 2|Minkowski's Inequality for Sums: index $2$]] }} {{eqn | r = \paren {\sum \paren {\map {d_{i'} } {x_i, y_i} + \map {d_{i'} } {y_i, z_i} }^2}^{\frac 1 2} | c = Definition of $r_i$ and $s_i$ }} {{eqn | o = \ge | r = \paren {\sum \paren {\map {d_{i'} } {x_i, z_i} }^2}^{\frac 1 2} | c = as $d_{i'}$ fulfils [[Definition:Metric Space Axioms|axiom $M2$]] }} {{eqn | r = \map {d_2} {x, z} | c = Definition of $d_2$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $M2$]] holds for $d_2$. {{qed|lemma}} === Proof of $M3$ === {{begin-eqn}} {{eqn | l = \map {d_2} {x, y} | r = \paren {\sum \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2} | c = Definition of $d_2$ }} {{eqn | r = \paren {\sum \paren {\map {d_{i'} } {y_i, x_i} }^2}^{\frac 1 2} | c = as $d_i$ fulfils [[Definition:Metric Space Axioms|axiom $M3$]] }} {{eqn | r = \map {d_2} {y, x} | c = Definition of $d_2$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $M3$]] holds for $d_2$. {{qed|lemma}} === Proof of $M4$ === {{begin-eqn}} {{eqn | l = x | o = \ne | r = y | c = }} {{eqn | ll= \leadsto | l = \exists k \in \closedint 1 n: x_k | o = \ne | r = y_k | c = }} {{eqn | ll= \leadsto | l = \map {d_k} {x_k, y_k} | o = > | r = 0 | c = as $d_k$ fulfils [[Definition:Metric Space Axioms|axiom $M4$]] }} {{eqn | ll= \leadsto | l = \paren {\sum \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2} | o = > | r = 0 | c = }} {{eqn | ll= \leadsto | l = \map {d_2} {x, y} | o = > | r = 0 | c = Definition of $d_2$ }} {{end-eqn}} So [[Definition:Metric Space Axioms|axiom $M4$]] holds for $d_2$. {{qed}}	0
Let $a \in H$. Let $\epsilon \in \R_{>0}$. Let $\delta = \epsilon$. Then: {{begin-eqn}} {{eqn | l = d_H \left({a, y}\right) | o = < | r = \delta | c = for some $y \in H$ }} {{eqn | ll= \implies | l = d \left({i_H \left({a}\right), i_H \left({y}\right)}\right) | r = d \left({a, y}\right) | c = }} {{eqn | r = d_H \left({a, y}\right) | c = }} {{eqn | o = < | r = \delta | c = }} {{eqn | r = \epsilon | c = }} {{end-eqn}} So by definition $d_H$ is [[Definition:Continuous at Point of Metric Space|continuous at $a$]]. As $a \in H$ is arbitrary, it follows that $d_H$ is [[Definition:Continuous on Metric Space|continuous on $H$]]. {{qed}}	0
We have: : [[Uncountable Discrete Space is not Second-Countable]] : [[Excluded Point Topology is Open Extension Topology of Discrete Topology]] The result follows from [[Condition for Open Extension Space to be Second-Countable]] {{qed}}	0
=== $(1)$ implies $(2)$ === Let $G \subseteq X$ be [[Definition:Open Set (Metric Space)|open]] in $\left({X, d}\right)$. Let $x \in G$. Let $\left\langle{x_n}\right\rangle$ be a [[Definition:Sequence|sequence]] in $X$ such that $x_n \to x$. By definition of [[Definition:Open Set (Metric Space)|open set]], there exists $\epsilon > 0$ such that: : $B_\epsilon \left({x}\right) \subseteq G$ where $B_\epsilon \left({x}\right)$ is the [[Definition:Open Ball|open $\epsilon$-ball]] of $x$. Since $x_n \to x$, there exists $n_0 \in \N$ such that: : $\forall n \ge n_0: d \left({x_n, x}\right) < \epsilon$ Thus: : $\forall n \ge n_0: x_n \in B_\epsilon \left({x}\right) \subseteq G$ {{qed|lemma}} === $(2)$ implies $(1)$ === Let $G \subseteq X$. Let: :$\forall x \in G: \forall \left\langle{x_n}\right\rangle \in X: x_n \to x: \exists n_0 \in \N: \forall n \ge n_0: \left\langle{x_n}\right\rangle \in G$ Aiming for a [[Definition:Contradiction|contradiction]], suppose $G$ is ''not'' [[Definition:Open Set (Metric Space)|open]] in $\left({X, d}\right)$. Then: : $\exists x \in G: \forall \epsilon \in \R_{>0}: B_\epsilon \left({x}\right) \nsubseteq G$ Therefore, for $n = 1, 2, \ldots$ we can [[Principle of Mathematical Induction|inductively]] find a [[Definition:Sequence|sequence]]: : $x_n \in B_{1/n} \left({x}\right) \cap \left({X \setminus G}\right) \implies x_n \notin G$ and: : $\forall n \in \N: d \left({x_n, x}\right) < \dfrac 1 n \implies x_n \to x \in G$ and $\forall n \in \N: x_n \notin G$ This contradicts hypothesis $(2)$. Thus, by [[Proof by Contradiction]], $G$ is [[Definition:Open Set (Metric Space)|open]] in $\left({X, d}\right)$. {{qed}}	0
Let $T_A = \struct {A, \tau_A}$ and $T_B = \struct {B, \tau_B}$ be [[Definition:Topological Space|topological spaces]]. Let $\phi: T_A \to T_B$ be a [[Definition:Everywhere Continuous Mapping (Topology)|continuous mapping]]. If $T_A$ is a [[Definition:First-Countable Space|first-countable space]], then it does not necessarily follow that $T_B$ is also [[Definition:First-Countable Space|first-countable]].	0
Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] of [[Definition:Positive Real Number|positive real numbers]]. Let $x_n$ [[Definition:Convergent Real Sequence|converge]] to $L$. Then $L \ge 0$.	0
Let $T = \struct {S, \tau_p}$ be an [[Definition:Uncountable Particular Point Topology|uncountable particular point space]]. Then $T$ is not [[Definition:Second-Countable Space|second-countable]].	0
We examine each condition for [[Definition:Equivalence Relation|equivalence]]. === Reflexivity === For any function $f: X \to Y$, define $H: X \times \closedint 0 1 \to Y$ by $\map H {x, t} := \map f x$. This yields a [[Definition:Homotopy|homotopy]] between $f$ and itself. Also, trivially, if $x \in K$ and $t \in \closedint 0 1$, then: :$\map f x = H \left({x, t}\right)$ so that $H$ is a [[Definition:Relative Homotopy|homotopy relative to $K$]]. Thus $\sim$ is a [[Definition:Reflexive Relation|reflexive relation]]. {{qed|lemma}} === Symmetry === Given a [[Definition:Relative Homotopy|$K$-relative homotopy]]: :$H: X \times \closedint 0 1 \to Y$ from $\map f x = \map H {x, 0}$ to $\map g x = \map H {x, 1}$, the function: :$\map G {x, t} = \map H {x, 1 - t}$ is a [[Definition:Relative Homotopy|$K$-relative homotopy]] from $g$ to $f$. Thus $\sim$ is a [[Definition:Symmetric Relation|symmetric relation]]. {{qed|lemma}} === Transitivity === Suppose that $f \sim g$ and $g \sim h$. Let $F, G: X \times \closedint 0 1 \to Y$ be [[Definition:Relative Homotopy|$K$-relative homotopies]] between $f$ and $g$, $g$ and $h$, respectively. Define $H: X \times \closedint 0 1 \to Y$ by: :$\map H {x, t} := \begin {cases} \map F {x, 2 t} & : 0 \le t \le \dfrac 1 2 \\ \map G {x, 2 t - 1} & : \dfrac 1 2 \le t \le 1 \end{cases}$ By [[Continuous Mapping on Finite Union of Closed Sets]], $H$ is a [[Definition:Relative Homotopy|$K$-relative homotopy]] between $f$ and $h$. Thus $\sim$ is a [[Definition:Transitive Relation|transitive relation]]. {{qed|lemma}} Having verified all three conditions, it follows that $\sim$ is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}} [[Category:Homotopy Theory]] k4bkjkcd1kcgumsnpf4mlv4xrsm2po4	0
Let $x \in X$. By [[Supremum of Singleton]]: :$\left\{ {x}\right\}$ admits a [[Definition:Supremum of Set|supremum]] and $\sup \left\{ {x}\right\} = x$ By definitions of [[Definition:Subset|subset]] and [[Definition:Singleton|singleton]]: :$\left\{ {x}\right\} \subseteq X$ By [[Singleton is Finite]]: :$\left\{ {x}\right\}$ is a [[Definition:Finite Set|finite set]]. Thus by definition of [[Definition:Finite Suprema Set|finite suprema set]]: :$x \in \operatorname{finsups}\left({X}\right)$ {{qed}}	0
The [[Definition:Integer Sequence|sequence]] of [[Definition:Square Number|square numbers]] which can be expressed as the [[Definition:Integer Addition|sum]] of a [[Definition:Integer Sequence|sequence]] of [[Definition:Odd Number|odd]] [[Definition:Cube Number|cubes]] from $1$ begins: :$1, 1225, 1 \, 413 \, 721, 1 \, 631 \, 432 \, 881, \dotsc$ {{OEIS|A046177}} The [[Definition:Integer Sequence|sequence]] of [[Definition:Square Root|square roots]] of this [[Definition:Integer Sequence|sequence]] is: :$1, 35, 1189, 40 \, 391, \dotsc$ {{OEIS|A046176}}	0
Let $\partial^A_\bullet$, $\partial^B_\bullet$ denote the differential on $A_\bullet$, respectively $B_\bullet$. First it will be demonstrated that: :$\forall a \in \ker \left({\partial^A_n}\right) \subseteq A_n: f_n \left({a}\right) \in \ker \left({\partial^B_n}\right)$ Thus: :$\partial^B_n f_n \left({a}\right) = f_{n - 1} \left({\partial^A_n a}\right) = f_{n - 1} \left({0}\right) = 0$ Thus there exists a map: :$\bar f_n: \ker \left({\partial^A_n}\right) \to \ker \left({\partial^B_n}\right)$ via the restriction of $f$. {{explain|Restriction of $f$ to what?}} Next will be shown that: :$\forall a \in \operatorname {Im} \left({\partial^A_{n + 1} }\right): f_n \left({a}\right) \in \operatorname {Im} \left({\partial^B_{n + 1} }\right)$ Let $a = \partial^A_{n + 1}a'$ Then: :$\partial^B_{n + 1} \left({f_{n + 1} \left({a'}\right)}\right) = f_n \left({\partial^A_n a'}\right) = f_n \left({a}\right)$. Let $\pi: \ker \left({\partial^B_n}\right) \to H_n \left({B_\bullet}\right)$ be the quotient map. Let $\rho = \pi \circ \bar f_n$. From above: :$\operatorname{Im} \left({\partial^A_{n + 1} }\right) \subseteq \ker \left({\rho}\right)$ Thus $\rho$ factors through a map: :$\tilde f_n: H_n \left({A_\bullet}\right) \to H_n \left({B_\bullet}\right)$ completing the proof. {{qed}} [[Category:Homological Algebra]] 6jscnmx37cpml3xy9rzvr19jleqx6rr	0
{{begin-eqn}} {{eqn | l = S_n | r = \sum_{j \mathop = 1}^n \paren {2 j - 1} | c = [[Odd Number Theorem]] }} {{eqn | r = \sum_{j \mathop = 1}^{n - 1} \paren {2 j - 1} + \paren {2 n - 1} | c = {{Defof|Summation}} }} {{eqn | r = S_{n - 1} + \paren {2 n - 1} | c = [[Odd Number Theorem]] }} {{end-eqn}} {{qed}} [[Category:Odd Number Theorem]] e5imnpnznujgeroh2tc2wl2ur5udrfg	0
Let $B_R \paren \xi$ denote the [[Definition:Open Ball|open $R$-ball]] of $\xi$. Let $z \in B_R \paren \xi$. Then $S \paren z$ [[Definition:Absolutely Convergent Series|converges absolutely]]. If $R = +\infty$, we define $B_R \paren \xi = \C$.	0
Let $\sequence {a_n}$ be a [[Definition:Real Sequence|sequence in $\R$]]. Then $\sequence {a_n}$ is a [[Definition:Real Cauchy Sequence|Cauchy sequence]] {{iff}} it is [[Definition:Convergent Real Sequence|convergent]].	0
Let $S_1, S_2, \ldots, S_j, \ldots, S_n$ be [[Definition:Set|sets]]. Let $\displaystyle \prod_{i \mathop = 1}^n S_i$ be the [[Definition:Finite Cartesian Product|Cartesian product]] of $S_1, S_2, \ldots, S_n$. For each $j \in \set {1, 2, \ldots, n}$, the '''$j$th projection on $\displaystyle S = \prod_{i \mathop = 1}^n S_i$''' is the [[Definition:Mapping|mapping]] $\pr_j: S \to S_j$ defined by: :$\map {\pr_j} {s_1, s_2, \ldots, s_j, \ldots, s_n} = s_j$ for all $\tuple {s_1, s_2, \ldots, s_n} \in S$.	0
Let $T = \struct {S, \tau}$ be a [[Definition:Countable Complement Space|countable complement space]]. Let $H \subseteq S$ be an [[Definition:Uncountable Set|uncountable]] [[Definition:Subset|subset]] of $S$. Then every point of $S$ is a [[Definition:Limit Point of Set|limit point]] of $H$. Let $H \subseteq S$ be an [[Definition:Countable Set|countable]] or [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] of $S$. Then $H$ contains all its [[Definition:Limit Point of Set|limit points]].	0
Let $T = \struct {S, \tau}$ be the [[Definition:Arens-Fort Space|Arens-Fort space]]. Then $T$ is not a [[Definition:Countably Compact Space|countably compact space]].	0
Let $R \subset \mathbb{SO} \left({3}\right)$ be the [[Definition:Group|group]] [[Definition:Generator of Group|generated]] by the $\pi$ and $\dfrac {2 \pi} 3$ rotations around different axes. The elements: :$\psi = \begin{pmatrix} -\tfrac 1 2 & \tfrac{\sqrt 3} 2 & 0 \\ -\tfrac{\sqrt 3} 2 & -\tfrac 1 2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}$ :$\phi = \begin{pmatrix} -\cos \left({\vartheta}\right) & 0 & \sin \left({\vartheta}\right) \\ 0 & -1 & 0 \\ \sin \left({\vartheta}\right) & 0 & \cos \left({\vartheta}\right) \\ \end{pmatrix}$ form a [[Definition:Basis (Topology)|basis]] for $R$ for some $\vartheta$. We have: :$\psi^3 = \phi^2 = \mathbf I_3$ where $\mathbf I_3$ is the [[Definition:Identity Mapping|identity mapping]] in $\mathbb R^3$. Therefore $\forall r \in R: r \ne \mathbf I_3, \psi, \phi, \psi^2$ there exists some $n \in \N$ and some set of numbers $m_k \in \left\{{1, 2}\right\}, 1 \le k \le n$ such that $r$ can written as one of the following: :$\displaystyle \text{(a)}: r = \prod_{k \mathop = 1}^n \phi \psi^{m_k}$ :$\displaystyle \text{(b)}: r = \psi^{m_1} \left({\prod_{k \mathop = 2}^n \phi \psi^{m_k}}\right) \phi$ :$\displaystyle \text{(c)}: r = \left({\prod_{k \mathop = 1}^n \phi \psi^{m_k}}\right) \phi$ :$\displaystyle \text{(d)}: r = \psi^{m_1} \left({\prod_{k \mathop = 2}^n \phi \psi^{m_k}}\right)$ Now we fix $\vartheta$ such that $\mathbf I_3$ cannot be written in any of the ways $\text{(a)}$, $\text{(b)}$, $\text{(c)}$, $\text{(d)}$. The action of $R$ on $\mathbb S^2$ will leave two points unchanged for each element of $R$ (the intersection of the axis of rotation and the sphere, to be exact). Since $R$ is finitely generated, it is a [[Definition:Countable|countable]] group. Therefore the set of points of $\mathbb S^2$ which are unchanged by at least one element of $R$ is also countable. We call this set $D \subset \mathbb S^2$, so that $R$ acts freely on $\mathbb S^2 - D$. By [[Set of Orbits forms Partition]], this [[Definition:Set Partition|partitions]] $\mathbb S^2 - D$ into [[Definition:Orbit (Group Theory)|orbits]]. By the [[Axiom:Axiom of Choice|Axiom of Choice]], there is a set $X$ containing one element of each [[Definition:Orbit (Group Theory)|orbit]]. For any $r \in R$, let $X_r$ be the [[Definition:Group Action|action]] of $r$ on $X$. We have: :$\displaystyle \mathbb S^2 - D = \bigcup_{r \mathop \in R} X_r$ Define the sets $A, B, C$ to be the smallest sets satisfying :$X \subseteq A$ :$\text{If } X_r \subset A, B, C, \text{ then } X_{r\phi} \subset B, A, C, \text{respectively.}$ :$\text{If } X_r \subset A, B, C, \text{ then } X_{r\psi} \subset B, C, A, \text{respectively.}$ :$\text{If } X_r \subset A, B, C, \text{ then } X_{r\phi^2} \subset C, A, B, \text{respectively.}$ These sets are defined due to the uniqueness of the properties $\text{(a)}$ to $\text{(d)}$ Also, $A, B, C, B \cup C$ are congruent since they are rotations of each other, namely: :$A_\psi = B, B_{\psi^2} = C, A_\phi = B \cup C$ Hence we have constructed the sets $A, B, C, D$ of the theorem. {{qed}} {{AoC}} Whether you view this result as a [[Definition:Veridical Paradox|veridical paradox]] or an [[Definition:Antinomy|antinomy]] depends your acceptance or otherwise of the [[Axiom:Axiom of Choice|Axiom of Choice]]. {{namedfor|Felix Hausdorff|cat = Hausdorff}}	0
By continuity of $x \mapsto 1 / x$: :$\ds \lim_{N \mathop \to \infty} \prod_{n \mathop = 1}^N \frac 1 {1 + a_n} = \frac 1 a$ It remains to prove the absolute convergence. Because $\ds \prod_{n \mathop = 1}^\infty \paren {1 + a_n}$ [[Definition:Absolute Convergence of Product|converges absolutely]], $\ds \sum_{n \mathop = 1}^\infty a_n$ [[Definition:Absolutely Convergent Series|converges absolutely]]. By [[Factors in Absolutely Convergent Product Converge to One]], $\norm {a_n} \le \dfrac 1 2 $ for $n$ [[Definition:Sufficiently Large|sufficiently large]]. Thus $\norm {\dfrac 1 {a_n + 1} - 1} = \norm {\dfrac {a_n} {a_n + 1} } \le 2 \norm {a_n}$ for $n$ [[Definition:Sufficiently Large|sufficiently large]]. By the [[Comparison Test]], $\ds \sum_{n \mathop = 1}^\infty \paren {\frac 1 {a_n + 1} - 1}$ [[Definition:Absolutely Convergent Series|converges absolutely]]. Thus $\ds \prod_{n \mathop = 1}^\infty \frac 1 {1 + a_n}$ [[Definition:Absolute Convergence of Product|converges absolutely]]. {{qed}} [[Category:Infinite Products]] bvj860sg8ra24s02t8zbbtpfd9gxcwc	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] which is a [[Definition:T3 1/2 Space|$T_{3 \frac 1 2}$ space]]. Let $T_H = \struct {H, \tau_H}$, where $\O \subset H \subseteq S$, be a [[Definition:Topological Subspace|subspace]] of $T$. Then $T_H$ is a [[Definition:T3 1/2 Space|$T_{3 \frac 1 2}$ space]].	0
For all $n \in \N_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$ === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \dfrac {x^1 - 1} {x - 1} | r = 1 | c = }} {{eqn | r = 2^0 | c = }} {{eqn | r = \sum_{j \mathop = 0}^{1 - 1} x^j | c = }} {{end-eqn}} so $\map P 1$ holds. This is our [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now we need to show that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is our [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_{j \mathop = 0}^{k - 1} x^j = \frac {x^k - 1} {x - 1}$ Then we need to show: :$\displaystyle \sum_{j \mathop = 0}^k x^j = \frac {x^{k + 1} - 1} {x - 1}$ === Induction Step === This is our [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \sum_{j \mathop = 0}^k x^j | r = \sum_{j \mathop = 0}^{k - 1} x^j + x^k | c = }} {{eqn | r = \frac {x^k - 1} {x - 1} + x^k | c = }} {{eqn | r = \frac {x^k - 1 + \paren {x - 1} x^k} {x - 1} | c = }} {{eqn | r = \frac {x^k - 1 + x^{k + 1} - x^k} {x - 1} | c = }} {{eqn | r = \frac {x^{k + 1} - 1} {x - 1} | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \N_{> 0}: \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$ {{qed}}	0
Let $\left\langle{a_n}\right\rangle$ be a [[Definition:Sequence|sequence]] of [[Definition:Complex Number|complex numbers]]. Let $\log$ denote the [[Definition:Complex Logarithm|complex logarithm]]. {{TFAE|def = Absolute Convergence of Product}}	0
{{proof wanted|proof from the fact that an isometry is a homeomorphism}}	0
Let $M = \struct {A, d}$ be a [[Definition:Metric Space|metric space]]. Then $M$ is a [[Definition:Paracompact Space|paracompact space]].	0
The nature of the [[Definition:Real Secant Function|secant function]] on the [[Definition:Real Number|set of real numbers]] $\R$ is as follows: :$(1): \quad \sec x$ is [[Definition:Continuous on Interval|continuous]] and [[Definition:Strictly Increasing Real Function|strictly increasing]] on the [[Definition:Half-Open Real Interval|intervals]] $\hointr 0 {\dfrac \pi 2}$ and $\hointl {\dfrac \pi 2} \pi$ :$(2): \quad \sec x$ is [[Definition:Continuous on Interval|continuous]] and [[Definition:Strictly Decreasing Real Function|strictly decreasing]] on the [[Definition:Half-Open Real Interval|intervals]] $\hointr {-\pi} {-\dfrac \pi 2}$ and $\hointl {-\dfrac \pi 2} 0$ :$(3): \quad \sec x \to + \infty$ as $x \to -\dfrac \pi 2^+$ :$(4): \quad \sec x \to + \infty$ as $x \to \dfrac \pi 2^-$ :$(5): \quad \sec x \to - \infty$ as $x \to \dfrac \pi 2^+$ :$(6): \quad \sec x \to - \infty$ as $x \to \dfrac {3 \pi} 2^-$	0
=== [[Triple Angle Formulas/Sine|Triple Angle Formula for Sine]] === {{:Triple Angle Formulas/Sine}} === [[Triple Angle Formulas/Cosine|Triple Angle Formula for Cosine]] === {{:Triple Angle Formulas/Cosine}} === [[Triple Angle Formulas/Tangent|Triple Angle Formula for Tangent]] === {{:Triple Angle Formulas/Tangent}}	0
{{begin-eqn}} {{eqn | l = \map \arccos {-x} | r = y | c = }} {{eqn | ll= \leadstoandfrom | l = -x | r = \cos y: | rr= 0 \le y \le \pi | c = {{Defof|Arccosine}} }} {{eqn | ll= \leadstoandfrom | l = x | r = -\cos y: | rr= -\pi \le y \le 0 | c = }} {{eqn | ll= \leadstoandfrom | l = x | r = \map \cos {\pi - y}: | rr= 0 \le y \le \pi | c = [[Cosine of Supplementary Angle]] }} {{eqn | ll= \leadstoandfrom | l = \arccos x | r = \pi - y | c = {{Defof|Arccosine}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sin \sqrt t | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {\sqrt t}^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Real Sine Function}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n} {\paren {2 n + 1}!} t^{n + \frac 1 2} | c = }} {{eqn | ll= \leadsto | l = \laptrans {\sin \sqrt t} | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\map \Gamma {n + \frac 3 2} } {\paren {2 n + 1}! s^{n + \frac 3 2} } | c = [[Laplace Transform of Power]], [[Linear Combination of Laplace Transforms]] }} {{eqn | r = \frac 1 {s^{3/2} } \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {n + \frac 1 2} \map \Gamma {n + \frac 1 2} } {\paren {2 n + 1}! s^n} | c = [[Gamma Difference Equation]] }} {{eqn | r = \frac {\sqrt \pi} {2 s^{3/2} } \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {\paren {2 n + 1} \paren {2 n}!} {2^{2 n} n! \paren {2 n + 1}! s^n} | c = [[Gamma Function of Positive Half-Integer]] }} {{eqn | r = \frac {\sqrt \pi} {2 s^{3/2} } \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n} {2^{2 n} n! s^n} | c = }} {{eqn | r = \frac {\sqrt \pi} {2 s^{3/2} } \sum_{n \mathop = 0}^\infty \frac 1 {n!} \paren {-\frac 1 {2^2 s} }^n | c = }} {{eqn | r = \dfrac {\sqrt \pi} {2 s^{3/2} } e^{-1/\paren {2^2 s} } | c = {{Defof|Exponential Function/Real|subdef = Sum of Series|Exponential Function}} }} {{eqn | r = \dfrac {\sqrt \pi} {2 s^{3/2} } \map \exp {-\dfrac 1 {4 s} } | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \cos^2 x \rd x | r = \frac x 2 + \frac {\sin 2 x} 4 + C | c = [[Primitive of Square of Cosine of a x|Primitive of $\cos^2 x$]] }} {{eqn | ll= \leadsto | l = \int_0^{\frac \pi 2} \cos^2 x \rd x | r = \intlimits {\frac x 2 + \frac {\sin 2 x} 4} 0 {\frac \pi 2} | c = }} {{eqn | r = \paren {\frac \pi 4 + \frac {\sin \pi} 4} - \paren {\frac 0 2 + \frac {\sin 0} 4} | c = }} {{eqn | r = \frac \pi 4 | c = [[Sine of Multiple of Pi]] and simplifying }} {{end-eqn}} {{qed}}	0
It is apparent by inspection that: :$(1): \quad g$ is an [[Definition:Extension of Mapping|extension]] of $f$ :$(2): \quad g$ is an [[Definition:Even Function|even function]]. Let $\map T x$ be the [[Definition:Fourier Series|Fourier series]] representing $g$: :$\map g x \sim \map T x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos \frac {n \pi x} \lambda + b_n \sin \frac {n \pi x} \lambda}$ where for all $n \in \Z_{> 0}$: :$a_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \cos \frac {n \pi x} \lambda \rd x$ :$b_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \sin \frac {n \pi x} \lambda \rd x$ From [[Fourier Sine Coefficients for Even Function over Symmetric Range]]: :$\forall n \in \Z_{> 0}: b_n = 0$ and from [[Fourier Cosine Coefficients for Even Function over Symmetric Range]]: :$\forall n \in \Z_{>0}: a_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map g x \cos \frac {n \pi x} \lambda \rd x$ But on $\openint 0 \lambda$, by definition: :$\forall x \in \openint 0 \lambda: \map g x = \map f x$ Hence: :$\map T x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren a_n \cos \frac {n \pi x} \lambda$ where: :$a_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map f x \cos \frac {n \pi x} \lambda \rd x$ That is, $\map T x$ is the same as $\map S x$. Hence the result. {{qed}} [[Category:Half-Range Fourier Series]] 4jkesnhoecrlce0p4zm4l919iuqymah	0
As [[Sine Function is Absolutely Convergent]] and [[Cosine Function is Absolutely Convergent]], we have: {{begin-eqn}} {{eqn | l = \cos z + i \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {z^{2 n} } {\paren {2 n}!} + i \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Complex Cosine Function}} and {{Defof|Complex Sine Function}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\paren {-1}^n \dfrac {z^{2 n} } {\paren {2 n}!} + i \paren {-1}^n \dfrac {z^{2 n + 1} } {\paren {2 n + 1}!} } | c = [[Sum of Absolutely Convergent Series]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\dfrac {\paren {i z}^{2 n} } {\paren {2 n}!} + \dfrac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = {{Defof|Imaginary Unit}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \dfrac {\paren {i z}^n} {n!} }} {{eqn | r = e^{i z} | c = {{Defof|Exponential Function/Complex|subdef = Sum of Series|Complex Exponential Function}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan x \paren {\tan x + \cot x} | r = \frac {\sin x} {\cos x} \paren {\frac {\sin x} {\cos x} + \frac {\cos x} {\sin x} } | c = {{Defof|Tangent Function}} and {{Defof|Cotangent}} }} {{eqn | r = \frac {\sin x} {\cos x} \paren {\frac {\sin^2 x + \cos^2 x} {\cos x \sin x} } }} {{eqn | r = \frac {\sin x} {\cos x} \paren {\frac 1 {\cos x \sin x} } | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \frac 1 {\cos^2 x} }} {{eqn | r = \sec^2 x | c = {{Defof|Secant Function}} }} {{end-eqn}} {{qed}}	0
The '''haversine''' of an [[Definition:Angle|angle]] is defined as: {{begin-eqn}} {{eqn | l = \hav \theta | r = \dfrac 1 2 \paren {1 - \cos \theta} | c = }} {{eqn | r = \dfrac 1 2 \vers \theta | c = where $\vers \theta$ denotes the [[Definition:Versed Sine|versed sine]] of $\theta$ }} {{eqn | r = \sin^2 \dfrac {\theta} 2 | c = [[Double Angle Formulas/Cosine/Corollary 2|Double Angle Formula for Cosine: Corollary 2]] }} {{end-eqn}}	0
{{begin-eqn}} {{eqn | l = \sec i | r = \sech 1 | c = [[Hyperbolic Secant in terms of Secant]] }} {{eqn | r = \frac 2 {e^1 + e^{-1} } | c = {{Defof|Hyperbolic Secant}} }} {{eqn | r = \frac {2 e} {e^2 + 1} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $e$ }} {{end-eqn}} {{qed}}	0
Let $\map S x$ be a [[Definition:Trigonometric Series|trigonometric series]]: :$\map S x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos n x + b_n \sin n x}$ Let the [[Definition:Series|series]]: :$\displaystyle \sum_{n \mathop = 1}^\infty \paren {\size {a_n} + \size {b_n} }$ be [[Definition:Convergent Series of Numbers|convergent]]. Then $S$ is a [[Definition:Convergent Series of Numbers|convergent series]].	0
:$\sec 180 \degrees = \sec \pi = -1$	0
Let a [[Definition:Point|point]] $P = \tuple {x, y}$ be placed in a [[Definition:Cartesian Plane|cartesian plane]] with [[Definition:Origin|origin]] $O$ such that $OP$ forms an [[Definition:Angle|angle]] $\theta$ with the [[Definition:X-Axis|$x$-axis]]. Then: {{begin-eqn}} {{eqn | l = \frac {\cos \theta} {\sin \theta} | r = \frac {x / r} {y / r} | c = [[Cosine of Angle in Cartesian Plane]] and [[Sine of Angle in Cartesian Plane]] }} {{eqn | r = \frac x r \frac r y | c = }} {{eqn | r = \frac x y | c = }} {{eqn | r = \cot \theta | c = [[Cotangent of Angle in Cartesian Plane]] }} {{end-eqn}} When $\sin \theta = 0$ the expression $\dfrac {\cos \theta} {\sin \theta}$ is not defined. {{qed}}	0
:$\cos 300 \degrees = \cos \dfrac {5 \pi} 3 = \dfrac 1 2$	0
{{begin-eqn}} {{eqn | l = \csc i | r = \frac 1 {\sin i} | c = Definition of [[Definition:Complex Cosecant Function|Complex Cosecant]] }} {{eqn | r = \frac 1 {\left({ \frac e 2 - \frac 1 {2e} }\right) i} | c = [[Sine of i|Sine of $i$]] }} {{eqn | r = \left({ \frac 1 {\frac 1 {2e} - \frac e {2} } }\right) i | c = [[Reciprocal of i|Reciprocal of $i$]] }} {{eqn | r = \left({\frac {2 e} {1 - e^2} }\right) i | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $2 e$ }} {{end-eqn}} {{qed}}	0
:$\map \sec {x + \dfrac {3 \pi} 2} = \csc x$	0
{{begin-eqn}} {{eqn | l = \map \sec {\frac \pi 2 - \theta} | r = \frac 1 {\map \cos {\frac \pi 2 - \theta} } | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 1 {\sin \theta} | c = [[Cosine of Complement equals Sine]] }} {{eqn | r = \csc \theta | c = [[Cosecant is Reciprocal of Sine]] }} {{end-eqn}} The above is valid only where $\sin \theta \ne 0$, as otherwise $\dfrac 1 {\sin \theta}$ is undefined. From [[Sine of Multiple of Pi]] it follows that this happens when $\theta \ne n \pi$. {{qed}}	0
:$\displaystyle \int e^{a x} \sin b x \rd x = \frac {e^{a x} \paren {a \sin b x - b \cos b x} } {a^2 + b^2} + C$	0
{{begin-eqn}} {{eqn | l = \map \sec {\frac {3 \pi} 2 - \theta} | r = \frac 1 {\map \cos {\frac {3 \pi} 2 - \theta} } | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 1 {-\sin \theta} | c = [[Cosine of Three Right Angles less Angle]] }} {{eqn | r = -\csc \theta | c = [[Cosecant is Reciprocal of Sine]] }} {{end-eqn}} {{qed}}	0
A direct implementation of [[Sine of Multiple of Pi]]: :$\forall n \in \Z: \sin n \pi = 0$ In this case, $n = 2$ and so: :$\sin 2 \pi = 0$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \arccot {-x} | r = y | c = }} {{eqn | ll= \leadstoandfrom | l = -x | r = \cot y: | rr= 0 \le y \le \pi | c = {{Defof|Arccotangent}} }} {{eqn | ll= \leadstoandfrom | l = x | r = -\cot y: | rr= -\pi \le y \le 0 | c = }} {{eqn | ll= \leadstoandfrom | l = x | r = \map \cot {\pi - y}: | rr= 0 \le y \le \pi | c = [[Cotangent of Supplementary Angle]] }} {{eqn | ll= \leadstoandfrom | l = \arccot x | r = \pi - y | c = {{Defof|Arccotangent}} }} {{end-eqn}} {{qed}}	0
:$\laptrans {\dfrac {\sin a t} t} = \arctan \dfrac a s$	0
{{begin-eqn}} {{eqn | l = \csc 195^\circ | r = \csc \left({360^\circ - 165^\circ}\right) | c = }} {{eqn | r = -\csc 165^\circ | c = [[Cosecant of Conjugate Angle]] }} {{eqn | r = - \left({\sqrt 6 + \sqrt 2}\right) | c = [[Cosecant of 165 Degrees]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan 15 \degrees | r = \tan \frac {30 \degrees} 2 | c = }} {{eqn | r = \frac {1 - \cos 30 \degrees} {\sin 30 \degrees} | c = [[Half Angle Formulas/Tangent/Corollary 2|Half Angle Formula for Tangent: Corollary 2]] }} {{eqn | r = \frac {1 - \frac {\sqrt 3} 2} {\frac 1 2} | c = [[Cosine of 30 Degrees|Cosine of $30 \degrees$]] and [[Sine of 30 Degrees|Sine of $30 \degrees$]] }} {{eqn | r = 2 - \sqrt 3 | c = multiplying top and bottom by $2$ }} {{end-eqn}} {{qed}}	0
:$\sin x + \cos x = \sqrt 2 \, \map \cos {x - \dfrac \pi 4}$	0
:$\displaystyle \frac {\tan x} {\sec x + 1} = \frac {\sec x - 1} {\tan x}$	0
{{begin-eqn}} {{eqn | l = \sin 255^\circ | r = \sin \left({360^\circ - 105^\circ}\right) | c = }} {{eqn | r = - \sin 105^\circ | c = [[Sine of Conjugate Angle]] }} {{eqn | r = - \frac {\sqrt 6 + \sqrt 2} 4 | c = [[Sine of 105 Degrees]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot 330^\circ | r = \cot \left({360^\circ - 30^\circ}\right) | c = }} {{eqn | r = -\cot 30^\circ | c = [[Cotangent of Conjugate Angle]] }} {{eqn | r = -\sqrt 3 | c = [[Cotangent of 30 Degrees]] }} {{end-eqn}} {{qed}}	0
The following values of the [[Definition:Cosecant|cosecant function]] can be expressed as exact [[Definition:Algebraic Number|algebraic numbers]]. This list is non-exhaustive. === [[Cosecant of Zero]] === {{:Cosecant of Zero}} === [[Cosecant of 15 Degrees]] === {{:Cosecant of 15 Degrees}} === [[Cosecant of 30 Degrees]] === {{:Cosecant of 30 Degrees}} === [[Cosecant of 45 Degrees]] === {{:Cosecant of 45 Degrees}} === [[Cosecant of 60 Degrees]] === {{:Cosecant of 60 Degrees}} === [[Cosecant of 75 Degrees]] === {{:Cosecant of 75 Degrees}} === [[Cosecant of Right Angle]] === {{:Cosecant of Right Angle}} === [[Cosecant of 105 Degrees]] === {{:Cosecant of 105 Degrees}} === [[Cosecant of 120 Degrees]] === {{:Cosecant of 120 Degrees}} === [[Cosecant of 135 Degrees]] === {{:Cosecant of 135 Degrees}} === [[Cosecant of 150 Degrees]] === {{:Cosecant of 150 Degrees}} === [[Cosecant of 165 Degrees]] === {{:Cosecant of 165 Degrees}} === [[Cosecant of Straight Angle]] === {{:Cosecant of Straight Angle}} === [[Cosecant of 195 Degrees]] === {{:Cosecant of 195 Degrees}} === [[Cosecant of 210 Degrees]] === {{:Cosecant of 210 Degrees}} === [[Cosecant of 225 Degrees]] === {{:Cosecant of 225 Degrees}} === [[Cosecant of 240 Degrees]] === {{:Cosecant of 240 Degrees}} === [[Cosecant of 255 Degrees]] === {{:Cosecant of 255 Degrees}} === [[Cosecant of Three Right Angles]] === {{:Cosecant of Three Right Angles}} === [[Cosecant of 285 Degrees]] === {{:Cosecant of 285 Degrees}} === [[Cosecant of 300 Degrees]] === {{:Cosecant of 300 Degrees}} === [[Cosecant of 315 Degrees]] === {{:Cosecant of 315 Degrees}} === [[Cosecant of 330 Degrees]] === {{:Cosecant of 330 Degrees}} === [[Cosecant of 345 Degrees]] === {{:Cosecant of 345 Degrees}} === [[Cosecant of Full Angle]] === {{:Cosecant of Full Angle}}	0
{{begin-eqn}} {{eqn | o = | r = 2 \sin \alpha \cos \beta }} {{eqn | r = 2 \paren {\dfrac {\exp \paren {i \alpha} - \exp \paren {-i \alpha} } {2 i} } \paren {\dfrac {\exp \paren {i \beta} + \exp \paren {-i \beta} } 2} | c = [[Sine Exponential Formulation]] and [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 1 {2 i} \paren {\exp \paren {i \alpha} - \exp \paren {-i \alpha} } \paren {\exp \paren {i \beta} + \exp \paren {-i \beta} } }} {{eqn | r = \frac 1 {2 i} \paren {\exp \paren {i \paren {\alpha + \beta} } - \exp \paren {-i \paren {\alpha + \beta} } + \exp \paren {i \paren {\alpha - \beta} } - \exp \paren {-i \paren {\alpha - \beta} } } }} {{eqn | r = \frac {\exp \paren {i \paren {\alpha + \beta} } - \exp \paren {-i \paren {\alpha + \beta} } } {2 i} + \frac {\exp \paren {i \paren {\alpha - \beta} } - \exp \paren {-i \paren {\alpha - \beta} } } {2 i} }} {{eqn | r = \sin \paren {\alpha + \beta} + \sin \paren {\alpha - \beta} }} {{end-eqn}} {{qed}}	0
Let $\triangle A'B'C'$ be the [[Definition:Polar Triangle|polar triangle]] of $\triangle ABC$. Let the [[Definition:Side of Spherical Triangle|sides]] $a', b', c'$ of $\triangle A'B'C'$ be [[Definition:Opposite (in Triangle)|opposite]] $A', B', C'$ respectively. From [[Spherical Triangle is Polar Triangle of its Polar Triangle]] we have that: :not only is $\triangle A'B'C'$ be the [[Definition:Polar Triangle|polar triangle]] of $\triangle ABC$ :but also $\triangle ABC$ is the [[Definition:Polar Triangle|polar triangle]] of $\triangle A'B'C'$. Let $s' = \dfrac {a' + b' + c'} 2$. We have: {{begin-eqn}} {{eqn | l = \cos \dfrac {A'} 2 | r = \sqrt {\dfrac {\sin s' \, \map \sin {s' - a'} } {\sin b' \sin c'} } | c = [[Cosine of Half Angle for Spherical Triangles]] }} {{eqn | ll= \leadsto | l = \cos \dfrac {\pi - a} 2 | r = \sqrt {\dfrac {\sin s' \, \map \sin {s' - a'} } {\map \sin {\pi - B} \, \map \sin {\pi - C} } } | c = [[Side of Spherical Triangle is Supplement of Angle of Polar Triangle]] }} {{eqn | ll= \leadsto | l = \map \cos {\dfrac \pi 2 - \dfrac a 2} | r = \sqrt {\dfrac {\sin s' \, \map \sin {s' - a'} } {\sin B \sin C} } | c = [[Sine of Supplementary Angle]] }} {{eqn | ll= \leadsto | l = \sin \dfrac a 2 | r = \sqrt {\dfrac {\sin s' \, \map \sin {s' - a'} } {\sin B \sin C} } | c = [[Sine of Complement equals Cosine]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = s' - a' | r = \dfrac {\paren {\pi - A} + \paren {\pi - B} + \paren {\pi - C} } 2 - \paren {\pi - A} | c = [[Side of Spherical Triangle is Supplement of Angle of Polar Triangle]] }} {{eqn | r = \dfrac \pi 2 - \dfrac {A + B + C} 2 + A | c = simplifying }} {{eqn | r = \dfrac \pi 2 - \paren {S - A} | c = where $S = \dfrac {A + B + C} 2$ }} {{eqn | ll= \leadsto | l = \map \sin {s' - a'} | r = \map \sin {\dfrac \pi 2 - \paren {S - A} } | c = }} {{eqn | r = \map \cos {S - A} | c = [[Sine of Complement equals Cosine]] }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = s' | r = \dfrac {\paren {\pi - A} + \paren {\pi - B} + \paren {\pi - C} } 2 | c = [[Side of Spherical Triangle is Supplement of Angle of Polar Triangle]] }} {{eqn | r = \dfrac {3 \pi} 2 - \dfrac {A + B + C} 2 | c = simplifying }} {{eqn | r = \dfrac {3 \pi} 2 - S | c = where $S = \dfrac {A + B + C} 2$ }} {{eqn | ll= \leadsto | l = \sin s' | r = \map \sin {\dfrac {3 \pi} 2 - S} | c = }} {{eqn | ll= \leadsto | l = \sin s' | r = -\map \sin {\dfrac \pi 2 - S} | c = [[Sine of Angle plus Straight Angle]] }} {{eqn | r = -\cos S | c = [[Sine of Complement equals Cosine]] }} {{end-eqn}} The result follows. {{qed}}	0
:$\displaystyle \int \tan^2 a x \ \mathrm d x = \frac {\tan a x} a - x + C$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arctan \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\d u} {\d x} | r = \frac a {x^2 + a^2} | c = [[Derivative of Arctangent of x over a|Derivative of $\arctan \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = 1 | c = }} {{eqn | ll= \implies | l = v | r = x | c = [[Primitive of Constant]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \arctan \frac x a \rd x | r = x \arctan \frac x a - \int x \paren {\frac a {x^2 + a^2} } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = x \arctan \frac x a - a \int \frac {x \rd x} {x^2 + a^2} + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = x \arctan \frac x a - a \paren {\frac 1 2 \ln \paren {x^2 + a^2} } + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{eqn | r = x \arctan \frac x a - \frac a 2 \ln \paren {x^2 + a^2} + C | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $\map f t := \map \Si t = \displaystyle \int_0^t \dfrac {\sin u} u \rd u$. Then: :$\map f 0 = 0$ and: {{begin-eqn}} {{eqn | l = \map \Si t | r = \int_0^t \dfrac {\sin u} u \rd u | c = {{Defof|Sine Integral Function}} }} {{eqn | r = \int_0^t \dfrac 1 u \paren {u - \dfrac {u^3} {3!} + \dfrac {u^5} {5!} - \dfrac {u^7} {7!} + \dotsb} \rd u | c = {{Defof|Real Sine Function}} }} {{eqn | r = t - \dfrac {t^3} {3 \times 3!} + \dfrac {t^5} {5 \times 5!} - \dfrac {t^7} {7 \times 7!} + \dotsb | c = [[Primitive of Power]] }} {{eqn | ll= \leadsto | l = \laptrans {\map \Si t} | r = \laptrans {t - \dfrac {t^3} {3 \times 3!} + \dfrac {t^5} {5 \times 5!} - \dfrac {t^7} {7 \times 7!} + \dotsb} | c = }} {{eqn | r = \dfrac 1 {s^2} - \dfrac 1 {3 \times 3!} \dfrac {3!} {s^4} + \dfrac 1 {5 \times 5!} \dfrac {5!} {s^6} - \dfrac 1 {7 \times 7!} \dfrac {7!} {s^8} + \dotsb | c = [[Laplace Transform of Positive Integer Power]] }} {{eqn | r = \dfrac 1 {s^2} - \dfrac 1 {3 s^4} + \dfrac 1 {5 s^6} - \dfrac 1 {7 s^8} + \dotsb | c = simplifying }} {{eqn | r = \dfrac 1 s \paren {\dfrac {\paren {1 / s} } 1 - \dfrac {\paren {1 / s}^3} 3 + \dfrac {\paren {1 / s}^5} 5 - \dfrac {\paren {1 / s}^7} 7 + \dotsb} | c = rearranging }} {{eqn | r = \dfrac 1 s \arctan \dfrac 1 s | c = [[Power Series Expansion for Real Arctangent Function]] }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arccos \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {-1} {\sqrt {a^2 - x^2} } | c = [[Derivative of Arccosine of x over a|Derivative of $\arccos \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^2} 2 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \arccos \frac x a \rd x | r = \frac {x^2} 2 \arccos \frac x a - \int \frac {x^2} 2 \paren {\frac {-1} {\sqrt {a^2 - x^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a + \frac 1 2 \int \frac {x^2 \rd x} {\sqrt {a^2 - x^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a + \frac 1 2 \paren {\frac {-x \sqrt {a^2 - x^2} } 2 + \frac {a^2} 2 \arcsin \frac x a} + C | c = [[Primitive of x squared over Root of a squared minus x squared|Primitive of $\dfrac {x^2} {\sqrt {a^2 - x^2} }$]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + \frac {a^2} 4 \paren {\frac \pi 2 - \arccos \frac x a} + C | c = [[Sum of Arcsine and Arccosine]] }} {{eqn | r = \paren {\frac {x^2} 2 - \frac {a^2} 4} \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + \frac {\pi a^2} 8 + C | c = simplifying }} {{eqn | r = \paren {\frac {x^2} 2 - \frac {a^2} 4} \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + C | c = subsuming $\dfrac {\pi a^2} 8$ into the [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
:$\ds \int x^4 \cos a x \rd x = \frac {\sin a x} a x^4 + \frac {4 \cos a x} {a^2} x^3 - \frac {12 \sin a x} {a^3} x^2 - \frac {24 \cos a x} {a^4} x + \frac {24 \sin a x} {a^5} + C$	0
{{begin-eqn}} {{eqn | l = \sin x | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Real Sine Function}} }} {{eqn | r = \left({-1}\right)^0 \frac{x^{2 \cdot 0 + 1} } { \left({2 \cdot 0 + 1}\right)!} + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} }} {{eqn | r = x + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \lim_{x \mathop \to 0} \frac {\sin x} x | r = \lim_{x \mathop \to 0} \frac {x + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} } x }} {{eqn | r = \lim_{x \mathop \to 0} \frac x x + \lim_{x \mathop \to 0} \frac{\sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n + 1} } {\paren {2 n + 1}!} } x }} {{eqn | r = 1 + \lim_{x \mathop \to 0} \frac {\sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!} } 1 | c = [[Power Series is Differentiable on Interval of Convergence]] and [[L'Hôpital's Rule]] }} {{eqn | r = 1 + \lim_{x \mathop \to 0} \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {x^{2 n} } {\paren {2 n}!} }} {{eqn | r = 1 + \sum_{n \mathop = 1}^\infty \paren {-1}^n \frac {0^{2 n} } {\paren {2 n}!} | c = [[Real Polynomial Function is Continuous]] }} {{eqn | r = 1 }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {\sin a t} | r = \laptrans {\frac {e^{i a t} - e^{-i a t} } {2 i} } | c = [[Sine Exponential Formulation]] }} {{eqn | r = \frac 1 {2 i} \paren {\laptrans {e^{i a t} } - \laptrans {e^{-i a t} } } | c = [[Linear Combination of Laplace Transforms]] }} {{eqn | r = \frac 1 {2 i} \paren {\frac 1 {s - i a} - \frac 1 {s + i a} } | c = [[Laplace Transform of Exponential]] }} {{eqn | r = \frac 1 {2 i} \paren {\frac {s + i a - s + i a} {s^2 + a^2} } | c = simplifying }} {{eqn | r = \frac 1 {2 i} \paren {\frac {2 i a} {s^2 + a^2} } | c = simplifying }} {{eqn | r = \frac a {s^2 + a^2} | c = simplifying }} {{end-eqn}} {{qed}}	0
Let $f \left({x}\right) = e^x \cos x$. By definition of [[Definition:Maclaurin Series|Maclaurin series]]: :$(1): \quad f \left({x}\right) \sim \displaystyle \sum_{n \mathop = 0}^\infty \frac {x^n} {n!} f^{\left({n}\right)} \left({0}\right)$ where $f^{\left({n}\right)} \left({0}\right)$ denotes the [[Definition:Higher Derivative|$n$th derivative]] of $f$ {{WRT|Differentiation}} $x$ evaluated at $x = 0$. It remains to be shown that: :$f^{\left({n}\right)} \left({0}\right) = 2^{n / 2} \cos \left({\dfrac {n \pi} 4}\right)$ The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 0}$, let $P \left({n}\right)$ be the [[Definition:Proposition|proposition]]: :$f^{\left({n}\right)} \left({x}\right) = 2^{n / 2} e^x \cos \left({x + \dfrac {n \pi} 4}\right)$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = f \left({x}\right) | r = e^x \cos x | c = }} {{eqn | r = 2^{0 / 2} e^x \cos \left({x + \dfrac {0 \pi} 4}\right) | c = }} {{end-eqn}} Thus $P \left({0}\right)$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({k}\right)$ is true, where $k \ge 0$, then it logically follows that $P \left({k + 1}\right)$ is true. So this is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$f^{\left({k}\right)} \left({x}\right) = 2^{k / 2} e^x \cos \left({x + \dfrac {k \pi} 4}\right)$ from which it is to be shown that: :$f^{\left({k + 1}\right)} \left({x}\right) = 2^{\left({k + 1}\right) / 2} e^x \cos \left({x + \dfrac {\left({k + 1}\right) \pi} 4}\right)$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = f^{\left({k + 1}\right)} \left({x}\right) | r = \frac \d {\d x} f^{\left({k}\right)} \left({x}\right) | c = }} {{eqn | r = \frac \d {\d x} 2^{k / 2} e^x \cos \left({x + \dfrac {k \pi} 4}\right) | c = [[Power Series Expansion for Exponential of x by Cosine of x#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = 2^{k / 2} \left({\cos \left({x + \dfrac {k \pi} 4}\right) \frac \d {\d x} e^x + e^x \frac \d {\d x} \cos \left({x + \dfrac {k \pi} 4}\right)}\right) | c = [[Product Rule]] }} {{eqn | r = 2^{k / 2} \left({\cos \left({x + \dfrac {k \pi} 4}\right) \frac \d {\d x} e^x - e^x \sin \left({x + \dfrac {k \pi} 4}\right)}\right) | c = [[Derivative of Cosine Function]], [[Derivatives of Function of a x + b|Derivatives of Function of $a x + b$]] }} {{eqn | r = 2^{k / 2} e^x \left({\cos \left({x + \dfrac {k \pi} 4}\right) - \sin \left({x + \dfrac {k \pi} 4}\right)}\right) | c = [[Derivative of Exponential Function]] and simplification }} {{eqn | r = 2^{k / 2} e^x \sqrt 2 \cos \left({x + \dfrac {k \pi} 4 + \dfrac \pi 4}\right) | c = [[Cosine of x minus Sine of x/Cosine Form|Cosine of x minus Sine of x: Cosine Form]] }} {{eqn | r = 2^{\left({k + 1}\right) / 2} e^x \cos \left({x + \dfrac {\left({k + 1}\right) \pi} 4}\right) | c = simplification }} {{end-eqn}} So $P \left({k}\right) \implies P \left({k + 1}\right)$ and by the [[Principle of Mathematical Induction]]: :$\forall n \in \Z_{\ge 0}: f^{\left({n}\right)} \left({x}\right) = 2^{n / 2} e^x \cos \left({x + \dfrac {n \pi} 4}\right)$ The result follows by setting $x = 0$ and substituting for $f^{\left({n}\right)} \left({0}\right)$ in $(1)$. {{qed}}	0
Let $\map f t := \map \Ci t = \displaystyle \int_t^\infty \dfrac {\cos u} u \rd u$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = -\dfrac {\cos t} t | c = }} {{eqn | ll= \leadsto | l = t \map {f'} t | r = -\cos t | c = }} {{eqn | ll= \leadsto | l = \laptrans {t \map {f'} t} | r = -\laptrans {\cos t} | c = }} {{eqn | r = -\dfrac s {s^2 + 1} | c = [[Laplace Transform of Cosine]] }} {{eqn | ll= \leadsto | l = -\dfrac \d {\d s} \laptrans {\map {f'} t} | r = -\dfrac s {s^2 + 1} | c = [[Derivative of Laplace Transform]] }} {{eqn | ll= \leadsto | l = \map {\dfrac \d {\d s} } {s \laptrans {\map f t} - \map f 0} | r = \dfrac s {s^2 + 1} | c = [[Laplace Transform of Derivative]] }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \int \dfrac s {s^2 + 1} \rd s | c = $\map f 0 = 0$, and integrating both sides {{WRT|Integration}} $s$ }} {{eqn | ll= \leadsto | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{end-eqn}} By the [[Initial Value Theorem of Laplace Transform]]: :$\displaystyle \lim_{s \mathop \to \infty} s \laptrans {\map f t} = \lim_{t \mathop \to 0} \map f t = \map f 0 = 0$ which leads to: :$c = 0$ Thus: {{begin-eqn}} {{eqn | l = s \laptrans {\map f t} | r = \dfrac 1 2 \map \ln {s^2 + 1} | c = }} {{eqn | ll= \leadsto | l = \laptrans {\map f t} | r = \dfrac {\map \ln {s^2 + 1} } {2 s} | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sec x | r = \map \sech {i x} | c = [[Secant in terms of Hyperbolic Secant]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {E_n \paren {i x}^n} {n!} | c = {{Defof|Euler Numbers}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {E_{2 n} \paren {i x}^{2 n} } {\paren {2 n}!} | c = [[Definition:Odd Integer|Odd]] terms vanish }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^n E_{2 n} x^{2 n} } {\paren {2 n}!} }} {{end-eqn}} {{qed}}	0
Let $z$ be a [[Definition:Complex Number|complex number]]. Let $\tan z$ denote the [[Definition:Complex Tangent Function|tangent function]] and $i$ denote the [[Definition:Imaginary Unit|imaginary unit]]: $i^2 = -1$. Then:	0
Let $\lambda \in \R \setminus \Z$ be a [[Definition:Real Number|real number]] which is not an [[Definition:Integer|integer]]. Then: :$\displaystyle \pi \csc \pi \lambda = \sum_{n \mathop = 1}^\infty \paren {-1}^n \paren {\frac 1 {n + \lambda} + \frac 1 {n - 1 - \lambda} }$	0
Let: {{begin-eqn}} {{eqn | l = u | r = \sin a x | c = }} {{eqn | l = \frac {\d u} {\d x} | r = a \cos a x | c = [[Derivative of Sine of a x|Derivative of $\sin a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {\cos a x \paren {1 - \sin a x} } | r = \int \frac {\cos a x \rd x} {\cos^2 a x \paren {1 - \sin a x} } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $\cos a x$ }} {{eqn | r = \int \frac {\cos a x \rd x} {\paren {1 - \sin^2 a x} \paren {1 - \sin a x} } | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \frac 1 a \int \frac {\d u} {\paren {1 - u^2} \paren {1 - u} } | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 a \int \frac {\d u} {\paren {1 + u} \paren {1 - u}^2} | c = [[Difference of Two Squares]] }} {{eqn | r = \frac 1 a \paren {\frac 1 2 \paren {\frac 1 {1 - u} + \frac 1 2 \ln \size {\frac {1 + u} {1 - u} } } } + C | c = [[Primitive of Reciprocal of a x + b squared by p x + q|Primitive of $\dfrac 1 {\paren {a x + b}^2 \paren {p x + q} }$]] }} {{eqn | r = \frac 1 {2 a \paren {1 - u} } + \frac 1 {4 a} \ln \size {\frac {1 + u} {1 - u} } + C | c = simplifying }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\frac {1 + \sin a x} {1 - \sin a x} } + C | c = substituting for $u$ }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\frac {\frac 1 2 \map {\sec^2} {\frac \pi 4 + \frac {a x} 2} } {\frac 1 2 \map {\sec^2} {\frac \pi 4 - \frac {a x} 2} } } + C | c = [[Reciprocal of One Minus Sine|Reciprocal of $1 - \sin$]] and [[Reciprocal of One Plus Sine|Reciprocal of $1 + \sin$]] }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\frac {\map {\cos^2} {\frac \pi 4 - \frac {a x} 2} } {\map {\cos^2} {\frac \pi 4 + \frac {a x} 2} } } + C | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\frac {\map {\sin^2} {\frac \pi 2 - \paren {\frac \pi 4 - \frac {a x} 2} } } {\map {\cos^2} {\frac \pi 4 + \frac {a x} 2} } } + C | c = [[Sine of Complement equals Cosine]] }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\frac {\map {\sin^2} {\frac \pi 4 + \frac {a x} 2} } {\map {\cos^2} {\frac \pi 4 + \frac {a x} 2} } } + C | c = }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {4 a} \ln \size {\map {\tan^2} {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac 1 {2 a \paren {1 - \sin a x} } + \frac 1 {2 a} \ln \size {\map \tan {\frac \pi 4 + \frac {a x} 2} } + C | c = [[Logarithm of Power]] }} {{end-eqn}} {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \cos 5 \theta + i \sin 5 \theta | r = \paren {\cos \theta + i \sin \theta}^5 | c = [[De Moivre's Formula]] }} {{eqn | r = \paren {\cos \theta}^5 + \binom 5 1 \paren {\cos \theta}^4 \paren {i \sin \theta} + \binom 5 2 \paren {\cos \theta}^3 \paren {i \sin \theta}^2 }} {{eqn | o = | ro=+ | r = \binom 5 3 \paren {\cos \theta}^2 \paren {i \sin \theta}^3 + \binom 5 4 \paren {\cos \theta} \paren {i \sin \theta}^4 + \paren {i \sin \theta}^5 | c = [[Binomial Theorem]] }} {{eqn | r = \cos^5 \theta + 5 i \cos^4 \theta \sin \theta - 10 \cos^3 \theta \sin^2 \theta | c = substituting for [[Definition:Binomial Coefficient|binomial coefficients]] }} {{eqn | o = | ro=- | r = 10 i \cos^2 \theta \sin^3 \theta + 5 \cos \theta \sin^4 \theta + i \sin^5 \theta | c = and using $i^2 = -1$ }} {{eqn | n = 1 | r = \cos^5 \theta - 10 \cos^3 \theta \sin^2 \theta + 5 \cos \theta \sin^4 \theta }} {{eqn | o = | ro=+ | r = i \paren {5 \cos^4 \theta \sin \theta - 10 \cos^2 \theta \sin^3 \theta + \sin^5 \theta} | c = rearranging }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \sin 5 \theta | r = 5 \cos^4 \theta \sin \theta - 10 \cos^2 \theta \sin^3 \theta + \sin^5 \theta | c = equating [[Definition:Imaginary Part|imaginary parts]] in $(1)$ }} {{eqn | r = 5 \paren {1 - \sin^2 \theta}^2 \sin \theta - 10 \paren {1 - \sin^2 \theta} \sin^3 \theta + \sin^5 \theta | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 5 \sin \theta - 20 \sin^3 \theta + 16 \sin^5 \theta | c = multiplying out and gathering terms }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int x \csc a x \rd x = \frac 1 {a^2} \paren {a x + \frac {\paren {a x}^3} {18} + \frac {7 \paren {a x}^5} {1800} + \cdots + \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_n \paren {a x}^{2 n + 1} } {\paren {2 n + 1}!} + \cdots} + C$ where $B_{2 n}$ is the $2 n$th [[Definition:Bernoulli Numbers|Bernoulli number]].	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arccos \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac {-1} {\sqrt {a^2 - x^2} } | c = [[Derivative of Arccosine of x over a|Derivative of $\arccos \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^2} 2 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \arccos \frac x a \rd x | r = \frac {x^2} 2 \arccos \frac x a - \int \frac {x^2} 2 \paren {\frac {-1} {\sqrt {a^2 - x^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a + \frac 1 2 \int \frac {x^2 \rd x} {\sqrt {a^2 - x^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a + \frac 1 2 \paren {\frac {-x \sqrt {a^2 - x^2} } 2 + \frac {a^2} 2 \arcsin \frac x a} + C | c = [[Primitive of x squared over Root of a squared minus x squared|Primitive of $\dfrac {x^2} {\sqrt {a^2 - x^2} }$]] }} {{eqn | r = \frac {x^2} 2 \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + \frac {a^2} 4 \paren {\frac \pi 2 - \arccos \frac x a} + C | c = [[Sum of Arcsine and Arccosine]] }} {{eqn | r = \paren {\frac {x^2} 2 - \frac {a^2} 4} \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + \frac {\pi a^2} 8 + C | c = simplifying }} {{eqn | r = \paren {\frac {x^2} 2 - \frac {a^2} 4} \arccos \frac x a - \frac {x \sqrt {a^2 - x^2} } 4 + C | c = subsuming $\dfrac {\pi a^2} 8$ into the [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = \arcsin \frac x a | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = \frac 1 {\sqrt {a^2 - x^2} } | c = [[Derivative of Arcsine of x over a|Derivative of $\arcsin \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = x^2 | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {x^3} 3 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^2 \arcsin \frac x a \rd x | r = \frac {x^3} 3 \arcsin \frac x a - \int \frac {x^3} 3 \paren {\frac 1 {\sqrt {a^2 - x^2} } } \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \arcsin \frac x a - \frac 1 3 \int \frac {x^3 \rd x} {\sqrt {a^2 - x^2} } + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \arcsin \frac x a - \frac 1 3 \paren {\frac {\paren {\sqrt {a^2 - x^2} }^3} 3 - a^2 \sqrt {a^2 - x^2} } + C | c = [[Primitive of x cubed over Root of a squared minus x squared|Primitive of $\dfrac {x^3} {\sqrt {a^2 - x^2} }$]] }} {{eqn | r = \frac {x^3} 3 \arcsin \frac x a + \frac {\paren {x^2 + 2 a^2} \sqrt {a^2 - x^2} } 9 + C | c = simplifying }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int x \csch a x \rd x | r = \frac 1 {a^2} \int \theta \csch \theta \rd \theta | c = [[Integration by Substitution|Substitution of $a x \to \theta$]] }} {{eqn | r = \frac 1 {a^2} \int \theta \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} \, \theta^{2 n - 1} } {\paren {2 n}!} \rd \theta | c = [[Power Series Expansion for Cosecant Function]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} } {\paren {2 n}!} \int \theta^{2 n} \rd \theta | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 {a^2} \sum_{n \mathop = 0}^\infty \frac {\paren {-1}^{n - 1} 2 \paren {2^{2 n - 1} - 1} B_{2 n} \paren {a x}^{2 n + 1} } {\paren {2 n + 1}!} + C | c = [[Integration by Substitution|Substituting back $\theta \to ax$]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot 105^\circ | r = \cot \left({90^\circ + 15^\circ}\right) | c = }} {{eqn | r = - \tan 15^\circ | c = [[Cotangent of Angle plus Right Angle]] }} {{eqn | r = - \left({2 - \sqrt 3}\right) | c = [[Tangent of 15 Degrees]] }} {{end-eqn}} {{qed}}	0
For $x \in \openint {-\pi} \pi$: :$\displaystyle \size x = \frac \pi 2 - \frac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\map \cos {2 n - 1} x} {\paren {2 n - 1}^2}$	0
:$\displaystyle \int \frac {\mathrm d x} {p \sin a x + q \left({1 + \cos a x}\right)} = \frac 1 {a p} \ln \left\vert{q + p \tan \frac {a x} 2}\right\vert + C$	0
{{begin-eqn}} {{eqn | l = \csc 345 \degrees | r = \map \csc {360 \degrees - 15 \degrees} | c = }} {{eqn | r = -\csc 15 \degrees | c = [[Cosecant of Conjugate Angle]] }} {{eqn | r = -\paren {\sqrt 6 + \sqrt 2} | c = [[Cosecant of 15 Degrees|Cosecant of $15 \degrees$]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sec z | r = \frac 1 {\cos z} | c = {{Defof|Complex Secant Function}} }} {{eqn | r = 1 / \frac {e^{i z} + e^{-i z} } 2 | c = [[Sine Exponential Formulation]] and [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 2 {e^{i z} + e^{-i z} } | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $2$ }} {{end-eqn}} {{qed}}	0
From the trigonometric definitions of [[Definition:Sine of Angle|sine]] and [[Definition:Cosine of Angle|cosine]]: {{begin-eqn}} {{eqn | l = \sin x | r = \frac{\text{opposite} } {\text{hypotenuse} } }} {{eqn | l = \cos x | r = \frac{\text{adjacent} } {\text{hypotenuse} } }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \sin^2 x + \cos^2 x | r = \frac{\text{opposite}^2 + \text{adjacent}^2} {\text{hypotenuse}^2} | c [[Definition:Square|squaring]] both sides and adding }} {{eqn | r = \frac{\text{hypotenuse}^2} {\text{hypotenuse}^2} | c = [[Pythagoras's Theorem]] }} {{eqn | r = 1 | c = }} {{end-eqn}} {{qed}}	0
By [[Derivative of Sine Integral Function]], we have: :$\displaystyle \frac \d {\d x} \paren {\map \Si x} = \frac {\sin x} x$ So: {{begin-eqn}} {{eqn | l = \int \map \Si x \rd x | r = \int 1 \times \map \Si x \rd x }} {{eqn | r = x \map \Si x - \int x \frac {\sin x} x \rd x | c = [[Integration by Parts]] }} {{eqn | r = x \map \Si x - \int \sin x \rd x }} {{eqn | r = x \map \Si x + \cos x + C | c = [[Primitive of Sine Function]] }} {{end-eqn}} {{qed}} [[Category:Primitives]] [[Category:Sine Integral Function]] r25o0orymphqj2y701esrh7ugpyc8d5	0
First we address the special case where $m = n$. In this case $m + n = m + m = 2 m$ is [[Definition:Even Integer|even]]. We have: {{begin-eqn}} {{eqn | l = \int \sin m x \cos m x \rd x | r = \frac {\sin^2 m x} {2 m} + C | c = [[Primitive of Sine of a x by Cosine of a x|Primitive of $\sin m x \cos m x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \cos m x \rd x | r = \left[{\frac {\sin^2 m x} {2 m} }\right]_0^\pi | c = }} {{eqn | r = \frac {\sin^2 m \pi} {2 m} - \frac {\sin^2 0} {2 m} | c = }} {{eqn | r = 0 | c = [[Sine of Multiple of Pi]] }} {{end-eqn}} So in this case, $\displaystyle \int_0^\pi \sin m x \cos n x \rd x = 0$ for $m + m$ [[Definition:Even Integer|even]]. Let $m \ne n$. {{begin-eqn}} {{eqn | l = \int \sin m x \cos n x \rd x | r = \frac {-\cos \left({m - n}\right) x} {2 \left({m - n}\right)} - \frac {\cos \left({m + n}\right) x} {2 \left({m + n}\right)} + C | c = [[Primitive of Sine of p x by Cosine of q x|Primitive of $\sin m x \cos n x$]] }} {{eqn | ll= \leadsto | l = \int_0^\pi \sin m x \cos n x \rd x | r = \left[{\frac {-\cos \left({m - n}\right) x} {2 \left({m - n}\right)} - \frac {\cos \left({m + n}\right) x} {2 \left({m + n}\right)} }\right]_0^\pi | c = }} {{eqn | r = \left({\frac {-\cos \left({\left({m - n}\right) \pi}\right)} {2 \left({m - n}\right)} - \frac {\cos \left({\left({m + n}\right) \pi}\right)} {2 \left({m + n}\right)} }\right) | c = }} {{eqn | o = | ro= - | r = \left({\frac {-\cos 0} {2 \left({m - n}\right)} - \frac {\cos 0} {2 \left({m + n}\right)} }\right) | c = }} {{eqn | r = \frac {-\cos \left({\left({m - n}\right) \pi}\right)} {2 \left({m - n}\right)} - \frac {\cos \left({\left({m + n}\right) \pi}\right)} {2 \left({m + n}\right)} + \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} | c = [[Cosine of Zero is One]] and simplifying }} {{eqn | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac {\left({-1}\right)^{m - n} } {2 \left({m - n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m + n}\right)} | c = [[Cosine of Multiple of Pi]] and rearranging }} {{eqn | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m - n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m + n}\right)} | c = $m + n$ and $m - n$ have the same [[Definition:Parity of Integer|parity]] }} {{end-eqn}} {{qed|lemma}} When $m + n$ is an [[Definition:Even Integer|even integer]], we have: {{begin-eqn}} {{eqn | o = | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m - n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m + n}\right)} | c = }} {{eqn | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac 1 {2 \left({m - n}\right)} - \frac 1 {2 \left({m + n}\right)} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} This shows that in all cases $\displaystyle \int_0^\pi \sin m x \cos n x \rd x = 0$ for $m + m$ [[Definition:Even Integer|even]]. When $m + n$ is an [[Definition:Odd Integer|odd integer]], we have: {{begin-eqn}} {{eqn | o = | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m - n}\right)} - \frac {\left({-1}\right)^{m + n} } {2 \left({m + n}\right)} | c = }} {{eqn | r = \frac 1 {2 \left({m - n}\right)} + \frac 1 {2 \left({m + n}\right)} - \frac {-1} {2 \left({m - n}\right)} - \frac {-1} {2 \left({m + n}\right)} | c = }} {{eqn | r = \frac 1 {m - n} + \frac 1 {m + n} | c = simplification }} {{eqn | r = \frac {\left({m + n}\right) + \left({m - n}\right)} {\left({m + n}\right) \left({m - n}\right)} | c = }} {{eqn | r = \frac {2 m} {\left({m + n}\right) \left({m - n}\right)} | c = }} {{eqn | r = \frac {2 m} {m^2 - n^2} | c = [[Difference of Two Squares]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \paren {\cos 45 \degrees}^2 | r = 1 - \paren {\sin 45 \degrees}^2 | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 1 - \paren {\frac {\sqrt 2} 2}^2 | c = [[Sine of 45 Degrees|Sine of $45 \degrees$]] }} {{eqn | r = \frac 1 2 | c = }} {{eqn | ll= \leadsto | l = \cos 45 \degrees | r = \sqrt {\frac 1 2} | c = [[Definition:Positive Real Number|positive]] because $\cos 45 \degrees$ is in [[Definition:Cosine/Definition from Circle/First Quadrant|Quadrant I]] }} {{eqn | r = \dfrac {\sqrt 2} 2 | c = }} {{end-eqn}} {{qed}}	0
:$\cot z = i \dfrac {e^{i z} + e^{-i z} } {e^{i z} - e^{-i z} }$	0
{{begin-eqn}} {{eqn | r = \cosh a \cosh b + \sinh a \sinh b | o = | c = }} {{eqn | r = \frac {e^a + e^{-a} } 2 \frac {e^b + e^{-b} } 2 + \frac {e^a - e^{-a} } 2 \frac {e^b - e^{-b} } 2 | c = {{Defof|Hyperbolic Sine}} and {{Defof|Hyperbolic Cosine}} }} {{eqn | r = \frac {e^{a + b} + e^{-a + b} + e^{a + b} + e^{-a - b} } 4 | c = [[Exponential of Sum]] }} {{eqn | o = | ro= + | r = \frac {e^{a + b} - e^{-a + b} - e^{a - b} + e^{-a - b} } 4 | c = }} {{eqn | r = \frac {2 e^{a + b} + 2 e^{-\paren {a + b} } } 4 | c = }} {{eqn | r = \frac {e^{a + b} + e^{-\paren {a + b} } } 2 | c = }} {{eqn | r = \map \cosh {a + b} | c = {{Defof|Hyperbolic Cosine}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \sec 15 \degrees | r = \frac 1 {\cos 15 \degrees} | c = [[Secant is Reciprocal of Cosine]] }} {{eqn | r = \frac 4 {\sqrt 6 + \sqrt 2} | c = [[Cosine of 15 Degrees|Cosine of $15 \degrees$]] }} {{eqn | r = \frac {4 \paren {\sqrt 6 - \sqrt 2} } {\paren {\sqrt 6 + \sqrt 2} \paren {\sqrt 6 - \sqrt 2} } | c = multiplying top and bottom by $\sqrt 6 - \sqrt 2$ }} {{eqn | r = \frac {4 \paren {\sqrt 6 - \sqrt 2} } {6 - 2} | c = [[Difference of Two Squares]] }} {{eqn | r = \sqrt 6 - \sqrt 2 | c = simplifying }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = 1 | c = [[Derivative of Identity Function]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = \csc^2 a x | c = }} {{eqn | ll= \implies | l = v | r = \frac {-\cot a x} a | c = [[Primitive of Square of Cosecant of a x|Primitive of $\csc^2 a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x \csc^2 a x \ \mathrm d x | r = \frac {-x \cot a x} a - \int \frac {-\cot a x} a \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-x \cot a x} a + \frac 1 a \int \cot a x \ \mathrm d x + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {-x \cot a x} a + \frac 1 a \left({\frac 1 a \ln \left\vert{\sin a x}\right\vert}\right) + C | c = [[Primitive of Cotangent of a x|Primitive of $\cot a x$]] }} {{eqn | r = \frac {-x \cot a x} a + \frac 1 {a^2} \ln \left\vert{\sin a x}\right\vert + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \sin^n a x \cos a x \rd x = \frac {\sin^{n + 1} a x} {\paren {n + 1} a} + C$	0
{{begin-eqn}} {{eqn | l = \cot 150 \degrees | r = \map \cot {90 \degrees + 60 \degrees} | c = }} {{eqn | r = -\tan 60 \degrees | c = [[Cotangent of Angle plus Right Angle]] }} {{eqn | r = -\sqrt 3 | c = [[Tangent of 60 Degrees|Tangent of $60 \degrees$]] }} {{end-eqn}} {{qed}}	0
:$\ds \int \cos^2 x \rd x = \frac x 2 + \frac {\sin 2 x} 4 + C$	0
Recall the definition of the [[Definition:Complex Sine Function|sine function]]: {{begin-eqn}} {{eqn | l = \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = }} {{eqn | r = z - \frac {z^3} {3!} + \frac {z^5} {5!} - \frac {z^7} {7!} + \cdots + \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1}!} + \cdots | c = }} {{end-eqn}} Recall the definition of the [[Definition:Exponential Function/Complex/Sum of Series|exponential as a power series]]: {{begin-eqn}} {{eqn | l = \exp z | r = \sum_{n \mathop = 0}^\infty \frac {z^n} {n!} | c = }} {{eqn | r = 1 + \frac z {1!} + \frac {z^2} {2!} + \frac {z^3} {3!} + \cdots + \frac {z^n} {n!} + \cdots | c = }} {{end-eqn}} Then, starting from the {{RHS}}: {{begin-eqn}} {{eqn | l = \frac {\exp \paren {i z} - \exp \paren {-i x} } {2 i} | r = \frac 1 {2 i} \paren {\sum_{n \mathop = 0}^\infty \frac {\paren {i z}^n} {n!} - \sum_{n \mathop = 0}^\infty \frac {\paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^n - \paren {-i z}^n} {n!} } | c = }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \paren {\frac {\paren {i z}^{2 n} - \paren {-i z}^{2 n} } {\paren {2 n}!} + \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = split into [[Definition:Even Integer|even]] and [[Definition:Odd Integer|odd]] $n$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} - \paren {-i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-i z}^{2 n} = \paren {i z}^{2 n}$ }} {{eqn | r = \frac 1 {2 i} \sum_{n \mathop = 0}^\infty \frac {2 \paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = as $\paren {-1}^{2 n + 1} = -1$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} | c = cancel $2$ }} {{eqn | r = \frac 1 i \sum_{n \mathop = 0}^\infty \frac {i \paren {-1}^n z^{2 n + 1} } {\paren {2 n + 1}!} | c = as $i^{2 n + 1} = i \paren {-1})^n $ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {z^{2 n + 1} } {\paren {2 n + 1!} } | c = cancel $i$ }} {{eqn | r = \sin z }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \frac 1 {2 \pi} \int_{-\pi}^\pi \size {\map f x}^2 \rd x | r = \frac 1 {2 \pi} \int_{-\pi}^\pi \map f x \overline {\map f x} \rd x | c = [[Modulus in Terms of Conjugate]] }} {{eqn | r = \frac 1 {2 \pi} \int_{-\pi}^\pi \sum_{n \mathop = -\infty}^\infty c_n e^{i n x} \overline {\sum_{m \mathop = -\infty}^\infty c_m e^{i m x} } \rd x }} {{eqn | r = \frac 1 {2 \pi} \int_{-\pi}^\pi \sum_{n \mathop = -\infty}^\infty c_n e^{i n x} \sum_{m \mathop = -\infty}^\infty \overline {c_m} e^{-i m x} \rd x | c = [[Sum of Complex Conjugates]] }} {{eqn | r = \frac 1 {2 \pi} \int_{-\pi}^\pi \sum_{n \mathop = -\infty}^\infty \sum_{m \mathop = -\infty}^\infty c_n \overline {c_m} e^{i x \left({n - m}\right)} \rd x }} {{eqn | r = \frac 1 {2 \pi} \sum_{n \mathop = -\infty}^\infty \sum_{m \mathop = -\infty}^\infty c_n \overline {c_m} \int_{-\pi}^\pi e^{i x \left({n - m}\right)} \rd x | c = [[Fubini's Theorem]] }} {{eqn | r = \frac 1 {2 \pi} \sum_{n \mathop = -\infty}^\infty \sum_{m \mathop = -\infty}^\infty c_n \overline {c_m} 2 \pi \delta_{n m} | c = [[Integral over 2 pi of Exponential of i by n x]] }} {{eqn | r = \frac {2 \pi} {2 \pi} \sum_{n \mathop = -\infty}^\infty c_n \overline {c_n} }} {{eqn | r = \sum_{n \mathop = -\infty}^\infty \size {c_n}^2 }} {{end-eqn}} {{qed}} {{Namedfor|Marc-Antoine Parseval}} [[Category:Parseval's Theorem]] n7lwtqwrxeozi2tv31js1j5lz1lxomz	0
: $\csc \left({x + 2 \pi}\right) = \csc x$	0
Define: :$\displaystyle \map I \alpha = \int_0^\infty \frac {e^{-\alpha x} \sin x} x \rd x$ Using [[Definite Integral of Partial Derivative]]: {{begin-eqn}} {{eqn | l = \map {I'} \alpha | r = \frac \partial {\partial \alpha} \int_0^\infty \frac {e^{-\alpha x} \sin x} x \rd x }} {{eqn | r = \int_0^\infty \frac \partial {\partial \alpha} \frac {e^{-\alpha x} \sin x} x \rd x | c = [[Leibniz's Integral Rule]] }} {{eqn | r = -\int_0^\infty e^{-\alpha x} \sin x \rd x | c = [[Derivative of Exponential Function]] }} {{eqn | r = \intlimits {-\frac {e^{-\alpha x} \paren {-\alpha \sin x + \cos x} } {\paren {-\alpha}^2 + 1} } 0 \infty | c = [[Primitive of Exponential of a x by Sine of b x|Primitive of $e^{\alpha x} \sin b x$]] }} {{eqn | r = -\frac 1 {\alpha^2 + 1} }} {{end-eqn}} Integrating with respect to $\alpha$: {{begin-eqn}} {{eqn | l = \map I \alpha | r = -\int \frac 1 {\alpha^2 + 1} \rd \alpha }} {{eqn | r = C - \arctan \alpha | c = [[Primitive of Reciprocal of x squared plus a squared/Arctangent Form|Primitive of $\dfrac 1 {x^2 + a^2}$]] }} {{end-eqn}} To determine $C$, take $\alpha \to \infty$: {{begin-eqn}} {{eqn | l = \lim_{\alpha \mathop \to \infty} \int_0^\infty \frac {e^{-\alpha x} \sin x} x \rd x | r = \lim_{\alpha \mathop \to \infty} \paren {C - \arctan \alpha} }} {{eqn | ll= \leadsto | l = \int_0^\infty \frac {0 \times \sin x} x | r = C - \frac \pi 2 }} {{eqn | ll= \leadsto | l = C | r = \frac \pi 2 }} {{end-eqn}} Hence: :$\displaystyle \int_0^\infty \frac {e^{-\alpha x} \sin x} x \rd x = \frac \pi 2 - \arctan \alpha$ Setting $\alpha = 0$ yields: :$\displaystyle \int_0^\infty \frac {\sin x} x \rd x = \frac \pi 2$ {{qed}}	0
:$\displaystyle \int_0^\infty \frac {\cos m x} {x^2 + a^2} \rd x = \frac \pi {2 a} e^{-m a}$	0
By [[De Moivre's Formula]]: :$\cos n \theta + i \sin n \theta = \paren {\cos \theta + i \sin \theta}^n$ As $n \in \Z_{>0}$, we use the [[Binomial Theorem]] on the {{RHS}}, resulting in: :$\displaystyle \cos n \theta + i \sin n \theta = \sum_{k \mathop \ge 0} \binom n k \paren {\cos^{n - k} \theta} \paren {i \sin \theta}^k$ When $k$ is [[Definition:Odd Integer|odd]], the expression being summed is [[Definition:Imaginary Number|imaginary]]. Equating the [[Definition:Imaginary Part|imaginary parts]] of both sides of the equation, replacing $k$ with $2 k + 1$ to make $k$ [[Definition:Odd Integer|odd]], gives: {{begin-eqn}} {{eqn | l = \sin n \theta | r = \sum_{k \mathop \ge 0} \paren {-1}^k \dbinom n {2 k + 1} \paren {\cos^{n - \paren {2 k + 1} } \theta} \paren {\sin^{2 k + 1} \theta} | c = }} {{eqn | r = \cos^n \theta \sum_{k \mathop \ge 0} \paren {-1}^k \dbinom n {2 k + 1} \paren {\tan^{2 k + 1} \theta} | c = factor out $\cos^n \theta$ }} {{end-eqn}} {{qed}} [[Category:Sine of Integer Multiple of Argument]] ixx0qmzthb6kv3ovxu3cq4c13kqywd1	0
Let $P = \tuple {x, y}$ be a [[Definition:Point|point]] on the [[Definition:Circumference of Circle|circumference]] of a [[Definition:Unit Circle|unit circle]] whose [[Definition:Center of Circle|center]] is at the [[Definition:Origin|origin]] of a [[Definition:Cartesian Plane|cartesian plane]]. From [[Sine of Angle in Cartesian Plane]] and [[Cosine of Angle in Cartesian Plane]]: :$P = \tuple {\cos \theta, \sin \theta}$ The [[Definition:Graph of Relation|graph]] of the [[Definition:Unit Circle|unit circle]] is the [[Definition:Locus|locus]] of: :$x^2 + y^2 = 1$ as given by [[Equation of Circle]]. Substituting $x = \cos \theta$ and $y = \sin \theta$ yields: :$\cos^2 \theta + \sin^2 \theta = 1$ {{qed}}	0
:$\ds \int \tan a x \rd x = \frac {-\ln \size {\cos a x} } a + C$	0
{{begin-eqn}} {{eqn | l = 1 - 2 \sin^2 x | r = \cos 2 x | c = [[Double Angle Formulas/Cosine/Corollary 2|Double Angle Formula for Cosine: Corollary 2]] }} {{eqn | ll= \leadsto | l = \sin^2 x | r = \frac {\cos 2 x - 1} {-2} | c = solving for $\sin^2x$ }} {{eqn | r = \frac {1 - \cos 2 x} 2 | c = multiplying top and bottom by $-1$ and rearranging terms }} {{end-eqn}} {{qed}}	0
: $\cos \paren {x + 2 \pi} = \cos x$	0
Let $n \ge 2$. Let: : $\displaystyle I_n := \int \sin^n x \ \mathrm d x$ Then: {{begin-eqn}} {{eqn | l = I_n | r = \int \sin^n x \ \mathrm d x | c = }} {{eqn | r = \int \sin^{n-1} x \sin x \ \mathrm d x | c = }} {{eqn | r = \int \sin^{n-1} x \ \mathrm d \left({-\cos x}\right) | c = [[Derivative of Cosine Function]] }} {{eqn | r = - \sin^{n-1} x \cos x - \int \left({-\cos x}\right) \mathrm d \left({\sin^{n-1} x }\right) | c = [[Integration by Parts]] }} {{eqn | r = - \sin^{n-1} x \cos x - \int \left({-\cos x}\right) \left({n - 1}\right)\sin^{n-2} x \cos x \ \mathrm d x | c = [[Derivative of Power]] and [[Chain Rule for Derivatives]] }} {{eqn | r = - \sin^{n-1} x \cos x + \left({n - 1}\right) \int \sin^{n-2} x \cos^2 x \ \mathrm d x | c = [[Linear Combination of Integrals]] and rearranging }} {{eqn | r = - \sin^{n-1} x \cos x + \left({n - 1}\right) \int \sin^{n-2} x \left({1 - \sin^2 x}\right) \ \mathrm d x | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = - \sin^{n-1} x \cos x + \left({n - 1}\right) \int \sin^{n-2} x \ \mathrm d x - \left({n - 1}\right) \int \sin^n x \ \mathrm d x | c = }} {{eqn | r = - \sin^{n-1} x \cos x + \left({n - 1}\right) I_{n-2} - \left({n - 1}\right) I_n | c = }} {{eqn | ll= \implies | l = n I_n | r = - \sin^{n-1} x \cos x + \left({n - 1}\right) I_{n-2} | c = adding $\left({n - 1}\right) I_n$ to both sides }} {{eqn | ll= \implies | l = I_n | r = \dfrac {n - 1} n I_{n-2} - \dfrac {\sin^{n-1} x \cos x} n | c = dividing by $n$ and rearranging }} {{end-eqn}} thus demonstrating the identity for all $n \ge 2$. When $n = 1$ this degenerates to: {{begin-eqn}} {{eqn | l = \int \sin x \ \mathrm d x | r = \dfrac 0 1 \int \frac 1 {\sin x} \ \mathrm d x - \dfrac {\sin^0 x \cos x} 1 | c = }} {{eqn | r = 0 - 1 \cdot \cos x | c = }} {{eqn | r = -\cos x | c = }} {{end-eqn}} From [[Primitive of Sine Function]] this shows that the identity still holds for $n = 1$. {{qed}}	0
:$\sin \theta = \dfrac 1 {\csc \theta}$	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \csc^{n - 2} a x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = -a \left({n - 2}\right) \csc^{n - 3} a x \csc a x \cot a x | c = [[Derivative of Power]], [[Derivative of Cosecant Function|Derivative of $\csc$]], [[Chain Rule for Derivatives]] }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = -a \left({n - 2}\right) \csc^{n - 2} a x \cot a x | c = }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = \csc^2 a x | c = }} {{eqn | ll= \implies | l = v | r = \frac {-\cot a x} a | c = [[Primitive of Square of Cosecant of a x|Primitive of $\csc^2 a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \csc^n a x \ \mathrm d x | r = \int \csc^{n - 2} a x \csc^2 a x \ \mathrm d x | c = }} {{eqn | r = \csc^{n - 2} a x \left({\frac {-\cot a x} a}\right) - \int \left({\frac {-\cot a x} a}\right) \left({-a \left({n - 2}\right) \csc^{n - 2} a x \cot a x }\right) \ \mathrm d x | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\csc^{n - 2} a x \cot a x} a - \left({n - 2}\right) \int \cot^2 a x \csc^{n - 2} a x \ \mathrm d x | c = simplifying }} {{eqn | r = \frac {-\csc^{n - 2} a x \cot a x} a - \left({n - 2}\right) \int \left({\csc^2 a x - 1}\right) \csc^{n - 2} a x \ \mathrm d x | c = [[Difference of Squares of Cosecant and Cotangent|Difference of $\csc^2$ and $\cot^2$]] }} {{eqn | r = \frac {-\csc^{n - 2} a x \cot a x} a - \left({n - 2}\right) \int \csc^n a x \ \mathrm d x | c = [[Linear Combination of Integrals]] }} {{eqn | o = | ro= + | r = \left({n - 2}\right) \int \csc^{n - 2} a x \ \mathrm d x | c = }} {{eqn | l = \left({n - 1}\right) \int \csc^n a x \ \mathrm d x | r = \frac {-\csc^{n - 2} a x \cot a x} a + \left({n - 2}\right) \int \csc^{n - 2} a x \ \mathrm d x | c = gathering terms }} {{eqn | l = \int \csc^n a x \ \mathrm d x | r = \frac{-\csc^{n - 2} a x \cot a x} {a \left({n - 1}\right)} + \frac {n - 2} {n - 1} \int \csc^{n - 2} a x \ \mathrm d x | c = dividing by $n - 1$ }} {{end-eqn}} {{qed}}	0
:$\sin 4 \theta = 4 \sin \theta \cos \theta - 8 \sin^3 \theta \cos \theta$	0
:$\sin 300 \degrees = \sin \dfrac {5 \pi} 3 = -\dfrac {\sqrt 3} 2$	0
Consider the [[Definition:Real Cosine Function|(real) cosine function]] $\cos: \R \to \R$. Then: :$\map \FF {\cos} = \operatorname {II}$ where: :$\FF$ denotes the [[Definition:Fourier Transform|Fourier transform]] :$\operatorname {II}$ denotes the [[Definition:Even Impulse Pair Function|even impulse pair function]].	0
{{begin-eqn}} {{eqn | l = \sin 285^\circ | r = \sin \left({360^\circ - 75^\circ}\right) | c = }} {{eqn | r = - \sin 75^\circ | c = [[Sine of Conjugate Angle]] }} {{eqn | r = - \dfrac {\sqrt 6 + \sqrt 2} 4 | c = [[Sine of 75 Degrees|Sine of $75^\circ$]] }} {{end-eqn}} {{qed}}	0
:[[File:Spherical-Cosine-Formula-Analog.png|500px]] Suppose $c$ is less than $\dfrac \pi 2$. Let $BA$ be [[Definition:Production|produced]] to $D$ so that $BD = \dfrac \pi 2$. Then: :$AD = \dfrac \pi 2 - c$ and: :$\angle CAD = pi - A$ Let $C$ and $D$ be joined by an [[Definition:Arc of Circle|arc]] of a [[Definition:Great Circle|great circle]], denoted $x$. From the [[Definition:Spherical Triangle|triangle]] $\sphericalangle DAC$, using the [[Spherical Law of Cosines]]: {{begin-eqn}} {{eqn | l = \cos x | r = \map \cos {\dfrac \pi 2 - c} \cos b + \map \sin {\dfrac \pi 2 - c} \sin b \, \map \cos {\pi - A} | c = }} {{eqn | r = \sin c \cos b - \cos c \sin b \cos A | c = }} {{end-eqn}} From the [[Definition:Spherical Triangle|triangle]] $\sphericalangle DBC$, using the [[Spherical Law of Cosines]]: {{begin-eqn}} {{eqn | l = \cos x | r = \cos \dfrac \pi 2 \cos a + \sin \pi 2 \sin a \cos B | c = }} {{eqn | r = \sin a \cos B | c = }} {{end-eqn}} Hence: :$\sin a \cos B = \sin c \cos b - \cos c \sin b \cos A$ The case where $c > \dfrac \pi 2$ is worked similarly, but by making $D$ the [[Definition:Point|point]] between $A$ and $B$ such that $BD$ is $\dfrac \pi 2$. {{qed}}	0
{{begin-eqn}} {{eqn | l = \cos x | r = 2 \cos^2 \frac x 2 - 1 | c = [[Double Angle Formulas/Cosine/Corollary 1|Cosine Double Angle Formula]] }} {{eqn | ll= \leadstoandfrom | l = 1 + \cos x | r = 2 \cos^2 \frac x 2 | c = adding $1$ to both sides }} {{eqn | ll= \leadstoandfrom | l = \frac 1 {1 + \cos x} | r = \frac 1 2 \frac 1 {\cos^2 \frac x 2} | c = taking the [[Definition:Reciprocal|reciprocal]] of both sides }} {{eqn | r = \frac 1 2 \sec^2 \frac x 2 | c = {{Defof|Secant Function}} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot 15^\circ | r = \frac {\cos 15^\circ} {\sin 15^\circ} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac{\frac {\sqrt 6 + \sqrt 2} 4} {\frac {\sqrt 6 - \sqrt 2} 4} | c = [[Cosine of 15 Degrees]] and [[Sine of 15 Degrees]] }} {{eqn | r = \frac {\sqrt 6 + \sqrt 2} {\sqrt 6 - \sqrt 2} | c = simplifying }} {{eqn | r = \frac {\left({\sqrt 6 + \sqrt 2}\right)^2} {\left({\sqrt 6 - \sqrt 2}\right) \left({\sqrt 6 + \sqrt 2}\right)} | c = multiplying top and bottom by $\sqrt 6 + \sqrt 2$ }} {{eqn | r = \frac {6 + 2 \sqrt 6 \sqrt 2 + 2 } {6 - 2} | c = multiplying out, and [[Difference of Two Squares]] }} {{eqn | r = \frac {8 + 4 \sqrt 3} 4 | c = simplifying }} {{eqn | r = 2 + \sqrt 3 | c = dividing top and bottom by $4$ }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | o = | r = 2 \map \sin {\dfrac {\alpha + \beta} 2} \map \cos {\dfrac {\alpha - \beta} 2} | c = }} {{eqn | r = 2 \frac {\map \sin {\dfrac {\alpha + \beta} 2 + \dfrac {\alpha - \beta} 2} + \map \sin {\dfrac {\alpha + \beta} 2 - \dfrac {\alpha - \beta} 2} } 2 | c = [[Simpson's Formula for Sine by Cosine]] }} {{eqn | r = \sin \frac {2 \alpha} 2 + \sin \frac {2 \beta} 2 | c = }} {{eqn | r = \sin \alpha + \sin \beta | c = }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \arcsin \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac 1 {\sqrt {a^2 - x^2} } | c = [[Derivative of Arcsine of x over a|Derivative of $\arcsin \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = \frac 1 {x^2} | c = }} {{eqn | ll= \implies | l = v | r = \frac {-1} x | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {\arcsin \frac x a \ \mathrm d x} {x^2} | r = \arcsin \frac x a \left({\frac {-1} x}\right) - \int \left({\frac {-1} x}\right) \left({\frac 1 {\sqrt {a^2 - x^2} } }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {-\arcsin \frac x a} x + \int \frac {\mathrm d x} {x \sqrt {a^2 - x^2} } \ \mathrm d x + C | c = simplifying }} {{eqn | r = \frac {-\arcsin \frac x a} x - \frac 1 a \ln \left({\frac {a + \sqrt {a^2 - x^2} } x}\right) + C | c = [[Primitive of Reciprocal of x by Root of a squared minus x squared/Logarithm Form|Primitive of $\dfrac 1 {x \sqrt {a^2 - x^2} }$]] }} {{end-eqn}} {{qed}}	0
:$\sin 240 \degrees = \sin \dfrac {4 \pi} 3 = -\dfrac {\sqrt 3} 2$	0
{{begin-eqn}} {{eqn | l = \int \sin^2 x \rd x | r = \int \paren {\frac {1 - \cos 2 x} 2} \rd x | c = [[Square of Sine]] }} {{eqn | r = \frac 1 2 \int \d x - \frac 1 2 \int \cos 2 x \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac x 2 - \frac 1 2 \int \cos 2 x \rd x + C | c = [[Primitive of Constant]] }} {{eqn | r = \frac x 2 - \frac 1 2 \paren {\frac {\sin 2 x} 2} + C | c = [[Primitive of Cosine of a x|Primitive of $\cos a x$]] }} {{eqn | r = \frac x 2 - \frac {\sin 2 x} 4 + C | c = rearranging }} {{end-eqn}} {{Qed}}	0
{{begin-eqn}} {{eqn | l = \csc 240 \degrees | r = \map \csc {360 \degrees - 120 \degrees} | c = }} {{eqn | r = -\csc 120 \degrees | c = [[Cosecant of Conjugate Angle]] }} {{eqn | r = -\frac {2 \sqrt 3} 3 | c = [[Cosecant of 120 Degrees|Cosecant of $120 \degrees$]] }} {{end-eqn}} {{qed}}	0
The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{> 0}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \int_0^{\frac \pi 2} \cos^{2 n} x \rd x = \dfrac {\paren {2 n}!} {\paren {2^n n!}^2} \dfrac \pi 2$ === Basis for the Induction === $\map P 1$ is the case: {{begin-eqn}} {{eqn | l = \int_0^{\frac \pi 2} \cos^2 x \rd x | r = \frac \pi 4 | c = [[Definite Integral from 0 to Half Pi of Square of Cosine x]] }} {{eqn | r = \dfrac 1 2 \times \dfrac \pi 2 | c = }} {{eqn | r = \dfrac 2 {\paren {2 \times 1}^2} \dfrac \pi 2 | c = }} {{eqn | r = \dfrac {\paren {2 \times 1}!} {\paren {2^1 \times 1!}^2} \dfrac \pi 2 | c = }} {{end-eqn}} Thus $\map P 1$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\displaystyle \int_0^{\frac \pi 2} \cos^{2 k} x \rd x = \dfrac {\paren {2 k}!} {\paren {2^k k!}^2} \dfrac \pi 2$ from which it is to be shown that: :$\displaystyle \int_0^{\frac \pi 2} \cos^{2 \paren {k + 1} } x \rd x = \dfrac {\paren {2 \paren {k + 1} }!} {\paren {2^{k + 1} \paren {k + 1}!}^2} \dfrac \pi 2$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: Let $I_k = \displaystyle \int_0^{\frac \pi 2} \cos^{2 k} x \rd x$. {{begin-eqn}} {{eqn | l = I_{k + 1} | r = \frac {2 \paren {k + 1} - 1} {2 \paren {k + 1} } I_k | c = [[Reduction Formula for Definite Integral of Power of Cosine]] }} {{eqn | r = \frac {2 \paren {k + 1} - 1} {2 \paren {k + 1} } \dfrac {\paren {2 k}!} {\paren {2^k k!}^2} \dfrac \pi 2 | c = [[Definite Integral from 0 to Half Pi of Even Power of Cosine x#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \frac {2 \paren {k + 1} \paren {2 \paren {k + 1} - 1} } {2^2 \paren {k + 1}^2} \dfrac {\paren {2 k}!} {\paren {2^k k!}^2} \dfrac \pi 2 | c = }} {{eqn | r = \dfrac {\paren {2 \paren {k + 1} }!} {\paren {2^{k + 1} \paren {k + 1}!}^2} \dfrac \pi 2 | c = }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \forall n \in \Z_{> 0}: \int_0^{\frac \pi 2} \cos^{2 n} x \rd x = \dfrac {\paren {2 n}!} {\paren {2^n n!}^2} \dfrac \pi 2$ {{qed}}	0
:$\displaystyle \int x \arccot \frac x a \rd x = \frac {x^2 + a^2} 2 \arccot \frac x a + \frac {a x} 2 + C$	0
:$\dfrac {\mathrm d \left({\operatorname{arcsec} x }\right)} {\mathrm d x} = \dfrac 1 {x^2 \sqrt {1 - \frac 1 {x^2}}}$	0
:$\displaystyle \int x^2 \sin a x \rd x = \frac {2 x \sin a x} {a^2} + \paren {\frac 2 {a^3} - \frac {x^2} a} \cos a x + C$	0
{{begin-eqn}} {{eqn | l = \sin^3 x | r = \paren {\frac {e^{i x} - e^{-i x} } {2 i} }^3 | c = [[Sine Exponential Formulation]] }} {{eqn | r = \frac {\paren {e^{i x} - e^{-i x} }^3} {8 i^3} | c = rearranging }} {{eqn | r = -\frac 1 {8 i} \paren {\paren {e^{i x} }^3 - 3 \paren {e^{i x} }^2 \paren {e^{-i x} } + 3 \paren {e^{i x} } \paren {e^{-i x} }^2 - \paren {e^{-i x} }^3} | c = multiplying out }} {{eqn | r = -\frac 1 {8 i} \paren {e^{3 i x} - 3 e^{i x} + 3 e^{-i x} - e^{-3 i x} } | c = simplifying }} {{eqn | r = \frac 3 4 \paren {\frac {e^{i x} - e^{-i x} } {2 i} } - \frac 1 4 \paren {\frac {e^{3 i x} - e^{-3 i x} } {2 i} } | c = gathering terms }} {{eqn | r = \frac {3 \sin x - \sin 3 x} 4 | c = [[Sine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int e^{a x} \sin^n b x \ \mathrm d x = \frac {e^{a x} \sin^{n - 1} b x} {a^2 + n^2 b^2} \left({a \sin b x - n b \cos b x}\right) + \frac {n \left({n - 1}\right) b^2} {a^2 + n^2 b^2} \int e^{a x} \sin^{n - 2} b x \ \mathrm d x + C$	0
{{begin-eqn}} {{eqn | l = y | r = \map {\sin^{-1} } {i x} | c = }} {{eqn | ll= \leadsto | l = \sin y | r = i x | c = {{Defof|Complex Inverse Sine}} }} {{eqn | ll= \leadsto | l = i \sin y | r = -x | c = $i^2 = -1$ }} {{eqn | ll= \leadsto | l = \map {\sin^{-1} } {i y} | r = -x | c = [[Sine in terms of Hyperbolic Sine]] }} {{eqn | ll= \leadsto | l = i y | r = \map {\sinh^{-1} } {-x} | c = {{Defof|Inverse Hyperbolic Sine}} }} {{eqn | ll= \leadsto | l = i y | r = -\sinh^{-1} x | c = [[Inverse Hyperbolic Sine is Odd Function]] }} {{eqn | ll= \leadsto | l = y | r = i \sinh^{-1} x | c = multiplying both sides by $-i$ }} {{end-eqn}} {{qed}} {{explain|Investigate the multifunctional nature of [[Definition:Complex Inverse Sine]] and similarly for Hyperbolic Sine}}	0
{{begin-eqn}} {{eqn | l = \frac {\cos x - 1} x | r = \frac {\cos x - \cos 0} x | c = [[Cosine of Zero is One]] }} {{eqn | o = \to | r = \left.{\dfrac {\mathrm d} {\mathrm dx} \cos x}\right \vert_{x \mathop = 0} | c = as $x \to 0$, from definition of [[Definition:Derivative of Real Function at Point|derivative at a point]] }} {{eqn | r = \sin x \vert_{x \mathop = 0} | c = [[Derivative of Cosine Function]] }} {{eqn | r = 0 | c = [[Sine of Zero is Zero]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cos x | r = +\frac 1 {\sqrt {1 + \tan^2 x} } | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ }} {{eqn | l = \cos x | r = -\frac 1 {\sqrt {1 + \tan^2 x} } | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ }} {{end-eqn}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arccot} \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac {-a} {x^2 + a^2} | c = [[Derivative of Arccotangent of x over a|Derivative of $\operatorname{arccot} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = 1 | c = }} {{eqn | ll= \implies | l = v | r = x | c = [[Primitive of Constant]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \operatorname{arccot} \frac x a \ \mathrm d x | r = x \operatorname{arccot} \frac x a - \int x \left({\frac {-a} {x^2 + a^2} }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = x \operatorname{arccot} \frac x a + a \int \frac {x \ \mathrm d x} {x^2 + a^2} + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = x \operatorname{arccot} \frac x a + a \left({\frac 1 2 \ln \left ({x^2 + a^2}\right)}\right) + C | c = [[Primitive of x over x squared plus a squared|Primitive of $\dfrac x {x^2 + a^2}$]] }} {{eqn | r = x \operatorname{arccot} \frac x a + \frac a 2 \ln \left({x^2 + a^2}\right) + C | c = simplifying }} {{end-eqn}} {{qed}}	0
The result follows from [[Real Cosine Function is Bounded]] and [[Bounded Function is of Exponential Order Zero]]. {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\ds \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = x | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = 1 | c = [[Derivative of Identity Function]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \frac 1 {1 - \sin a x} | c = }} {{eqn | ll= \leadsto | l = v | r = \frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} | c = [[Primitive of Reciprocal of 1 minus Sine of a x]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {x \rd x} {1 - \sin a x} | r = x \paren {\frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} } - \int \paren {\frac 1 a \map \tan {\frac \pi 4 + \frac {a x} 2} } \times 1 \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac x a \map \tan {\frac \pi 4 + \frac {a x} 2} - \frac 1 a \int \map \tan {\frac \pi 4 + \frac {a x} 2} \rd x + C | c = simplifying }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = z | r = \frac \pi 4 + \frac {a x} 2 }} {{eqn | l = \frac {\d z} {\d x} | r = \frac a 2 | c = [[Derivative of Power]] }} {{eqn | ll= \implies | l = \frac 1 a \int \map \tan {\frac \pi 4 + \frac {a x} 2} \rd x | r = \frac 1 a \int \frac 2 a \tan z \rd z | c = [[Integration by Substitution]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\cos z} + C | c = [[Primitive of Tangent Function/Cosine Form|Primitive of $\tan z$: Cosine Form]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \cos {\frac \pi 4 + \frac {a x} 2} } + C | c = substituting back for $z$ }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \sin {\frac \pi 2 - \paren {\frac \pi 4 + \frac {a x} 2} } } + C | c = [[Sine of Complement equals Cosine]] }} {{eqn | r = -\frac 2 {a^2} \ln \size {\map \sin {\frac \pi 4 - \frac {a x} 2} } + C | c = }} {{end-eqn}} Putting it all together: :$\ds \int \frac {x \rd x} {1 - \sin a x} = \frac x a \map \tan {\frac \pi 4 + \frac {a x} 2} + \frac 2 {a^2} \ln \size {\map \sin {\frac \pi 4 - \frac {a x} 2} } + C$ {{qed}}	0
Let $\lambda \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real number]]. Let $\map f x: \openint {-\lambda} \lambda \to \R$ be the [[Definition:Absolute Value|absolute value function]] on the [[Definition:Open Real Interval|open real interval]] $\openint {-\lambda} \lambda$: :$\forall x \in \openint {-\lambda} \lambda: \map f x = \size x$ The [[Definition:Fourier Series|Fourier series]] of $f$ over $\openint {-\lambda} \lambda$ can be given as: {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \sum_{n \mathop = 0}^\infty \frac 1 {\paren {2 n + 1}^2} \cos \dfrac {\paren {2 n + 1} \pi x} \lambda | c = }} {{eqn | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \paren {\cos \dfrac {\pi x} \lambda + \frac 1 {3^2} \cos \dfrac {3 \pi x} \lambda + \frac 1 {5^2} \dfrac {5 \pi x} \lambda + \dotsb} | c = }} {{end-eqn}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\d v} {\d x} \rd x = u v - \int v \frac {\d u} {\d x} \rd x$ let: {{begin-eqn}} {{eqn | l = u | r = x^2 | c = }} {{eqn | ll= \leadsto | l = \frac {\d u} {\d x} | r = 2 x | c = [[Derivative of Power]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\d v} {\d x} | r = \cos a x | c = }} {{eqn | ll= \leadsto | l = v | r = \frac {\sin a x} a | c = [[Primitive of Cosine of a x|Primitive of $\cos a x$]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^2 \map \cos {a x} \rd x | r = x^2 \paren {\frac {\sin a x} a} - \int \paren {2 x \frac {\sin a x} a} \rd x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^2 \sin a x} a - \frac 2 a \int x \sin a x \rd x + C | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac {x^2 \sin a x} a - \frac 2 a \paren {\frac {\sin a x} {a^2} - \frac {x \cos a x} a} + C | c = [[Primitive of x by Sine of a x|Primitive of $x \sin a x$]] }} {{eqn | r = \frac {2 x \cos a x} {a^2} + \paren {\frac {x^2} a - \frac 2 {a^3} } \sin a x + C | c = simplification }} {{end-eqn}} {{qed}}	0
:$\cos 180^\circ = \cos \pi = - 1$	0
{{begin-eqn}} {{eqn | l = \tan 45 \degrees | r = \frac {\sin 45 \degrees} {\cos 45 \degrees} | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac {\frac {\sqrt 2} 2} {\frac {\sqrt 2} 2} | c = [[Sine of 45 Degrees|Sine of $45 \degrees$]] and [[Cosine of 45 Degrees|Cosine of $45 \degrees$]] }} {{eqn | r = 1 | c = dividing top and bottom by $\sqrt 2 / 2$ }} {{end-eqn}} {{qed}}	0
Let $\theta$ be an [[Definition:Angle|angle]]. {{TFAE|def = Secant of Angle|view = secant}}	0
{{begin-eqn}} {{eqn | l = \sin 15^\circ | r = \sin \frac {30^\circ} 2 | c = }} {{eqn | r = \sqrt {\frac {1 - \cos 30^\circ} 2} | c = [[Half Angle Formula for Sine]]: $\theta$ is in [[Definition:Sine/Definition from Circle/First Quadrant|Quadrant I]] }} {{eqn | r = \sqrt {\frac {1 - \frac {\sqrt 3} 2} 2} | c = [[Cosine of 30 Degrees|Cosine of $30^\circ$]] }} {{eqn | r = \sqrt {\frac {2 - \sqrt 3} 4} | c = }} {{eqn | r = \sqrt {\frac {8 - 4 \sqrt 3} {16} } | c = }} {{eqn | r = \sqrt {\frac {6 + 2 - 2 \sqrt 2 \sqrt 6} {16} } | c = }} {{eqn | r = \sqrt {\frac {\left({\sqrt 6 - \sqrt 2}\right)^2} {16} } | c = }} {{eqn | r = \frac {\sqrt 6 - \sqrt 2} 4 | c = positive because $\theta$ is in the [[Definition:Sine/Definition from Circle/First Quadrant|first quadrant]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \cos^2 x \rd x | r = \int \paren {\frac {1 + \cos 2 x} 2} \rd x | c = [[Square of Cosine]] }} {{eqn | r = \int \frac 1 2 \rd x + \int \paren {\frac {\cos 2 x} 2} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac x 2 + C + \int \paren {\frac {\cos 2 x} 2} \rd x | c = [[Primitive of Constant]] }} {{eqn | r = \frac x 2 + C + \frac 1 2 \int \cos 2 x \rd x | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac x 2 + \frac 1 2 \paren {\frac {\sin 2 x} 2} + C | c = [[Primitive of Function of Constant Multiple]] and [[Primitive of Cosine Function]] }} {{eqn | r = \frac x 2 + \frac {\sin 2 x} 4 + C | c = [[Primitive of Function of Constant Multiple]] and [[Primitive of Cosine Function]] }} {{end-eqn}} {{Qed}}	0
{{Proofread}} Assume $y \in \R$, $ -\dfrac \pi 2 \le y \le \dfrac \pi 2 $. {{begin-eqn}} {{eqn | l = y | r = \arccot x }} {{eqn | ll= \leadstoandfrom | l = x | r = \cot y }} {{eqn | ll= \leadstoandfrom | l = x | r = i \frac {1 + e^{2 i y} } {1 - e^{2 i y} } | c = [[Cotangent Exponential Formulation]] }} {{eqn | ll= \leadstoandfrom | l = i x | r = \frac {e^{2 i y} + 1} {e^{2 i y} - 1} | c = $ i^2 = -1 $ }} {{eqn | ll= \leadstoandfrom | l = i x \paren {e^{2 i y} - 1} | r = e^{2 i y} + 1 }} {{eqn | ll= \leadstoandfrom | l = i x e^{2 i y} - i x | r = e^{2 i y} + 1 }} {{eqn | ll= \leadstoandfrom | l = e^{2 i y} - i x e^{2 i y} | r = 1 + i x }} {{eqn | ll= \leadstoandfrom | l = e^{2 i y} | r = \frac {1 + i x} {1 - i x} }} {{eqn | ll= \leadstoandfrom | l = y | r = \frac 1 2 i \map \ln {\frac {1 + i x} {1 - i x} } }} {{end-eqn}} {{qed}}	0
A direct implementation of [[Sine of Multiple of Pi]]: :$\forall n \in \Z: \sin n \pi = 0$ In this case, $n = 1$ and so: :$\sin \pi = 0$ {{qed}}	0
Let $z$ be a [[Definition:Complex Number|complex number]]. Let $\csc z$ denote the [[Definition:Complex Cosecant Function|cosecant function]] and $i$ denote the [[Definition:Imaginary Unit|imaginary unit]]: $i^2 = -1$. Then: :$\csc z = \dfrac {2 i} {e^{i z} - e^{-i z} }$	0
:$\cot 225^\circ = \cot \dfrac {5 \pi} 4 = 1$	0
{{begin-eqn}} {{eqn | l = \csc i | r = \frac 1 {\sin i} | c = Definition of [[Definition:Complex Cosecant Function|Complex Cosecant]] }} {{eqn | r = \frac 1 {\left({ \frac e 2 - \frac 1 {2e} }\right) i} | c = [[Sine of i|Sine of $i$]] }} {{eqn | r = \left({ \frac 1 {\frac 1 {2e} - \frac e {2} } }\right) i | c = [[Reciprocal of i|Reciprocal of $i$]] }} {{eqn | r = \left({\frac {2 e} {1 - e^2} }\right) i | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $2 e$ }} {{end-eqn}} {{qed}}	0
Let it be assumed that $p \sin x + q \cos x$ can be expressed in the form $M \sin \left({x + \phi}\right)$. Then: {{begin-eqn}} {{eqn | l = p \sin x + q \cos x | r = M \sin \left({x + \phi}\right) | c = }} {{eqn | ll= \implies | l = \frac p M \sin x + \frac q M \cos x | r = \sin \left({x + \phi}\right) | c = }} {{eqn | r = \sin x \cos \phi + \cos x \sin \phi | c = [[Sine of Sum]] }} {{eqn | ll= \implies | l = \sin x \cos \phi | r = \frac p M \sin x | c = equating terms in $\sin x$ }} {{eqn | l = \cos x \sin \phi | r = \frac q M \cos x | c = and $\cos x$ }} {{eqn | n = 1 | ll= \implies | l = \sin \phi | r = \frac q M | c = }} {{eqn | n = 2 | l = \cos \phi | r = \frac p M | c = }} {{eqn | ll= \implies | l = \sin^2 \phi + \cos^2 \phi | r = \left({\frac q M}\right)^2 + \left({\frac p M}\right)^2 | c = squaring and adding $(1)$ and $(2)$ }} {{eqn | ll= \implies | l = 1 | r = \frac {p^2 + q^2} {M^2} | c = }} {{eqn | ll= \implies | r = M | l = \sqrt {p^2 + q^2} | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \tan \phi | r = \frac {\sin \phi} {\cos \phi} | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac {q / M} {p / M} | c = from $(1)$ and $(2)$ }} {{eqn | r = \frac q p | c = simplifying }} {{end-eqn}} Hence the result. {{qed}} [[Category:Sine Function]] [[Category:Cosine Function]] oc56d623463q7jxezd0o7ngan8b9zd8	0
{{begin-eqn}} {{eqn | l = \cos^{2 n + 1} \theta | r = \paren {\frac {e^{i \theta} + e^{-i \theta} } 2}^{2 n + 1} | c = [[Cosine Exponential Formulation]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {e^{i \theta} + e^{-i \theta} }^{2 n + 1} | c = [[Exponent Combination Laws/Power of Product|Exponent Combination Laws: Power of Product]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \sum^{2 n + 1}_{k \mathop = 0} \binom {2 n + 1} k e^{k i \theta} e^{-\paren {2 n - k + 1} i \theta} | c = [[Binomial Theorem]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \sum^{2 n + 1}_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} | c = [[Exponential of Sum]] }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^{2 n + 1}_{k \mathop = n + 1} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} } | c = Partitioning the sum }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^n_{k \mathop = 0} \binom {2 n + 1} {2 n + 1 - k} e^{\paren {2 \paren {2 n - k + 1} - 2 n - 1} i \theta} } | c = $k \mapsto 2 n + 1 - k$ }} {{eqn | r = \frac 1 {2^{2 n + 1} } \paren {\sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{-\paren {2 n - 2 k + 1} i \theta} + \sum^n_{k \mathop = 0} \binom {2 n + 1} k e^{\paren {2 n - 2 k + 1} i \theta} } | c = [[Symmetry Rule for Binomial Coefficients]] }} {{eqn | r = \frac 1 {2^{2 n} } \sum^n_{k \mathop = 0} \binom {2 n + 1} k \, \map \cos {2 n - 2 k + 1} \theta | c = [[Cosine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
From [[Cosine of Zero is One]] we have that $\cos 0 = 1$. From [[Cosine Function is Even]] we have that $\cos x = \map \cos {-x}$. As the [[Cosine Function is Continuous]], it follows that: :$\exists \xi > 0: \forall x \in \openint {-\xi} \xi: \cos x > 0$ {{AimForCont}} $\cos x$ were [[Definition:Positive Real Function|positive]] everywhere on $\R$. From [[Derivative of Cosine Function]]: :$\map {D_{xx} } {\cos x} = \map {D_x} {-\sin x} = -\cos x$ Thus $-\cos x$ would always be [[Definition:Negative Real Function|negative]]. Thus from [[Second Derivative of Concave Real Function is Non-Positive]], $\cos x$ would be [[Definition:Concave Real Function|concave]] everywhere on $\R$. But from [[Real Cosine Function is Bounded]], $\cos x$ is [[Definition:Bounded Real-Valued Function|bounded on $\R$]]. By [[Differentiable Bounded Concave Real Function is Constant]], $\cos x$ would then be a [[Definition:Constant Function|constant function]]. This [[Definition:Contradiction|contradicts]] the fact that $\cos x$ is not a [[Definition:Constant Function|constant function]]. Thus by [[Proof by Contradiction]] $\cos x$ can not be [[Definition:Positive Real Function|positive]] everywhere on $\R$. Therefore, there must exist a smallest [[Definition:Positive Real Number|positive]] $\eta \in \R_{>0}$ such that $\cos \eta = 0$. By definition, $\cos \eta = \map \cos {-\eta} = 0$ and $\cos x > 0$ for $-\eta < x < \eta$. Now we show that $\sin \eta = 1$. From [[Sum of Squares of Sine and Cosine]]: :$\cos^2 x + \sin^2 x = 1$ Hence as $\cos \eta = 0$ it follows that $\sin^2 \eta = 1$. So either $\sin \eta = 1$ or $\sin \eta = -1$. But $\map {D_x} {\sin x} = \cos x$. On the [[Definition:Closed Real Interval|interval]] $\closedint {-\eta} \eta$, it has been shown that $\cos x > 0$. Thus by [[Derivative of Monotone Function]], $\sin x$ is [[Definition:Increasing Real Function|increasing]] on $\closedint {-\eta} \eta$. Since $\sin 0 = 0$ it follows that $\sin \eta > 0$. So it must be that $\sin \eta = 1$. Now we apply [[Sine of Sum]] and [[Cosine of Sum]]: :$\map \sin {x + \eta} = \sin x \cos \eta + \cos x \sin \eta = \cos x$ :$\map \cos {x + \eta} = \cos x \cos \eta - \sin x \sin \eta = -\sin x$ Hence it follows, after some algebra, that: :$\map \sin {x + 4 \eta} = \sin x$ :$\map \cos {x + 4 \eta} = \cos x$ Thus $\sin$ and $\cos$ are [[Definition:Periodic Function|periodic]] on $\R$ with [[Definition:Period of Function|period]] $4 \eta$. {{qed}}	0
:$\sin 3^\circ = \sin \dfrac \pi {60} = \dfrac {\sqrt{30} + \sqrt{10} - \sqrt 6 - \sqrt 2 - 2 \sqrt {15 + 3 \sqrt 5} + 2 \sqrt {5 + \sqrt 5} } {16}$ where $\sin$ denotes the [[Definition:Sine Function|sine function]].	0
Starting from the right, we have: {{begin-eqn}} {{eqn | l = \cos^2 \theta - \sin^2\theta | r = \left({\frac 1 2 \left({e^{i \theta} + e^{-i \theta} }\right)}\right)^2 - \left({\frac 1 {2 i} \left({e^{i \theta} - e^{-i \theta} }\right)}\right)^2 | c = [[Cosine Exponential Formulation]], [[Sine Exponential Formulation]] }} {{eqn | r = \frac 1 4 \left({e^{i \theta} + e^{-i \theta} }\right)^2 + \frac 1 4 \left({e^{i \theta} - e^{-i \theta} }\right)^2 | c = $i$ is the [[Definition:Imaginary Unit|imaginary unit]] }} {{eqn | r = \frac 1 4 \left({e^{2 i \theta} + 2 + e^{-2 i \theta} + e^{2 i \theta} - 2 + e^{-2 i \theta} }\right) | c = [[Square of Sum]], [[Square of Difference]] }} {{eqn | r = \frac 1 2 \left({e^{2 i \theta} + e^{-2 i \theta} }\right) | c = Simplifying }} {{eqn | r = \cos 2 \theta | c = [[Cosine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
We have: {{begin-eqn}} {{eqn | l = \cos 5 \theta + i \sin 5 \theta | r = \paren {\cos \theta + i \sin \theta}^5 | c = [[De Moivre's Formula]] }} {{eqn | r = \paren {\cos \theta}^5 + \binom 5 1 \paren {\cos \theta}^4 \paren {i \sin \theta} + \binom 5 2 \paren {\cos \theta}^3 \paren {i \sin \theta}^2 | c = [[Binomial Theorem]] }} {{eqn | o = | ro=+ | r = \binom 5 3 \paren {\cos \theta}^2 \paren {i \sin \theta}^3 + \binom 5 4 \paren {\cos \theta} \paren {i \sin \theta}^4 + \paren {i \sin \theta}^5 }} {{eqn | r = \cos^5 \theta + 5 i \cos^4 \theta \sin \theta - 10 \cos^3 \theta \sin^2 \theta | c = substituting for [[Definition:Binomial Coefficient|binomial coefficients]] }} {{eqn | o = | ro=- | r = 10 i \cos^2 \theta \sin^3 \theta + 5 \cos \theta \sin^4 \theta + i \sin^5 \theta | c = and using $i^2 = -1$ }} {{eqn | n = 1 | r = \cos^5 \theta - 10 \cos^3 \theta \sin^2 \theta + 5 \cos \theta \sin^4 \theta }} {{eqn | o = | ro=+ | r = i \paren {5 \cos^4 \theta \sin \theta - 10 \cos^2 \theta \sin^3 \theta + \sin^5 \theta} | c = rearranging }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \cos 5 \theta | r = \cos^5 \theta - 10 \cos^3 \theta \sin^2 \theta + 5 \cos \theta \sin^4 \theta | c = equating [[Definition:Real Part|real parts]] in $(1)$ }} {{eqn | r = \cos^5 \theta - 10 \cos^3 \theta \paren {1 - \cos^2 \theta} + 5 \cos \theta \paren {1 - \cos^2 \theta}^2 | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 16 \cos^5 \theta - 20 \cos^3 \theta + 5 \cos \theta | c = multiplying out and gathering terms }} {{end-eqn}} {{qed}}	0
By definition of [[Definition:Half-Range Fourier Cosine Series|half-range Fourier cosine series]]: :$\displaystyle \map f x \sim \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty a_n \cos \dfrac {n \pi x} \lambda$ where for all $n \in \Z_{> 0}$: :$a_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map f x \cos \dfrac {n \pi x} \lambda \rd x$ Thus by definition of $f$: {{begin-eqn}} {{eqn | l = a_0 | r = \frac 2 \lambda \int_0^\lambda \map f x \rd x | c = [[Cosine of Zero is One]] }} {{eqn | r = \frac 2 \lambda \int_0^\lambda x \rd x | c = Definition of $f$ }} {{eqn | r = \frac 2 \lambda \intlimits {\frac {x^2} 2} 0 \lambda | c = [[Primitive of Power]] }} {{eqn | r = \frac 2 \lambda \paren {\frac {l^2} 2 - \frac {0^2} 2} | c = }} {{eqn | r = \lambda | c = simplification }} {{end-eqn}} {{qed|lemma}} For $n > 0$: {{begin-eqn}} {{eqn | l = a_n | r = \frac 2 \lambda \int_0^l \map f x \cos \dfrac {n \pi x} \lambda \rd x | c = }} {{eqn | r = \frac 2 \lambda \int_0^l x \cos \dfrac {n \pi x} \lambda \rd x | c = Definition of $f$ }} {{eqn | r = \frac 2 \lambda \intlimits {\frac {l^2} {\pi^2 n^2} \cos \dfrac {n \pi x} \lambda + \frac \lambda {\pi n} x \sin \dfrac {n \pi x} \lambda} 0 \lambda | c = [[Primitive of x by Cosine of a x|Primitive of $x \cos n x$]] }} {{eqn | r = \frac 2 \lambda \paren {\paren {\frac {\lambda^2} {\pi^2 n^2} \cos n \pi + \frac {\lambda^2} {\pi n} \sin n \pi} - \paren {\frac {\lambda^2} {\pi^2 n^2} \cos 0 + \frac \lambda {\pi n} 0 \sin 0} } | c = }} {{eqn | r = \frac {2 \lambda} {\pi^2 n^2} \paren {\cos n \pi - \cos 0} | c = [[Sine of Multiple of Pi]] and simplification }} {{eqn | r = \frac {2 \lambda} {\pi^2 n^2} \paren {\paren {-1}^n - 1} | c = [[Cosine of Multiple of Pi]] }} {{end-eqn}} When $n$ is [[Definition:Even Integer|even]], $\paren {-1}^n = 1$. We can express $n = 2 r$ for $r \ge 1$. Hence in that case: {{begin-eqn}} {{eqn | l = a_{2 r} | r = \frac {2 \lambda} {\pi^2 n^2} \paren {\paren {-1}^n - 1} | c = }} {{eqn | r = \frac {2 \lambda} {\pi^2 n^2} \paren {1 - 1} | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} When $n$ is [[Definition:Odd Integer|odd]], $\paren {-1}^n = -1$. We can express $n = 2 r + 1$ for $r \ge 0$. Hence in that case: {{begin-eqn}} {{eqn | l = a_{2 r - 1} | r = \frac {2 \lambda} {\pi^2 \paren {2 r + 1}^2} \paren {-1 - 1} | c = }} {{eqn | r = -\frac {4 \lambda} {\pi^2} \paren {\frac 1 {\paren {2 r + 1}^2} } | c = rearranging }} {{end-eqn}} Finally: {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty a_n \cos n x | c = }} {{eqn | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \sum_{r \mathop = 0}^\infty \frac 1 {\paren {2 r + 1}^2} \cos \dfrac {\paren {2 r + 1} \pi x} \lambda | c = substituting for $a_0$ and $a_n$ from above }} {{eqn | r = \frac \lambda 2 - \frac {4 \lambda} {\pi^2} \sum_{n \mathop = 0}^\infty \frac 1 {\paren {2 n + 1}^2} \cos \dfrac {\paren {2 n + 1} \pi x} \lambda | c = changing the name of the variable }} {{end-eqn}} {{qed}} [[Category:Half-Range Fourier Series for Identity Function]] 6ban72q3brqfa31nsk4okvynnqfo5s8	0
First note that: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arccot} \frac x a | c = }} {{eqn | ll= \implies | l = x | r = a \cot u | c = Definition of [[Definition:Arccotangent|Arccotangent]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arccot} \frac x a | c = }} {{eqn | ll= \implies | l = \frac{\mathrm d u} {\mathrm d x} | r = \frac {-a} {a^2 + x^2} | c = [[Derivative of Arccotangent Function/Corollary|Derivative of Arccotangent Function: Corollary]] }} {{eqn | ll= \implies | l = \int F \left({\operatorname{arccot} \frac x a}\right) \ \mathrm d x | r = \int F \left({u}\right) \ \frac {a^2 + x^2} {-a} \ \mathrm d u | c = [[Primitive of Composite Function]] }} {{eqn | r = \int F \left({u}\right) \ \frac {a^2 + a^2 \cot^2 u} {-a} \ \mathrm d u | c = Definition of $x$ }} {{eqn | r = \int F \left({u}\right) \ a^2 \frac {1 + \cot^2 u} {-a} \ \mathrm d u | c = }} {{eqn | r = \int F \left({u}\right) \ \left({-a}\right) \csc^2 u \ \mathrm d u | c = [[Difference of Squares of Cosecant and Cotangent]] }} {{eqn | r = -a \int F \left({u}\right) \ \csc^2 u \ \mathrm d u | c = [[Primitive of Constant Multiple of Function]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int F \left({\operatorname{arccsc} \frac x a}\right) \ \mathrm d x = -a \int F \left({u}\right) \left\vert{\csc u}\right\vert \cot u \ \mathrm d u$ where $u = \operatorname{arccsc} \dfrac x a$.	0
For the first part: {{begin-eqn}} {{eqn | l = \cos x | o = > | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ | cc= [[Sign of Cosine]] }} {{eqn | ll= \leadsto | l = \frac 1 {\cos x} | o = > | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ | cc= [[Reciprocal of Strictly Positive Real Number is Strictly Positive]] }} {{eqn | ll= \leadsto | l = \sec x | o = > | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n - \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 1 2} \pi$ | cc= [[Secant is Reciprocal of Cosine]] }} {{end-eqn}} For the second part: {{begin-eqn}} {{eqn | l = \cos x | o = < | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ | cc= [[Sign of Cosine]] }} {{eqn | ll= \leadsto | l = \frac 1 {\cos x} | o = < | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ | cc= [[Reciprocal of Strictly Negative Real Number is Strictly Negative]] }} {{eqn | ll= \leadsto | l = \sec x | o = < | r = 0 | c = if there exists an [[Definition:Integer|integer]] $n$ such that $\paren {2 n + \dfrac 1 2} \pi < x < \paren {2 n + \dfrac 3 2} \pi$ | cc= [[Secant is Reciprocal of Cosine]] }} {{end-eqn}} {{qed}}	0
:$\csc 15^\circ = \csc \dfrac \pi {12} = \sqrt 6 + \sqrt 2$	0
This is established in [[Zeroes of Sine and Cosine]]. {{qed}}	0
The value of a [[Definition:Radian|radian]] in [[Definition:Degree of Arc|degrees]] is given by: :$1 \radians = \dfrac {180 \degrees} {\pi} \approx 57.29577 \ 95130 \ 8232 \ldots \degrees$ {{OEIS|A072097}}	0
It is apparent by inspection that: :$(1): \quad g$ is an [[Definition:Extension of Mapping|extension]] of $f$ :$(2): \quad g$ is an [[Definition:Odd Function|odd function]]. Let $\map T x$ be the [[Definition:Fourier Series|Fourier series]] representing $g$: :$\map g x \sim \map T x = \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos \frac {n \pi x} \lambda + b_n \sin \frac {n \pi x} \lambda}$ where for all $n \in \Z_{> 0}$: :$a_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \cos \frac {n \pi x} \lambda \rd x$ :$b_n = \displaystyle \frac 1 \lambda \int_{-\lambda}^\lambda \map g x \sin \frac {n \pi x} \lambda \rd x$ From [[Fourier Cosine Coefficients for Odd Function over Symmetric Range]]: :$\forall n \in \Z_{\ge 0}: a_n = 0$ and from [[Fourier Sine Coefficients for Odd Function over Symmetric Range]]: :$\forall n \in \Z_{>0}: b_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map g x \sin \frac {n \pi x} \lambda \rd x$ But on $\openint 0 \lambda$, by definition: :$\forall x \in \openint 0 \lambda: \map g x = \map f x$ Hence: :$\map T x = \displaystyle \sum_{n \mathop = 1}^\infty \paren b_n \sin \frac {n \pi x} \lambda$ where: :$b_n = \displaystyle \frac 2 \lambda \int_0^\lambda \map f x \sin \frac {n \pi x} \lambda \rd x$ That is, $\map T x$ is the same as $\map S x$. Hence the result. {{qed}} [[Category:Half-Range Fourier Series]] f8yrsjeo4qkda53fgo1p4mkfp8kpmj5	0
:$\displaystyle \int \cosh a x \sin p x \ \mathrm d x = \frac {a \sinh a x \sin p x - p \cosh a x \cos p x} {a^2 + p^2} + C$	0
:$\displaystyle \cos^{2n+1} \theta = \frac 1 {2^{2n}} \sum_{k \mathop = 0}^n \binom {2n+1} k \cos \left({2n - 2k + 1}\right) \theta$	0
{{begin-eqn}} {{eqn | l = \tan z | r = \frac {\sin z} {\cos z} | c = {{Defof|Complex Tangent Function}} }} {{eqn | r = \frac {\frac 1 2 i \paren {e^{-i z} - e^{i z} } } {\frac 1 2 \paren {e^{-i z} + e^{i z} } } | c = [[Sine Exponential Formulation]] and [[Cosine Exponential Formulation]] }} {{eqn | r = i \frac {e^{-i z} - e^{i z} } {e^{-i z} + e^{i z} } }} {{eqn | r = i \frac {1 - e^{2 i z} } {1 + e^{2 i z} } | c = multiplying [[Definition:Numerator|numerator]] and [[Definition:Denominator|denominator]] by $e^{i z}$ }} {{end-eqn}} {{qed}} [[Category:Tangent Exponential Formulation]] jupkkkg9vg2be4qs1g0evp04zf8wctk	0
:$\ds \int \frac {x \rd x} {1 + \sin a x} = -\frac x a \map \tan {\frac \pi 4 - \frac {a x} 2} + \frac 2 {a^2} \ln \size {\map \sin {\frac \pi 4 + \frac {a x} 2} } + C$	0
:$\paren {\sin x + \cos x} \paren {\tan x + \cot x} = \sec x + \csc x$	0
{{begin-eqn}} {{eqn | l = \sin c \sin a \cos B | r = \cos b - \cos c \cos a | c = [[Spherical Law of Cosines]] }} {{eqn | r = \cos b - \cos c \paren {\cos b \cos c + \sin b \sin c \cos A} | c = [[Spherical Law of Cosines]] }} {{eqn | r = \cos b \paren {1 - \cos^2 c} - \sin b \sin c \cos c \cos A | c = rearranging }} {{eqn | r = \sin^2 c \cos b - \sin b \sin c \cos c \cos A | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | ll= \leadsto | l = \sin a \cos B | r = \sin c \cos b - \sin b \cos c \cos A | c = simplifying }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \sin a \sin b \cos C | r = \cos c - \cos a \cos b | c = [[Spherical Law of Cosines]] }} {{eqn | r = \cos c - \cos b \paren {\cos b \cos c + \sin b \sin c \cos A} | c = [[Spherical Law of Cosines]] }} {{eqn | r = \cos c \paren {1 - \cos^2 b} - \sin b \sin c \cos b \cos A | c = rearranging }} {{eqn | r = \sin^2 b \cos c - \sin b \sin c \cos b \cos A | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | ll= \leadsto | l = \sin a \cos C | r = \cos b \sin c - \sin c \cos b \cos A | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\cos 345 \degrees = \cos \dfrac {23 \pi} {12} = \dfrac {\sqrt 6 + \sqrt 2} 4$	0
By definition of [[Definition:Fourier Series over Range 2 Pi|Fourier series]]: :$\displaystyle \map f x \sim \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty \paren {a_n \cos n x + b_n \sin n x}$ where: {{begin-eqn}} {{eqn | l = a_n | r = \dfrac 1 \pi \int_0^{2 \pi} \map f x \cos n x \rd x }} {{eqn | l = b_n | r = \dfrac 1 \pi \int_0^{2 \pi} \map f x \sin n x \rd x }} {{end-eqn}} for all $n \in \Z_{>0}$. Thus: {{begin-eqn}} {{eqn | l = a_0 | r = \frac 1 \pi \int_0^{2 \pi} \map f x \rd x | c = [[Cosine of Zero is One]] }} {{eqn | r = \frac 1 \pi \int_0^\pi -\pi \rd x + \frac 1 \pi \int_\pi^{2 \pi} \paren {x - \pi} \rd x | c = }} {{eqn | r = -\int_0^\pi \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \rd x - \int_\pi^{2 \pi} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = -\int_0^{2 \pi} \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \rd x | c = [[Sum of Integrals on Adjacent Intervals for Continuous Functions]] }} {{eqn | r = -\big[{x}\big]_0^{2 \pi} + \frac 1 \pi \int_\pi^{2 \pi} x \rd x | c = [[Primitive of Constant]] }} {{eqn | r = -\paren {2 \pi - 0} + \frac 1 \pi \int_\pi^{2 \pi} x \rd x | c = }} {{eqn | r = -2 \pi + \frac 1 \pi \paren {\intlimits {\frac {x^2} 2} \pi {2 \pi} } | c = [[Primitive of Power]] }} {{eqn | r = -2 \pi + \frac 1 \pi \paren {\frac {\paren {2 \pi}^2} 2 - \frac {\pi^2} 2} | c = }} {{eqn | r = -2 \pi + 2 \pi - \frac \pi 2 | c = }} {{eqn | r = -\frac \pi 2 | c = }} {{end-eqn}} {{qed|lemma}} For $n > 0$: {{begin-eqn}} {{eqn | l = a_n | r = \frac 1 \pi \int_0^{2 \pi} \map f x \cos n x \rd x | c = }} {{eqn | r = \frac 1 \pi \int_0^\pi \paren {-\pi} \cos n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} \paren {x - \pi} \cos n x \rd x | c = Definition of $f$ }} {{eqn | r = -\int_0^\pi \cos n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \cos n x \rd x - \int_\pi^{2 \pi} \cos n x \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = -\int_0^{2 \pi} \cos n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \cos n x \rd x | c = [[Sum of Integrals on Adjacent Intervals for Continuous Functions]] }} {{eqn | r = 0 + \frac 1 \pi \int_\pi^{2 \pi} x \cos n x \rd x | c = [[Integral over 2 pi of Cosine of n x|Integral over $2 \pi$ of $\cos n x$]] }} {{eqn | r = \frac 1 \pi \intlimits {\frac {\cos n x} {n^2} + \frac {x \sin n x} n} \pi {2 \pi} | c = [[Primitive of x by Cosine of a x|Primitive of $x \cos n x$]] }} {{eqn | r = \frac 1 \pi \paren {\frac {\cos 2 n \pi} {n^2} + \frac {2 \pi \sin 2 n \pi} n} - \frac 1 \pi \paren {\frac {\cos n \pi} {n^2} + \frac {\pi \sin n \pi} n} | c = }} {{eqn | r = \frac 1 \pi \paren {\frac {\cos 2 n \pi} {n^2} - \frac {\cos n \pi} {n^2} } | c = [[Sine of Multiple of Pi]] }} {{eqn | r = \frac {1 - \paren {-1}^n} {n^2 \pi} | c = [[Cosine of Multiple of Pi]] and simplifying }} {{eqn | r = \begin {cases} \dfrac 2 {n^2 \pi} & : \text {$n$ odd} \\ 0 & : \text {$n$ even} \end {cases} | c = separating out the [[Definition:Odd Integer|odd]] and [[Definition:Even Integer|even]] cases }} {{eqn | r = \dfrac 2 {\pi \paren {2 r + 1}^2} | c = setting $n = 2 r + 1$ for [[Definition:Odd Integer|odd]] $n$ }} {{end-eqn}} {{qed|lemma}} Now for the $\sin n x$ terms: {{begin-eqn}} {{eqn | l = b_n | r = \frac 1 \pi \int_0^{2 \pi} \map f x \sin n x \rd x | c = }} {{eqn | r = \frac 1 \pi \int_0^\pi \paren {-\pi} \sin n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} \paren {x - \pi} \sin n x \rd x | c = Definition of $f$ }} {{eqn | r = -\int_0^\pi \sin n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \sin n x \rd x - \int_\pi^{2 \pi} \sin n x \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = -\int_0^{2 \pi} \sin n x \rd x + \frac 1 \pi \int_\pi^{2 \pi} x \sin n x \rd x | c = [[Sum of Integrals on Adjacent Intervals for Continuous Functions]] }} {{eqn | r = 0 + \frac 1 \pi \int_\pi^{2 \pi} x \sin n x \rd x | c = [[Integral over 2 pi of Sine of n x|Integral over $2 \pi$ of $\sin n x$]] }} {{eqn | r = \frac 1 \pi \intlimits {\frac {\sin n x} {n^2} - \frac {x \cos n x} n} \pi {2 \pi} | c = [[Primitive of x by Sine of a x|Primitive of $x \sin n x$]] }} {{eqn | r = \frac 1 \pi \paren {\frac {\sin 2 n \pi} {n^2} - \frac {2 \pi \cos 2 n \pi} n} - \frac 1 \pi \paren {\frac {\sin \pi} {n^2} - \frac {\pi \cos \pi} n} | c = }} {{eqn | r = \frac {-2 \cos 2 n \pi + \cos n \pi} n | c = [[Sine of Multiple of Pi]] and simplifying }} {{eqn | r = \frac {-2 + \paren {-1}^n} n | c = [[Cosine of Multiple of Pi]] }} {{eqn | r = -\frac {2 - \paren {-1}^n} n | c = rearranging }} {{end-eqn}} {{qed|lemma}} Finally: {{begin-eqn}} {{eqn | l = \map f x | o = \sim | r = \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty \paren {a_n \cos n x + b_n \sin n x} | c = }} {{eqn | r = \dfrac 1 2 \paren {-\frac \pi 2} + \sum_{r \mathop = 0}^\infty \frac 2 {\pi \paren {2 r + 1}^2} \cos \paren {2 r + 1} x + \sum_{n \mathop = 1}^\infty \paren {-\frac {2 - \paren {-1}^n} n} \sin n x | c = substituting for $a_0$, $a_n$ and $b_n$ from above }} {{eqn | r = -\frac \pi 4 + \frac 2 \pi \sum_{r \mathop = 0}^\infty \frac {\cos \paren {2 r + 1} x} {\paren {2 r + 1}^2} - \sum_{n \mathop = 1}^\infty \frac {2 - \paren {-1}^n \sin n x} n | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\sec a x} x \ \mathrm d x = \ln \size x + \frac {\paren {a x}^2} 4 + \frac {5 \paren {a x}^4} {96} + \frac {61 \paren {a x}^6} {4320} + \cdots + \frac {\paren {-1}^n E_n \paren {a x}^{2 n} } {\paren {2 n} \paren {2 n}!} + \cdots + C$ where $E_n$ is the $n$th [[Definition:Euler Numbers|Euler number]].	0
{{begin-eqn}} {{eqn | l = \tan i | r = i \tanh 1 | c = [[Hyperbolic Tangent in terms of Tangent]] }} {{eqn | r = \paren {\frac {e^1 - e^{-1} } {e^1 + e^{-1} } } i | c = {{Defof|Hyperbolic Tangent}} }} {{eqn | r = \paren {\frac {e^2 - 1} {e^2 + 1} } i | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $e$ }} {{end-eqn}} {{qed}}	0
By [[L'Hôpital's Rule]]: {{begin-eqn}} {{eqn |l = \lim_{x \mathop \to 0} \frac {\tan x} x |r = \lim_{x \mathop \to 0} \frac {\sec^2 x} 1 |c = [[Derivative of Tangent Function]] }} {{eqn |r = 1 |c = [[Secant of Zero]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \cos^2 x \rd x | r = \frac x 2 + \frac {\sin 2 x} 4 + C | c = [[Primitive of Square of Cosine of a x|Primitive of $\cos^2 x$]] }} {{eqn | ll= \leadsto | l = \int_0^{\frac \pi 2} \cos^2 x \rd x | r = \intlimits {\frac x 2 + \frac {\sin 2 x} 4} 0 {\frac \pi 2} | c = }} {{eqn | r = \paren {\frac \pi 4 + \frac {\sin \pi} 4} - \paren {\frac 0 2 + \frac {\sin 0} 4} | c = }} {{eqn | r = \frac \pi 4 | c = [[Sine of Multiple of Pi]] and simplifying }} {{end-eqn}} {{qed}}	0
:$\csc 240 \degrees = \csc \dfrac {4 \pi} 3 = -\dfrac {2 \sqrt 3} 3$	0
For $n \in \Z_{>0}$:	0
{{begin-eqn}} {{eqn | l = \map \cot {x + \frac {3 \pi} 2} | r = \frac {\map \cos {x + \frac {3 \pi} 2} } {\map \sin {x + \frac {3 \pi} 2} } | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac {\sin x} {-\cos x} | c = [[Cosine of Angle plus Right Angle]] and [[Sine of Angle plus Right Angle]] }} {{eqn | r = -\tan x | c = [[Tangent is Sine divided by Cosine]] }} {{end-eqn}} {{qed}}	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Then: :$i \cot z = -\coth \paren {i z}$ where: : $\cot$ denotes the [[Definition:Complex Cotangent Function|cotangent function]] : $\coth$ denotes the [[Definition:Hyperbolic Cotangent|hyperbolic cotangent]] : $i$ is the [[Definition:Imaginary Unit|imaginary unit]]: $i^2 = -1$.	0
{{begin-eqn}} {{eqn | l = \csc 90^\circ | r = \frac 1 {\sin 90^\circ} | c = [[Cosecant is Reciprocal of Sine]] }} {{eqn | r = \frac 1 1 | c = [[Sine of 90 Degrees]] }} {{eqn | r = 1 | c = }} {{end-eqn}} {{qed}}	0
With a view to expressing the [[Definition:Primitive|primitive]] in the form: :$\displaystyle \int u \frac {\mathrm d v}{\mathrm d x} \ \mathrm d x = u v - \int v \frac {\mathrm d u}{\mathrm d x} \ \mathrm d x$ let: {{begin-eqn}} {{eqn | l = u | r = \operatorname{arccot} \frac x a | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d u} {\mathrm d x} | r = \frac {-a} {x^2 + a^2} | c = [[Derivative of Arccotangent of x over a|Derivative of $\operatorname{arccot} \dfrac x a$]] }} {{end-eqn}} and let: {{begin-eqn}} {{eqn | l = \frac {\mathrm d v} {\mathrm d x} | r = x^2 | c = }} {{eqn | ll= \implies | l = v | r = \frac {x^3} 3 | c = [[Primitive of Power]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int x^2 \operatorname{arccot} \frac x a \ \mathrm d x | r = \frac {x^3} 3 \operatorname{arccot} \frac x a - \int \frac {x^3} 3 \left({\frac {-a} {x^2 + a^2} }\right) \ \mathrm d x + C | c = [[Integration by Parts]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arccot} \frac x a + \frac a 3 \int \frac {x^3 \ \mathrm d x} {x^2 + a^2} + C | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arccot} \frac x a + \frac a 3 \left({\frac {x^2} 2 - \frac {a^2} 2 \ln \left ({x^2 + a^2}\right) }\right) + C | c = [[Primitive of x cubed over x squared plus a squared|Primitive of $\dfrac {x^3} {x^2 + a^2}$]] }} {{eqn | r = \frac {x^3} 3 \operatorname{arccot} \frac x a + \frac {a x^2} 6 - \frac {a^3} 6 \ln \left({x^2 + a^2}\right) + C | c = simplifying }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int x^m \arcsin \frac x a \rd x = \frac {x^{m + 1} } {m + 1} \arcsin \frac x a - \frac 1 {m + 1} \int \frac {x^{m + 1} \rd x} {\sqrt{a^2 - x^2} } + C$	0
