text	label
=== Proof of $(1)$ === Let $\lambda \in \Bbb F$, $h \in H, k \in K$. Then: {{begin-eqn}} {{eqn|l = \left\langle{\left({\lambda A}\right)h, k}\right\rangle_K |r = \lambda \left\langle{Ah, k}\right\rangle_K |c = Property $(2)$ of [[Definition:Inner Product|inner products]] }} {{eqn|r = \lambda \left\langle{h, A^*k}\right\rangle_H |c = Definition of [[Definition:Adjoint Linear Transformation|adjoint]] }} {{eqn|r = \left\langle{h, \left({\overline \lambda A^*}\right) k}\right\rangle_H |c = Properties $(1), (2)$ of [[Definition:Inner Product|inner products]] }} {{end-eqn}} Thus, by [[Existence and Uniqueness of Adjoint]], $\left({\lambda A}\right)^* = \overline \lambda A^*$. {{qed|lemma}}	1
If $n$ is [[Definition:Prime Number|prime]], the result is immediate. Let $n$ be [[Definition:Composite Number|composite]]. Then by [[Composite Number has Two Divisors Less Than It]]: :$\exists r, s \in \Z: n = r s, 1 < r < n, 1 < s < n$ This being the case, the set $S_1 = \set {d: d \divides n, 1 < d < n}$ is [[Definition:Non-Empty Set|nonempty]], and [[Definition:Bounded Below Set|bounded below]] by $1$. By [[Set of Integers Bounded Below by Integer has Smallest Element]], $S_1$ has a [[Definition:Smallest Element|smallest element]], which we will call $p_1$. {{AimForCont}} $p_1$ is [[Definition:Composite Number|composite]]. By [[Composite Number has Two Divisors Less Than It]], there exist $a, b$ such that $a, b \divides p_1$ and $1 < a < p_1, 1 < b < p_1$. But by [[Divisor Relation on Positive Integers is Partial Ordering]], it follows that $a, b \divides n$ and hence $a, b \in S$. This [[Definition:Contradiction|contradicts]] the assertion that $p_1$ is the [[Definition:Smallest Element|smallest element]] of $S_1$. Thus, $p_1$ is necessarily [[Definition:Prime Number|prime]]. We may now write $n = p_1 n_1$, where $n > n_1 > 1$. If $n_1$ is [[Definition:Prime Number|prime]], the proof is complete. Otherwise, the set $S_2 = \set {d: d \divides n_1, 1 < d < n_1}$ is [[Definition:Non-Empty Set|nonempty]], and [[Definition:Bounded Below Set|bounded below]] by $1$. By the above argument, the [[Definition:Smallest Element|smallest element]] $p_2$ of $S_2$ is [[Definition:Prime Number|prime]]. Thus we may write $n_1 = p_2 n_2$, where $1 < n_2 < n_1$. This gives us $n = p_1 p_2 n_2$. If $n_2$ is [[Definition:Prime Number|prime]], we are done. Otherwise, we continue this process. Since $n > n_1 > n_2 > \cdots > 1$ is a [[Definition:Strictly Decreasing Sequence|(strictly) decreasing sequence]] of [[Definition:Positive Integer|positive integers]], there must be a [[Definition:Finite Set|finite number]] of $n_i$'s. That is, we will arrive at some [[Definition:Prime Number|prime number]] $n_{k - 1}$, which we will call $p_k$. This results in the [[Definition:Prime Decomposition|prime decomposition]] $n = p_1 p_2 \cdots p_k$. {{qed}}	1
We refer to the elements of: :$\begin {vmatrix} 1 & 0 & \cdots & 0 \\ b_{2 1} & b_{2 2} & \cdots & b_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ b_{n 1} & b_{n 2} & \cdots & b_{n n} \end {vmatrix}$ as $\begin {vmatrix} b_{i j} \end {vmatrix}$. Thus $b_{1 1} = 1, b_{1 2} = 0, \ldots, b_{1 n} = 0$. Then from the definition of [[Definition:Determinant of Matrix|determinant]]: {{begin-eqn}} {{eqn | l = D | r = \sum_\lambda \paren {\map \sgn \lambda \prod_{k \mathop = 1}^n b_{k \map \lambda k} } | c = }} {{eqn | r = \sum_{\lambda} \map \sgn \lambda b_{1 \map \lambda 1} b_{2 \map \lambda 2} \cdots b_{n \map \lambda n} | c = }} {{end-eqn}} Now we note: {{begin-eqn}} {{eqn | l = \map \lambda 1 = 1 | o = \implies | r = b_{1 \map \lambda 1} b_{2 \map \lambda 2} \cdots b_{n \map \lambda n} = 1 | c = }} {{eqn | l = \map \lambda 1 \ne 1 | o = \implies | r = b_{1 \map \lambda 1} b_{2 \map \lambda 2} \cdots b_{n \map \lambda n} = 0 | c = }} {{end-eqn}} So only those [[Definition:Permutation on n Letters|permutations]] on $\N^*_n$ such that $\map \lambda 1 = 1$ contribute towards the final [[Definition:Summation|summation]]. Thus we have: :$\displaystyle D = \sum_\mu \map \sgn \mu b_{2 \map \mu 2} \cdots b_{n \map \mu n}$ where $\mu$ is the collection of all [[Definition:Permutation on n Letters|permutations]] on $\N^*_n$ which [[Definition:Fixed Element of Permutation|fix]] $1$. Hence the result. {{Qed}}	1
:$T$ is [[Definition:Separable Space|separable]] {{iff}} :there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $T$ which is [[Definition:Everywhere Dense|dense]] by definition of [[Definition:Separable Space|separable space]] {{iff}} :there exists a [[Definition:Subset|subset]] $A$ of $T$ such that $A$ is [[Definition:Everywhere Dense|dense]] and exists an [[Definition:Injection|injection]] $A \to \N$ by definition of [[Definition:Countable Set|countable set]] {{iff}} :there exists a [[Definition:Subset|subset]] $A$ of $T$ such that $A$ is [[Definition:Everywhere Dense|dense]] and $\left\vert{A}\right\vert \leq \left\vert{\N}\right\vert$ by [[Injection iff Cardinal Inequality]] {{iff}} :there exists a [[Definition:Subset|subset]] $A$ of $T$ such that $A$ is [[Definition:Everywhere Dense|dense]] and $\left\vert{A}\right\vert \leq \aleph_0$ by [[Aleph Zero equals Cardinality of Naturals]] {{iff}} :$d \left({T}\right) \leq \aleph_0$ by definition of [[Definition:Density of Topological Space|density]] where $\left\vert{A}\right\vert$ denotes the [[Definition:Cardinality|cardinality]] of $A$. {{qed}}	1
Let: {{begin-eqn}} {{eqn | l = \alpha_{1 1} x_1 + \alpha_{1 2} x_2 + \ldots + \alpha_{1 n} x_n | r = 0 | c = }} {{eqn | l = \alpha_{2 1} x_1 + \alpha_{2 2} x_2 + \ldots + \alpha_{2 n} x_n | r = 0 | c = }} {{eqn | o = \vdots }} {{eqn | l = \alpha_{m 1} x_1 + \alpha_{m 2} x_2 + \ldots + \alpha_{m n} x_n | r = 0 | c = }} {{end-eqn}} be the system of equations to be solved. Suppose the [[Definition:Elementary Row Operation|elementary row operation]] of multiplying one row $i$ by a non-zero scalar $\lambda$ is performed. Recall, the $i$th row of the matrix [[Definition:Matrix Representation of Simultaneous Linear Equations|represents]] the $i$th equation of the system to be solved. Then this is [[Definition:Logically Equivalent|logically equivalent]] to multiplying the $i$th equation on both sides by the scalar $\lambda$: {{begin-eqn}} {{eqn | l = \alpha_{i 1} x_1 + \alpha_{i 2} x_2 + \ldots + \alpha_{i n} x_n | r = 0 }} {{eqn | ll= \to | l = \lambda \alpha_{i 1} x_1 + \lambda \alpha_{i 2} x_2 + \ldots + \lambda \alpha_{i n} x_n | r = 0 | c = $r_i \to \lambda r_i$ }} {{end-eqn}} which clearly has the same solutions as the original equation. Suppose the [[Definition:Elementary Row Operation|elementary row operation]] of adding a scalar multiple of row $i$ to another row $j$ is performed. Recall that the $i$th and $j$th row of the matrix [[Definition:Matrix Representation of Simultaneous Linear Equations|represent]] the $i$th and $j$th equation in the system to be solved. {{explain|Woolly. The matrix (by which I presume you mean $\mathbf A$) contains the coefficients and so no part of it "represents" an equation. The act of multiplying $\mathbf x$ by it to obtain $\mathbf b$ represents the equation.}} Thus this is [[Definition:Logically Equivalent|logically equivalent]] to manipulating the $i$th and $j$th equations as such: {{begin-eqn}} {{eqn | l = \alpha_{i 1} x_1 + \alpha_{i 2} x_2 + \ldots + \alpha_{i n} x_n | r = 0 | c = }} {{eqn | l = \alpha_{j 1} x_1 + \alpha_{j 2} x_2 + \ldots + \alpha_{j n} x_n | r = 0 | c = }} {{eqn | ll= \to | l = \alpha_{j 1} x_1 + \alpha_{j 2} x_2 + \ldots + \alpha_{j n} x_n + \lambda \paren {\alpha_{i 1} x_1 + \alpha_{i 2} x_2 + \ldots + \alpha_{i n} x_n} | r = 0 | c = $r_i \to r_i + \lambda r_j$ }} {{end-eqn}} As both sides of equation $i$ are equal to each other, this operation is simply performing the same act on both sides of equation $j$. This clearly will have no effect on the solution set of the system of equations. Suppose the [[Definition:Elementary Row Operation|elementary row operation]] of interchanging row $i$ and row $j$ is performed. Recall that the $i$th and $j$th row of the matrix [[Definition:Matrix Representation of Simultaneous Linear Equations|represent]] the $i$th and $j$th equation in the system to be solved. Then, interchanging row $i$ and row $j$ is [[Definition:Logically Equivalent|logically equivalent]] to switching the $i$th equation and the $j$th equation of the system to be solved. But clearly the system containing the following two equations: {{begin-eqn}} {{eqn | l = \alpha_{i 1} x_1 + \alpha_{i 2} x_2 + \cdots + \alpha_{i n} x_n | r = 0 | c = }} {{eqn | l = \alpha_{j 1} x_1 + \alpha_{j 2} x_2 + \cdots + \alpha_{j n} x_n | r = 0 | c = }} {{end-eqn}} has the same [[Definition:Solution Set to System of Simultaneous Equations|solution set]] as a system instead containing the following two equations: {{begin-eqn}} {{eqn | l = \alpha_{j 1} x_1 + \alpha_{j 2} x_2 + \cdots + \alpha_{j n} x_n | r = 0 | c = }} {{eqn | l = \alpha_{i 1} x_1 + \alpha_{i 2} x_2 + \cdots + \alpha_{i n} x_n | r = 0 | c = $r_i \leftrightarrow r_j$ }} {{end-eqn}} Hence the result, by the definition of [[Definition:Row Equivalence|row equivalence]]. {{qed}} {{proofread}} [[Category:Linear Algebra]] ix7loxpm1evprncf15v9560q41jslzx	1
Let $C$ be the [[Definition:Circle|circle]] embedded in the [[Definition:Complex Plane|complex plane]] given by the equation: :$z = R e^{i \theta}$ Then: :$\cmod {e^{i z} } = e^{-R \sin \theta}$	1
Since $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $l \ne 0$, by [[Sequence Converges to Within Half Limit/Normed Division Ring|Sequence Converges to Within Half Limit]] then: :$\exists k \in \N: \forall n \in \N: \dfrac {\norm l} 2 < \norm {x_{k + n} }$ By {{NormAxiom|1}}: :$\forall n \in \N : x_{k + n} \ne 0$ Let $\sequence {y_n}$ be the [[Definition:Subsequence|subsequence]] of $\sequence {x_n}$ where $y_n = x_{k + n}$. By [[Limit of Subsequence equals Limit of Sequence/Normed Division Ring|Limit of Subsequence equals Limit of Sequence]], $\sequence {y_n}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} y_n = l$ It also follows that: :$\forall n \in \N : y_n \ne 0$ So $\sequence { {y_n}^{-1} }$ is [[Definition:Well-Defined|well-defined]] and: :$\sequence { {y_n}^{-1} } = \sequence { {x_{k + n} }^{-1} }$ === [[Combination Theorem for Sequences/Normed Division Ring/Inverse Rule/Lemma|Lemma]] === {{:Combination Theorem for Sequences/Normed Division Ring/Inverse Rule/Lemma}}{{qed}}	1
From [[Ring of Square Matrices over Ring with Unity]] we have that $\struct {\map {\MM_R} n, +, \times}$ is a [[Definition:Ring with Unity|ring with unity]]. However, [[Matrix Multiplication is not Commutative]]. Hence $\struct {\map {\MM_R} n, +, \times}$ is not a [[Definition:Commutative Ring|commutative ring]] for $n \ge 2$. For $n = 1$ we have that: {{begin-eqn}} {{eqn | lo= \forall \mathbf A, \mathbf B \in \map {\MM_R} 1: | l = \mathbf A \mathbf B | r = a_{11} b_{11} | c = where $\mathbf A = \begin {pmatrix} a_11 \end {pmatrix}$ and $\mathbf B = \begin {pmatrix} b_11 \end {pmatrix}$ }} {{eqn | r = b_{11} a_{11} | c = as $R$ is a [[Definition:Commutative Ring|commutative ring]] }} {{eqn | r = \mathbf {B A} | c = }} {{end-eqn}} Thus, for $n = 1$, $\struct {\map {\MM_R} n, +, \times}$ ''is'' a [[Definition:Commutative Ring|commutative ring]]. {{qed}}	1
From the [[Definition:Vector Space Axioms|vector space axioms]] we have that $\exists \mathbf 0 \in \mathbf V$. It remains to be proved that $\map T {\mathbf 0} = \mathbf 0'$: {{begin-eqn}} {{eqn | l = \map T {\mathbf 0} | r = \map T {\mathbf 0 + \mathbf 0} }} {{eqn | r = \map T {\mathbf 0} + \map T {\mathbf 0} | c = {{Defof|Linear Transformation on Vector Space}} }} {{eqn | ll= \leadsto | l = \mathbf 0' | r = \map T {\mathbf 0} | c = subtracting $\map T {\mathbf 0}$ from both sides }} {{end-eqn}} {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A, B \subseteq H$ be [[Definition:Subset|subsets]] of $H$ such that $B \subseteq A^\perp$, where $A^\perp$ is the [[Definition:Orthocomplement|orthocomplement]] of $A$. Then $A \subseteq B^\perp$.	1
First it is shown that $(2)$ is equivalent to $(3)$. Then, equivalence to $(1)$ is shown. === $(2)$ iff $(3)$ === Suppose $P Q = Q$. Then $P = P + Q - P Q$, and by [[Product of Projections]], necessarily $P Q = Q P$. That is, $Q P = P Q = Q$. The converse is obtained by swapping the rôles of $P$ and $Q$ in [[Product of Projections]]. {{qed|lemma}} === $(2), (3)$ imply $(1)$ === Suppose $P Q = Q P = Q$. Then as $P, Q$ are [[Definition:Projection (Hilbert Spaces)|projections]]: :$\paren {P - Q}^2 = P^2 + Q^2 - P Q - Q P = P - Q$ That is, $P - Q$ is [[Definition:Idempotent Operator|idempotent]]. From [[Adjoining is Linear]], $\paren {P - Q}^* = P^* - Q^* = P - Q$. An application of [[Characterization of Projections]], statement $(4)$ shows that $P - Q$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. {{qed|lemma}} === $(1)$ implies $(2)$ === Let $P - Q$ be a [[Definition:Projection (Hilbert Spaces)|projection]]. Then by [[Characterization of Projections]], statement $(6)$, one has: :$\innerprod {P h} h_H - \innerprod {Q h}, h_H \ge 0$ Applying this statement on $P, Q$ also, one obtains that: :$P h = 0 \implies Q h = 0$ that is: :$\ker P \subseteq \ker Q$ [[Orthocomplement Reverses Subset]] and that $P, Q$ are [[Definition:Projection (Hilbert Spaces)|projections]] combine to state $\Rng Q \subseteq \Rng P$. So for every $h \in H$, there is a $p \in H$ with $Q h = P p$. It follows that: :$P Q h = P P p = P p = Q h$ Hence $PQ = Q$. {{qed}}	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $1 \le r < s \le n$. Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] that exchanging [[Definition:Row of Matrix|rows]] $r$ and $s$. Let $\mathbf B = \map e {\mathbf A}$. Let $\mathbf E$ be the [[Definition:Elementary Row Matrix|elementary row matrix]] corresponding to $e$. From [[Elementary Row Operations as Matrix Multiplications]]: :$\mathbf B = \mathbf E \mathbf A$ From [[Determinant of Elementary Row Matrix/Exchange Rows|Determinant of Elementary Row Matrix: Exchange Rows]]: :$\map \det {\mathbf E} = -1$ Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf B} | r = \map \det {\mathbf E \mathbf A} | c = [[Determinant of Matrix Product]] }} {{eqn | r = -\map \det {\mathbf A} | c = as $\map \det {\mathbf E} = -1$ }} {{end-eqn}} Hence the result. {{qed}}	1
Let $\struct {R, \norm {\,\cdot\,} }$ be a [[Definition:Normed Division Ring |normed division ring]]. Let $\sequence {x_n}$, $\sequence {y_n}$ be [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequences in $R$]]. Let $a, b \in R$. The following results hold:	1
By [[Characterisation of Non-Archimedean Division Ring Norms]] then: :$\exists n \in \N$ such that $\norm n > 1$ Let $n_0 = \min \set {n \in \N : \norm n > 1}$ By [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity]] then: :$n_0 > 1$ Let $\alpha = \dfrac {\log \norm {n_0} } {\log n_0}$ Since $n_0, \norm n_0 > 1$ then: :$\alpha > 0$ ==== [[Ostrowski's Theorem/Archimedean Norm/Lemma 1.1|Lemma 1.1]] ==== {{:Ostrowski's Theorem/Archimedean Norm/Lemma 1.1}}{{qed|lemma}} ==== [[Ostrowski's Theorem/Archimedean Norm/Lemma 1.2|Lemma 1.2]] ==== {{:Ostrowski's Theorem/Archimedean Norm/Lemma 1.2}}{{qed|lemma}} Hence: :$\forall n \in \N: \norm n = n^\alpha = \size n^\alpha$ By [[Equivalent Norms on Rational Numbers|Equivalent Norms on Rational Numbers]] then $\norm {\, \cdot \,}$ is [[Definition:Equivalent Division Ring Norms|equivalent]] to the [[Definition:Absolute Value|absolute value]] $\size {\, \cdot \,}$.	1
Let $\mathbf E$ be an [[Definition:Elementary Row Matrix|elementary row matrix]]. The [[Definition:Determinant of Matrix|determinant]] of $\mathbf E$ is as follows:	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]]. Let $T \in B_0 \left({H, K}\right)$ be a [[Definition:Compact Linear Transformation|compact linear transformation]]. Then $T$ is also a [[Definition:Bounded Linear Transformation|bounded linear transformation]]. That is, $B_0 \left({H, K}\right) \subseteq B \left({H, K}\right)$.	1
Proving each of the [[Definition:Norm on Division Ring|norm axioms]] one by one: === Proving $(N1) : \forall x \in R: \norm x = 0 \iff x = 0_R$ === This follows directly from the definition of the [[Definition:Trivial Norm on Division Ring|trivial norm]]. {{qed|lemma}} === Proving $(N2) : \forall x, y \in R: \norm {x \circ y} = \norm x \times \norm y$ === If $x = 0_R$: {{begin-eqn}} {{eqn | l = \norm {x \circ y} | r = \norm {0_R \circ y} | c = by substitution }} {{eqn | r = \norm {0_R} | c = [[Ring Product with Zero]] }} {{eqn | r = 0 | c = {{Defof|Trivial Norm on Division Ring}} }} {{eqn | r = 0 \times \norm y | c = }} {{eqn | r = \norm x \times \norm y | c = since $x = 0_R$ }} {{end-eqn}} The reasoning is similar if $y = 0_R$. If $x,y \ne 0_R$, then $x \circ y \ne 0_R$ by alternative definition $(3)$ of [[Definition:Division Ring|division rings]]. We get: {{begin-eqn}} {{eqn | l = \norm {x \circ y} | r = 1 | c = since $x \circ y \ne 0_R$ }} {{eqn | r = 1 \times 1 | c = }} {{eqn | r = \norm x \times \norm y | c = since $x, y \ne 0_R$ }} {{end-eqn}} {{qed|lemma}} === Proving $(N3) : \forall x, y \in R: \norm {x + y} \le \norm x + \norm y$ === If $x = 0_R$: {{begin-eqn}} {{eqn | l = \norm {x + y} | r = \norm {0_R + y} | c = }} {{eqn | r = \norm y | c = }} {{eqn | r = 0 + \norm y | c = }} {{eqn | r = \norm x + \norm y | c = since $x = 0_R$ }} {{end-eqn}} The reasoning is similar if $y = 0_R$. If $x, y \ne 0_R$: {{begin-eqn}} {{eqn | l = \norm {x + y} | o = \le | r = 1 | c = }} {{eqn | o = < | r = 1 + 1 | c = }} {{eqn | r = \norm x + \norm y | c = since $x, y \ne 0_R$ }} {{end-eqn}} {{qed}} [[Category:Division Rings]] [[Category:Trivial Norms]] maljxp1yu5tbbkzkk6jv4dfoj081df2	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A3}} that the [[Definition:Identity Element|identity element]] of [[Definition:Ring Addition|ring addition]] is the [[Definition:Ring Zero|ring zero]] $0_R$. The result then follows directly from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
{{qed}} [[Category:Completion of Normed Division Ring]] 6vhsk55vh13gm7nei3hhwfm8rx013m3	1
First we demonstrate the operation has the specified property: {{begin-eqn}} {{eqn | l = \frac a b + \frac c d | r = a \circ b^{-1} + c \circ d^{-1} | c = {{Defof|Division Product}} }} {{eqn | r = a \circ b^{-1} \circ d \circ d^{-1} + c \circ d^{-1} \circ b \circ b^{-1} | c = {{Defof|Inverse Element}} and {{Defof|Identity Element}} under $\circ$ }} {{eqn | r = \paren {a \circ d} \circ \paren {d^{-1} \circ b^{-1} } + \paren {b \circ c} \circ \paren {d^{-1} \circ b^{-1} } | c = {{Defof|Commutative Operation}} }} {{eqn | r = \paren {a \circ d + b \circ c} \circ \paren {b \circ d}^{-1} | c = {{Defof|Distributive Operation}} $\circ$ over $+$ }} {{eqn | r = \frac {a \circ d + b \circ c} {b \circ d} | c = {{Defof|Division Product}} }} {{end-eqn}} Notice that this works only if $\struct {R, +, \circ}$ is [[Definition:Commutative Ring|commutative]]. Now we show that $+$ is [[Definition:Well-Defined Operation|well-defined]]. Let $a, c, a', c' \in D, b, d, b', d' \in D^*$ such that $\dfrac a b = \dfrac {a'} {b'}$ and $\dfrac c d = \dfrac {c'} {d'}$. Then: {{begin-eqn}} {{eqn | l = a \circ b' | r = \paren {a \circ b^{-1} } \circ b \circ b' | c = }} {{eqn | r = \paren {a' \circ \paren {b'}^{-1} } \circ b \circ b' | c = as $a / b = a' / b'$ }} {{eqn | r = a' \circ b | c = }} {{end-eqn}} Similarly, $c \circ d' = c' \circ d$. Hence: {{begin-eqn}} {{eqn | l = \paren {\paren {a' \circ d'} + \paren {c' \circ b'} } \circ b \circ d | r = a' \circ b \circ d' \circ d + c' \circ d \circ b' \circ b | c = }} {{eqn | r = a \circ b' \circ d' \circ d + c \circ d' \circ b' \circ b | c = }} {{eqn | r = \paren {\paren {a \circ d} + \paren {c \circ b} } \circ b' \circ d' | c = }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = \frac {\paren {a' \circ d'} + \paren {c' \circ b'} } {b' \circ d'} | r = \paren {\paren {\paren {a' \circ d'} + \paren {c' \circ b'} } \circ b \circ d} \circ \paren {b^{-1} \circ d^{-1} \circ \paren {b'}^{-1} \circ \paren {d'}^{-1} } | c = }} {{eqn | r = \paren {\paren {\paren {a \circ d} + \paren {c \circ b} } \circ b' \circ d'} \circ \paren {b^{-1} \circ d^{-1} \circ \paren {b'}^{-1} \circ \paren {d'}^{-1} } | c = }} {{eqn | r = \paren {\paren {a \circ d} + \paren {c \circ b} } \circ b^{-1} \circ d^{-1} | c = }} {{eqn | r = \frac {\paren {a \circ d} + \paren {c \circ b} } {b \circ d} | c = }} {{end-eqn}} showing that $+$ is indeed [[Definition:Well-Defined Operation|well-defined]]. {{Qed}}	1
Let $T = \struct {S, \tau}$ be a [[Definition:Finite Complement Topology|finite complement topology]] on an [[Definition:Infinite Set|infinite]] set $S$. Then $T$ is a [[Definition:Separable Space|separable space]].	1
Since $x, p > 0$ then $a > 0$. {{AimForCont}} for some $c \in \Z:c^k = a$. Since $c^k \in \Z$, by [[Nth Root of Integer is Integer or Irrational]] then: :$c \in \Z$ Suppose $k$ is [[Definition:Odd Integer|odd]]. Since $a > 0$, by [[Odd Power Function is Strictly Increasing]] then $c > 0$ Hence $a = \size c^k$ On the other hand, suppose $k$ is [[Definition:Even Integer|even]], that is $k = 2l$ for some $l \in Z_{> 0}$. Then: {{begin-eqn}} {{eqn | l = a | r = c^{2l} }} {{eqn | r = \paren {c^2}^l }} {{eqn | r = \paren {\size c^2}^l | c = [[Equivalence of Definitions of Absolute Value Function]] }} {{eqn | r = \size c^{2l} }} {{eqn | r = \size c^k }} {{end-eqn}} In either case $\size c \in \Z_{> 0}$ and $\size c^k = a$ Let $d = \size c$ By the definition of $a$ it follows that $d^k = x^k + p$ Hence: {{begin-eqn}} {{eqn | l = p | r = d^k - x^k }} {{eqn | r = \paren {d - x} \paren {d^{k - 1} + d^{k - 2} x + d^{k - 3} x^2 + \dotsb + c x^{k - 2} + x^{k - 1} } | c = [[Difference of Two Powers]] }} {{end-eqn}} Let $y = d^{k - 1} + d^{k - 2} x + d^{k - 3} x^2 + \dotsb + d x^{k - 2} + x^{k - 1}$ Since $d, x \in \Z_{> 0}$ then $d - x \in \Z$ and $y \in \Z$ So $d - x$ and $y$ are [[Definition:Factor|factors]] of $p$ The [[Definition:Factor|factors]] of $p$ by definition are: :$\pm 1$ and $\pm p$ Since $d, x \in \Z_{> 0}$ then: {{begin-eqn}} {{eqn | l = d^{k - 1} + d^{k - 2} x + d^{k - 3} x^2 + \dotsb + d x^{k - 2} + x^{k - 1} | o = \ge | r = d^{k - 1} + x^{k - 1} }} {{eqn | o = \ge | r = d + x }} {{eqn | o = \ge | r = 1 + 1 }} {{eqn | o = \ge | r = 2 }} {{end-eqn}} Hence $y = p$ Then: {{begin-eqn}} {{eqn | l = p | r = d^{k - 1} + d^{k - 2} x + d^{k - 3} x^2 + \dotsb + d x^{k - 2} + x^{k - 1} }} {{eqn | o = \ge | r = d^{k - 1} + x^{k - 1} }} {{eqn | o = \ge | r = d + x }} {{end-eqn}} It also follows that $d - x = 1$, that is, $d = x + 1$ Then {{begin-eqn}} {{eqn | l = d + x | r = \paren {x + 1} + x }} {{eqn | r = 2x + 1 }} {{eqn | o = > | r = 2x }} {{eqn | o = > | r = p + 1 }} {{eqn | o = > | r = p }} {{end-eqn}} This [[Definition:Contradiction|contradicts]] the previous conclusion that $p \ge d + x$ So: :$\nexists \,c \in \Z : c^k = a$ {{qed}} [[Category:P-adic Norm not Complete on Rational Numbers]] phaadqtr1ve7gbf201hybvqikj8ieqs	1
Let $a, b, c, d \in \R$ be [[Definition:Real Number|real numbers]]. Let $\theta_{a, b}: \R \to \R$ be the [[Definition:Real Function|real function]] defined as: :$\forall x \in \R: \map {\theta_{a, b} } x = a x + b$ Let $\theta_{c, d} \circ \theta_{a, b}$ denote the [[Definition:Composition of Mappings|composition]] of $\theta_{c, d}$ with $\theta_{a, b}$. Then: :$\theta_{c, d} \circ \theta_{a, b} = \theta_{a, b} \circ \theta_{c, d}$ {{iff}}: :$b c + d = a d + b$	1
This is a special case of [[Direct Product of Unitary Modules is Unitary Module]].	1
Let $n$ and $k$ be [[Definition:Natural Number|natural numbers]]. Let $\Omega \subset \R^{n + k}$ be [[Definition:Open Set|open]]. Let $f: \Omega \to \R^k$ be [[Definition:Continuous Function|continuous]]. Let the [[Definition:Partial Derivative|partial derivatives]] of $f$ with respect to $\R^k$ be [[Definition:Continuous Function|continuous]]. Let $\tuple {a, b} \in \Omega$, with $a\in \R^n$ and $b\in \R^k$. Let $\map f {a, b} = 0$. For $\tuple {x_0, y_0} \in \Omega$, let $D_2 \map f {x_0, y_0}$ denote the [[Definition:Total Derivative|total derivative]] of the function $y \mapsto \map f {x_0, y}$ at $y_0$. Let the [[Definition:Linear Mapping|linear map]] $D_2 \map f {a, b}$ be [[Definition:Invertible Linear Mapping|invertible]]. Then there exist [[Definition:Neighborhood|neighborhoods]] $U \subset \Omega$ of $a$ and $V \subset \R^k$ of $b$ such that there exists a unique [[Definition:Function|function]] $g: U \to V$ such that $\map f {x, \map g x} = 0$ for all $x \in U$. Moreover, $g$ is [[Definition:Continuous Mapping|continuous]].	1
Let $\struct {X, \norm {\,\cdot\,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Let $\sequence {x_n}$ be a [[Definition:Sequence|sequence]] in $\struct {X, \norm {\,\cdot\,} }$. Then $\sequence {x_n}$ can have at most one [[Definition:Limit of Sequence in Normed Vector Space|limit]].	1
By [[Bézout's Identity]] then: :$\exists n, m \in \Z : m a + n b = 1$ By [[Properties of Norm on Division Ring/Norm of Unity|Norm of Unity]] then: :$\norm {m a + n b} = 1$ By [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 5|Corollary 5 of Characterisation of Non-Archimedean Division Ring Norms]] then: :$\norm a, \norm b, \norm n, \norm m \le 1$ Let $\norm a \lt 1$. By [[Definition:Norm on Division Ring|Norm axiom $(\text N 2)$: Multiplicativity]]: :$\norm {m a} = \norm m \norm a \lt 1$ Hence: :$\norm {m a} < \norm {m a + n b}$ By [[Three Points in Ultrametric Space have Two Equal Distances/Corollary 4|Corollary 4]]: :$\norm {n b} = \norm {m a + n b} = 1$ By [[Definition:Norm on Division Ring|Norm axiom $(\text N 2)$: Multiplicativity]]: :$\norm n \norm b = 1$. Hence $\norm b = 1$. The result follows. {{qed}}	1
An [[Definition:Element of Matrix|element]] $a b_{ij}$ of $\mathbf {A B}$ is formed by [[Definition:Real Multiplication|multiplying]] each [[Definition:Element of Matrix|element]] of [[Definition:Row of Matrix|row]] $i$ of $\mathbf A$ by its corresponding [[Definition:Element of Matrix|element]] of [[Definition:Column of Matrix|column]] $j$ of $\mathbf B$. No more than $1$ [[Definition:Element of Matrix|element]] of [[Definition:Row of Matrix|row]] $i$ equals $1$, and the rest equal $0$. No more than $1$ [[Definition:Column of Matrix|column]] $k$ of $\mathbf B$ contains $1$ in its $i$th [[Definition:Element of Matrix|element]], and the rest contain $0$. So of all the [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $i$ of $\mathbf {A B}$, only $a b_{ik}$ is $1$, and the rest are $0$. By the same argument, each [[Definition:Row of Matrix|row]] of $\mathbf {A B}$ contains no more than one $1$, and all the rest of the [[Definition:Element of Matrix|elements]] are $0$. In $\mathbf B$, each [[Definition:Column of Matrix|column]] $j$ has no more than one [[Definition:Element of Matrix|element]] equal to $1$, and all are in a different [[Definition:Row of Matrix|row]] $k$. Thus each [[Definition:Row of Matrix|row]] contains its $1$, if it has one, in a different [[Definition:Column of Matrix|column]] from all the other [[Definition:Row of Matrix|rows]]. So there is no more than one $1$ in each [[Definition:Column of Matrix|column]] of $\mathbf {A B}$. Hence the result by definition of [[Definition:Rook Matrix|rook matrix]]. {{qed}}	1
Let $\mathbf x$ be a [[Definition:Quaternion|quaternion]] such that: : $\mathbf x = a \mathbf 1 + b \mathbf i + c \mathbf j + d \mathbf k$ When the [[Definition:Quaternion|quaternion]] basis is [[Quaternions Defined by Matrices|expressed in the form of matrices]]: :$\mathbf 1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \qquad \mathbf i = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} \qquad \mathbf j = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \qquad \mathbf k = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix}$ the general [[Definition:Quaternion|quaternion]] $\mathbf x$ has the form: :$\mathbf x = \begin{bmatrix} a + bi & c + di \\ -c + di & a - bi \end{bmatrix}$ or: :$\mathbf x = \begin{bmatrix} w & z \\ -\overline z & \overline w \end{bmatrix}$ where : :$w$ and $z$ are [[Definition:Complex Number|complex numbers]] :$\overline z$ is the [[Definition:Complex Conjugate|complex conjugate]] of $z$.	1
Let $A = \struct {A_F, \oplus}$ be a [[Definition:Normed Division Algebra|normed divison algebra]] as defined in the hypothesis. The fact that $A$ is a [[Definition:Unitary Algebra|unitary algebra]] is a consequence of the definition of [[Definition:Normed Division Algebra|normed divison algebra]]. From the definition of a [[Definition:Norm on Vector Space|norm]], we have that: :$\forall a \in A: \norm a = 0 \iff a = 0_A$ So, let $a, b \in A \setminus \set {0_A}$. We have: {{begin-eqn}} {{eqn | l = \norm a \norm b | o = \ne | r = 0 | c = }} {{eqn | ll= \leadsto | l = \norm {a \oplus b} | o = \ne | r = 0 | c = {{Defof|Normed Division Algebra}}: $\norm {a \oplus b} = \norm a \norm b$ }} {{eqn | ll= \leadsto | l = a \oplus b | o = \ne | r = 0_A | c = {{Defof|Norm on Vector Space}} }} {{end-eqn}} Thus for any arbitrary $a, b \ne 0_A$ we have shown that $a \oplus b \ne 0_A$. Thus $A$ is a [[Definition:Division Algebra|division algebra]]. {{qed|lemma}} Next: {{begin-eqn}} {{eqn | o = | r = \norm a \norm {1_A} | c = }} {{eqn | r = \norm {a \oplus 1_A} | c = {{Defof|Normed Division Algebra}}: $\norm {a \oplus b} = \norm a \norm b$ }} {{eqn | r = \norm a | c = {{Defof|Norm on Vector Space}} }} {{end-eqn}} demonstrating that $\norm {1_A} = 1$. {{qed}}	1
By the definition of a [[Definition:Complex Number|complex number]], let $z = a + i b$ where $a$ and $b$ are [[Definition:Real Number|real numbers]]. Then: {{begin-eqn}} {{eqn | l = z \overline z | r = \paren {a + i b} \paren {a - i b} | c = {{Defof|Complex Conjugate}} }} {{eqn | r = a^2 + a \cdot i b + a \cdot \paren {-i b} + i \cdot \paren {-i} \cdot b^2 | c = {{Defof|Complex Multiplication}} }} {{eqn | r = a^2 + i a b - i a b + b^2 | c = }} {{eqn | r = a^2 + b^2 | c = }} {{eqn | r = \paren {\sqrt {a^2 + b^2} }^2 | c = }} {{eqn | r = \cmod z^2 | c = {{Defof|Complex Modulus}} }} {{end-eqn}} As $a^2 + b^2$ is [[Definition:Wholly Real|wholly real]], the result follows. {{qed}}	1
First, suppose $k \perp m$. That is: :$\gcd \set {k, m} = 1$ Then, by [[Bézout's Identity]]: :$\exists u, v \in \Z: u k + v m = 1$ Thus: :$\eqclass {u k + v m} m = \eqclass {u k} m = \eqclass u m \eqclass k m = \eqclass 1 m$ Thus $\eqclass u m$ is an [[Definition:Multiplicative Inverse|inverse]] of $\eqclass k m$. Suppose that: :$\exists u \in \Z: \eqclass u m \eqclass k m = \eqclass {u k} m = 1$. Then: :$u k \equiv 1 \pmod m$ and: :$\exists v \in \Z: u k + v m = 1$ Thus from [[Bézout's Identity]]: :$k \perp m$ {{qed}}	1
=== [[Subset of Normed Vector Space is Everywhere Dense iff Closure is Normed Vector Space/Necessary Condition|Necessary Condition]] === {{:Subset of Normed Vector Space is Everywhere Dense iff Closure is Normed Vector Space/Necessary Condition}}{{qed|lemma}} === [[Subset of Normed Vector Space is Everywhere Dense iff Closure is Normed Vector Space/Sufficient Condition|Sufficient Condition]] === {{:Subset of Normed Vector Space is Everywhere Dense iff Closure is Normed Vector Space/Sufficient Condition}}{{qed}}	1
Consider this [[Simultaneous Linear Equations/Examples/Arbitrary System 1|system of simultaneous linear equations]]: {{begin-eqn}} {{eqn | n = 1 | l = x_1 - 2 x_2 + x_3 | r = 1 }} {{eqn | n = 2 | l = 2 x_1 - x_2 + x_3 | r = 2 }} {{end-eqn}} From its [[Simultaneous Linear Equations/Examples/Arbitrary System 2|evaluation]] it has the following [[Definition:Solution to System of Simultaneous Equations|solutions]]: {{begin-eqn}} {{eqn | l = x_1 | r = 1 - \dfrac t 3 }} {{eqn | l = x_2 | r = \dfrac t 3 }} {{eqn | l = x_3 | r = t }} {{end-eqn}} where $t$ is any [[Definition:Number|number]]. Hence the are as many [[Definition:Solution to System of Simultaneous Equations|solutions]] as the [[Definition:Cardinality|cardinality]] of the [[Definition:Domain of Variable|domain]] of $t$. {{qed}}	1
Let $V$ be a [[Definition:Vector Space|$K$-vector space]]. Let $M, N$ be [[Definition:Vector Subspace|linear subspaces]] of $V$. Then $L := M + N$ is also a [[Definition:Vector Subspace|linear subspace]] of $V$, where $+$ denotes [[Definition:Setwise Addition|setwise addition]].	1
Let $\mathbf A$ and $\mathbf B$ be [[Definition:Square Matrix|square matrices of order $n$]]. Let $\mathbf A \mathbf B$ denote the [[Definition:Matrix Product (Conventional)|matrix product]] of $\mathbf A$ and $\mathbf B$. Let $\mathbf I$ be the $n \times n$ [[Definition:Unit Matrix|unit matrix]]. Let $\mathbf A$ and $\mathbf B$ be [[Definition:Invertible Matrix|invertible]]. Then: :$\mathbf A \mathbf B$ is [[Definition:Invertible Matrix|invertible]] {{iff}} :both $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible]].	1
Let $\struct {R, +, \circ}$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $\struct {U_R, \circ}$ be the [[Definition:Group of Units of Ring|group of units]] of $\struct {R, +, \circ}$. Then: :$\displaystyle \forall x \in R: -\frac x z = \frac {-x} z = \frac x {-z}$ where $\dfrac x z$ is defined as $x \circ \paren {z^{-1} }$, that is the [[Definition:Division Product|division product]] of $x$ by $z$.	1
An '''inner product space''' is a [[Definition:Vector Space|vector space]] together with an associated [[Definition:Inner Product|inner product]].	1
It is not possible to extend the [[Definition:Complex Number|complex numbers]] to an [[Definition:Algebra over Field|algebra]] of $3$ [[Definition:Dimension (Linear Algebra)|dimensions]] with [[Definition:Real Number|real]] [[Definition:Scalar (Vector Space)|scalars]].	1
Let the [[Definition:Conjugation (Abstract Algebra)|conjugation]] operator on $A$ be $*$. Let $\left({a, b}\right), \left({c, d}\right) \in A'$. Let $A$ be a [[Definition:Real Star-Algebra|real algebra]]. {{begin-eqn}} {{eqn | l = \left({a, b}\right) \oplus' \left({c, d}\right) | r = \left({a \oplus c - d \oplus b^*, a^* \oplus d + c \oplus b}\right) | c = }} {{eqn | r = \left({a \oplus c - d \oplus b^*, a^* \oplus d + c \oplus b}\right) | c = }} {{eqn | r = \left({a \oplus c - d \oplus b, a \oplus d + c \oplus b}\right) | c = As $A$ is [[Definition:Real Algebra|real]], $b^* = b$ and $a^* = a$ }} {{eqn | r = \left({c \oplus a - b \oplus d, c \oplus b + a \oplus d}\right) | c = [[Real Star-Algebra is Commutative]], [[Real Addition is Commutative]] }} {{eqn | r = \left({c \oplus a - b \oplus d^*, c^* \oplus b + a \oplus d}\right) | c = As $A$ is real, $d = d^*$ and $c = c^*$ }} {{eqn | r = \left({c, d}\right) \oplus' \left({a, b}\right) | c = }} {{end-eqn}} So $A'$ is a [[Definition:Commutative Algebra|commutative algebra]]. {{qed|lemma}} Let $A'$ be a [[Definition:Commutative Algebra|commutative algebra]]. By picking apart the above equations, it is clear that for $A'$ to be a [[Definition:Commutative Algebra|commutative algebra]] it is necessary for $A$ to be both [[Definition:Real Star-Algebra|real]] and [[Definition:Commutative Algebra|commutative]]. Hence the result. {{qed}}	1
Follows directly from [[Dimension of Proper Subspace is Less Than its Superspace]]. {{qed}}	1
=== Necessary Condition === Let $G$ be a [[Definition:Basis of Vector Space|basis]] for $E$. From [[Cardinality of Basis of Vector Space]], $\card G = n$. {{qed}} === Sufficient Condition === Let $\card G = n$. From [[Sufficient Conditions for Basis of Finite Dimensional Vector Space]], $G$ is a [[Definition:Basis of Vector Space|basis]] for $E$. {{qed}}	1
The '''matrix of change of basis from $A$ to $B$''' is the [[Definition:Matrix|matrix]] whose columns are the [[Definition:Coordinate Vector|coordinate vectors]] of the elements of the '''new basis''' $\left \langle {b_n} \right \rangle$ relative to the '''original basis''' $\left \langle {a_n} \right \rangle$.	1
Let $\map {\MM_R} {m, n}$ denote the [[Definition:Matrix Space|$m \times n$ metric space]] over $R$. Let $I_m$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order]] $m$. Then: :$\forall \mathbf A \in \map {\MM_R} {m, n}: \mathbf I_m \mathbf A = \mathbf A$	1
Let $\norm {\,\cdot\,}_p$ be the [[Definition:P-adic Norm|$p$-adic norm]] on the [[Definition:Rational Numbers|rationals $\Q$]] for some [[Definition:Prime Number|prime]] $p$. Then: :the [[Definition:Valued Field|valued field]] $\struct {\Q, \norm {\,\cdot\,}_p}$ is not [[Definition:Complete Normed Division Ring|complete]]. That is, there exists a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]] in $\struct {\Q, \norm{\,\cdot\,}_p}$ which does not [[Definition:Convergent Sequence in Normed Division Ring|converge]] to a [[Definition:Limit of Sequence (Normed Division Ring)|limit]] in $\Q$.	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $V$ be a [[Definition:Finitely Generated Module|finitely generated]] [[Definition:Vector Space|vector space]] over $K$. Then $V$ has a [[Definition:Finite Set|finite]] [[Definition:Basis of Vector Space|basis]].	1
The '''Vandermonde determinant of order $n$''' is the [[Definition:Determinant of Matrix|determinant]] defined as follows: :$V_n = \begin {vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n - 2} & x_1^{n - 1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n - 2} & x_2^{n - 1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n - 2} & x_n^{n - 1} \end {vmatrix}$ Its value is given by: :$\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {x_j - x_i}$	1
: [[Matrix Multiplication is Closed]]. : [[Matrix Multiplication is Associative]]. : The [[Unit Matrix is Unity of Ring of Square Matrices]]. {{qed}}	1
Let $\mathbf v, \mathbf w \in \map {\mathrm N} {\mathbf A}$. By the definition of [[Definition:Null Space|null space]]: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf v | r = \mathbf 0 }} {{eqn | l = \mathbf A \mathbf w | r = \mathbf 0 }} {{end-eqn}} Next, observe that: {{begin-eqn}} {{eqn | l = \mathbf A \paren {\mathbf v + \mathbf w} | r = \mathbf A \mathbf v + \mathbf A \mathbf w | c = [[Matrix Multiplication Distributes over Matrix Addition]] }} {{eqn | r = \mathbf 0 + \mathbf 0 }} {{eqn | r = \mathbf 0 }} {{end-eqn}} The [[Definition:Order of Matrix|order]] is correct, [[Definition:By Hypothesis|by hypothesis]]. Hence the result, by the definition of [[Definition:Null Space|null space]]. {{qed}}	1
By the definition of $\alpha$: :$\norm {n_0} = n_0^\alpha$ By the definition of $n_0$: :$n_0^\alpha > 1$ Let $n \in \N$. By [[Basis Representation Theorem]] then $n$ can be written: :$n = a_0 + a_1 n_0 + a_2 n_0^2 + \cdots + a_s n_0^s$ where $0 \le a_i < n_0$ and $a_s \ne 0$ By [[Bounds for Integer Expressed in Base k]]: :$n_0^{s + 1} > n \ge n_0^s$ By [[Ostrowski's Theorem/Archimedean Norm/Lemma 1.1|Lemma 1.1]]: :$\norm {n_0^{s + 1} - n} \le \paren {n_0^{s + 1} - n}^\alpha$ Hence: {{begin-eqn}} {{eqn | l = \norm n | o = \ge | r = \norm {n_0^{s + 1} } - \norm {n_0^{s + 1} - n} | c = [[Reverse Triangle Inequality/Normed Division Ring|Reverse Triangle Inequality]] }} {{eqn | r = \norm {n_0}^{s + 1} - \norm {n_0^{s + 1} - n} | c = {{Norm Axiom|2}} }} {{eqn | r = n_0^{\alpha \paren {s + 1} } - \norm {n_0^{s + 1} - n} | c = as $\norm {n_0} = n_0^\alpha$ }} {{eqn | o = \ge | r = n_0^{\alpha \paren {s + 1} } - \paren {n_0^{s + 1} - n}^\alpha | c = as $\norm {n_0^{s + 1} - n} \le \paren {n_0^{s + 1} - n}^\alpha$ }} {{eqn | o = \ge | r = n_0^{\alpha \paren {s + 1} } - \paren {n_0^{s + 1} - n_0^s}^\alpha | c = as $n \ge n_0^s$ }} {{eqn | r = n_0^{\alpha \paren {s + 1} } \paren {1 - \paren {1 - \frac 1 {n_0} }^\alpha} | c = }} {{eqn | o = \ge | r = n^\alpha \paren {1 - \paren {1 - \frac 1 {n_0} }^\alpha} | c = as $n_0^{s+1}>n$ }} {{end-eqn}} Let $C' = \paren{1 - \paren{1 - \frac 1 {n_0} }^\alpha}$. Then: :$\norm n \ge C' n^\alpha$ As $n \in \N$ was arbitrary: :$\forall n \in N: \norm n \ge C' n^\alpha$ Let $n, N \in N$. Then: :$\norm {n^N} \ge C' \paren {n^N}^\alpha$ Now: {{begin-eqn}} {{eqn | l = \norm {n^N} \ge C' \paren {n^N}^\alpha | o = \leadsto | r = \norm n^N \ge C' \paren {n^N}^\alpha | c = {{NormAxiom|2}} }} {{eqn | o = \leadsto | r = \norm n^N \ge C' \paren {n^\alpha}^N | c = }} {{eqn | o = \leadsto | r = \norm n \ge \sqrt [N] {C'} n^\alpha | c = taking $N$th roots }} {{end-eqn}} By [[Limit of Root of Positive Real Number]]: :$\sqrt [N] {C'} \to 1$ as $N \to \infty$ By the [[Multiple Rule for Real Sequences]]: :$\sqrt [N] {C'} n^\alpha \to n^\alpha$ as $N \to \infty$ By [[Inequality Rule for Real Sequences]], letting $N \to \infty$ for fixed $n$: :$\norm n \ge n^\alpha$ The result follows. {{qed}}	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $I \subset F$ be a [[Definition:Non-Null Ideal|non-null]] [[Definition:Ideal of Ring|ideal]] of $F$. Let $a \in I$ be non-[[Definition:Field Zero|zero]]. Since $F$ is a [[Definition:Field (Abstract Algebra)|field]], $a^{-1}$ exists. We have that $1 = a^{-1} \cdot a \in I$. Since $1 \in I$, for every [[Definition:Element|element]] $b \in F$: :$b = b \cdot 1 \in I$ we have that $I = F = \ideal 1$ if $I \ne \set 0$. Thus the only [[Definition:Ideal of Ring|ideals]] of $F$ are $\ideal 0 = \set 0$ and $\ideal 1 = F$, which are both [[Definition:Principal Ideal Domain|principal ideals]]. Hence $F$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. {{qed}} [[Category:Principal Ideal Domains]] [[Category:Field Theory]] 82o7uz5ng4f9faonjp1mx046d66y0sm	1
{{MissingLinks}} {{proofread}} Let $J$ be an [[Definition:Ideal of Ring|ideal]] of $\struct {\Z_m, +, \times}$. $\struct {J, +}$ is a [[Definition:Subgroup|subgroup]] of $\struct {\Z_m, +}$. Let $\struct {G, +}$ be a [[Definition:Subgroup|subgroup]] of $\struct {\Z_m, +}$. Then $\struct {G, +}$ is a [[Definition:Cyclic Group|cyclic]] [[Definition:Subgroup|subgroup]] generated by $\gen d$, where $d \divides m$. We know that for a [[Definition:Finite Group|finite]] [[Definition:Cyclic Group|cyclic group]] of [[Definition:Order of Group|order $k$]], the [[Definition:Order of Group|order]] of every [[Definition:Subgroup|subgroup]] is a [[Definition:Divisor of Integer|divisor]] of $k$. Also there is exactly one [Definition:Subgroup|subgroup]] for each [[Definition:Divisor of Integer|divisor]]. It follows that all [[Definition:Ideal of Ring|ideals]] of $\struct {\Z_m, +, \times}$ are of form $\gen d$, where $d$ is a [[Definition:Strictly Positive Integer|positive]] [[Definition:Divisor of Integer|divisor]] of $m$. {{qed}}	1
Let $S$ be a [[Definition:Subset|subset]] of $R$. Let $x, y \in S: x \ne y$ Let $r \in \R_{>0} : r = \norm {x - y}$ Consider the [[Definition:Open Ball of Normed Division Ring|open ball]] $\map {B_r} x$ such that: :$x \in \map {B_r} x$ :$y \notin \map {B_r} x$ By [[Topological Properties of Non-Archimedean Division Rings/Open Balls are Clopen|Open Balls are Clopen]] then $\map {B_r} x$ is both [[Definition:Open Set (Topology)|open]] and [[Definition:Closed Set (Topology)|closed]]. By [[Complement of Clopen Set is Clopen]] then $R \setminus \map {B_r} x$ is [[Definition:Open Set (Topology)|open]]. Hence $S$ is not [[Definition:Connected Topological Space|connected]]. The result follows. {{qed}}	1
A [[Definition:Euclidean Domain|Euclidean domain]] is a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
By [[Cauchy Sequence is Bounded in Normed Division Ring|Cauchy Sequence is Bounded]]: :$\exists M \in \R_{>0}: \forall n, \norm {x_n} \le M$ Given $\epsilon > 0$. Since $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $0$ then: :$\exists N \in \N: \forall n > N, \norm {x_n} < \dfrac \epsilon M$ Hence: {{begin-eqn}} {{eqn | l = \norm {x_n y_n - 0} | r = \norm {x_n y_n} }} {{eqn | r = \norm {x_n} \norm {y_n} | c = {{NormAxiom|2}} }} {{eqn | o = < | r = \norm {x_n} M | c = $\sequence {y_n}$ is [[Definition:Bounded Real Sequence|bounded]] by $M$ }} {{eqn | o = < | r = \paren { \dfrac \epsilon M } M | c = $\sequence {x_n}$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $0$ }} {{eqn | r = \epsilon }} {{end-eqn}} Similarly, $\norm {y_n x_n - 0} < \epsilon$ The result follows from [[Definition:Convergent Sequence in Normed Division Ring|convergence in normed division rings]]. {{qed}}	1
Let $\R$ be the [[Definition:Set|set]] of [[Definition:Real Numbers|real numbers]]. Let $\size {\, \cdot \,}$ be the [[Definition:Absolute Value|absolute value]]. Then $\struct {\R, \size {\, \cdot \,}}$ is a [[Definition:Normed Vector Space|normed vector space]].	1
Let $\mathbf a = \left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert\mathbf u$. Then: {{begin-eqn}} {{eqn | l=\cos \angle \mathbf u, \mathbf a | r=\frac {\mathbf u \cdot \mathbf a} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Cosine Formula for Dot Product]] }} {{eqn | r=\frac {\mathbf u \cdot \left({ \left\Vert{ \mathbf u }\right\Vert \mathbf v + \left\Vert{ \mathbf v }\right\Vert \mathbf u }\right)} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a} \right\Vert} | c= }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf u }\right) } {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Properties of Dot Product]] }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf u }\right\Vert^2} {\left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Dot Product of Vector with Itself]] }} {{eqn | r=\frac {\mathbf u \cdot \mathbf v + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert} {\left\Vert{ \mathbf a }\right\Vert} | c= }} {{end-eqn}} {{begin-eqn}} {{eqn | l=\cos \angle \mathbf a, \mathbf v | r=\frac {\mathbf v \cdot \mathbf a} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Cosine Formula for Dot Product]] }} {{eqn | r=\frac {\mathbf v \cdot \left({ \left\Vert{ \mathbf u }\right\Vert \mathbf v + \left\Vert{ \mathbf v }\right\Vert \mathbf u }\right)} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= }} {{eqn | r=\frac {\left\Vert{ \mathbf u }\right\Vert \left({ \mathbf v \cdot \mathbf v }\right) + \left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right)} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= from [[Properties of Dot Product]] }} {{eqn | r=\frac {\left\Vert{ \mathbf v }\right\Vert \left({ \mathbf u \cdot \mathbf v }\right) + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert^2} {\left\Vert{ \mathbf v }\right\Vert \left\Vert{ \mathbf a }\right\Vert} | c= [[Dot Product of Vector with Itself]] }} {{eqn | r=\frac {\mathbf u \cdot \mathbf v + \left\Vert{ \mathbf u }\right\Vert \left\Vert{ \mathbf v }\right\Vert} {\left\Vert{ \mathbf a }\right\Vert} | c= }} {{end-eqn}} Comparing the two expressions gives us: :$\cos \angle \mathbf u, \mathbf a = \cos \angle \mathbf a, \mathbf v$ Since the angle used in the dot product is always taken to be between $0$ and $\pi$ and [[Definition:Cosine|cosine]] is [[Definition:Injection|injective]] on this interval (from [[Shape of Cosine Function]]), we have: :$\angle \mathbf u, \mathbf a = \angle \mathbf a, \mathbf v$ The result follows. {{qed}}	1
{{begin-eqn}} {{eqn | l = \norm {\mathbf v} \norm {\mathbf w} \cos \theta | r = \mathbf v \cdot \mathbf w | c = [[Cosine Formula for Dot Product]] }} {{eqn | l = \cos \theta | r = \frac {\mathbf v \cdot \mathbf w} {\norm {\mathbf v} \norm {\mathbf w} } | c = because $\mathbf v, \mathbf w \ne \mathbf 0 \implies \norm {\mathbf v}, \norm {\mathbf w} \ne 0$ }} {{eqn | l = \arccos \paren {\cos \theta} | r = \arccos \paren {\frac {\mathbf v \cdot \mathbf w} {\norm {\mathbf v} \norm {\mathbf w} } } | c = [[Definition:Angle Between Vectors#Comment|because $0 \le \theta \le \pi$]] $\implies -1 \le \cos \theta \le 1$ }} {{eqn | l = \theta | r = \arccos \paren {\frac {\mathbf v \cdot \mathbf w} {\norm {\mathbf v} \norm {\mathbf w} } } | c = [[Composite of Bijection with Inverse is Identity Mapping]] }} {{end-eqn}} {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]] whose [[Definition:Ring Zero|zero]] is $0_R$. Let $\map {\MM_R} {m, n}$ be a [[Definition:Matrix Space|$m \times n$ matrix space]] over $\struct {R, +, \circ}$. Let $\mathbf A$ be an [[Definition:Element|element]] of $\map {\MM_R} {m, n}$. Let $-\mathbf A$ be the [[Definition:Negative Matrix|negative]] of $\mathbf A$. Then $-\mathbf A$ is the [[Definition:Inverse Element|inverse]] for the operation $+$, where $+$ is [[Definition:Matrix Entrywise Addition over Ring|matrix entrywise addition]].	1
Let $p$ be [[Definition:Prime Number|prime]]. From [[Irreducible Elements of Ring of Integers]], we have that $p$ is [[Definition:Irreducible Element of Ring|irreducible]] in the [[Definition:Ring of Integers|ring of integers]] $\struct {\Z, +, \times}$. From [[Ring of Integers is Principal Ideal Domain]], $\struct {\Z, +, \times}$ is a [[Definition:Principal Ideal Domain|principal ideal domain]]. Thus by [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]], $\ideal p$ is a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $\struct {\Z, +, \times}$. Hence by [[Maximal Ideal iff Quotient Ring is Field]], $\Z / \ideal p$ is a [[Definition:Field (Abstract Algebra)|field]]. But $\Z / \ideal p$ is exactly $\struct {\Z_p, +, \times}$. {{qed|lemma}} Let $p$ be [[Definition:Composite Number|composite]]. Then $p$ is not [[Definition:Irreducible Element of Ring|irreducible]] in $\struct {\Z, +, \times}$. Thus by [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]], $\ideal p$ is not a [[Definition:Maximal Ideal of Ring|maximal ideal]] of $\struct {\Z, +, \times}$. Hence by [[Maximal Ideal iff Quotient Ring is Field]], $\Z / \ideal p$ is not a [[Definition:Field (Abstract Algebra)|field]]. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n}$ be an [[Definition:Matrix|$m \times n$ matrix]] over a [[Definition:Field (Abstract Algebra)|field]] $K$. Then there exists a [[Definition:Row Operation|row operation]] to convert $\mathbf A$ into another [[Definition:Matrix|$m \times n$ matrix]] $\mathbf B = \sqbrk b_{m n}$ with the following properties: :$(1): \quad$ Except possibly for [[Definition:Element of Matrix|element]] $b_{1 1}$, all the [[Definition:Element of Matrix|elements]] of [[Definition:Column of Matrix|column]] $1$ are $0$ :$(2): \quad$ If $b_{1 1} \ne 0$, then $b_{1 1} = 1$. This process is referred to as '''clearing the first column'''.	1
Let $\sequence {\tuple {x_n, y_n}}_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence|Cauchy sequence]] in $V$: : $\forall \epsilon \in \R_{>0}: \exists N \in \N: \forall m, n \in \N: m, n > N: \norm {\tuple {x_n, y_n} - \tuple {x_m, y_m} } < \epsilon$ We have that: {{begin-eqn}} {{eqn | l = \norm {x_n - x_m} | o = \le | r = \map \max {\norm {x_n - x_m}, \norm {y_n - y_m} } }} {{eqn | r = \norm {\tuple {x_n - x_m, y_n - y_m} } | c = {{defof|Direct Product Norm}} }} {{eqn | r = \norm {\tuple {x_n, y_n} - \tuple {x_m, y_m} } | c = [[Definition:Operation Induced by Direct Product|Induced component-wise operations]] }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Hence, $\sequence {x_n}_{n \mathop \in \N}$ is [[Definition:Cauchy Sequence|Cauchy sequence]] in $X$. By assumption, $X$ is a [[Definition:Banach Space|Banach space]]. Therefore, $\sequence {x_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $x \in X$. By analogous arguments, $\sequence {y_n}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $y \in Y$. Then there exists $\epsilon \in \R_{> 0}$ such that: :$\exists N_x \in \N : \forall n \in \N : n > N_x : \norm {x - x_n}_n < \epsilon$ :$\exists N_y \in \N : \forall n \in \N : n > N_y : \norm {y - y_n}_n < \epsilon$ Let $N := \map \max {N_x, N_y}$. Then: :$\forall n \in \N : n > N : \paren {\norm {x - x_n}_n < \epsilon} \land \paren{\norm {y - y_n}_n < \epsilon}$ Thus: {{begin-eqn}} {{eqn | l = \norm {\tuple {x, y} - \tuple {x_n, y_n} } | r = \norm {\tuple {x - x_n, y - y_n} } }} {{eqn | r = \map \max {\norm {x - x_n}, \norm {y - y_n} } | c = {{defof|Direct Product Norm}} }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Therefore, $\sequence {\tuple {x_n, y_n}}_{n \mathop \in \N}$ [[Definition:Convergent Sequence in Normed Vector Space|converges]] to $\tuple {x, y}$. By definition, $V$ is a [[Definition:Banach Space|Banach space]]. {{qed}}	1
Let $\mathbf A = \sqbrk a_{m n} \in \map {\MM_R} {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \paren {-\mathbf A} | r = \sqbrk a_{m n} + \paren {-\sqbrk a_{m n} } | c = Definition of $\mathbf A$ }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {-a}_{m n} | c = {{Defof|Negative Matrix over Ring}} }} {{eqn | r = \sqbrk {a + \paren {-a} }_{m n} | c = {{Defof|Matrix Entrywise Addition over Ring}} }} {{eqn | r = \sqbrk {0_R}_{m n} | c = {{Defof|Ring Negative}} }} {{eqn | ll= \leadsto | l = \mathbf A + \paren {-\mathbf A} | r = \mathbf 0_R | c = {{Defof|Zero Matrix over Ring}} }} {{end-eqn}} The result follows from [[Zero Matrix is Identity for Matrix Entrywise Addition over Ring]]. {{qed}}	1
Let $k$ be a [[Definition:Division Ring|division ring]]. Let $X$ be a [[Definition:Set|set]]. Let $k^{\paren X}$ be the [[Definition:Free Vector Space on Set|free vector space]] on $X$. The [[Definition:Vector Space|vector space]] $k^{\paren X}$ has [[Definition:Dimension of Vector Space|dimension]] the [[Definition:Cardinality|cardinality]] of $X$.	1
{{begin-eqn}} {{eqn | l = \frac a c | r = \frac b d | c = }} {{eqn | ll= \leadstoandfrom | l = a \circ c^{-1} | r = b \circ d^{-1} | c = {{Defof|Division Product}} }} {{eqn | ll= \leadstoandfrom | l = a \circ c^{-1} \circ c \circ d | r = b \circ d^{-1} \circ c \circ d | c = {{Defof|Cancellable Element}} of $U_R$ }} {{eqn | ll= \leadstoandfrom | l = \paren {a \circ d} \circ \paren {c^{-1} \circ c} | r = \paren {b \circ c} \circ \paren {d^{-1} \circ d} | c = {{Defof|Commutative Operation}} }} {{eqn | ll= \leadstoandfrom | l = a \circ d | r = b \circ c | c = {{Defof|Identity Element}} and {{Defof|Inverse Element}} }} {{end-eqn}} {{qed}} {{refactor|Construct a proof based on the below}} Alternatively, a proof can be built using [[Addition of Division Products]].	1
By definition, $T$ is [[Definition:Separable Space|separable]] {{iff}} there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]] in $T$. Consider $U := \set p \subseteq S$. By definition, $U$ is [[Definition:Open Set (Topology)|open]] in $T$. From [[Closure of Open Set of Particular Point Space]] we have that $U^- = S$, where $U^-$ is the [[Definition:Closure (Topology)|closure]] of $U$. By definition, $U$ is [[Definition:Everywhere Dense|everywhere dense in $T$]]. $U$ is (trivially) [[Definition:Countable Set|countable]]. Hence the result, by definition of a [[Definition:Separable Space|separable space]]. {{qed}}	1
Let $\mathbf A = \sqbrk \alpha_{m n}$ be an [[Definition:Matrix|$m \times n$ matrix]] over the [[Definition:Complex Number|complex numbers]] $\C$. Then the '''Hermitian conjugate''' of $\mathbf A$ is denoted $\mathbf A^\dagger$ and is defined as: :$\mathbf A^\dagger = \sqbrk \beta_{n m}: \forall i \in \set {1, 2, \ldots, n}, j \in \set {1, 2, \ldots, m}: \beta_{i j} = \overline {\alpha_{j i} }$ where $\overline {\alpha_{j i} }$ denotes the [[Definition:Complex Conjugate|complex conjugate]] of $\alpha_{j i}$.	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Then $\struct {R, +, \times, \times}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +, \times}$.	1
Checking in turn each of the criteria for [[Definition:Equivalence Relation|equivalence]]: === Reflexivity === We have that for all $\mathbf A \in S$: :$\mathbf A^r = \mathbf A^r$ for all $r \in \N$. It follows by definition of $\alpha$ that: :$\mathbf A \mathrel \alpha \mathbf A$ Thus $\alpha$ is seen to be [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetry === {{begin-eqn}} {{eqn | l = \mathbf A | o = \alpha | r = \mathbf B | c = }} {{eqn | ll= \implies | l = \mathbf A^r | r = \mathbf B^s | c = for some $r, s \in \N$ }} {{eqn | ll= \implies | l = \mathbf B^s | r = \mathbf A^r | c = }} {{eqn | ll= \implies | l = \mathbf B | o = \alpha | r = \mathbf A | c = }} {{end-eqn}} Thus $\alpha$ is seen to be [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitivity === Let: : $\mathbf A \mathrel \alpha \mathbf B$ and $\mathbf B \mathrel \alpha \mathbf C$ for [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]] $\mathbf A, \mathbf B, \mathbf C$. Then by definition: {{begin-eqn}} {{eqn | l = \mathbf A^r | r = \mathbf B^s | c = for some $r, s \in \N$ }} {{eqn | l = \mathbf B^u | r = \mathbf C^v | c = for some $u, v \in \N$ }} {{eqn | ll= \leadsto | l = \mathbf A^{r u} | r = \mathbf B^{s u} | c = raising both sides to [[Definition:Power of Matrix|$u$th power]] }} {{eqn | l = \mathbf B^{s u} | r = \mathbf C^{s v} | c = raising both sides to [[Definition:Power of Matrix|$s$th power]] }} {{eqn | ll= \leadsto | l = \mathbf A^{r u} | r = \mathbf C^{s v} | c = }} {{end-eqn}} Thus $\alpha$ is seen to be [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} $\alpha$ has been shown to be [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. Hence by definition it is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
:$\mathbf u \cdot \mathbf u = 0 \iff \mathbf u = \mathbf 0$	1
:$\sequence {a x_n}$ is a [[Definition:Cauchy Sequence in Normed Division Ring|Cauchy sequence]].	1
Let $m \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\Z_m, +, \times}$ denote the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]]. The [[Definition:Ideal of Ring|ideals]] of $\struct {\Z_m, +, \times}$ are of the form: :$d \Z / m \Z$ where $d$ is a [[Definition:Divisor of Integer|divisor]] of $m$.	1
Let the [[Definition:Radius Vector|radius vector]] $\mathbf r$ from the [[Definition:Origin|origin]] to $p$ be expressed as: :$(1): \quad \mathbf r = r \mathbf u_r$ :[[File:MotionInPolarPlane.png|600px]] From [[Derivatives of Unit Vectors in Polar Coordinates]]: {{begin-eqn}} {{eqn | n = 2 | l = \dfrac {\mathrm d \mathbf u_r} {\mathrm d \theta} | r = \mathbf u_\theta | c = }} {{eqn | n = 3 | l = \dfrac {\mathrm d \mathbf u_\theta} {\mathrm d \theta} | r = -\mathbf u_r | c = }} {{end-eqn}} From [[Velocity Vector in Polar Coordinates]]: :$\mathbf v = r \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_\theta + \dfrac {\mathrm d r} {\mathrm d t} \mathbf u_r$ where $\mathbf v$ is the [[Definition:Velocity|velocity]] of $p$. The [[Definition:Acceleration|acceleration]] of $p$ is by definition the [[Definition:Rate of Change|rate of change]] in its [[Definition:Velocity|velocity]]: {{begin-eqn}} {{eqn | l = \mathbf a | r = \dfrac {\mathrm d \mathbf v} {\mathrm d t} | c = }} {{eqn | r = r \dfrac {\mathrm d^2 \theta} {\mathrm d t^2} \mathbf u_\theta + \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_\theta + r \dfrac {\mathrm d \theta} {\mathrm d t} \dfrac {\mathrm d \mathbf u_\theta} {\mathrm d t} + \dfrac {\mathrm d^2 r} {\mathrm d t^2} \mathbf u_r + \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \mathbf u_r} {\mathrm d t} | c = [[Product Rule for Derivatives]] }} {{eqn | r = r \dfrac {\mathrm d^2 \theta} {\mathrm d t^2} \mathbf u_\theta + \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_\theta + r \dfrac {\mathrm d \theta} {\mathrm d t} \dfrac {\mathrm d \mathbf u_\theta} {\mathrm d \theta} \dfrac {\mathrm d \theta} {\mathrm d t} + \dfrac {\mathrm d^2 r} {\mathrm d t^2} \mathbf u_r + \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \mathbf u_r} {\mathrm d \theta} \dfrac {\mathrm d \theta} {\mathrm d t} | c = [[Chain Rule for Derivatives]] }} {{eqn | r = r \dfrac {\mathrm d^2 \theta} {\mathrm d t^2} \mathbf u_\theta + \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_\theta - r \dfrac {\mathrm d \theta} {\mathrm d t} \mathbf u_r \dfrac {\mathrm d \theta} {\mathrm d t} + \dfrac {\mathrm d^2 r} {\mathrm d t^2} \mathbf u_r + \dfrac {\mathrm d r} {\mathrm d t} \mathbf u_\theta \dfrac {\mathrm d \theta} {\mathrm d t} | c = substituting from $(2)$ and $(3)$ }} {{eqn | r = \left({r \dfrac {\mathrm d^2 \theta} {\mathrm d t^2} + 2 \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \theta} {\mathrm d t} }\right) \mathbf u_\theta + \left({\dfrac {\mathrm d^2 r} {\mathrm d t^2} - r \left({\dfrac {\mathrm d \theta} {\mathrm d t} }\right)^2}\right) \mathbf u_r | c = gathering terms }} {{end-eqn}} {{qed}}	1
Let $\mathbf I_n$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order $n$]] over $R$. The proof proceeds by [[Principle of Mathematical Induction|induction]]. For all $n \in \Z_{\ge 1}$, let $\map P n$ be the [[Definition:Proposition|proposition]]: :$\map \det {\mathbf I_n} = 1_R$ By definition of [[Definition:Determinant of Order 1|Determinant of Order $1$]]: :$\begin {vmatrix} a_{1 1} \end {vmatrix} = a_{1 1}$ In this case $a_{1 1} = 1_R$. Thus $\map P 1$ is seen to hold. === Basis for the Induction === {{begin-eqn}} {{eqn | l = \map \det {\mathbf I_2} | r = \begin {vmatrix} 1_R & 0_R \\ 0_R & 1_R \end {vmatrix} | c = }} {{eqn | r = 1_R \cdot 1_R - 0_R \cdot 0_R | c = {{Defof|Determinant of Order 2}} }} {{eqn | r = 1_R | c = }} {{end-eqn}} Thus $\map P 2$ is seen to hold. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that if $\map P k$ is true, where $k \ge 1$, then it logically follows that $\map P {k + 1}$ is true. So this is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\map \det {\mathbf I_k} = 1_R$ from which it is to be shown that: :$\map \det {\mathbf I_{k + 1} } = 1_R$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = \mathbf I_{k + 1} | r = \begin {bmatrix} 1_R & 0_R \\ 0_R & \mathbf I_n \end {bmatrix} | c = {{Defof|Unit Matrix}} }} {{eqn | ll= \leadsto | l = \map \det {\mathbf I_{k + 1} } | r = \begin {vmatrix} 1_R & 0_R \\ 0_R & \mathbf I_n \end {vmatrix} | c = {{Defof|Determinant of Matrix}} }} {{eqn | r = \map \det {\mathbf I_k} | c = [[Determinant with Unit Element in Otherwise Zero Row]] }} {{eqn | r = 1_R | c = [[Determinant of Unit Matrix#Induction Hypothesis|Induction Hypothesis]] }} {{end-eqn}} So $\map P k \implies \map P {k + 1}$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\forall n \in \Z_{\ge 0}: \map \det {\mathbf I_n} = 1_R$ {{qed}}	1
[[Definition:Matrix Product (Conventional)|Matrix multiplication (conventional)]] is [[Definition:Distributive Operation|distributive]] over [[Definition:Matrix Entrywise Addition|matrix entrywise addition]].	1
By the [[Combination Theorem for Sequences/Normed Division Ring/Inverse Rule|Inverse Rule for Normed Division Ring]]: :$\exists k \in \N : \forall n \in \N : y_{k + n} \ne 0$ and the [[Definition:Sequence|sequence]]: :$\sequence { {y_{k + n} }^{-1} }$ is well-defined and [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} {y_{k + n} }^{-1} = m^{-1}$ By [[Limit of Subsequence equals Limit of Sequence/Normed Division Ring|Limit of Subsequence equals Limit of Sequence]], $\sequence {x_{k + n} }$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] with: :$\displaystyle \lim_{n \mathop \to \infty} x_{k + n} = l$ By [[Combination Theorem for Sequences/Normed Division Ring/Product Rule|Product Rule for Normed Division Ring Sequences]]: :$\displaystyle \lim_{n \mathop \to \infty} x_{k + n} \ {y_{k + n} }^{-1} = l m^{-1}$ :$\displaystyle \lim_{n \mathop \to \infty} {y_{k + n} }^{-1} \ x_{k + n} = m^{-1} l$ {{qed}} [[Category:Combination Theorem for Sequences in Normed Division Rings]] cfkhm7xuqj4mr0odhepsxdcjkooqcwm	1
By definition of [[Definition:Proper Orthogonal Matrix|proper orthogonal matrix]]: :$\mathbf A$ is [[Definition:Orthogonal Matrix|orthogonal]] :the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$ is equal to $1$. By [[Inverse of Orthogonal Matrix is Orthogonal]], $\mathbf A^{-1}$ is [[Definition:Orthogonal Matrix|orthogonal]]. By [[Determinant of Inverse Matrix]]: :$\det \mathbf A^{-1} = \dfrac 1 {\det \mathbf A} = 1$ Hence, by definition, $\mathbf A^{-1}$ is [[Definition:Proper Orthogonal Matrix|proper orthogonal]]. {{qed}} [[Category:Orthogonal Matrices]] [[Category:Inverse Matrices]] sxpg2zjfvvqirrj1e87ohv80ib8a4k4	1
We assume the two hypotheses of the theorem. We have that: {{begin-eqn}} {{eqn | l = \map {\frac \d {\d t} } {\map \Phi {t + T} } | r = \map {\Phi'} {t + T} | c = }} {{eqn | r = \map {\mathbf A} {t + T} \map \Phi {t + T} | c = }} {{eqn | r = \map {\mathbf A} t \map \Phi {t + T} | c = }} {{end-eqn}} So the first implication of the theorem holds, that is: that $\map \Phi {t + T}$ is a [[Definition:Fundamental Matrix|fundamental matrix]]. Because $\map \Phi t$ and $\map \Phi {t + T}$ are both fundamental matrices, there must exist some [[Definition:Matrix|matrix]] $\mathbf C$ such that: :$\map \Phi {t + T} = \map \Phi t \mathbf C$ Hence by the [[Existence of Matrix Logarithm|existence of the matrix logarithm]], there exists a [[Definition:Matrix|matrix]] $\mathbf B$ such that: :$\mathbf C = e^{\mathbf BT}$ Defining $\map {\mathbf P} t = \map \Phi t e^{-\mathbf B t}$, it follows that: {{begin-eqn}} {{eqn | l = \map {\mathbf P} {t + T} | r = \map \Phi {t + T} e^{-\mathbf B t - \mathbf B T} | c = }} {{eqn | r = \map \Phi t C e^{-\mathbf B T} e^{-\mathbf B t} | c = }} {{eqn | r = \map \Phi t e^{-\mathbf B t} | c = }} {{eqn | r = \map {\mathbf P} t | c = }} {{end-eqn}} and hence $\map {\mathbf P} t$ is periodic with period $T$. As $\map \Phi t = \map {\mathbf P} t e^{\mathbf B t}$, the second implication also holds. {{qed}}	1
Let: : $\mathbf f: x \mapsto \left\langle{f_1 \left({x}\right), f_2 \left({x}\right), \ldots, f_n \left({x}\right)}\right\rangle$ : $\mathbf g: x \mapsto \left\langle{g_1 \left({x}\right), g_2 \left({x}\right), \ldots, g_n \left({x}\right)}\right\rangle$ : $\mathbf h: x \mapsto \left\langle{h_1 \left({x}\right), h_2 \left({x}\right), \ldots, h_n \left({x}\right)}\right\rangle$ be [[Definition:Differentiable Vector-Valued Function|differentiable]] [[Definition:Vector-Valued Function|vector-valued functions]]. The [[Definition:Derivative of Vector-Valued Function|derivative]] of their [[Definition:Scalar Triple Product|scalar triple product]] is given by: :$\dfrac \d {\d x} \left({\mathbf f \cdot \left({\mathbf g \times \mathbf h}\right)}\right) = \dfrac {\d \mathbf f} {\d x} \cdot \left({\mathbf g \times \mathbf h}\right) + \mathbf f \cdot \left({\dfrac {\d \mathbf g} {\d x} \times \mathbf h}\right) + \mathbf f \cdot \left({\mathbf g \times \dfrac {\d \mathbf h} {\d x} }\right)$	1
Let the first [[Definition:Column of Matrix|column]] of $\mathbf A$ containing a non-[[Definition:Field Zero|zero]] [[Definition:Element of Matrix|element]] be [[Definition:Column of Matrix|column]] $j$. Let such a non-[[Definition:Field Zero|zero]] [[Definition:Element of Matrix|element]] be in [[Definition:Row of Matrix|row]] $i$. Take [[Definition:Element of Matrix|element]] $a_{i j} \ne 0$ and perform the [[Definition:Elementary Row Operation|elementary row operations]]: :$(1): \quad r_i \to \dfrac {r_i} {a_{i j}}$ :$(2): \quad r_1 \leftrightarrow r_i$ This gives a [[Definition:Matrix|matrix]] with $1$ in the $\tuple {1, j}$ position: :$\begin {bmatrix} 0 & \cdots & 0 & 1 & b_{1, j + 1} & \cdots & b_{1 n} \\ 0 & \cdots & 0 & b_{2 j} & b_{2, j + 1} & \cdots & b_{2 n} \\ \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & b_{m j} & b_{m, j + 1} & \cdots & b_{m n} \\ \end {bmatrix}$ Now the [[Definition:Elementary Row Operation|elementary row operations]] $r_k \to r_k - b_{k j} r_1, k \in \set {2, 3, \ldots, m}$ gives the [[Definition:Matrix|matrix]]: :$\begin{bmatrix} 0 & \cdots & 0 & 1 & c_{1, j + 1} & \cdots & c_{1 n} \\ 0 & \cdots & 0 & 0 & c_{2, j + 1} & \cdots & c_{2 n} \\ \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & 0 & c_{m, j + 1} & \cdots & c_{m n} \\ \end{bmatrix}$ If some [[Definition:Zero Row or Column|zero rows]] have appeared, do some further [[Definition:Elementary Row Operation|elementary row operations]], that is row interchanges, to put them at the bottom. We now repeat the process with the remaining however-many-there-are [[Definition:Row of Matrix|rows]]: :$\begin{bmatrix} \cdots & 0 & 1 & d_{1, j + 1} & \cdots & d_{1, k - 1} & d_{1 k} & d_{1, k + 1} & \cdots & d_{1 n} \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 1 & d_{2, k + 1} & \cdots & d_{2 n} \\ \cdots & 0 & 0 & 0 & \cdots & 0 & d_{3 k} & d_{3, k + 1} & \cdots & d_{3 n} \\ \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \cdots & 0 & 0 & 0 & \cdots & 0 & d_{n k} & d_{m, k + 1} & \cdots & d_{m n} \\ \end{bmatrix}$ Then we can get the [[Definition:Reduced Echelon Matrix|reduced echelon form]] by: :$r_i \to r_i - d_{i k} r_2, i \in \set {1, 3, 4, \ldots, m}$ as follows: :$\begin{bmatrix} \cdots & 0 & 1 & {e_{1, j + 1 } } & \cdots & {e_{1, k - 1} } & 0 & {e_{1, k + 1} } & \cdots & {e_{1 n} } \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 1 & {e_{2, k + 1} } & \cdots & {e_{2 n} } \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 0 & {e_{3, k + 1} } & \cdots & {e_{3 n} } \\ \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 0 & {e_{m, k + 1} } & \cdots & {e_{m n} } \\ \end{bmatrix}$ Thus we progress, until the entire [[Definition:Matrix|matrix]] is in [[Definition:Reduced Echelon Matrix|reduced echelon]] form. {{Qed}}	1
Let $\struct {S, \cdot}$ be a [[Definition:Monoid|monoid]] whose [[Definition:Identity Element|identity]] is $e$. Let $\map {\MM_S} {m, n}$ be an [[Definition:Matrix Space|$m \times n$ matrix space]] over $S$. Let $\mathbf e = \sqbrk e_{m n}$ be the [[Definition:Zero Matrix over General Monoid|zero matrix]] of $\map {\MM_S} {m, n}$. Then $\mathbf e$ is the [[Definition:Identity Element|identity element]] for [[Definition:Hadamard Product|Hadamard product]].	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in \map B H$ be a [[Definition:Normal Operator|normal operator]]. Then: :$\ker A = \ker A^*$ where: :$\ker$ denotes [[Definition:Kernel of Linear Transformation|kernel]] :$A^*$ denotes the [[Definition:Adjoint Linear Transformation|adjoint]] of $A$.	1
Let $\overline z$ denote the [[Definition:Complex Conjugate|complex conjugate]] of $z$. Then: {{begin-eqn}} {{eqn | l = \cmod {z_1 z_2} | r = \sqrt {\paren {z_1 z_2} \overline {\paren {z_1 z_2} } } | c = [[Modulus in Terms of Conjugate]] }} {{eqn | r = \sqrt {z_1 \overline {z_1} z_2 \overline {z_2} } | c = [[Product of Complex Conjugates]], [[Complex Multiplication is Commutative]] }} {{eqn | r = \sqrt {z_1 \overline {z_1} } \sqrt {z_2 \overline {z_2} } | c = [[Exponent Combination Laws/Power of Product|Power of Product]] }} {{eqn | r = \cmod {z_1} \cmod {z_2} }} {{end-eqn}} {{qed}}	1
Let $V$ be a [[Definition:Vector Space|vector space]] of [[Definition:Dimension (Linear Algebra)|dimension]] $n$. Let $\mathcal B = \set {\mathbf x_1, \mathbf x_2, \ldots, \mathbf x_n}$ be a [[Definition:Basis of Vector Space|basis]] for $V$. Let $\mathbf x \in V$ be any [[Definition:Vector (Linear Algebra)|vector]] of $V$. Then $\mathbf x$ can be expressed as a [[Definition:Unique|unique]] [[Definition:Linear Combination|linear combination]] of elements of $\mathcal B$. {{explain|The definition we have of linear combination doesn't really work for this. We probably need to expand that definition.}}	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number as Vector|complex numbers in vector form]] such that $z_1 \ne 0$ and $z_2 \ne 0$. Then $z_1$ and $z_2$ are [[Definition:Perpendicular|perpendicular]] {{iff}}: :$z_1 \circ z_2 = 0$ where $z_1 \circ z_2$ denotes the [[Definition:Complex Dot Product|complex dot product]] of $z_1$ with $z_2$.	1
We have that [[Real Numbers form Ordered Integral Domain]]. Therefore [[Sum of Absolute Values on Ordered Integral Domain]] applies directly. {{qed}}	1
Let $\mathbb J = \set {x \in \R: a \le x \le b}$ be a [[Definition:Closed Real Interval|closed interval]] of the [[Definition:Real Number Line|real number line]] $\R$. Let $\map \RR {\mathbb J}$ be the set of all [[Definition:Riemann Integrable Function|Riemann integrable functions]] on $\mathbb J$. Then $\struct {\map \RR {\mathbb J}, +, \times}_\R$ is a [[Definition:Vector Subspace|subspace]] of the [[Definition:Vector Space|$\R$-vector space]] $\struct {\R^{\mathbb J}, +, \times}_\R$.	1
It is noted that: :$\sup \set {\size n: n \in \Z} = +\infty$ By a [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 3|corollary of Characterisation of Non-Archimedean Division Ring Norms]] then $\size {\,\cdot\,}$ is [[Definition:Archimedean Division Ring Norm|Archimedean]]. By [[P-adic Norm is Non-Archimedean Norm]] then $\norm {\,\cdot\,}_p$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]]. By [[Equivalent Norms are both Non-Archimedean or both Archimedean]], $\norm {\,\cdot\,}_p$ and $\size {\,\cdot\,}$ are not [[Definition:Equivalent Division Ring Norms|equivalent norms]]. {{qed}}	1
By [[Scalar Product with Identity]] we have: :$\forall \lambda \in R: \lambda \circ e = e$ Hence the result by definition of [[Definition:Linearly Dependent Set|linearly dependent]]. {{qed}}	1
Let $J$ be the [[Definition:Ideal of Ring|ideal]] formed from the [[Definition:Set|set]] of [[Definition:Polynomial over Ring in One Variable|polynomials over $\Z$ in $X$]] which have a [[Definition:Constant Term of Polynomial|constant term]] which is [[Definition:Even Integer|even]]. From [[Polynomials in Integers with Even Constant Term forms Ideal]], $J$ is indeed an [[Definition:Ideal of Ring|ideal]]. {{AimForCont}} $J$ is a [[Definition:Principal Ideal of Ring|principal ideal of]] $\Z \sqbrk X$ such that $J = \ideal f$. But $2 \in J$, and so $2$ is a [[Definition:Multiple of Integer|multiple]] of $f$ in $\Z \sqbrk X$. So $f = \pm 2$ or $f = \pm 1$. But this [[Definition:Contradiction|contradicts]] the fact that $J = \ideal f$. Hence the result by [[Proof by Contradiction]]. {{qed}}	1
From [[Direct Product of Modules is Module]], $M$ is an $R$-[[Definition:Module|module]]. It remains to verify that: :$\forall x \in M: 1 \circ x = x$ We have: {{begin-eqn}} {{eqn | l = 1_R \circ (m_i)_{i \in I} | r = \left({ 1_R \circ_i m_i }\right)_{i \in I} }} {{eqn | r = (m_i)_{i \in I} }} {{end-eqn}} {{qed}} [[Category:Module Theory]] [[Category:Direct Products]] 7icx8rqtkyen4nvys9dfq0v8ggic4qi	1
=== 1 implies 2 === Let there exist an $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]] $A_f \to A_g$. By [[Existence of Homomorphism between Localizations of Ring]], $f$ is contained in the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of the [[Definition:Set|set]] of [[Definition:Power of Ring Element|powers]] $\set {g^n : n \in \N}$. That is, $f$ [[Definition:Divisor of Ring Element|divides]] some power of $g$. {{qed|lemma}} === 2 implies 1 === Let $f$ [[Definition:Divisor of Ring Element|divide]] some power of $g$. Say $f d = g^n$. Then for all $m \in \N$, $f^m d^m = g^{nm}$. Thus $f^m$ [[Definition:Divisor of Ring Element|divides]] some power of $g$. Thus the [[Definition:Set|set]] of [[Definition:Power of Ring Element|powers]] $\set {f^m : m \in \N}$ is [[Definition:Set Containment|contained]] in the [[Definition:Saturation of Multiplicatively Closed Subset of Ring|saturation]] of $\set {g^n : n \in \N}$. By [[Existence of Homomorphism between Localizations of Ring]], there exists an $A$-[[Definition:Unital Associative Commutative Algebra Homomorphism|algebra homomorphism]] $A_f \to A_g$. {{qed|lemma}} === 3 iff 4 === By definition, a [[Definition:Principal Open Subset of Spectrum|principal open subset]] $\map D f$ is the [[Definition:Relative Complement|complement]] in the [[Definition:Spectrum of Ring|spectrum]] $\Spec A$ of the [[Definition:Vanishing Set of Subset of Ring|vanishing set]] $\map V f$. By [[Relative Complement inverts Subsets]], $\map V f \subseteq \map V g$ {{iff}} $\map D f \supseteq \map D g$. {{qed|lemma}} === 2 implies 3 === Let $I$ be a [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]] containing $f$. Let $f$ [[Definition:Divisor of Ring Element|divide]] some power of $g$. Say $f d = g^n$. By definition of [[Definition:Ideal of Ring|ideal]], $g^n = f d \in I$. By definition of [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]], $g \in I$. So every [[Definition:Prime Ideal of Commutative and Unitary Ring|prime ideal]] [[Definition:Element of Set|containing]] $f$ also contains $g$. {{qed|lemma}} === 3 implies 2 === {{proof wanted}} [[Category:Localization of Rings]] ij451qc4p6k2jezeki27pu6q472klvj	1
{{begin-eqn}} {{eqn | l = 0_F \circ \mathbf v | r = \paren {0_F + 0_F} \circ \mathbf v | c = {{Field-axiom|A3}} }} {{eqn | r = 0_F \circ \mathbf v + 0_F \circ \mathbf v | c = {{Vector-space-axiom|5}} }} {{eqn | ll= \leadsto | l = 0_F \circ \mathbf v + \paren {-0_F \circ \mathbf v} | r = \paren {0_F \circ \mathbf v + 0_F \circ \mathbf v} + \paren {-0_F \circ \mathbf v} | c = adding $-0_F \circ \mathbf v$ to both sides }} {{eqn | r = 0_F \circ \mathbf v + \paren {0_F \circ \mathbf v + \paren {-0_F \circ \mathbf v} } | c = {{Vector-space-axiom|2}} }} {{eqn | ll= \leadsto | l = \bszero | r = 0_F \circ \mathbf v + \bszero | c = {{Vector-space-axiom|4}} }} {{eqn | r = 0_F \circ \mathbf v | c = {{Vector-space-axiom|3}} }} {{end-eqn}} {{qed}}	1
Let $\norm {\, \cdot \,}$ be a [[Definition:Nontrivial Division Ring Norm|non-trivial]] [[Definition:Norm on Division Ring|norm]] on the [[Definition:Rational Numbers|rational numbers]] $\Q$. === [[Ostrowski's Theorem/Archimedean Norm|Archimedean Norm Case]] === Let $\norm {\, \cdot \,}$ be an [[Definition:Archimedean Division Ring Norm|Archimedean norm]]. {{:Ostrowski's Theorem/Archimedean Norm}}{{qed|lemma}} === [[Ostrowski's Theorem/Non-Archimedean Norm|Non-Archimedean Norm Case]] === Let $\norm {\, \cdot \,}$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean Norm]]. {{:Ostrowski's Theorem/Non-Archimedean Norm}}{{qed}}	1
If $p$ is [[Definition:Irreducible Element of Ring|irreducible]], it is proven. Suppose $p$ is not [[Definition:Irreducible Element of Ring|irreducible]]. Then $p = r_1 r_2$ where neither $r_1$ nor $r_2$ are [[Definition:Unit of Ring|units]]. If $r_1$ and $r_2$ are [[Definition:Irreducible Element of Ring|irreducible]], then the proof is complete. If $r_1$ is not [[Definition:Irreducible Element of Ring|irreducible]], then $r_1 = r_{11} r_{12}$ where neither $r_{11}$ nor $r_{12}$ are [[Definition:Unit of Ring|units]]. If we continue, we get a [[Definition:Proper Subset|proper inclusion]] of [[Definition:Ideal of Ring|ideals]] $\ideal r \subset \ideal {r_1} \subset \ideal {r_{11} } \subset \cdots \subset R$. If this process finishes in a [[Definition:Finite Set|finite number]] of steps, the proof is complete. {{AimForCont}} the process does ''not'' complete in a [[Definition:Finite Set|finite number]] of steps. Thus the [[Definition:Chain (Set Theory)|chain]] $\ideal r \subset \ideal {r_1} \subset \ideal {r_{11} } \subset \cdots \subset R$ is [[Definition:Infinite Set|infinite]]. From [[Principal Ideal Domain fulfills Ascending Chain Condition]], this cannot happen. Thus, the process ends in a [[Definition:Finite Set|finite number]] of steps. {{qed}} [[Category:Factorization]] [[Category:Ideal Theory]] [[Category:Principal Ideal Domains]] 05ldswkzlatcaniyeaqrq59aonytg7i	1
Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]]. Let $S \subseteq G$. Then the [[Definition:Generator of Module|submodule $H$ generated by $S$]] is the [[Definition:Set|set]] of all [[Definition:Linear Combination|linear combinations]] of $S$.	1
Let $V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{vmatrix}$. By [[Multiple of Row Added to Row of Determinant]], we can subtract [[Definition:Row of Matrix|row]] 1 from each of the other rows and leave $V_n$ unchanged: :$V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 0 & x_2 - x_1 & x_2^2 - x_1^2 & \cdots & x_2^{n-2} - x_1^{n-2} & x_2^{n-1} - x_1^{n-1} \\ 0 & x_3 - x_1 & x_3^2 - x_1^2 & \cdots & x_3^{n-2} - x_1^{n-2} & x_3^{n-1} - x_1^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & x_{n-1} - x_1 & x_{n-1}^2 - x_1^2 & \cdots & x_{n-1}^{n-2} - x_1^{n-2} & x_{n-1}^{n-1} - x_1^{n-1} \\ 0 & x_n - x_1 & x_n^2 - x_1^2 & \cdots & x_n^{n-2} - x_1^{n-2} & x_n^{n-1} - x_1^{n-1} \end{vmatrix}$ Similarly without changing the value of $V_n$, we can subtract, in order: :$x_1$ times [[Definition:Column of Matrix|column]] $n-1$ from [[Definition:Column of Matrix|column]] $n$ :$x_1$ times [[Definition:Column of Matrix|column]] $n-2$ from [[Definition:Column of Matrix|column]] $n-1$ and so on, till we subtract: : $x_1$ times [[Definition:Column of Matrix|column]] $1$ from [[Definition:Column of Matrix|column]] $2$. The first row will vanish all apart from the first element $a_{11} = 1$. On all the other rows, we get, with new $i$ and $j$: :$a_{ij} = \left({x_i^{j-1} - x_1^{j-1}}\right) - \left({x_1 x_i^{j-2} - x_1^{j-1}}\right) = \left({x_i - x_1}\right) x_i^{j-2}$: :$V_n = \begin{vmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & x_2 - x_1 & \left({x_2 - x_1}\right) x_2 & \cdots & \left({x_2 - x_1}\right) x_2^{n-3} & \left({x_2 - x_1}\right) x_2^{n-2} \\ 0 & x_3 - x_1 & \left({x_3 - x_1}\right) x_3 & \cdots & \left({x_3 - x_1}\right) x_3^{n-3} & \left({x_3 - x_1}\right) x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & x_{n-1} - x_1 & \left({x_{n-1} - x_1}\right) x_{n-1} & \cdots & \left({x_{n-1} - x_1}\right) x_{n-1}^{n-3} & \left({x_{n-1} - x_1}\right) x_{n-1}^{n-2}\\ 0 & x_n - x_1 & \left({x_n - x_1}\right) x_n & \cdots & \left({x_n - x_1}\right) x_n^{n-3} & \left({x_n - x_1}\right) x_n^{n-2} \end{vmatrix}$ For all rows apart from the first, the $k$th row has the constant factor $\left({x_k - x_1}\right)$. So we can extract all these as factors, and from [[Determinant with Row Multiplied by Constant]], we get: :$\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) \begin{vmatrix} 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & x_2 & \cdots & x_2^{n-3} & x_2^{n-2} \\ 0 & 1 & x_3 & \cdots & x_3^{n-3} & x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 1 & x_{n-1} & \cdots & x_{n-1}^{n-3} & x_{n-1}^{n-2}\\ 0 & 1 & x_n & \cdots & x_n^{n-3} & x_n^{n-2} \end{vmatrix}$ From [[Determinant with Unit Element in Otherwise Zero Row]], we can see that this directly gives us: :$\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) \begin{vmatrix} 1 & x_2 & \cdots & x_2^{n-3} & x_2^{n-2} \\ 1 & x_3 & \cdots & x_3^{n-3} & x_3^{n-2} \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & \cdots & x_{n-1}^{n-3} & x_{n-1}^{n-2}\\ 1 & x_n & \cdots & x_n^{n-3} & x_n^{n-2} \end{vmatrix}$ and it can be seen that: : $\displaystyle V_n = \prod_{k \mathop = 2}^n \left({x_k - x_1}\right) V_{n-1}$ $V_2$, by the time we get to it (it will concern elements $x_{n-1}$ and $x_n$), can be calculated directly using the formula for calculating a [[Definition:Determinant of Order 2|Determinant of Order 2]]: : $V_2 = \begin{vmatrix} 1 & x_{n-1} \\ 1 & x_n \end{vmatrix} = x_n - x_{n-1}$ The result follows. {{qed}}	1
Follows directly from [[Product of Linear Transformations]]. {{Qed}}	1
:$\psi$ is a [[Definition:Surjection|surjective mapping]].	1
{{ProofWanted}} [[Category:Implicit Functions]] op8r7n6voyto8kaqajoonhlny7b7fra	1
By definition of a [[Definition:Left Ideal|left ideal]] then $\circ$ is [[Definition:Well-Defined|well-defined]]. === $(\text M 1)$ : Scalar Multiplication (Left) Distributes over Module Addition === Follows directly from {{Ring-axiom|D}} {{qed|lemma}} === $(\text M 2)$ : Scalar Multiplication (Right) Distributes over Scalar Addition === Follows directly from {{Ring-axiom|D}} {{qed|lemma}} === $(\text M 3)$ : Associativity of Scalar Multiplication === Follows directly from {{Ring-axiom|M1}} {{qed}}	1
Let $\mathbf a, \mathbf b, \mathbf c$ be [[Definition:Vector Quantity|vectors]]. Then: :$\mathbf a + \paren {\mathbf b + \mathbf c} = \paren {\mathbf a + \mathbf b} + \mathbf c$	1
The [[Definition:Sufficient Condition|sufficient condition]] is proved in [[Vector Scaled by Zero is Zero Vector]], and in [[Zero Vector Scaled is Zero Vector]]. The [[Definition:Necessary Condition|necessary condition]] is proved in [[Vector Product is Zero only if Factor is Zero]]. {{qed}}	1
By [[Linear Subspaces Closed under Setwise Addition]], $M + N$ is a [[Definition:Vector Subspace|linear subspace]] of $H$. Now to show that it is [[Definition:Closed Set (Topology)|closed]]. Let $P: H \to H$ denote the [[Definition:Orthogonal Projection|orthogonal projection]] on $M$. Denote by $I - P$ the [[Definition:Complementary Projection|complementary projection]] of $P$. Define $N' := \left\{{ n - Pn: n \in N}\right\}$. $N'$ is a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$. {{explain|prove the above claim}} Observe $m + n = \left({m + Pn}\right) + \left({n - Pn}\right) \in M + N'$; hence, $M + N \subseteq M + N'$. By $m + \left({n - Pn}\right) = \left({m - Pn}\right) + n \in M + N$, conclude that $M + N' \subseteq M + N$, hence equality. Furthermore, $N' \subseteq \operatorname{ran} \left({I - P}\right) = \operatorname{ker} P$ by [[Range of Idempotent is Kernel of Complementary Idempotent]]. That is, $N' \subseteq M^\perp$ by [[Properties of Orthogonal Projection]], and hence $M \perp N'$. Denote by $P'$ the [[Definition:Orthogonal Projection|orthogonal projection]] on $N'$. Suppose now that $h \in M + N'$. Then: {{begin-eqn}} {{eqn | l = h | r = \left({P + \left({I - P}\right)}\right)h }} {{eqn | r = \left({P + \left({I - P}\right)}\right) \left({P' + \left({I - P'}\right)}\right)h | c = $I$ is the [[Definition:Identity Operator|identity operator]] }} {{eqn | r = PP'h + \left({I - P}\right)P'h + P\left({I - P'}\right)h \left({I - P}\right)\left({I - P'}\right)h }} {{eqn | r = 0 + P'h + Ph + 0 }} {{end-eqn}} For this last equality, observe $M \perp N'$, hence $M \subseteq N'^\perp$, $N' \subseteq M^\perp$ and $\left({M + N'}\right)^\perp \subseteq M^\perp \cup N'^\perp$. {{finish|link to results establishing the implications of these relations for the associated projections. These results don't exist at the moment}} The conclusion is that every $h \in M + N'$ can be uniquely decomposed as $P'h + Ph$, with $P'h \in N', Ph \in M$. So suppose there is a sequence $h_n \to h$ in $M + N'$. Then $Ph_n$ and $P'h_n$ are sequences in $M, N'$, respectively. As those are [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$, there are $m \in M, n \in N'$ with $Ph_n \to m, P'h_n \to n$. It follows that $h = m + n \in M + N'$. That is, $M + N' = M + N$ is a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$. {{qed}} [[Category:Hilbert Spaces]] 0gbg8zzxrz1hypc60x8yl9c087m1opm	1
Since $A$ is [[Definition:Finitely Generated R-Algebra|finitely generated]], we prove this by [[Proof by Mathematical Induction|induction]] on the number $m$ of [[Definition:Generator of Algebra|generators]] as a [[Definition:K-Algebra|$k$-algebra]]. === Basis for the induction === If $m = 0$, then $A = k$ and thus there is nothing to prove. === Induction step === Let $A$ be a $k$-algebra generated by the elements $y_1, \dotsc, y_m, y_{m + 1}$. If $y_1, \dotsc, y_m, y_{m + 1}$ are algebraically independent, then $A$ is isomorphic to $k \sqbrk {x_1, \dotsc, x_m, x_{m + 1} }$. In this case the theorem is obvious, so assume that $y_{m + 1}$ depends algebraically on $y_1, \ldots, y_m$. So there exists a polynomial $P \in k \sqbrk {x_1, \dotsc, x_m, X}$ such that $\map P {y_1, \dotsc, y_n, y_{n + 1} } = 0$ in $A$. Let $\mu \in \N^n$ and define: :$f_\mu: k \sqbrk {x_1, \dotsc, x_n, X} \to k \sqbrk {x_1, \dotsc, x_n, X}: x_i \mapsto x_i + X^{\mu_i}, X \mapsto X$ This is easily seen to be an isomorphism. {{handwaving|easily seen}} Next, we want to show that there exists $\mu \in \N^n$ such that $\map {f_\mu} P$ is a polynomial in $X$ with leading coefficient in $k$. Let $e$ be the biggest natural number such that $P$ involves $x_e$ non-trivially. If there is no such number, define $e$ to be $0$. We proceed by induction on $e$. The case $e = 0$ is easy since we then have a polynomial $f \in k \sqbrk X$. Thus $\mu = \tuple {0, \dotsc, 0}$ will suffice. Suppose it holds for $e$ and suppose $P$ depends only on $x_1, \dotsc, x_e, x_{e + 1}$. Then we can write $P$ as: :$\displaystyle P = \sum_{i \mathop = 0}^l a_i x_{e + 1}^i$ where $a_i \in k \sqbrk {x_1, \dotsc, x_e, X}$ and $a_l \ne 0$. Hence by the induction hypothesis, there exists a $\mu' \in \N^n $ such that $\map {f_{\mu'} } {a_l}$ has a leading coefficient in $k$. Note that we can choose that $\mu'_l$ for $l > e$ since $\map {f_\mu} {a_i}$ is independent of these components. Hence we define $\mu$ as $\mu_i = \mu'_i$ for every $i \ne e + 1$. Now observe that: :$\displaystyle \map {f_\mu} P = \sum_{i \mathop = 0}^l \map {f_\mu} {a_i} \, \map {f_\mu} {x_{e + 1} }^i = \sum_{i \mathop = 0}^l \map {f_{\mu'} } {a_i} \paren {x_{e + 1} + X^{\mu_{e + 1} } }^i$ We have that $\map {f_{\mu'} } {a_l}$ has a leading coefficient in $k$. Thus by choosing $\mu_{e + 1}$ big enough, $map {f_\mu} P$ has a leading coefficient in $k$. Consider now thus the elements $z_i = \map {f_\mu^{-1} } {y_i} \in A$. These elements still generate $A$ since $f_\mu$ is an isomorphism. On the other hand, we find that: {{begin-eqn}} {{eqn | l = \map {f_\mu} P \tuple {z_1, \dotsc, z_m, z_{m + 1} } | r = \map {f_\mu} P \tuple {\map {f_\mu^{-1} } {y_1}, \dotsc, \map {f_\mu^{-1} } {y_m}, \map {f_\mu^{-1} } {y_{m + 1} } } }} {{eqn | r = \map P {y_1, \dotsc, y_m, y_{m + 1} } }} {{eqn | r = 0 }} {{end-eqn}} By the induction hypothesis, we have the existence of: :$\alpha: k \sqbrk {x_1, \dotsc, x_n} \to A'$ where $A'$ is generated by $z_1, \dotsc, z_m$. By extending this $\alpha$ to $A$ by the natural inclusion, we find that $\alpha$ is finite, since $z_i$ is integral by construction and $z_{m + 1}$ is integral over the others since it satisfies $\map {f_\mu} P$. {{explain|"integral over the others" -- what specifically are the "others"? The above sentence needs to be rewritten as a series of simple statements in standard {{ProofWiki}} style.}} Hence it is the wanted finite injective morphism. {{qed}} {{Namedfor|Emmy Noether|cat = Noether E}}	1
Let $\mathbf r$ be a [[Definition:Vector Quantity|vector quantity]] embedded in a [[Definition:Cartesian 3-Space|Cartesian $3$-space]]. Let $\mathbf r$ be expressed in terms of its [[Definition:Component of Vector|components]]: :$\mathbf r = x \mathbf i + y \mathbf j + z \mathbf k$ where $\mathbf i$, $\mathbf j$ and $\mathbf k$ denote the [[Definition:Unit Vector|unit vectors]] in the [[Definition:Positive Direction|positive directions]] of the [[Definition:X-Axis|$x$-axis]], [[Definition:Y-Axis|$y$-axis]] and [[Definition:Z-Axis|$z$-axis]] respectively. Then: :$\size {\mathbf r} = \sqrt {x^2 + y^2 + z^2}$ where $\size {\mathbf r}$ denotes the [[Definition:Magnitude|magnitude]] of $\mathbf r$.	1
{{begin-eqn}} {{eqn | l = \dfrac 1 {\size {\mathbf a} } \mathbf a | r = \mathbf {\hat a} | c = {{Defof|Unit Vector}} }} {{eqn | ll= \leadsto | l = \size {\mathbf a} \times \dfrac 1 {\size {\mathbf a} } \mathbf a | r = \size {\mathbf a} \mathbf {\hat a} | c = }} {{eqn | ll= \leadsto | l = \mathbf a | r = \size {\mathbf a} \mathbf {\hat a} | c = }} {{end-eqn}} {{qed}}	1
$T$ is [[Definition:Separable Space|separable]] {{iff}} there exists a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]]. $T$ is [[Definition:Metacompact Space|metacompact]] {{iff}} every [[Definition:Open Cover|open cover]] of $S$ has an [[Definition:Open Refinement|open refinement]] which is [[Definition:Point Finite|point finite]]. $T$ is a [[Definition:Lindelöf Space|Lindelöf space]] if every [[Definition:Open Cover|open cover]] of $S$ has a [[Definition:Countable Subcover|countable subcover]]. Having established the definitions, we proceed. Let $T = \left({S, \tau}\right)$ be [[Definition:Separable Space|separable]] and [[Definition:Metacompact Space|metacompact]]. {{AimForCont}} there exists an [[Definition:Open Cover|open cover]] $\UU$ of $S$ which has no [[Definition:Countable Subcover|countable subcover]]. As $T$ is [[Definition:Metacompact Space|metacompact]], $\UU$ has an [[Definition:Open Refinement|open refinement]] $\VV$ which is [[Definition:Point Finite|point finite]]. By nature of $\UU$, which has no [[Definition:Countable Subcover|countable subcover]], $\VV$ is [[Definition:Uncountable Set|uncountable]]. {{explain|How would $\VV$ being countable lead to a countable subcover of $\UU$? ACC is helpful for this.}} [[Definition:By Hypothesis|By hypothesis]], $T$ is [[Definition:Separable Space|separable]]. Let $\SS$ be a [[Definition:Countable Set|countable]] [[Definition:Subset|subset]] of $S$ which is [[Definition:Everywhere Dense|everywhere dense]]. Then each $V \in \VV$ contains some $s \in \SS$. {{explain|why? Also, this is not true if $\VV$ contains the empty set.}} So some $s \in \SS$ is contained in an [[Definition:Uncountable Set|uncountable]] number of elements of $\VV$. Thus, by definition, $\VV$ is not [[Definition:Point Finite|point finite]]. Thus no [[Definition:Uncountable Set|uncountable]] [[Definition:Open Refinement|open refinement]] $\VV$ of $\UU$ exists which is [[Definition:Point Finite|point finite]]. It follows that $\VV$ must be [[Definition:Countable Set|countable]]. Thus $\UU$ has a [[Definition:Countable Subcover|countable subcover]]. That is, by definition, $T$ is a [[Definition:Lindelöf Space|Lindelöf space]]. {{qed}}	1
Let $\mathcal L$ be a [[Definition:Vertical Line|vertical line]] embedded in the [[Definition:Cartesian Plane|Cartesian plane]] $\mathcal C$. Then the [[Definition:Equation of Geometric Figure|equation]] of $\mathcal L$ can be given by: :$x = a$ where $\tuple {a, 0}$ is the [[Definition:Point|point]] at which $\mathcal L$ [[Definition:Intersection (Geometry)|intersects]] the [[Definition:X-Axis|$x$-axis]]. :[[File:Graph-of-vertical-line.png|520px]]	1
Follows from the fact that $\left({G, +_G, \circ}\right)_R$ has to be, by definition, a [[Definition:Trivial Module|trivial module]]: $\circ$ can only be defined as: :$\forall \lambda \in R: \forall x \in G: \lambda \circ x = e_G$ {{qed}} [[Category:Module Theory]] 98hsbt8bc90fqn1tff9vjbo2blkvish	1
Let $\mathbf I$ denote the [[Definition:Unit Matrix|unit matrix]] of [[Definition:Order of Square Matrix|order]] $n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $e$ be an [[Definition:Elementary Column Operation|elementary column operation]] on $\mathbf I$. Let $\mathbf E$ be the [[Definition:Elementary Column Matrix|elementary column matrix]] of [[Definition:Order of Square Matrix|order]] $n$ [[Definition:Unique|uniquely]] defined as: :$\mathbf E = e \paren {\mathbf I}$ where $\mathbf I$ is the [[Definition:Unit Matrix|unit matrix]]. Let $\kappa_k$ denote the $k$th [[Definition:Column of Matrix|column]] of $\mathbf I$ for $1 \le k \le n$.	1
Follows directly from [[Module of All Mappings is Module]] and the definition of [[Definition:Vector Space|vector space]].	1
Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A + \paren {-\mathbf A} | r = \sqbrk a_{m n} + \paren {-\sqbrk a_{m n} } | c = Definition of $\mathbf A$ }} {{eqn | r = \sqbrk a_{m n} + \sqbrk {-a}_{m n} | c = {{Defof|Negative Matrix}} }} {{eqn | r = \sqbrk {a + \paren {-a} }_{m n} | c = {{Defof|Matrix Entrywise Addition}} }} {{eqn | r = \sqbrk 0_{m n} | c = }} {{eqn | ll= \leadsto | l = \mathbf A + \paren {-\mathbf A} | r = \mathbf 0 | c = {{Defof|Zero Matrix}} }} {{end-eqn}} The result follows from [[Zero Matrix is Identity for Matrix Entrywise Addition]]. {{qed}}	1
Consider a [[Definition:Particle|particle]] $p$ moving in the [[Definition:Plane|plane]]. Let the [[Definition:Position|position]] of $p$ at [[Definition:Time|time]] $t$ be given in [[Definition:Polar Coordinates|polar coordinates]] as $\left\langle{r, \theta}\right\rangle$. Then the [[Definition:Acceleration|acceleration]] $\mathbf a$ of $p$ can be expressed as: :$\mathbf a = \left({r \dfrac {\mathrm d^2 \theta} {\mathrm d t^2} + 2 \dfrac {\mathrm d r} {\mathrm d t} \dfrac {\mathrm d \theta} {\mathrm d t} }\right) \mathbf u_\theta + \left({\dfrac {\mathrm d^2 r} {\mathrm d t^2} - r \left({\dfrac {\mathrm d \theta} {\mathrm d t} }\right)^2}\right) \mathbf u_r$ where: :$\mathbf u_r$ is the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Radial Coordinate|radial coordinate]] of $p$ :$\mathbf u_\theta$ is the [[Definition:Unit Vector|unit vector]] in the direction of the [[Definition:Angular Coordinate|angular coordinate]] of $p$	1
Let $e$ be the [[Definition:Elementary Row Operation|elementary row operation]] acting on $\mathbf I$ as: {{begin-axiom}} {{axiom | n = \text {ERO} 1 | t = For some $\lambda \in K_{\ne 0}$, [[Definition:Matrix Scalar Product|multiply]] [[Definition:Row of Matrix|row]] $k$ of $\mathbf I$ by $\lambda$ | m = r_k \to \lambda r_k }} {{end-axiom}}	1
:[[File:Angular Bisector Vector Diagram.png|400px]] As shown above: :Let the [[Definition:Angle|angle]] between $\mathbf u$ and $\mathbf v$ be $\gamma$. :Let the angle between $\left\Vert{\mathbf u}\right\Vert \mathbf v$ and $\left\Vert{\mathbf v}\right\Vert \mathbf u$ be $\alpha$. :Let the angle between $\mathbf u$ and $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ be $\beta$. Note that $\left\Vert{ \mathbf u }\right\Vert \mathbf v$ is $\mathbf v$ multiplied by the length of $\mathbf u$. By [[Vector Times Magnitude Same Length As Magnitude Times Vector]] the vectors $\left\Vert{\mathbf u}\right\Vert \mathbf v$ and $\left\Vert{\mathbf v}\right\Vert \mathbf u$ have equal length. So $\left\Vert{\mathbf u}\right\Vert \mathbf v$, $\left\Vert{\mathbf v}\right\Vert \mathbf u$ and $\left\Vert{\mathbf u}\right\Vert \mathbf v + \left\Vert{\mathbf v}\right\Vert \mathbf u$ make an [[Definition:Isosceles Triangle|isosceles triangle]]. Therefore: {{begin-eqn}} {{eqn | l = 2 \beta + \alpha | r = 180^\circ }} {{eqn | l = \beta | r = 90^\circ - \frac 1 2 \alpha }} {{eqn | l = 2 \beta | r = 180^\circ - \alpha }} {{end-eqn}} But since $\mathbf v$ and $\left\Vert{ \mathbf u }\right\Vert \mathbf v$ are [[Definition:Parallel Vectors|parallel]], we also have: {{begin-eqn}} {{eqn | l = \delta | r = \alpha | c = [[Parallelism implies Equal Corresponding Angles]] }} {{eqn | l = \delta + \gamma | r = 180^\circ }} {{eqn | l = \alpha + \gamma | r = 180^\circ }} {{eqn | l = \gamma | r = 180^\circ - \alpha }} {{end-eqn}} Thus $\gamma = 2 \beta$, and the result follows. {{qed}}	1
Let $\displaystyle z = \int_X f \rd \mu \in \C$. By [[Complex Multiplication as Geometrical Transformation]], there is a [[Definition:Complex Number|complex number]] $\alpha$ with $\cmod \alpha = 1$ such that: :$\alpha z = \cmod z \in \R$ Let $u = \map \Re {\alpha f}$, where $\Re$ denotes the [[Definition:Real Part|real part]] of a [[Definition:Complex Number|complex number]]. By [[Modulus Larger than Real Part]], we have that: :$u \le \cmod {\alpha f} = \cmod f$ Thus we get the inequality: {{begin-eqn}} {{eqn | l = \cmod {\int_X f \rd \mu} | r = \alpha \int_X f \rd \mu | c = }} {{eqn | r = \int_X \alpha f \rd \mu | c = [[Integral of Integrable Function is Homogeneous]] }} {{eqn | r = \int_X u \rd \mu | c = }} {{eqn | o = \le | r = \int_X \cmod f \rd \mu | c = [[Integral of Integrable Function is Monotone]] }} {{end-eqn}} {{qed}}	1
Let $A$ be a [[Definition:Commutative and Unitary Ring|commutative ring with unity]]. Let $S \subseteq A$ be a [[Definition:Multiplicatively Closed Subset|multiplicatively closed subset]] with $0 \notin S$. Then there exists a [[Definition:Localization of Ring|localization]] $\struct {A_S, \iota}$ of $A$ at $S$.	1
By [[Convergent Sequence is Cauchy Sequence/Normed Division Ring|every convergent sequence is a Cauchy sequence]] then $\NN \subseteq \CC$. From [[Constant Sequence Converges to Constant in Normed Division Ring]], the [[Definition:Unity of Ring|unity]] $\tuple {1, 1, 1, \dotsC}$ of $\CC$ [[Definition:Convergent Sequence in Normed Division Ring|converges]] to $1 \in R$, and therefore $\tuple {1, 1, 1, \dotsc} \in \CC \setminus \NN$ So $\NN \subsetneq \CC$. {{qed}}	1
=== Definition 1 === :$V_n = \begin{bmatrix} x_1 & \cdots & x_n \\ x_1^2 & \cdots & x_n^2 \\ \vdots & \ddots & \vdots \\ x_1^{n} & \cdots & x_n^{n} \\ \end{bmatrix} ,\quad V = \begin{bmatrix} 1 & \cdots & 1 \\ x_1 & \cdots & x_n \\ \vdots & \ddots & \vdots \\ x_1^{n-1} & \cdots & x_n^{n-1} \\ \end{bmatrix} \quad $ [[Definition:Vandermonde Matrix|Vandermonde matrices]] :$D = \begin{bmatrix} x_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & x_n \\ \end{bmatrix}, \quad P = \begin{bmatrix} \map {p_1} {x_1} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \map {p_n} {x_n} \\ \end{bmatrix} \quad $ [[Definition:Diagonal Matrix]] :$ E = \begin{bmatrix} E_{11} & \cdots & E_{1n} \\ \vdots & \ddots & \vdots \\ E_{n1} & \cdots & E_{nn} \\ \end{bmatrix} \quad $ Matrix of [[Definition:Elementary Symmetric Function|symmetric functions]] :'''where for''' $\mathbf {1 \mathop \le i,j \mathop \le n}$: {{begin-eqn}} {{eqn | l = E_{ij} | r = \paren { -1 }^{n-j} \, e_{n-j} \paren { \set {x_1,\ldots,x_n} \setminus \set {x_i} } | c = [[Definition:Elementary Symmetric Function]] $e_m \paren {U}$ }} {{eqn | l = \displaystyle \map {p_j} x | r = \prod_{ k \mathop = 1,\, k \mathop \neq j }^n \paren { x - x_k } }} {{end-eqn}} === Lemma 1 === {{begin-eqn}} {{eqn | l = V_n | r = V\, D | c [[Definition:Matrix Product (Conventional)|Matrix Multiply]] }} {{eqn | l = V_n^{-1} | r = D^{-1} V^{-1} | c = provided the inverses exist: [[Inverse of Matrix Product]] }} {{end-eqn}} {{qed|lemma}} === Lemma 2 === :$ EV = P$ :$ V^{-1} = P^{-1} E\quad$ provided $\set {x_1,\ldots,x_n}$ is a set of distinct values. :$ V_n^{-1} = D^{-1} V^{-1}\quad$ provided $\set {x_1,\ldots,x_n}$ is a set of distinct values, all nonzero. '''Proof of Lemma 2''' Matrix multiply establishes $EV=P$, provided: :$(1)\quad \sum_{k = 1}^n E_{ik}\, x_j^{k-1} = \begin{cases} 0 & i \neq j \\ \map {p_i} {x_i} & i = j \end{cases} $ Polynomial $\map {p_i} {x}$ is zero for $x \in \set {x_1,\ldots,x_n} \setminus \set {x_i}$. Then (1) is equivalent to :$(2)\quad \sum_{k = 1}^n \paren { -1 }^{n-k} e_{n-k} \paren { \set {x_1,\ldots,x_n} \setminus \set {x_i} } \, x_j^{k-1} = \map {p_i} {x_j} $ Apply [[Viete's Formulas]] to degree $n-1$ monic polynomial $\map {p_i} {\mathbf {u} }$: :$(3)\quad \sum_{k = 1}^{n} \paren {-1}^{n-k} e_{n-k} \paren { \set {x_1,\ldots,x_n} \setminus \set {x_i} } \, {\mathbf {u} }^{k-1} = \map {p_i} {\mathbf {u} } $ Substitute ${\mathbf {u} } = x_j$ into (3), proving (2) holds. Then (2) and (1) hold, proving $EV=P$. Assume $\set {x_1,\ldots,x_n}$ is a set of distinct values. Then $\det \paren {P}$ is nonzero. By [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], $P$ has an inverse $P^{-1}$. Multiply $EV=P$ by $P^{-1}$, then: :$V^{-1} = P^{-1} E\quad$ [[Left or Right Inverse of Matrix is Inverse]] Similarly, $D^{-1}$ exists provided $\set {x_1,\ldots,x_n}$ is a set of nonzero values. Then $V_n^{-1} = D^{-1} V^{-1}$ by Lemma 1. {{qed|lemma}} '''Proof of the Theorem''' Assume $\set {x_1,\ldots,x_n}$ is a set of distinct values. Let $d_{ij}$ denote the entries in $V^{-1}$. Then: {{begin-eqn}} {{eqn | l = V^{-1} | r = P^{-1} E | c = Lemma 2 }} {{eqn | l = d_{ij} | r = \dfrac{ E_{ij} }{\map {p_i} {x_i} } | c = [[Definition:Matrix Product (Conventional)|Matrix multiply]] }} {{eqn | r = \dfrac { \displaystyle \paren { -1 }^{n-j} \, e_{n-j} \paren { \set {x_1,\ldots,x_n} \setminus \set {x_i} } } { \displaystyle \map {p_i} {x_i} } | c =Definition 1 }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | n = 4 | l = d_{ij} | r = \paren { -1 }^{n-j} \, \dfrac { \displaystyle \sum_{ \substack { 1 \mathop \le m_1 \mathop \lt \cdots \mathop \lt m_{n-j} \mathop \le n \\ m_1,\ldots,m_{n-j} \mathop \neq i } } x_{m_1} \cdots x_{m_{n-j} } } { \displaystyle \prod_{ \substack { 1 \mathop \le m \mathop \le n \\ m \mathop \neq i } } \paren { x_i - x_m } } | c = [[Definition:Elementary Symmetric Function]] $e_m \paren {U}$ Definition 1, equation for $\map {p_i} {x}$ }} {{end-eqn}} Assume $\set {x_1,\ldots,x_n}$ is a set of distinct values, all nonzero. Let $b_{ij}$ denote the entries in $V_n^{-1}$. Then: {{begin-eqn}} {{eqn | l = V_n^{-1} | r = D^{-1} V^{-1} | c = Lemma 1 }} {{eqn | l = b_{ij} | r = \dfrac{ 1 }{ x_i} \, d_{ij} | c = [[Definition:Matrix Product (Conventional)|Matrix multiply]] }} {{end-eqn}} Factor $\paren {-1}^{n-1}$ from the denominator of (4) to agree with Knuth (1997). {{qed}}	1
Let $V_n = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1} \end{vmatrix}$. Start by replacing number $x_n$ in $V_n$ with the unknown $x$. Thus $V_n$ is made into a function of $x$. :$P \left({x}\right) = \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \\ 1 & x & x^2 & \cdots & x^{n-2} & x^{n-1} \end{vmatrix}$. Let $x$ equal a value from the set $\set {x_1,\ldots,x_{n-1} }$. Then determinant $\map P {x}$ has [[Square Matrix with Duplicate Rows has Zero Determinant|equal rows]], giving: {{begin-eqn}} {{eqn | l = \map P {x} | r = 0 | c = for $x = x_1, \ldots, x_{n-1}$ }} {{end-eqn}} Perform row expansion by the last row. Then $P \left({x}\right)$ is seen to be a polynomial of degree $n-1$: :$P \left({x}\right) = \begin{vmatrix} x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ x_3 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \end{vmatrix} + \begin{vmatrix} 1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\ 1 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\ 1 & x_3^2 & \cdots & x_3^{n-2} & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} & x_{n-1}^{n-1} \end{vmatrix}x \ \ + \ \ \cdots \ \ + \ \ \begin{vmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-2} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-2} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-2} \end{vmatrix}x^{n-1}$. <!--- We can see that statement $P \left({x}\right) = 0$ holds true for all $x_1, x_2, \cdots x_{n-1}$ because if $x=x_1, x_2, \cdots x_{n-1}$ starting determinant would have two equal rows and by [[Square Matrix with Duplicate Rows has Zero Determinant]] would $V_n = 0$. {{explain|The meaning of the above statement needs to be made clearer, and the logical sense turned into a linear flow.}} Moved statement higher up so it reads linearly. Nov 6 2019 GBGustafson Also fixed one typo $x_{n-1} \to x^{n-1}$ and changed "returning" to "evaluating at". The cofactor expansion is missing checkerboard signs but the minors are correct. Not fixed. ---> By the [[Polynomial Factor Theorem]]: :$P \left({x}\right) = C \left({x - x_1}\right) \left({x - x_2}\right) \dotsm \left({x - x_{n-1} }\right)$ where $C$ is the leading coefficient (with $x^{n-1}$ power). Thus: :$P \left({x}\right) = V_{n-1} \left({x - x_1}\right) \left({x - x_2}\right) \dotsm \left({x - x_{n-1} }\right)$ which by evaluating at $x = x_n$ gives: :$V_n = V_{n-1} \left({x_n - x_1}\right) \left({x_n - x_2}\right) \dotsm \left({x_n - x_{n-1} }\right)$ Repeating the process: {{begin-eqn}} {{eqn | l = V_n | r = \prod_{1 \mathop \le i \mathop < n} \left({x_n - x_i}\right) V_{n-1} | c = }} {{eqn | r = \prod_{1 \mathop \le i \mathop < n} \left({x_n - x_i}\right) \prod_{1 \mathop \le i \mathop < n-1} \left({x_{n-1} - x_i}\right) V_{n-2} | c = }} {{eqn | r = \dotsm | c = }} {{eqn | r = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right) | c = }} {{end-eqn}} which establishes the solution. {{qed}}	1
By definition of [[Definition:Equivalence of Norms|equivalent norms]]: :$\exists m,M \in \R_{> 0} : m \le M : \forall x \in X: m \norm x_b \le \norm x_a \le M \norm x_b$ Since $U$ is [[Definition:Open Set in Normed Vector Space|open]] in $M_a$: :$\forall x \in U : \exists \epsilon_a \in \R_{> 0} : \map {B_{\epsilon_a}} x \subseteq U$ where $\map {B_{\epsilon_a}} x$ stands for an [[Definition:Open Ball in Normed Vector Space|open ball]], defined as: :$\map {B_{\epsilon_a}} x := \set {\forall y \in X : \norm {x - y}_a < \epsilon_a}$ Define an [[Definition:Open Ball in Normed Vector Space|open ball]] $\map {B_{\epsilon_b}} x$ as: :$\map {B_{\epsilon_b}} x := \set {\forall y \in X : \norm {x - y}_b < \epsilon_b}$ Then we have that: :$m \norm {x - y}_b \le \norm {x - y}_a < \epsilon_a$ So far, $\epsilon_b$ was unspecified. Define $\epsilon_b := \dfrac {\epsilon_a} m$. So: :$\forall x \in U : y \in \map {B_{\epsilon_a}} x \implies y \in \map {B_{\epsilon_b}} x$ with $\epsilon_a = m \epsilon_b$. Hence: :$\forall x \in U : \exists \epsilon_b \in \R_{> 0} : \map {B_{\epsilon_b}} x \subseteq U$ {{qed}}	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_F$ and whose [[Definition:Unity of Field|unity]] is $1_F$. Let $\struct {\mathbf V, +, \circ}_F$ be a [[Definition:Vector Space|vector space]] over $F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then: :$\forall \mathbf v \in \mathbf V: -\mathbf v = -1_F \circ \mathbf v$	1
Let $\struct {G, +}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $\mathbf 0$. Let $\struct {G, +, \circ}_K$ be a [[Definition:Vector Space|$K$-vector space]]. Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be a [[Definition:Sequence|sequence]] of distinct [[Definition:Zero Vector|non-zero vectors]] of $G$. Then $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ is [[Definition:Linearly Dependent Sequence|linearly dependent]] {{iff}}: :$\exists p \in \closedint 2 n: a_p$ is a [[Definition:Linear Combination|linear combination]] of $\sequence {a_k}_{1 \mathop \le k \mathop \le p - 1}$	1
Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]]. Then $\sequence {a_n}$ is an [[Definition:Ordered Basis|ordered basis]] of $G$ {{Iff}}: :For every $x \in G$ there exists [[Definition:Exactly One|one and only one]] [[Definition:Sequence|sequence]] $\sequence {\lambda_n}$ of [[Definition:Scalar (Module)|scalars]] such that $\displaystyle x = \sum_{k \mathop = 1}^n \lambda_k a_k$.	1
We will prove that all [[Definition:Norm on Vector Space|norms]] are [[Definition:Equivalence of Norms|equivalent]] to $\norm {\, \cdot \,}_2$. By definition, two [[Definition:Norm on Vector Space|norms]] are [[Definition:Equivalence of Norms|equivalent]] on $\R^d$ {{iff}}: :$\forall \mathbf x \in \R^d : \exists m, M \in \R_{> 0} : m \norm {\mathbf x}_a \le \norm {\mathbf x}_b \le M \norm {\mathbf x}_a$ === "Less or equal" condition === Let $\set {\mathbf e_1 \dots \mathbf e_n}$ be a [[Definition:Standard Basis/Vector Space|standard basis]] in $\R^d$. We have that each $\mathbf x \in \R^d$ is [[Expression of Vector as Linear Combination from Basis is Unique|uniquely]] expressible as: :$\displaystyle \mathbf x = \sum_{i \mathop = 1}^d x_i \mathbf e_i$ where $x_i$ is a [[Definition:Scalar (Vector Space)|scalar]]. Then: {{begin-eqn}} {{eqn | l = \norm {\sum_{i \mathop = 1}^d x_i \mathbf e_i} | o = \le | r = \sum_{i \mathop = 1}^d \norm {x_i \mathbf e_i} | c = [[Definition:Norm Axioms (Vector Space)|Norm axiom]]: triangle inequality }} {{eqn | r = \sum_{i \mathop = 1}^d \size {x_i} \norm {\mathbf e_i} | c = [[Definition:Norm Axioms (Vector Space)|Norm axiom]]: positive homogeneity }} {{eqn | r = \sqrt {\paren {\sum_{i \mathop = 1}^d \size {x_i} \norm {\mathbf e_i} }^2 } }} {{eqn | o = \le | r = \sqrt{\sum_{i \mathop = 1}^d \norm {\mathbf e_i}^2} \sqrt {\sum_{j \mathop = 1}^d \size {x_j}^2 } | c = [[Cauchy-Schwarz Inequality]] }} {{eqn | o = \le | r = \sqrt{\sum_{i \mathop = 1}^d \norm {\mathbf e_i}^2} \norm {\mathbf x}_2 | c = {{Defof|P-Norm|$p$-norm}} }} {{eqn | r = M \norm {\mathbf x}_2 | c = Define $\displaystyle M := \sqrt{\sum_{i \mathop = 1}^d \norm {\mathbf e_i}^2}$}} {{end-eqn}} By definition of [[Definition:Norm on Vector Space|norm]], its [[Definition:Image|image]] is the [[Definition:Set|set]] of [[Definition:Non-Negative Reals|non-negative real numbers]]: $M \in \R_{\ge 0}$. Hence: :$\forall \mathbf x \in \R^d : \exists M \in \R_{\ge 0} : \norm {\mathbf x} \le M \norm {\mathbf x}_2$ {{qed|lemma}} === Existence of $m$ === Let $K := \set {\mathbf y \in \R^d : \norm {\mathbf y}_2 = 1}$ be a [[Definition:Unit Sphere/Normed Vector Space|unit sphere]] in $\struct {\R^d, \norm {\, \cdot \,}_2}$. By [[Unit Sphere is Closed in Normed Vector Space]], $K$ is [[Definition:Closed Set of Normed Vector Space|closed]] in $\struct {\R^d, \norm{\, \cdot \,}_2}$. By [[Definition:Bounded Normed Vector Space|definition]], $K$ is [[Definition:Bounded Normed Vector Space|bounded]] in $\struct {\R^d, \norm{\, \cdot \,}_2}$. Hence, by the [[Heine-Borel Theorem/Normed Vector Space|Heine-Borel theorem]], $K$ is a [[Definition:Compact Normed Vector Space|compact set]]. By [[Norm on Vector Space is Continuous Function]], the [[Definition:Mapping|map]] $\norm {\, \cdot \,} : K \to \R_{\ge 0}$ is [[Definition:Continuous Mapping|continuous]] from $\struct {K, \norm {\, \cdot \,}_2}$ to $\struct {\R_{\ge 0}, \size {\, \cdot \,}}$: :$\forall \mathbf y_1, \mathbf y_2 \in K : \size {\norm {\mathbf y_1} - \norm {\mathbf y_2}} \le \norm {\mathbf y_1 - \mathbf y_2} \le M \norm {\mathbf y_1 - \mathbf y_2}_2$ By [[Weierstrass Extreme Value Theorem|Weierstrass theorem]], $\norm {\, \cdot \,} : K \to \R_{\ge 0}$ attains a [[Definition:Minimum Value of Real Function|minimum value]] $m$ for some $\mathbf y \in K$. Suppose $m = 0$. Then $\norm {\mathbf y} = 0$. By the [[Definition:Norm Axioms (Vector Space)|norm axiom]] of positive definiteness, $\mathbf y = 0$. Then $\mathbf y \notin K$. Hence, $m \ne 0$. So $m > 0$. Furthermore: :$\forall \mathbf y \in \R^d : \norm {\mathbf y}_2 = 1 : \norm {\mathbf y} \ge m $ {{qed|lemma}} === "Greater or equal" condition === Suppose $\mathbf x = 0$. Then we have [[Definition:Equality|equality]]. Suppose $\mathbf x \ne 0$. Let $\displaystyle \mathbf y = \frac {\mathbf x} {\norm {\mathbf x}_2}$. We have that $\norm {\mathbf y}_2 = 1$ and $\mathbf y \in K$. Then: {{begin-eqn}} {{eqn | l = m | o = \le | r = \norm {\mathbf y} }} {{eqn | r = \norm {\frac {\mathbf x} {\norm {\mathbf x}_2} } }} {{eqn | r = \frac {\norm {\mathbf x} } {\norm {\mathbf x}_2} }} {{end-eqn}} This implies that: :$m \norm {\mathbf x}_2 \le \norm {\mathbf x}$ Therefore: :$\forall \mathbf x \in \R^d : \exists m, M : m \norm {\mathbf x}_2 \le \norm {\mathbf x} \le M \norm {\mathbf x}_2$ By definition, all [[Definition:Norm on Vector Space|norms]] are [[Definition:Equivalence of Norms|equivalent]] to $\norm{\, \cdot \,}_2$. By [[Norm Equivalence is Equivalence]], the [[Definition:Equivalence Relation|equivalence relation]] $\norm {\, \cdot \,} \sim \norm {\, \cdot \,}_2$ is [[Definition:Transitive Relation|transitive]]. Thus, all [[Definition:Norm on Vector Space|norms]] are [[Definition:Equivalence of Norms|equivalent]]. {{qed}}	1
Let $d_1$ and $d_2$ be [[Definition:Topologically Equivalent Metrics|topologically equivalent metrics]]. Then: :$d_1$ and $d_2$ are [[Definition:Equivalent Metrics|convergently equivalent metrics]].	1
By [[P-Product Metric Induces Product Topology|$p$-Product Metric Induces Product Topology]] and [[Continuous Mapping is Continuous on Induced Topological Spaces]], it suffices to consider the case $p = \infty$. Let $\tuple {x_0, y_0} \in R \times R$. Let $\epsilon > 0$ be given. Let $\delta = \min \set {\dfrac \epsilon {1 + \norm {y_0} + \norm {x_0} }, 1}$ Since $1 + \norm {y_0} + \norm {x_0} > 0$ then $\delta > 0$ Let $\tuple {x,y} \in R \times R$ such that: :$\map {d_\infty} {\tuple {x, y}, \tuple{x_0, y_0} } < \delta$ By the definition of the [[Definition:P-Product Metric|$p$-product metric $d_\infty$]]: :$\max \set {\map d {x, x_0}, \map d {y, y_0}} < \delta$ or equivalently: :$\map d {x, x_0} < \delta$ :$\map d {y, y_0} < \delta$ Then: {{begin-eqn}} {{eqn | l = \norm y | r = \norm {y - y_0 + y_0} | c = }} {{eqn | o = \le | r = \norm {y - y_0} + \norm {y_0} | c = {{NormAxiom|3}} }} {{eqn | o = \le | r = \map d {y, y_0} + \norm {y_0} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | o = < | r = \delta + \norm {y_0} | c = }} {{eqn | o = \le | r = 1 + \norm {y_0} | c = }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \map d {x y, x_0 y_0} | r = \norm {x y - x_0 y_0} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm {x y - x_0 y + x_0 y - x_0 y_0} | c = }} {{eqn | o = \le | r = \norm {x y - x_0 y} + \norm {x_0 y - x_0 y_0} | c = {{NormAxiom|3}} }} {{eqn | o = \le | r = \norm {x - x_0} \norm y + \norm {x_0} \norm {y - y_0} | c = {{NormAxiom|2}} }} {{eqn | o = \le | r = \map d {x, x_0} \norm y + \norm {x_0} \map d {y, y_0} | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | o = < | r = \delta \norm y + \norm {x_0} \delta | c = }} {{eqn | o = \le | r = \delta \paren {\norm y + \norm {x_0} } | c = }} {{eqn | o = < | r = \delta \paren {1 + \norm {y_0} + \norm {x_0} } | c = }} {{eqn | o = \le | r = \dfrac \epsilon {1 + \norm {y_0} + \norm {x_0} } \paren {1 + \norm {y_0} + \norm {x_0} } | c = }} {{eqn | r = \epsilon }} {{end-eqn}} We have that $\tuple {x_0, y_0}$ and $\epsilon$ are arbitrary. Hence, by the definition of [[Definition:Continuous Mapping (Metric Space)|continuity]], the [[Definition:Mapping|mapping]]: :$* : \struct {R \times R, d_\infty} \to \struct {R, d}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]]. {{qed}}	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_K$ and [[Definition:Unity of Field|unity]] is $1_K$. Let $\SL {n, K}$ be the [[Definition:Special Linear Group|special linear group of order $n$ over $K$]]. Then $\SL {n, K}$ is not an [[Definition:Abelian Group|abelian group]].	1
By [[Euclid's Lemma for Prime Divisors]]: :$p \nmid k x_1^{k - 1}$ Hence: :$k x_1^{k - 1} \not \equiv 0 \mod p$ The [[Definition:Formal Derivative of Polynomial|formal derivative]] $\map {f'} X \in \Z \sqbrk X$ of $\map f X$ is by definition: :$k X^{k - 1}$ Then: :$\map {f'} {x_1} = k x_1^{k - 1} \not \equiv 0 \pmod p$ {{qed}} [[Category:P-adic Norm not Complete on Rational Numbers]] nvzcpl2u0gv7vtca4wdgk9kegsflq64	1
From [[Dot Product Operator is Bilinear]]: :$\paren {c \mathbf u + \mathbf v} \cdot \mathbf w = c \paren {\mathbf u \cdot \mathbf w} + \paren {\mathbf v \cdot \mathbf w}$ Setting $c = 1$ yields the result. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let its [[Definition:Ring Zero|zero]] be denoted by $0_R$. Let $\norm {\,\cdot\,}$ be a [[Definition:Multiplicative Norm on Ring|multiplicative norm]] on $R$. Then $R$ has no [[Definition:Proper Zero Divisor|proper zero divisors]]. That is: :$\forall x, y \in R^*: x \circ y \ne 0_R$ where $R^*$ is [[Definition:Ring Less Zero|defined as $R \setminus \set {0_R}$]].	1
Let $K$ be a [[Definition:Field (Abstract Algebra)|field]] whose [[Definition:Field Zero|zero]] is $0_K$ and [[Definition:Unity of Field|unity]] is $1_K$. Let $\SL {n, K}$ be the [[Definition:Special Linear Group|special linear group of order $n$ over $K$]]. Then $\SL {n, K}$ is a [[Definition:Subgroup|subgroup]] of the [[Definition:General Linear Group|general linear group]] $\GL {n, K}$.	1
Let $V$ be a [[Definition:Trivial Vector Space|nontrivial]] [[Definition:Finite Dimensional Vector Space|finite dimensional]] [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $A: V \to V$ be a [[Definition:Linear Operator|linear operator]] of $V$. Then the [[Definition:Determinant of Linear Operator|determinant]] $\det A$ of $A$ is [[Definition:Well Defined|well defined]], that is, does not depend on the choice of a [[Definition:Basis of Vector Space|basis]] of $V$.	1
$\operatorname{N} \left({\mathbf A}\right) \subseteq \R^n$, by construction. We have: * $\mathbf 0 \in \operatorname{N} \left({\mathbf A}\right)$, from [[Null Space Contains Zero Vector]] * $\forall \mathbf v, \mathbf w \in \operatorname{N} \left({\mathbf A}\right): \mathbf v + \mathbf w \in \operatorname{N} \left({\mathbf A}\right)$, from [[Null Space Closed under Vector Addition]] * $\forall\mathbf v \in \operatorname{N} \left({\mathbf A}\right), \lambda \in \R: \lambda \mathbf v \in \operatorname{N} \left({\mathbf A}\right)$, from [[Null Space Closed under Scalar Multiplication]] The result follows from [[Vector Subspace of Real Vector Space]]. {{qed}}	1
Let $\mathbf T_n$ be an [[Definition:Upper Triangular Matrix|upper triangular matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf T_n}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf T_n$. Then $\map \det {\mathbf T_n}$ is equal to the product of all the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf T_n$. That is: :$\displaystyle \map \det {\mathbf T_n} = \prod_{k \mathop = 1}^n a_{k k}$	1
Let $\mathbf A$ be a [[Definition:Square Matrix|square matrix]] over $R$ with two identical [[Definition:Column of Matrix|columns]]. Let $\mathbf A^\intercal$ denote the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Then $\mathbf A^\intercal$ has two identical [[Definition:Row of Matrix|rows]]. Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A} | r = \map \det {\mathbf A^\intercal} | c = [[Determinant of Transpose]] }} {{eqn | r = 0 | c = [[Square Matrix with Duplicate Rows has Zero Determinant]] }} {{end-eqn}} {{qed}}	1
The following [[Definition:Algorithm|algorithm]] generates a [[Definition:Sequence|sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] which convert $\mathbf A$ to $\mathbf B$. Let $\mathbf A' = \sqbrk {a'}_{m n}$ denote the state of $\mathbf A$ after having processed the latest step. After each step, an implicit step can be included that requires that the form of $\mathbf A'$ is inspected to see if it is in the form $\mathbf B$, and if so, terminating the algorithm, but this is not essential. :$(1): \quad$ Are all [[Definition:Element of Matrix|elements]] in the first [[Definition:Column of Matrix|column]] of $\mathbf A$ equal to $0$? :::If so, there is nothing to do, and the required [[Definition:Row Operation|row operation]] is the [[Definition:Unit Matrix|unit matrix]] $\mathbf I_m$. :::Otherwise, move on to step $(2)$. :$(2): \quad$ Is [[Definition:Element of Matrix|element]] $a_{1 1}$ equal to $0$? :::If so: ::::$\text (a): \quad$ find the smallest $k$ such that [[Definition:Row of Matrix|row]] $k$ of $\mathbf A$ such that $a_{k 1} \ne 0$ ::::$\text (b): \quad$ use the [[Definition:Elementary Row Operation|elementary row operation]] $r_1 \leftrightarrow r_k$ which will result $a'_{1 1} = a_{k 1}$ and $a'_{k 1} = 0$. :Move on to step $(3)$. :$(3): \quad$ Is [[Definition:Element of Matrix|element]] $a'_{1 1}$ equal to $1$? :::If so, use the [[Definition:Elementary Row Operation|elementary row operation]] $r_1 \to \lambda r_1$ where $\lambda = \dfrac 1 {a'_{1 1} }$, which will result $a'_{1 1} = 1$. :Move on to step $4$ :$(4): \quad$ For each [[Definition:Row of Matrix|row]] $j$ from $2$ to $m$, do the following: :::Is $a_{j 1} \ne 0$? ::::If so, use the [[Definition:Elementary Row Operation|elementary row operation]] $r_j \leftrightarrow r_j + \mu r_1$, where $\mu = -\dfrac {a'_{j 1} } {a'{1 1} }$, which will result in $a'_{j 1} = 0$. This will result in an [[Definition:Matrix|$m \times n$ matrix]] in the required form. Exercising the above [[Definition:Algorithm|algorithm]] will have generated a [[Definition:Sequence|sequence]] of [[Definition:Elementary Row Operation|elementary row operations]] $e_1, e_2, \ldots, e_t$. For each $e_k$ we create the [[Definition:Elementary Row Matrix|elementary row matrix]] $\mathbf E_k$. We then assemble the [[Definition:Matrix Product (Conventional)|matrix product]]: :$\mathbf R := \mathbf E_t \mathbf E_{t - 1} \mathbf E_{t - 2} \dotsm \mathbf E_2 \mathbf E_1$ From [[Row Operation is Equivalent to Pre-Multiplication by Product of Elementary Matrices]], $\mathbf R$ is the resulting [[Definition:Matrix|$m \times m$ matrix]] corresponding to the [[Definition:Row Operation|row operation]] which is used to convert $\mathbf A$ to $\mathbf B$. {{qed}}	1
Let $z = a + b i$. Then: {{begin-eqn}} {{eqn | l = \cmod z | r = \cmod {a + b i} | c = Definition of $z$ }} {{eqn | r = \sqrt {a^2 + b^2} | c = {{Defof|Complex Modulus}} }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \cmod {\overline z} | r = \cmod {\overline {a + b i} } | c = Definition of $z$ }} {{eqn | r = \cmod {a - b i} | c = {{Defof|Complex Conjugate}} }} {{eqn | r = \cmod {a + \paren {- b} i} | c = }} {{eqn | r = \sqrt {a^2 + \paren {- b}^2} | c = {{Defof|Complex Modulus}} }} {{eqn | r = \sqrt {a^2 + b^2} | c = }} {{eqn | r = \cmod z | c = }} {{end-eqn}} {{qed}}	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]] with [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]] $\norm {\, \cdot \,}$. Let $\struct {S, \norm {\, \cdot \,}_S }$ be a [[Definition:Normed Division Subring|normed division subring]] of $R$. Then: :$\norm {\, \cdot \,}_S$ is a [[Definition:Non-Archimedean Division Ring Norm|non-archimedean norm]].	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A3}} that the [[Definition:Identity Element|identity element]] of [[Definition:Ring Addition|ring addition]] is the [[Definition:Ring Zero|ring zero]] $0_R$. The result then follows directly from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
Suppose $\left\{{a_1, a_2, \ldots, a_n}\right\}$ is a [[Definition:Linearly Independent Set|linearly independent set]]. Then by [[Subset of Linearly Independent Set]], any [[Definition:Subset|subset]] $\left\{{b_1, b_2, \ldots, b_m}\right\}$ of $\left\{{a_1, a_2, \ldots, a_n}\right\}$ must itself be [[Definition:Linearly Independent Set|linearly independent]]. Thus if $\left\{{b_1, b_2, \ldots, b_m}\right\}$ is [[Definition:Linearly Dependent Set|linearly dependent]], then so must $\left\{{a_1, a_2, \ldots, a_n}\right\}$ be. {{qed}}	1
=== Necessary Condition === Let both $\mathbf A$ and $\mathbf B$ be [[Definition:Invertible Matrix|invertible]]. By [[Matrix is Invertible iff Determinant has Multiplicative Inverse]]: :$\map \det {\mathbf A} \ne 0$ and $\map \det {\mathbf B} \ne 0$ where $\map \det {\mathbf A}$ denotes the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. By [[Determinant of Matrix Product]]: :$\map \det {\mathbf A} \map \det {\mathbf B} = \map \det {\mathbf A \mathbf B}$ Thus as both $\map \det {\mathbf A} \ne 0$ and $\map \det {\mathbf B} \ne 0$, it follows that: :$\map \det {\mathbf A \mathbf B} = \ne 0$ Hence by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], $\map \det {\mathbf A \mathbf B}$ is [[Definition:Invertible Matrix|invertible]]. === Sufficient Condition === Let $\mathbf A \mathbf B$ be [[Definition:Invertible Matrix|invertible]]. {{AimForCont}} it is not the case that both $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible]]. Then by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], either: :$\map \det {\mathbf A} = 0$ or: :$\map \det {\mathbf B} = 0$ By [[Determinant of Matrix Product]]: :$\map \det {\mathbf A} \map \det {\mathbf B} = \map \det {\mathbf A \mathbf B}$ and so: :$\map \det {\mathbf A \mathbf B} = 0$ Hence by [[Matrix is Invertible iff Determinant has Multiplicative Inverse]], $\map \det {\mathbf A \mathbf B}$ is not [[Definition:Invertible Matrix|invertible]]. This [[Definition:Contradiction|contradicts]] the assumption that $\mathbf A \mathbf B$ is [[Definition:Invertible Matrix|invertible]]. Hence by [[Proof by Contradiction]] it follows that both $\mathbf A$ and $\mathbf B$ are [[Definition:Invertible Matrix|invertible]]. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $A \in B \left({H}\right)$ be a [[Definition:Bounded Linear Operator|bounded linear operator]]. Let $\begin{pmatrix} W & X \\ Y & Z \end{pmatrix}$ be the [[Definition:Matrix Notation (Bounded Linear Operator)|matrix notation]] for $A$ with respect to $M$. Let $M$ be a [[Definition:Closed Linear Subspace|closed linear subspace]] of $H$; denote by $P$ the [[Definition:Orthogonal Projection|orthogonal projection]] on $M$. Then the following three statements are equivalent: :$(1): \qquad M$ is an [[Definition:Invariant Subspace|invariant subspace]] for $A$ :$(2): \qquad PAP = AP$ :$(3): \qquad Y = 0$	1
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]] which is not a [[Definition:Field (Abstract Algebra)|field]]. Then $\struct {D, +, \circ}$ has an [[Definition:Infinite Set|infinite number]] of [[Definition:Distinct Elements|distinct]] [[Definition:Ideal of Ring|ideals]].	1
Let $\hat H$ be a Hermitian operator on an [[Definition:Inner Product Space|inner product space]] $V$ over the [[Definition:Field (Abstract Algebra)|field]] of [[Definition:Complex Number|complex numbers]], $\C$. That is, $\hat H = \hat H^\dagger$. Then for an eigenvector $\left\vert{x}\right\rangle \in V, \left\vert{x}\right\rangle \ne \left\vert{0}\right\rangle$ and eigenvalue $\lambda \in \C$: :$\left.{\hat H}\middle\vert{x}\right\rangle = \left.{\lambda}\middle\vert{x}\right\rangle$ We know for a general operator $\hat A$ on $V$, the following holds: :$\forall \left\vert{x}\right\rangle, \left\vert{y}\right\rangle \in V: \left\langle{x}\middle\vert{\hat A}\middle\vert{y}\right\rangle = \left\langle{y}\middle\vert{\hat A^\dagger}\middle\vert{x }\right\rangle^*$ where $^*$ denotes the [[Definition:Complex Conjugate|complex conjugate]]. Noting $\hat H = \hat H^\dagger$ gives: :$\left\langle{x}\middle\vert{\hat H}\middle\vert{y}\right\rangle = \left\langle{y}\middle\vert{\hat H}\middle\vert{x}\right\rangle^*$ Now we compute: {{begin-eqn}} {{eqn | l = \left\langle{x}\middle\vert{\hat H}\middle\vert{x}\right\rangle | r = \left\langle{x}\middle\vert{ \left({\hat H}\middle\vert{x}\right\rangle}\right) | c = }} {{eqn | r = \left\langle{x}\middle\vert{\lambda}\middle\vert{x}\right\rangle | c = }} {{eqn | r = \lambda \left\langle{x}\middle\vert{x}\right\rangle | c = }} {{end-eqn}} Using our previous result: {{begin-eqn}} {{eqn | l = \left\langle{x}\middle\vert{\hat H}\middle\vert{x}\right\rangle | r = \left\langle{x}\middle\vert{\hat H}\middle\vert{x}\right\rangle^* | c = }} {{eqn | r = \left({\lambda \left\langle{x}\middle\vert{x}\right\rangle}\right)^* | c = }} {{end-eqn}} Equating the two previous equations gives: :$\lambda \left\langle{x}\middle\vert{x}\right\rangle = \left({\lambda \left\langle{x}\middle\vert{x}\right\rangle}\right)^*$ Recalling the conjugate symmetry property of the [[Definition:Inner Product|inner product]], we can see that: :$\left\langle{x}\middle\vert{x}\right\rangle = \left\langle{x}\middle\vert{x}\right\rangle^*$ which is true {{iff}} $\left\langle{x}\middle\vert{x}\right\rangle \in \R$. So :$\lambda \left\langle{x}\middle\vert{x}\right\rangle = \lambda^* \left\langle{x}\middle\vert{x}\right\rangle$ and so: :$\lambda = \lambda^*$ Therefore $\lambda \in \R$. {{qed}} [[Category:Linear Algebra]] fvvr0rwh5xnun62ea9bf3lt0ll3idue	1
Let $\mathbf u \cdot \mathbf u = 0$. Then: {{begin-eqn}} {{eqn | l = 0 | r = \norm {\mathbf u }^2 \cos \angle \mathbf u, \mathbf u | c = {{Defof|Dot Product|index = 2}} }} {{eqn | r = \norm {\mathbf u}^2 \cos 0 | c = }} {{eqn | r = \norm {\mathbf u}^2 | c = }} {{end-eqn}} The only way for this to happen is if: :$\norm {\mathbf u} = 0$ which implies: :$\mathbf u = \mathbf 0$ Now suppose $\mathbf u = \mathbf 0$. Then: {{begin-eqn}} {{eqn | l = \mathbf u \cdot \mathbf u | r = \mathbf 0 \cdot \mathbf 0 | c = }} {{eqn | r = \norm {\mathbf 0}^2 \cos \angle \mathbf 0, \mathbf 0 | c = }} {{eqn | r = 0 | c = }} {{end-eqn}} {{qed}}	1
Let $X$ be a [[Definition:Non-Empty Set|non-empty set]]. Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Field (Abstract Algebra)|field]] (or [[Definition:Division Ring|division ring]]) $K$. Let $V^X$ denote the [[Definition:Set of All Mappings|set of all mappings]] from $X$ to $V$. Let $+$ denote [[Definition:Pointwise Addition of Mappings|pointwise addition]] on $V^X$. Let $\circ$ denote [[Definition:Pointwise Scalar Multiplication of Mappings|pointwise ($K$)-scalar multiplication]] on $V^X$. Then $\left({V^X, +, \circ}\right)_K$ is a [[Definition:Vector Space|vector space]] over $K$.	1
Let $R$ be a [[Definition:Principal Ideal Domain|principal ideal domain]]. Let $p \in R$ such that $p \ne 0$ and $p$ is not a [[Definition:Unit of Ring|unit]]. Then there exist [[Definition:Irreducible Element of Ring|irreducible elements]] $p_1, \ldots, p_n$ such that $p = p_1 \cdots p_n$.	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $\cmod z$ be the [[Definition:Modulus of Complex Number|modulus]] of $z$. Then: : $\cmod {z_1 + z_2} \ge \cmod {z_1} - \cmod {z_2}$	1
Let $m$ be a [[Definition:Positive Integer|positive integer]]. Then the [[Definition:Euclidean Space|Euclidean space]] $\R^m$, along with the [[Definition:Euclidean Norm|Euclidean norm]], forms a [[Definition:Banach Space|Banach space]] over $\R$.	1
Let $\left({G, +_G, \circ}\right)_R$ be an [[Definition:Module|$R$-module]]. Then the [[Definition:Null Module|null module]]: :$\left({\left\{{e_G}\right\}, +_G, \circ}\right)_R$ is a [[Definition:Submodule|submodule]] of $\left({G, +_G, \circ}\right)_R$.	1
Let $R$ be a [[Definition:Ring with Unity|ring with unity]]. Let $M$ be a [[Definition:Free Module|free $R$-module]] of [[Definition:Dimension (Linear Algebra)|finite dimension]] $n>0$. Let $\mathcal A$ and $\mathcal B$ be [[Definition:Ordered Basis|ordered bases]] of $M$. Let $\mathbf M_{\mathcal A, \mathcal B}$ be the [[Definition:Change of Basis Matrix|change of basis matrix]] from $\mathcal A$ to $\mathcal B$. Let $f : M\times M \to R$ be a [[Definition:Bilinear Form|bilinear form]]. Let $\mathbf M_{f, \mathcal A}$ be its [[Definition:Relative Matrix of Bilinear Form|matrix relative to]] $\mathcal A$. Then its [[Definition:Relative Matrix of Bilinear Form|matrix relative to]] $\mathcal B$ equals: :$\mathbf M_{f, \mathcal B} = \mathbf M_{\mathcal A, \mathcal B}^\intercal \mathbf M_{f, \mathcal A} \mathbf M_{\mathcal A, \mathcal B}$	1
A '''linear operator''' on a [[Definition:Vector Space|vector space]] is a [[Definition:Linear Transformation|linear transformation]] from a [[Definition:Vector Space|vector space]] into itself.	1
Let $\alpha_1, \alpha_2, \ldots, \alpha_n$ be a [[Definition:Generator of Vector Space|generator]] of $V$. Let $\xi_1, \xi_2, \ldots, \xi_r$ be a [[Definition:Linearly Independent Set|linearly independent set]] of [[Definition:Element|elements]] of $V$. Hence the [[Definition:Sequence|sequence]] $\sequence {\xi_1, \alpha_1, \alpha_2, \ldots, \alpha_n}$ is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]] of [[Definition:Element|elements]] of $V$. One of these [[Definition:Element|elements]], which cannot be $\xi_1$, is a [[Definition:Linear Combination of Subset|linear combination]] of the preceding [[Definition:Element|elements]]. Let this [[Definition:Element|elements]] be $\alpha_i$. So we can omit $\alpha_i$ from that [[Definition:Sequence|sequence]], and the remaining [[Definition:Set|set]] is still a [[Definition:Generator of Vector Space|generator]] of $V$. Therefore $\xi_2$ is a [[Definition:Linear Combination of Subset|linear combination]] of these. Thus $\sequence {\xi_2, \xi_1, \alpha_1, \alpha_2, \ldots, \alpha_{i - 1}, \alpha {i + 1}, \ldots, \alpha_n}$ is a [[Definition:Linearly Dependent Sequence|linearly dependent sequence]] of [[Definition:Element|elements]] of $V$. Again, one of them is a [[Definition:Linear Combination of Subset|linear combination]] of the preceding [[Definition:Element|elements]]. This cannot be $\xi_2$, as $\xi_2$ has no preceding [[Definition:Element|elements]]. Neither can it be $\xi_1$, as $\xi_1$ and $\xi_2$ are [[Definition:Linearly Independent Set|linearly independent]]. Thus we can omit whichever $\alpha_j$ it is, and we have a new [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$. This consists of $\xi_1$, $\xi_2$ and whichever $n - 2$ of the remaining [[Definition:Element|elements]] of $\set {\alpha_1, \alpha_2, \ldots, \alpha_n}$. After $p$ such steps, we have a [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$ which consists of: :$\xi_1, \xi_2, \ldots, \xi_p$ and $n - p$ of the [[Definition:Element|elements]] of $\set {\alpha_1, \alpha_2, \ldots, \alpha_n}$. {{AimForCont}} suppose $n < r$. Then when $p = n$, the remaining [[Definition:Set|set]] which is a [[Definition:Generator of Vector Space|generator]] of $V$ consists of: :$\xi_1, \xi_2, \ldots, \xi_n$ and there is at least one more [[Definition:Element|element]] $\xi_{n + 1}$. This is a [[Definition:Linear Combination of Subset|linear combination]] of $\set {\xi_1, \xi_2, \ldots, \xi_n}$. But this [[Definition:Contradiction|contradicts]] the supposition that $\set {\xi_1, \xi_2, \ldots, \xi_n, \xi_{n + 1} }$ is a [[Definition:Linearly Independent Set|linearly independent set]]. Hence, by [[Proof by Contradiction]], $n \ge r$. The result follows. {{qed}}	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $\family {M_i}_{i \mathop \in I}$ be an [[Definition:Indexed Family|$I$-indexed family]] of [[Definition:Closed Linear Subspace|closed linear subspaces]] of $H$. Then: :$\ds \bigcap_{i \mathop \in I} M_i^\perp = \paren {\vee \set {M_i : i \in I} }^\perp$ where: :$\perp$ denotes [[Definition:Orthocomplement|orthocomplementation]] :$\vee$ denotes [[Definition:Closed Linear Span|closed linear span]].	1
:$\NN \subsetneq \CC$.	1
Take the version of the [[Definition:Cauchy Matrix|Cauchy matrix]] defined such that $a_{ij} = \dfrac 1 {x_i + y_j}$. Subtract [[Definition:Column of Matrix|column]] $1$ from each of [[Definition:Column of Matrix|column]]s $2$ to $n$. Thus: {{begin-eqn}} {{eqn | l = a_{ij} | o = \gets | r = \frac 1 {x_i + y_j} - \frac 1 {x_i + y_1} | c = }} {{eqn | r = \frac {\left({x_i + y_1}\right) - \left({x_i + y_j}\right)} {\left({x_i + y_j}\right) \left({x_i + y_1}\right)} | c = }} {{eqn | r = \left({\frac {y_1 - y_j}{x_i + y_1} }\right) \left({\frac 1 {x_i + y_j} }\right) | c = }} {{end-eqn}} From [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the [[Definition:Determinant of Matrix|determinant]]. Now: :$1$: extract the factor $\dfrac 1 {x_i + y_1}$ from each [[Definition:Row of Matrix|row]] $1 \le i \le n$ :$2$: extract the factor $y_1 - y_j$ from each [[Definition:Column of Matrix|column]] $2 \le j \le n$. Thus from [[Determinant with Row Multiplied by Constant]] we have the following: :$\displaystyle D_n = \left({\prod_{i \mathop = 1}^n \frac 1 {x_i + y_1}}\right) \left({\prod_{j \mathop = 2}^n y_1 - y_j}\right) \begin{vmatrix} 1 & \dfrac 1 {x_1 + y_2} & \dfrac 1 {x_1 + y_3} & \cdots & \dfrac 1 {x_1 + y_n} \\ 1 & \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ 1 & \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ Now subtract [[Definition:Row of Matrix|row]] $1$ from each of [[Definition:Row of Matrix|row]]s $2$ to $n$. [[Definition:Column of Matrix|Column]] $1$ will go to $0$ for all but the first [[Definition:Row of Matrix|row]]. [[Definition:Column of Matrix|Column]]s $2$ to $n$ will become: {{begin-eqn}} {{eqn | l = a_{ij} | o = \gets | r = \frac 1 {x_i + y_j} - \frac 1 {x_1 + y_j} | c = }} {{eqn | r = \frac {\left({x_1 + y_j}\right) - \left({x_i + y_j}\right)} {\left({x_i + y_j}\right) \left({x_1 + y_j}\right)} | c = }} {{eqn | r = \left({\frac {x_1 - x_i} {x_1 + y_j} }\right) \left({\frac 1 {x_i + y_j} }\right) | c = }} {{end-eqn}} From [[Multiple of Row Added to Row of Determinant]] this will have no effect on the value of the [[Definition:Determinant of Matrix|determinant]]. Now: :$1$: extract the factor $x_1 - x_i$ from each [[Definition:Row of Matrix|row]] $2 \le i \le n$ :$2$: extract the factor $\dfrac 1 {x_1 + y_j}$ from each [[Definition:Column of Matrix|column]] $2 \le j \le n$. Thus from [[Determinant with Row Multiplied by Constant]] we have the following: :$\displaystyle D_n = \left({\prod_{i \mathop = 1}^n \frac 1 {x_i + y_1}}\right) \left({\prod_{j \mathop = 1}^n \frac 1 {x_1 + y_j}}\right) \left({\prod_{i \mathop = 2}^n x_1 - x_i}\right) \left({\prod_{j \mathop = 2}^n y_1 - y_j}\right) \begin{vmatrix} 1 & 1 & 1 & \cdots & 1 \\ 0 & \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ 0 & \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ From [[Determinant with Unit Element in Otherwise Zero Row]], and tidying up the products, we get: :$D_n = \frac {\displaystyle \prod_{i \mathop = 2}^n \left({x_i - x_1}\right) \left({y_i - y_1}\right)} {\displaystyle \prod_{1 \mathop \le i, j \mathop \le n} \left({x_i + y_1}\right) \left({x_1 + y_j}\right)} \begin{vmatrix} \dfrac 1 {x_2 + y_2} & \dfrac 1 {x_2 + y_3} & \cdots & \dfrac 1 {x_2 + y_n} \\ \dfrac 1 {x_3 + y_2} & \dfrac 1 {x_3 + y_3} & \cdots & \dfrac 1 {x_3 + y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n + y_2} & \dfrac 1 {x_n + y_3} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{vmatrix}$ Repeat the process for the remaining [[Definition:Row of Matrix|row]]s and [[Definition:Column of Matrix|column]]s $2$ to $n$. The result follows. {{qed}} A similar process obtains the result for the $a_{ij} = \dfrac 1 {x_i - y_j}$ form.	1
The [[Definition:Absolute Value|absolute value]] is a [[Definition:Norm on Division Ring|norm]] on the [[Definition:Real Number|set of real numbers]] $\R$.	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number|complex numbers]]. Let $z_1 \circ z_2$ denote the [[Definition:Complex Dot Product|(complex) dot product]] of $z_1$ and $z_2$. Then: :$\size {z_1 \circ z_2} = \size {z_2 \circ z_1}$ where $\size {\, \cdot \,}$ denotes the [[Definition:Absolute Value|absolute value function]].	1
A '''system of simultaneous [[Definition:Linear Equation|linear equations]]''' is a set of equations: :$\displaystyle \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$ That is: {{begin-eqn}} {{eqn | l = \beta_1 | r = \alpha_{1 1} x_1 + \alpha_{1 2} x_2 + \cdots + \alpha_{1 n} x_n }} {{eqn | l = \beta_2 | r = \alpha_{2 1} x_1 + \alpha_{2 2} x_2 + \cdots + \alpha_{2 n} x_n }} {{eqn | o = \cdots}} {{eqn | l = \beta_m | r = \alpha_{m 1} x_1 + \alpha_{m 2} x_2 + \cdots + \alpha_{m n} x_n }} {{end-eqn}}	1
Checking in turn each of the critera for [[Definition:Equivalence Relation|equivalence]]: === Reflexive === $\mathbf A = \mathbf{I_n}^{-1} \mathbf A \mathbf{I_n}$ trivially, for all order $n$ [[Definition:Square Matrix|square matrices]] $\mathbf A$. So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetric === Let $\mathbf B = \mathbf P^{-1} \mathbf A \mathbf P$. As $\mathbf P$ is [[Definition:Invertible Matrix|invertible]], we have: {{begin-eqn}} {{eqn | l = \mathbf P \mathbf B \mathbf P^{-1} | r = \mathbf P \mathbf P^{-1} \mathbf A \mathbf P \mathbf P^{-1} | c = }} {{eqn | r = \mathbf{I_n} \mathbf A \mathbf{I_n} | c = }} {{eqn | r = \mathbf A | c = }} {{end-eqn}} So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitive === Let $\mathbf B = \mathbf P_1^{-1} \mathbf A \mathbf P_1$ and $\mathbf C = \mathbf P_2^{-1} \mathbf B \mathbf P_2$. Then $\mathbf C = \mathbf P_2^{-1} \mathbf P_1^{-1} \mathbf A \mathbf P_1 \mathbf P_2$. The result follows from the definition of [[Definition:Invertible Matrix|invertible matrix]], that the product of two invertible matrices is itself invertible. So [[Definition:Matrix Similarity|matrix similarity]] is [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} So, by definition, [[Definition:Matrix Similarity|matrix similarity]] is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf x \times \mathbf x | r = \begin{vmatrix} \mathbf i & \mathbf j & \mathbf k \\ x_i & x_j & x_k \\ x_i & x_j & x_k \\ \end{vmatrix} | c = {{Defof|Vector Cross Product}} }} {{eqn | r = \mathbf 0 | c = [[Square Matrix with Duplicate Rows has Zero Determinant]] }} {{end-eqn}} {{qed}} [[Category:Vector Cross Product]] 1bl292w88tnbfskcppb6mc83yd4zye2	1
Let $C \subseteq S$ be a [[Definition:Countable Set|countable set]]. Since $S$ is [[Definition:Uncountable Set|uncountable]], by [[Uncountable Set less Countable Set is Uncountable]], so is $\relcomp S C$. Thus there exists some point $x \in \relcomp S C$ and $x \ne p$. By [[Clopen Points in Fort Space]], $\set x \in \tau_p$. By [[Empty Intersection iff Subset of Complement]], we have $C \cap \set x = \O$. Therefore $C$ is not [[Definition:Everywhere Dense|everywhere dense]]. Since $C$ is arbitrary, $T$ is not a [[Definition:Separable Space|separable space]]. {{qed}}	1
From: :[[Integers form Ring]] :[[Rational Numbers form Ring]] :[[Real Numbers form Ring]] :[[Complex Numbers form Ring]] the [[Definition:Standard Number System|standard number systems]] $\Z$, $\Q$, $\R$ and $\C$ are [[Definition:Ring (Abstract Algebra)|rings]] whose [[Definition:Zero Element|zero]] is the [[Definition:Zero (Number)|number $0$ (zero)]]. Hence we can apply [[Zero Matrix is Identity for Matrix Entrywise Addition over Ring]]. {{qed|lemma}} The above cannot be applied to the [[Definition:Natural Numbers|natural numbers]] $\N$, as they do not form a [[Definition:Ring (Abstract Algebra)|ring]]. However, from [[Natural Numbers under Addition form Commutative Monoid]], the [[Definition:Algebraic Structure|algebraic structure]] $\struct {\N, +}$ is a [[Definition:Commutative Monoid|commutative monoid]] whose [[Definition:Identity Element|identity]] is [[Definition:Zero (Number)|$0$ (zero)]]. By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' with respect to [[Definition:Addition|addition of numbers]]. The result follows from [[Zero Matrix is Identity for Hadamard Product]]. {{qed}}	1
=== Reflexivity === Let $\norm {\, \cdot \,}$ be a [[Definition:Norm|norm]] on $X$. Then for all $x \in X$ we have that: :$\norm x = 1 \cdot \norm {x}$. Therefore: :$1 \cdot \norm x \le \norm x \le 1 \cdot \norm x$ Hence: :$\norm {\, \cdot \,} \sim \norm {\, \cdot \,}$. {{qed|lemma}} === Symmetry === Suppose, $\norm {\, \cdot \,}_a \sim \norm {\, \cdot \,}_b$. Then: :$\exists m, M \in \R_{> 0} : m \le M : \forall x \in X : m \norm x_b \le \norm x_a \le M \norm x_b$ Consider two [[Definition:Inequality|inequalities]], obtained by [[Definition:Real Division|division]] by $M$ and $m$: :$\dfrac m M \norm x_b \le \dfrac 1 M \norm x_a \le \norm x_b$ :$\norm x_b \le \dfrac 1 m \norm x_a \le \dfrac M m \norm x_b$ Notice, that: :$\dfrac 1 M \norm x_a \le \norm x_b \le \dfrac 1 m \norm x_a$ We have that $m \le M$ implies $\dfrac 1 m \ge \dfrac 1 M$. Define $C := \dfrac 1 m$ and $c := \dfrac 1 M$. Hence, $c \le C$ and: :$\norm {\, \cdot \,}_b \sim \norm {\, \cdot \,}_a$ {{qed|lemma}} === Transitivity === Suppose, $\norm {\, \cdot \,}_a \sim \norm {\, \cdot \,}_b$ and $\norm {\, \cdot \,}_b \sim \norm {\, \cdot \,}_c$. Then: :$\exists m_{a b}, M_{a b} \in \R_{> 0} : m_{a b} \le M_{a b} : \forall x \in X : m_{a b} \norm x_b \le \norm x_a \le M_{a b} \norm x_b$ :$\exists m_{b c}, M_{b c} \in \R_{> 0} : m_{b c} \le M_{b c} : \forall x \in X : m_{b c} \norm x_c \le \norm x_b \le M_{b c} \norm x_c$ Generate $2$ more [[Definition:Inequality|inequalities]] by [[Definition:Real Multiplication|multiplying]] the second [[Definition:Inequality|inequality]] by $m_{a b}$ and $M_{a b}$: :$m_{a b} m_{b c} \norm x_c \le m_{a b} \norm x_b \le m_{a b} M_{b c} \norm x_c$ :$M_{a b} m_{b c} \norm x_c \le M_{a b} \norm x_b \le M_{a b} M_{b c} \norm x_c$ From above it follows that: {{begin-eqn}} {{eqn | l = m_{a b} m_{b c} \norm x_c | o = \le | r = m_{a b} \norm x_b }} {{eqn | o = \le | r = \norm x_a }} {{eqn | o = \le | r = M_{a b} \norm x_b }} {{eqn | o = \le | r = M_{a b} M_{b c} \norm x_c }} {{end-eqn}} Define $k := m_{a b} m_{b c}$ and $K := M_{a b} M_{b c}$. Then $k \le K$ and: :$k \norm x_c \le \norm x_a \le K \norm x_c$ Therefore: :$\norm {\, \cdot \,}_a \sim \norm {\, \cdot \,}_c$ {{qed}}	1
Let $x \in H$ be arbitrary. Then: {{begin-eqn}} {{eqn | l = x | o = \in | r = \ker A | c = }} {{eqn | ll= \leadstoandfrom | l = A x | r = \mathbf 0_H | c = {{Defof|Kernel of Linear Transformation}} }} {{eqn | ll= \leadstoandfrom | l = \gen {A x, A x} | r = 0 | c = {{Defof|Inner Product}} }} {{eqn | ll= \leadstoandfrom | l = \gen {A^*Ax, x} | r = 0 | c = {{Defof|Adjoint Linear Transformation}} }} {{eqn | ll= \leadstoandfrom | l = \gen {A A^* x, x} | r = 0 | c = {{Defof|Normal Operator}} }} {{eqn | ll= \leadstoandfrom | l = \gen {x, AA^*x} | r = 0 | c = {{Defof|Inner Product}} }} {{eqn | ll= \leadstoandfrom | l = \gen {A^*x, A^* x} | r = 0 | c = {{Defof|Adjoint Linear Transformation}} }} {{eqn | ll= \leadstoandfrom | l = A^*x | r = \mathbf 0_H | c = {{Defof|Inner Product}} }} {{eqn | ll= \leadstoandfrom | l = x | o = \in | r = \ker A^* | c = {{Defof|Kernel of Linear Transformation}} }} {{end-eqn}} Hence, by definition of [[Definition:Set Equality/Definition 1|set equality]]: :$\ker A = \ker A^*$ {{qed}} [[Category:Adjoints]] [[Category:Linear Transformations on Hilbert Spaces]] 8nnktytpzh3yrk7r6howhqwdvwzeglx	1
Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]], and let $H$ be an [[Definition:Module|$R$-module]]. Let $\phi: G \to H$ be a [[Definition:Mapping|mapping]]. Then $\phi$ is a [[Definition:Linear Transformation|linear transformation]] {{iff}}: :$\forall x, y \in G: \forall \lambda, \mu \in R: \map \phi {\lambda x + \mu y} = \lambda \map \phi x + \mu \map \phi y$	1
{{begin-eqn}} {{eqn | l = \norm {\mathbf x^p}_q | r = \paren {\sum_{n \mathop = 0}^\infty \size { {x_n}^p}^q}^{1 / q} | c = {{Defof|P-Norm|$p$-Norm}} }} {{eqn | r = \paren {\sum_{n \mathop = 0}^\infty \size {x_n}^{p q} }^{1 / q} | c = [[Power of Power]] }} {{eqn | r = \paren {\paren {\sum_{n \mathop = 0}^\infty \size {x_n}^{p q} }^{1 / p q} }^p | c = [[Power of Power]] }} {{eqn | r = \norm {\mathbf x}_{p q}^p | c = {{Defof|P-Norm|$p$-Norm}} }} {{end-eqn}} {{qed}} [[Category:Functional Analysis]] [[Category:Norm Theory]] [[Category:P-Sequence Metrics]] [[Category:P-Norms]] 7s0yq0l4umt2ll14z8pxdv54cwl87yc	1
Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be [[Definition:Element|elements]] of $\map {\MM_R} {m, n}$. Let $\sqbrk c_{m n} = \sqbrk a_{m n} + \sqbrk b_{m n}$. By definition of [[Definition:Matrix Entrywise Addition|matrix entrywise addition]]: :$\forall i \in \closedint 1 m, j \in \closedint 1 n: a_{i j} + b_{i j} = c_{i j}$ By {{Ring-axiom|A0}}, $R$ is [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Ring Addition|addition]]. Hence: :$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} \in R$ From the definition of [[Definition:Matrix Entrywise Addition|matrix entrywise addition]], $\sqbrk c_{m n}$ has the same [[Definition:Order of Matrix|order]] as both $\sqbrk a_{m n}$ and $\sqbrk b_{m n}$. Thus it follows that: :$\sqbrk c_{m n} \in \map {\MM_R} {m, n}$ Thus $\struct {\map {\MM_R} {m, n}, +}$, as it is defined, is [[Definition:Closure (Abstract Algebra)|closed]]. {{qed}}	1
Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\map {\mathcal M_R} n$ be the [[Definition:Matrix Space|order $n$ square matrix space]] over a [[Definition:Ring (Abstract Algebra)|ring]] $R$. Let $\struct {\map {\mathcal M_R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $R$]]. Let $\map {U_R} n$ be the [[Definition:Set|set]] of [[Definition:Upper Triangular Matrix|upper triangular matrices]] of [[Definition:Order of Square Matrix|order]] $n$ over $R$. Then $\map {U_R} n$ forms a [[Definition:Subring|subring]] of $\struct {\map {\mathcal M_R} n, +, \times}$. Similarly, let $\map {L_R} n$ be the [[Definition:Set|set]] of [[Definition:Lower Triangular Matrix|lower triangular matrices]] of [[Definition:Order of Square Matrix|order]] $n$ over $R$. Then $\map {L_R} n$ forms a [[Definition:Subring|subring]] of $\struct {\map {\mathcal M_R} n, +, \times}$.	1
[[Definition:By Hypothesis|By hypothesis]], let $H$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $E$ === Necessary Condition === Let $H$ be a [[Definition:Basis of Vector Space|basis]] for $E$. By definition of [[Definition:Dimension of Vector Space|dimension of vector space]], a [[Definition:Basis of Vector Space|basis]] for $E$ contains exactly $n$ [[Definition:Element|elements]]. By [[Bases of Finitely Generated Vector Space have Equal Cardinality]], it follows that $H$ also contains exactly $n$ [[Definition:Element|elements]]. {{qed|lemma}} === Sufficient Condition === Let $H$ contain exactly $n$ [[Definition:Element|elements]]. By [[Sufficient Conditions for Basis of Finite Dimensional Vector Space]] $H$ is itself a [[Definition:Basis of Vector Space|basis]] for $E$. {{Qed}}	1
Let $m\in M$, and $[m]_{\mathcal A}$ be its [[Definition:Coordinate Vector|coordinate vector]] with respect to $\mathcal A$. On the one hand: {{begin-eqn}} {{eqn | l = [g(f(m))]_{\mathcal C} | r = \mathbf M_{(g\mathop\circ f), \mathcal C, \mathcal A} \cdot [m]_{\mathcal A} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $g\circ f$ }} {{end-eqn}} On the other hand: {{begin-eqn}} {{eqn | l = [g(f(m))]_{\mathcal C} | r = \mathbf M_{g, \mathcal C, \mathcal B} \cdot [f(m)]_{\mathcal B} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $g$ }} {{eqn | l = | r = \mathbf M_{g, \mathcal C, \mathcal B} \cdot \mathbf M_{f, \mathcal B, \mathcal A} \cdot [m]_{\mathcal A} | c = [[Change of Coordinate Vectors Under Linear Mapping]] applied to $f$ }} {{end-eqn}} Thus $(\mathbf M_{(g\mathop\circ f), \mathcal C, \mathcal A} - \mathbf M_{g, \mathcal C, \mathcal B} \cdot \mathbf M_{f, \mathcal B, \mathcal A}) \cdot [m]_{\mathcal A} = 0$ for all $m\in M$. The result follows. {{explain|find a link why}} {{qed}}	1
Let $\mathbf A_1, \mathbf A_2, \ldots, \mathbf A_n$ be [[Definition:Matrix|matrices]]. Let the [[Definition:Order of Matrix|order]] of $\mathbf A_j$ be $d_j \times d_{j + 1}$. Let $\displaystyle \mathbf C := \prod_{j \mathop = 1}^n \mathbf A_j = \mathbf A_1 \mathbf A_2 \cdots \mathbf A_n$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A_1, \mathbf A_2, \ldots, \mathbf A_n$. Then: :$(1): \quad \displaystyle \map c {i_1, i_{n + 1} } = \sum_{i_n \mathop = 1}^{d_n} \dotsm \sum_{i_3 \mathop = 1}^{d_3} \sum_{i_2 \mathop = 1}^{d_2} \map {a_1} {i_1, i_2} \map {a_2} {i_2, i_3} \dotsm \map {a_{n - 1} } {i_{n - 1}, i_n} \map {a_n} {i_n, i_{n + 1} }$ where: :$\map {a_1} {i_1, i_2}$ (for example) denotes the [[Definition:Element of Matrix|element]] of $\mathbf A_1$ whose [[Definition:Index of Matrix Element|indices]] are $i_1$ and $i_2$ :the [[Definition:Order of Matrix|order]] of $\mathbf C$ is $d_1 \times d_{n+1}$.	1
Let $\left({G, +, \circ}\right)_R$ be an [[Definition:Module|$R$-module]] and $S$ be a [[Definition:Set|set]]. We need to show that $\left({G^{\left({S}\right)}, +'}\right)$ is a [[Definition:Group|group]]. Let $f, g \in G^{\left({S}\right)}$. Let: : $F = \left\{{x \in S: f \left({x}\right) \ne e}\right\}$ : $G = \left\{{x \in S: g \left({x}\right) \ne e}\right\}$ From the definition of $f$ and $g$, both $F$ and $G$ are [[Definition:Finite|finite]]. Since $e + e = e$ by definition of [[Definition:Identity Element|identity element]], it follows that if: : $f \left({x}\right) + g \left({x}\right) \ne e$ then necessarily $f \left({x}\right) \ne e$ or $g \left({x}\right) \ne e$. That is: : $\left({f +' g}\right) \left({x}\right) = f \left({x}\right) + g \left({x}\right) \ne e \implies x \in F \cup G$ But as $F$ and $G$ are both [[Definition:Finite|finite]], it follows that $F \cup G$ is also [[Definition:Finite|finite]]. Hence $f +' g \in G^{\left({S}\right)}$ and $\left({G^{\left({S}\right)}, +'}\right)$ is [[Definition:Closed Algebraic Structure|closed]]. Now let $f \in G^{\left({S}\right)}$. Let $f^*$ be the [[Induced Structure Inverse]] of $f$. Thus $f^* \left({x}\right) = - \left({f \left({x}\right)}\right)$. Again, let $F = \left\{{x \in S: f \left({x}\right) \ne e}\right\}$. It follows directly that $x \in S \setminus F \implies f^* \left({x}\right) = e$. Hence $f^* \left({x}\right) \ne e \implies x \in F$ and hence $f^* \in G^{\left({S}\right)}$. So by the [[Two-Step Subgroup Test]], it follows that $\left({G^{\left({S}\right)}, +'}\right)$ is a [[Definition:Subgroup|subgroup]] of $\left({G^S, +}\right)$. Hence $\left({G^{\left({S}\right)}, +', \circ}\right)_R$ is an [[Definition:Module|$R$-module]]. The result follows. {{qed}}	1
Let $\struct {R, +, \circ}$ be a [[Definition:Ring with Unity|ring with unity]] whose [[Definition:Ring Zero|zero]] is $0_R$ and whose [[Definition:Unity of Ring|unity]] is $1_R$. Let $\sequence {e_k}_{1 \mathop \le k \mathop \le n}$ be the [[Definition:Standard Ordered Basis|standard ordered basis]] of the [[Definition:Module on Cartesian Product|$R$-module $R^n$]]. The corresponding (unordered) set $\set {e_1, e_2, \ldots, e_n}$ is called the '''standard basis of $R^n$'''. === [[Definition:Standard Basis/Vector Space|Vector Space]] === The concept of a '''standard basis''' is often found in the context of [[Definition:Vector Space|vector spaces]]. {{:Definition:Standard Basis/Vector Space}}	1
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a [[Definition:Normed Vector Space|normed vector space]]. Then $X$ is [[Definition:Closed Set in Normed Vector Space|closed]] in $M$.	1
Let $\map \MM {m, n}$ be a [[Definition:Metric Space|metric space]] of [[Definition:Order of Matrix|order]] $m \times n$ over a [[Definition:Field (Abstract Algebra)|field]] $K$. Let $\mathbf A \in \map \MM {m, n}$ be a [[Definition:Matrix|matrix]]. Let $\Gamma$ be a [[Definition:Column Operation|column operation]] which transforms $\mathbf A$ to a new [[Definition:Matrix|matrix]] $\mathbf B \in \map \MM {m, n}$. Then there exists another [[Definition:Column Operation|column operation]] $\Gamma'$ which transforms $\mathbf B$ back to $\mathbf A$.	1
Recall that the [[Definition:P-adic Norm|$p$-adic norm]] is defined as: :$\forall q \in \Q: \norm r_p := \begin{cases} 0 & : r = 0 \\ p^{- k} & : r \ne 0 \end{cases}$ where: :$\displaystyle r = p^k \frac m n$ and: :$k, n \in \Z, m \in \Z_{\ne 0} : p \nmid m, n$ where $\nmid$ stands for [[Symbols:Number Theory/Does Not Divide|"does not divide"]]. We must show the following hold for all $r_1$, $r_2 \in \Q$: {{begin-axiom}} {{axiom | n = \text N 1 | q = \forall r \in \Q | ml= \norm r_p = 0 | mo= \iff | mr= x = 0 }} {{axiom | n = \text N 2 | q = \forall r_1, r_2 \in \Q | ml= \norm {r_1 r_2} | mo= = | mr= \norm {r_1}_p \times \norm {r_2}_p }} {{axiom | n = \text N 3 | q = \forall r_1, r_2 \in \Q | ml= \norm {r_1 + r_2}_p | mo= \le | mr= \norm {r_1}_p + \norm {r_2}_p }} {{end-axiom}} === Norm Axiom $(\text N 1)$ === Let $r \in \Q : r \ne 0$. Let $k, m\in \Z, n \in \Z_{\ne 0} : p \nmid m, n$. Suppose $r = 0$. By [[Definition:P-adic Norm|definition]]: :$\norm {r}_p = 0$ Suppose $\displaystyle r = p^k \frac m n \ne 0$ By [[Definition:P-adic Norm|definition]]: :$\displaystyle \norm {r}_p = \frac 1 {p^k} > 0$ Suppose $\norm r_p = 0$. By [[Definition:P-adic Norm|definition]]: :$r = 0$ {{qed|lemma}} === Norm Axiom $(\text N 2)$ === Suppose $r_1 = 0$ or $r_2 = 0$. From [[Definition:Norm Axioms|axiom $(\text N 1)$]], $\norm {r_1}_p = 0$ or $\norm {r_2}_p = 0$. Suppose $r_1 \ne 0 \ne r_2$. Let $k_1, k_2, m_1, m_2 \in \Z, n_1, n_2 \in \Z_{\ne 0} : p \nmid n_1, n_2, m_1, m_2$ Let $\displaystyle r_1 = p^{k_1} \frac {m_1} {n_1}, r_2 = p^{k_2} \frac {m_2} {n_2}$ Then: :$\displaystyle r_1 r_2 = p^{k_1 + k_2} \frac {m_1 m_2}{n_1 n_2}$ We have that $p \nmid m_1$, $p \nmid m_2$. Since $p$ is [[Definition:Prime Number|prime]]: :$p \nmid m_1 m_2$. Similarly: :$p \nmid n_1 n_2$. Therefore: {{begin-eqn}} {{eqn| l = \norm {r_1 r_2}_p | r = p^{- \paren {k_1 + k_2} } }} {{eqn| r = p^{-k_1} p^{-k_2} }} {{eqn| r = \norm {r_1}_p \norm {r_2}_p }} {{end-eqn}} {{qed|lemma}} === Norm Axiom $(\text N 3)$ === Suppose one of the following is true: :$r_1 = 0$ :$r_2 = 0$ :$r_1 + r_2 = 0$ Then the result is straightforward. Suppose $r_1 \ne 0$, $r_2 \ne 0$, $r_1 + r_2 \ne 0$. Let $\displaystyle r_1 = p^{k_1} \frac {m_1}{n_1}, r_2 = p^{k_2} \frac{m_2}{n_2}$ where: :$\displaystyle k_1, k_2, m_1, m_2 \in \Z, n_1, n_2 \in \Z_{\ne 0} : p \nmid m_1, m_2, n_1, n_2$ Then: {{begin-eqn}} {{eqn | l = r_1 + r_2 | r = \frac {p^{k_1} m_1 n_2 + p^{k_2} m_2 n_1} {n_1 n_2} }} {{eqn | r = p^{\map \min {k_1, k_2} } \frac {p^{k_1 \mathop - \map \min {k_1, k_2} } m_1 n_2 + p^{k_2 \mathop - \map \min {k_1, k_2} } n_1 m_2}{n_1 n_2} | c = {{Defof|Min Operation}} }} {{eqn | r = p^{\map \min {k_1, k_2} } \frac {\tilde m}{n_1 n_2} | c = $\displaystyle \tilde m := p^{k_1 \mathop - \map \min {k_1, k_2} } m_1 n_2 + p^{k_2 \mathop - \map \min {k_1, k_2} } n_1 m_2$ }} {{end-eqn}} By [[Fundamental Theorem of Arithmetic]]: :$\exists ! \tilde k \in \Z_{\ge 0} : \exists m \in \Z : p \nmid m : \tilde m = p^{\tilde k} m$ Obviously, $p \nmid n_1 n_2$ Hence: {{begin-eqn}} {{eqn | l = \norm {r_1 + r_2}_p | r = \frac 1 {p^{\tilde k + \map \min {k_1, k_2} } } }} {{eqn | o = \le | r = \frac 1 {p^{\map \min {k_1, k_2} } } }} {{eqn | r = \map \max {p^{-k_1}, p^{-k_2} } | c = {{Defof|Max Operation}} }} {{eqn | r = \map \max {\norm {r_1}_p, \norm {r_1}_p} }} {{eqn | o = \le | r = \map \max {\norm {r_1}_p, \norm {r_2}_p} + \map \min {\norm {r_1}_p, \norm {r_2}_p} }} {{eqn | r = \norm {r_1}_p + \norm {r_2}_p }} {{end-eqn}} {{qed|lemma}} All [[Definition:Norm Axioms|norm axioms]] are seen to be satisfied. Hence the result. {{qed}}	1
=== Proof of $(\text M 1)$ and $(\text M 4)$ === Let $x, y \in R$. Then $\map d {x, y} = \norm {x - y} \ge 0$, and furthermore: {{begin-eqn}} {{eqn | l = \map d {x, y} | r = 0 }} {{eqn | ll= \leadstoandfrom | l = \norm {x - y} | r = 0 }} {{eqn | ll= \leadstoandfrom | l = x - y | r = \mathbf 0_R | c = {{NormAxiom|1}} }} {{eqn | ll= \leadstoandfrom | l = x | r = y }} {{end-eqn}} {{qed|lemma}} === Proof of $(\text M 2)$ === Let $x, y, z \in R$. Then: {{begin-eqn}} {{eqn | l = \map d {x, z} | r = \norm {x - z} }} {{eqn | r = \norm {x - y + y - z} | c = }} {{eqn | o = \le | r = \norm {x - y} + \norm {y - z} | c = {{NormAxiom|3}} }} {{eqn | r = \map d {x, y} + \map d {y, z} }} {{end-eqn}} {{qed|lemma}} === Proof of $(\text M 3)$ === Let $x, y \in R$. Then: {{begin-eqn}} {{eqn | l = \map d {x, y} | r = \norm {x - y} }} {{eqn | r = \norm {-1 \paren {y - x} } }} {{eqn | r = \norm {-1} \times \norm {y - x} | c = {{NormAxiom|2}} }} {{eqn | r = \norm {y - x} }} {{eqn | r = \map d {y, x} }} {{end-eqn}} {{qed|lemma}} As $d$ satisfies the [[Definition:Metric Space Axioms|metric space axioms]], it is a [[Definition:Metric|metric]]. {{qed}}	1
Let $A$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]].	1
Let $H, K$ be [[Definition:Hilbert Space|Hilbert spaces]] over $\Bbb F \in \left\{{\R, \C}\right\}$. Let $u: H \times K \to \Bbb F$ be a [[Definition:Bounded Sesquilinear Form|bounded sesquilinear form]] with bound $M$. Then there exist unique [[Definition:Bounded Linear Transformation|bounded linear transformations]] $A \in B \left({H, K}\right), B \in B \left({K, H}\right)$ such that: :$\forall h \in H, k \in K: u \left({h, k}\right) = \left\langle{Ah, k}\right\rangle_K = \left\langle{h, Bk}\right\rangle_H$ Furthermore, $\left\Vert{A}\right\Vert, \left\Vert{B}\right\Vert \le M$.	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]], and let $L$ be a [[Definition:Linear Functional|linear functional]] on $H$. Then the following four statements are equivalent: :$(1):\quad L$ is [[Definition:Continuous Mapping (Topology)|continuous]] :$(2):\quad L$ is [[Definition:Continuous Mapping (Topology)#Continuous at a Point|continuous]] at $\mathbf{0}_H$ :$(3):\quad L$ is [[Definition:Continuous Mapping (Topology)#Continuous at a Point|continuous]] at some point :$(4):\quad \exists c > 0: \forall h \in H: \left|{Lh}\right| \le c \left\|{h}\right\|$	1
By the definition of a [[Definition:Distance-Preserving Mapping|distance-preserving mapping]] and a [[Definition:Ring Monomorphism|ring monomorphism]] it has to be shown that: :$(1): \quad \phi$ is a [[Definition:Ring Homomorphism|homomorphism]]. :$(2): \quad \phi$ is an [[Definition:Injection|injection]]. :$(3): \quad \phi$ is [[Definition:Distance-Preserving Mapping|distance-preserving]]. === $(1): \quad \phi$ is a [[Definition:Ring Homomorphism|homomorphism]] === By definition, $\phi$ is the [[Definition:Composition of Mappings|composition of two mappings]]: :$\phi = q \circ \phi'$ where: :$\text{(a)}: \quad \phi': R \to \CC$, defined by: $\forall a \in R, \map {\phi'} a = \sequence {a, a, a, \dotsc}$ :$\text{(b)}: \quad q$ is the [[Definition:Quotient Mapping|quotient mapping]] $q: \CC \to \CC \, \big / \NN$ defined by: $\map q {\sequence {x_n} } = \sequence {x_n} + \NN$ By [[Embedding Normed Division Ring into Ring of Cauchy Sequences]], $\phi'$ is a [[Definition:Ring Monomorphism|ring monomorphism]]. By [[Quotient Ring Epimorphism is Epimorphism]], then $q$ is a [[Definition:Ring Epimorphism|ring epimorphism]]. By [[Composition of Ring Homomorphisms is Ring Homomorphism]] then the composition $\phi = q \circ \phi'$ is a [[Definition:Ring Homomorphism|ring homomorphism]] {{qed|lemma}} === $(2): \quad \phi$ is an [[Definition:Injection|injection]] === Let $a, b \in R$. Suppose $\map \phi a = \map \phi b$. Then: {{begin-eqn}} {{eqn | l = \sequence {a, a, a, \dotsc} + \NN | r = \sequence {b, b, b, \dotsc} + \NN | c = Definition of $\phi$ }} {{eqn | ll= \leadsto | l = \sequence {a, a, a, \dotsc} - \sequence {b, b, b, \dotsc} | o = \in | r = \NN | c = [[Left Cosets are Equal iff Product with Inverse in Subgroup]] }} {{eqn | ll= \leadsto | l = \sequence {a - b, a - b, a - b, \dotsc} | o = \in | r = \NN | c = Ring operations on [[Definition:Ring of Cauchy Sequences|Ring of Cauchy Sequences]] }} {{eqn | ll= \leadsto | l = \lim_{n \mathop \to \infty} {a - b} | r = 0 | c = Definition of $\NN$ }} {{end-eqn}} By [[Constant Sequence Converges to Constant in Normed Division Ring]] then: :$\displaystyle \lim_{n \mathop \to \infty} {a - b} = a - b$ Hence $a-b = 0$. The result follows. {{qed|lemma}} === $(3): \quad \phi$ is [[Definition:Distance-Preserving Mapping|distance-preserving]] === Let $a, b \in R$. Then: {{begin-eqn}} {{eqn | l = \norm {\phi \paren {a} - \phi \paren {b} } | r = \norm {\phi \paren {a - b} } | c = $\phi$ is a [[Definition:Ring Homomorphism|homomorphism]] }} {{eqn | r = \norm {\sequence {a - b, a - b, a - b, \dots} + \NN} | c = Definition of $\phi$ }} {{eqn | r = \lim_{n \mathop \to \infty } \norm {a - b} | c = Definition of $\norm {\, \cdot \,}$ }} {{eqn | r = \norm {a - b} | c = [[Constant Sequence Converges to Constant in Normed Division Ring]] }} {{end-eqn}} {{qed}}	1
By [[Convergent Sequence is Cauchy Sequence/Normed Division Ring|every convergent sequence is a Cauchy sequence]] then $\NN \subseteq \CC$. The proof is completed in these steps: :$(1): \quad \NN$ is an [[Definition:Ideal of Ring|ideal]] of $\CC$. :$(2): \quad \NN$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]]. :$(3): \quad \NN$ is a [[Definition:Maximal Right Ideal of Ring|maximal right ideal]]. === [[Null Sequences form Maximal Left and Right Ideal/Lemma 1|Lemma 1]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 1}}{{qed|lemma}} === [[Null Sequences form Maximal Left and Right Ideal/Lemma 2|Lemma 2]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 2}}{{qed|lemma}} === [[Null Sequences form Maximal Left and Right Ideal/Lemma 3|Lemma 3]] === {{:Null Sequences form Maximal Left and Right Ideal/Lemma 3}}{{qed}}	1
Let $\mathbf A$ be a [[Definition:Hermitian Matrix|Hermitian matrix]]. Then, by definition, $\mathbf A = \mathbf A^\dagger$, where $^\dagger$ designates the [[Definition:Conjugate Transpose of Matrix|conjugate transpose]]. Let $\lambda$ be an [[Definition:Eigenvalue|eigenvalue]] of $\mathbf A$. Let $\mathbf v$ be an [[Definition:Eigenvector|eigenvector]] corresponding to the [[Definition:Eigenvalue|eigenvalue]] $\lambda$ of $\mathbf A$. Denote with $\left\langle{\cdot, \cdot}\right\rangle$ the [[Definition:Inner Product|inner product]] on $\C$. {{begin-eqn}} {{eqn | l = \lambda * \left\langle v, v\right\rangle | r = \left\langle \lambda*v, v\right\rangle | c = [[Properties of Complex Inner Product]] }} {{eqn | r = \left\langle \mathbf A*v, v\right\rangle | c = {{Defof|Eigenvector}}: $\lambda*v = \mathbf A*v$ }} {{eqn | r = \left\langle v, \mathbf A^\dagger * v\right\rangle | c = [[Properties of Adjugate]] }} {{eqn | r = \left\langle v, \mathbf A*v\right\rangle | c = $\mathbf A$ is [[Definition:Hermitian Matrix|Hermitian]], so $\mathbf A^\dagger = \mathbf A$ }} {{eqn | r = \left\langle v, \lambda*v\right\rangle | c = {{Defof|Eigenvector}}: $\lambda*v = \mathbf A*v$ }} {{eqn | r = \overline{\lambda}*\left\langle v, v\right\rangle | c = [[Properties of Complex Inner Product]] }} {{end-eqn}} We have that $v \ne 0$, and because of the [[Definition:Positive Definite|positive definiteness]], it must be that: : $\left\langle v, v\right\rangle \ne 0$ {{explain|A link is needed to whatever it is ($\left\langle v, v\right\rangle$, presumably) is [[Definition:Positive Definite|positive definite]].}} Thus: : $\left\langle v, v\right\rangle \ne 0$ So we can divide both sides by $\left\langle v, v\right\rangle$. Thus $\lambda = \overline{\lambda}$. By [[Complex Number equals Conjugate iff Wholly Real]], $\lambda$ is a [[Definition:Real Number|real number]]. $\lambda$ was arbitrary, so it follows that every [[Definition:Eigenvalue|eigenvalue]] is a [[Definition:Real Number|real number]]. Hence the result. {{qed}}	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $r \in R$. Let $r \, \mathbf I_n$ be the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]] defined by: :$\sqbrk {r \, \mathbf I_n}_{i j} = \begin{cases} r & : i = j \\ 0 & : i \ne j \end{cases}$ Then: :$\map \det {r \, \mathbf I_n} = r^n$ where $\det$ denotes [[Definition:Determinant of Matrix|determinant]].	1
Consider this [[Simultaneous Linear Equations/Examples/Arbitrary System 1|system of simultaneous linear equations]]: {{begin-eqn}} {{eqn | n = 1 | l = x_1 - 2 x_2 + x_3 | r = 1 }} {{eqn | n = 2 | l = 2 x_1 - x_2 + x_3 | r = 2 }} {{eqn | n = 3 | l = 4 x_1 + x_2 - x_3 | r = 1 }} {{end-eqn}} From its [[Simultaneous Linear Equations/Examples/Arbitrary System 1|evaluation]] it has the following [[Definition:Unique|unique]] [[Definition:Solution to System of Simultaneous Equations|solution]]: {{begin-eqn}} {{eqn | l = x_1 | r = -\dfrac 1 2 }} {{eqn | l = x_2 | r = \dfrac 1 2 }} {{eqn | l = x_3 | r = \dfrac 3 2 }} {{end-eqn}} Hence the result. {{qed}}	1
=== Sufficient Condition === Immediate from [[Separable Discrete Space is Countable]]. {{qed|lemma}} === Necessary Condition === Immediate from [[Countable Space is Separable]]. {{qed}}	1
The [[Definition:Square (Algebra)|square]] of the [[Definition:Vandermonde Matrix|Vandermonde matrix of order $n$]]: : $\mathbf V = \begin{bmatrix} x_1 & x_2 & \cdots & x_n \\ x_1^2 & x_2^2 & \cdots & x_n^2 \\ \vdots & \vdots & \ddots & \vdots \\ x_1^n & x_2^n & \cdots & x_n^n \end{bmatrix}$ is symmetrical in $x_1, \ldots, x_n$. {{questionable|The case $n {{=}} 2$ left me clueless to what could possibly be intended here; only $\mathbf {V V}^T$ is trivially seen symmetric in the $x_n$, but this can hardly be called a square}}	1
Let $S$ be a [[Definition:Simultaneous Equations|system of simultaneous equations]]. Then it is possible that $S$ may have a [[Definition:Solution Set to System of Simultaneous Equations|solution set]] which is a [[Definition:Singleton|singleton]].	1
Let $R$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\struct {\map {\MM_R} n, +, \times}$ denote the [[Definition:Ring of Square Matrices|ring of square matrices of order $n$ over $R$.]] Then $\struct {\map {\MM_R} n, +, \times}$ is a [[Definition:Ring (Abstract Algebra)|ring]].	1
Let $S \subseteq G$. Then $S$ is a '''linearly independent set (over $R$)''' {{iff}} every [[Definition:Finite Sequence|finite]] [[Definition:Sequence of Distinct Terms|sequence of distinct terms]] in $S$ is a [[Definition:Linearly Independent Sequence|linearly independent sequence]]. That is, such that: : $\displaystyle \forall \sequence {\lambda_n} \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0_R$ where $a_1, a_2, \ldots, a_k$ are distinct elements of $S$. === [[Definition:Linearly Independent/Set/Real Vector Space|Linearly Independent Set on a Real Vector Space]] === {{:Definition:Linearly Independent/Set/Real Vector Space}} === [[Definition:Linearly Independent/Set/Complex Vector Space|Linearly Independent Set on a Complex Vector Space]] === {{:Definition:Linearly Independent/Set/Complex Vector Space}}	1
By [[Characterization of Projections]], $A$ is [[Definition:Self-Adjoint Operator|self-adjoint]]. Then $\left({I - A}\right)^* = I^* - A^* = I - A$ from [[Adjoining is Linear]]. So $I - A$ is also [[Definition:Self-Adjoint Operator|self-adjoint]]. From [[Complementary Idempotent is Idempotent]], $I - A$ is [[Definition:Idempotent Operator|idempotent]]. Hence, applying [[Characterization of Projections]], $I - A$ is a [[Definition:Projection (Hilbert Spaces)|projection]]. {{qed}} [[Category:Linear Transformations on Hilbert Spaces]] 0o5qci51vwo2q9gzp82zog9zl95dky6	1
Let $H$ be a [[Definition:Hilbert Space|Hilbert space]]. Let $S = \set {h_n: n \in \N}$ be a [[Definition:Linearly Independent Set|linearly independent]] [[Definition:Subset|subset]] of $H$. Then there exists an [[Definition:Orthonormal Subset|orthonormal subset]] $E = \set {e_n: n \in \N}$ of $H$ such that: :$\forall k \in \N: \operatorname{span} \set {h_n: 0 \le n \le k} = \operatorname{span} \set {e_n: 0 \le n \le k}$ where $\operatorname{span}$ denotes [[Definition:Linear Span|linear span]].	1
[[Proof by Counterexample]]: Let $\mathbf a = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, $\mathbf b = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$, $\mathbf c = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ be [[Definition:Vector (Euclidean Space)|vectors in $\R^3$]]. {{begin-eqn}} {{eqn | l = \mathbf a \times \left({\mathbf b \times \mathbf c}\right) | r = \mathbf a \times \left({\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} }\right) }} {{eqn | r = \mathbf a \times \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \times \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} 0 \\ 0 \\ -1 \end{bmatrix} }} {{eqn | l = \left({\mathbf a \times \mathbf b}\right) \times \mathbf c | r = \left({\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} }\right) \times \mathbf c }} {{eqn | r = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \times \mathbf c }} {{eqn | r = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \times \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} }} {{eqn | r = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} }} {{end-eqn}} {{qed}} [[Category:Vector Cross Product]] 7bzmkkoao1qhhs5bmyk2u1rrudeclby	1
By the definition of [[Definition:Dimension (Representation Theory)|degree]] of a [[Definition:Linear Representation|linear representation]], it is known that $\dim \left({V}\right) = 1$. Let $W$ be a [[Definition:Proper Vector Subspace|proper vector subspace]] of $V$. It follows from [[Dimension of Proper Subspace is Less Than its Superspace]] that: :$\dim \left({W}\right) < 1$ and hence $\dim \left({W}\right) = 0$. Now from [[Trivial Vector Space iff Zero Dimension]], it follows that: :$W = \left\{{\mathbf 0}\right\}$ But this is not a non-[[Definition:Trivial Subspace|trivial]] [[Definition:Proper Vector Subspace|proper subspace]] of $V$. Thus $V$ has no non-[[Definition:Trivial Subspace|trivial]] [[Definition:Proper Vector Subspace|proper vector subspaces]]. Hence, by definition, $\rho$ is an [[Definition:Irreducible Linear Representation|irreducible linear representation]]. {{qed}} [[Category:Representation Theory]] danaed4hxwyj92gbmkbtb2z59clixql	1
We will first prove that there is an [[Definition:Injection|injection]] from $X$ to $Y$. Let $x \in X$. By [[Expression of Vector as Linear Combination from Basis is Unique/General Result|Expression of Vector as Linear Combination from Basis is Unique: General Result]], there is a unique [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $C_x$ of $R \times Y$ such that: :$\displaystyle x = \sum_{\tuple {r, v} \mathop \in C_x} r \cdot v$ and :$\forall \tuple {r, v} \in C_x: r \ne 0_R$ Define $\Phi: X \to \powerset Y$ by: :$\map \Phi x := \Img {C_x}$ We next show that $\sequence {\map \Phi x}_{x \mathop \in X}$ satisfies the '''marriage condition'''. That is, for every [[Definition:Finite Set|finite]] [[Definition:Subset|subset]] $F$ of $X$: :$\displaystyle \card F \le \card {\bigcup \map \Phi F}$ Since $X$ is a [[Definition:Basis of Vector Space|basis]], it is [[Definition:Linearly Independent Set|linearly independent]]. By [[Subset of Linearly Independent Set is Linearly Independent]], $F$ is also [[Definition:Linearly Independent Set|linearly independent]]. By the definition of $\Phi$: :$\displaystyle F \subseteq \operatorname {span} \bigcup \map \Phi F$. By [[Finite Union of Finite Sets is Finite]], $\displaystyle \bigcup \map \Phi F$ is [[Definition:Finite Set|finite]]. Thus by [[Size of Linearly Independent Subset is at Most Size of Finite Generator]]: :$\displaystyle \size F \le \size {\bigcup \map \Phi F}$ By [[Hall's Marriage Theorem/General Set|Hall's Marriage Theorem]], there is an [[Definition:Injection|injection]] from $X$ into $Y$. Precisely the same argument with $X$ and $Y$ interchanged shows that there is an injection from $Y$ into $X$ as well. Thus by the [[Cantor-Bernstein-Schröder Theorem]], $X$ and $Y$ are [[Definition:Equinumerous|equinumerous]]. {{qed}} {{BPI|Hall's Marriage Theorem/General Set|3}}	1
{{ProofWanted}} [[Category:Vector Spaces]] 2x9le09b6zjtlrg87lf3wlrpuwn2e3y	1
Let $z_1, z_2 \in \C$ be [[Definition:Complex Number|complex numbers]]. Let $L$ be a [[Definition:Straight Line|straight line]] through $z_1$ and $z_2$ in the [[Definition:Complex Plane|complex plane]]. === [[Equation for Line through Two Points in Complex Plane/Formulation 1|Formulation 1]] === {{:Equation for Line through Two Points in Complex Plane/Formulation 1}} === [[Equation for Line through Two Points in Complex Plane/Parametric Form 1|Parametric Form $1$]] === {{:Equation for Line through Two Points in Complex Plane/Parametric Form 1}} === [[Equation for Line through Two Points in Complex Plane/Parametric Form 2|Parametric Form $2$]] === {{:Equation for Line through Two Points in Complex Plane/Parametric Form 2}} === [[Equation for Line through Two Points in Complex Plane/Symmetric Form|Symmetric Form]] === {{:Equation for Line through Two Points in Complex Plane/Symmetric Form}}	1
Let $\mathbb K$ be a [[Definition:Field (Abstract Algebra)|field]]. Let $V$ be a [[Definition:Vector Space|vector space]] over $\mathbb K$. Let $b$ be a [[Definition:Bilinear Form|bilinear form]] on $V$. Then the following are [[Definition:Logically Equivalent|equivalent]]: :$(1): \quad$ $b$ is [[Definition:Reflexive Bilinear Form|reflexive]] :$(2): \quad$ $b$ is [[Definition:Symmetric Bilinear Form|symmetric]] or [[Definition:Alternating Bilinear Form|alternating]]	1
{{ProofWanted}} [[Category:Bilinear Forms]] q2o8hbxerxg39apkrawszwsjkb41w58	1
$L$ can be expressed by the equation: :$z = ...$ or: :$z = ...$ This form of $L$ is known as the '''parametric form''', where $t$ is the '''parameter'''.	1
[[Definition:Matrix Similarity|Matrix similarity]] is an [[Definition:Equivalence Relation|equivalence relation]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $G$ be a [[Definition:Unitary Module|unitary $R$-module]] whose [[Definition:Dimension of Module|dimension]] is [[Definition:Finite|finite]]. Then the [[Definition:Evaluation Linear Transformation|evaluation linear transformation]] $J: G \to G^{**}$ is called the '''evaluation isomorphism from $G$ to $G^{**}$.	1
We assume the two hypotheses of the theorem. We have that: {{begin-eqn}} {{eqn | l = \map {\frac \d {\d t} } {\map \Phi {t + T} } | r = \map {\Phi'} {t + T} | c = }} {{eqn | r = \map {\mathbf A} {t + T} \map \Phi {t + T} | c = }} {{eqn | r = \map {\mathbf A} t \map \Phi {t + T} | c = }} {{end-eqn}} So the first implication of the theorem holds, that is: that $\map \Phi {t + T}$ is a [[Definition:Fundamental Matrix|fundamental matrix]]. Because $\map \Phi t$ and $\map \Phi {t + T}$ are both fundamental matrices, there must exist some [[Definition:Matrix|matrix]] $\mathbf C$ such that: :$\map \Phi {t + T} = \map \Phi t \mathbf C$ Hence by the [[Existence of Matrix Logarithm|existence of the matrix logarithm]], there exists a [[Definition:Matrix|matrix]] $\mathbf B$ such that: :$\mathbf C = e^{\mathbf BT}$ Defining $\map {\mathbf P} t = \map \Phi t e^{-\mathbf B t}$, it follows that: {{begin-eqn}} {{eqn | l = \map {\mathbf P} {t + T} | r = \map \Phi {t + T} e^{-\mathbf B t - \mathbf B T} | c = }} {{eqn | r = \map \Phi t C e^{-\mathbf B T} e^{-\mathbf B t} | c = }} {{eqn | r = \map \Phi t e^{-\mathbf B t} | c = }} {{eqn | r = \map {\mathbf P} t | c = }} {{end-eqn}} and hence $\map {\mathbf P} t$ is periodic with period $T$. As $\map \Phi t = \map {\mathbf P} t e^{\mathbf B t}$, the second implication also holds. {{qed}}	1
Let $F$ be a [[Definition:Field (Abstract Algebra)|field]]. Then $F$ is a [[Definition:Principal Ideal Domain|principal ideal domain]].	1
By definition, [[Definition:Matrix Entrywise Addition|matrix entrywise addition]] is the '''[[Definition:Hadamard Product|Hadamard product]]''' of $\mathbf A$ and $\mathbf B$ with respect to [[Definition:Ring Addition|ring addition]]. We have from {{Ring-axiom|A1}} that [[Definition:Ring Addition|ring addition]] is [[Definition:Associative Operation|associative]]. The result then follows directly from [[Associativity of Hadamard Product]]. {{qed}}	1
{{begin-eqn}} {{eqn | l = \mathbf A \mathbf 0 | r = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m 1} & a_{m 2} & \cdots & a_{m n} \\ \end {bmatrix} \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix} }} {{eqn | r = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix} }} {{eqn | r = \mathbf 0 }} {{end-eqn}} The [[Definition:Order of Matrix|order]] is correct [[Definition:By Hypothesis|by hypothesis]]. The result follows by the definition of [[Definition:Null Space|null space]]. {{qed}}	1
It needs to be demonstrated that $L$ is [[Definition:Closed Algebraic Structure|closed]] under $+$ and $\circ$. So let $m_1 + n_1, m_2 + n_2 \in L$. Then $\paren {m_1 + n_1} + \paren {m_2 + n_2} = \paren {m_1 + m_2} + \paren {n_1 + n_2} \in L$. It follows that $L$ is [[Definition:Closed Algebraic Structure|closed]] under $+$. Now let $\lambda \in K, m + n \in L$. Then $\lambda \circ \paren {m + n} = \paren {\lambda \circ m} + \paren {\lambda \circ n} \in L$. It follows that $L$ is [[Definition:Closed for Scalar Product |closed]] under $\circ$. Hence the result, by definition of [[Definition:Vector Subspace|linear subspace]]. {{qed}} [[Category:Vector Subspaces]] fkp9zb9z6qnfaekdo3j3vmer3oxbvlp	1
In [[Unit Matrix is Identity for Matrix Multiplication]], it is demonstrated that: :$\forall \mathbf A \in \map {\MM_R} n: \mathbf A \mathbf I_n = \mathbf A = \mathbf I_n \mathbf A$ Hence the result, by definition of [[Definition:Identity Element|identity element]] {{qed}}	1
Let $S$ be a [[Definition:Simultaneous Equations|system of simultaneous equations]]. Then it is possible that $S$ may have a [[Definition:Solution Set to System of Simultaneous Equations|solution set]] which is [[Definition:Empty Set|empty]].	1
Let $\struct {\mathbf V, +, \circ}_{\mathbb F}$ be a [[Definition:Vector Space|vector space]] over $\mathbb F$, as defined by the [[Definition:Vector Space Axioms|vector space axioms]]. Then the [[Definition:Zero Vector|zero vector]] in $\mathbf V$ is [[Definition:Unique|unique]]: :$\exists! \mathbf 0 \in \mathbf V: \forall \mathbf x \in \mathbf V: \mathbf x + \mathbf 0 = \mathbf x$	1
Let $V$ be a [[Definition:Vector Space|vector space]] over $K$ with [[Definition:Zero Vector|zero vector]] $\mathbf 0$. The [[Definition:Zero Subspace|zero subspace]] $\left\{{\mathbf 0}\right\}$ is a [[Definition:Vector Subspace|subspace]] of $V$.	1
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $n$ be a [[Definition:Positive Integer|positive integer]]. Let $\mathbf A$ and $\mathbf B$ be [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Matrix|order]] $n$ over $R$. Then $\mathbf A$ and $\mathbf B$ are '''congruent''' {{Iff}} there exists an [[Definition:Invertible Matrix|invertible matrix]] $\mathbf P\in R^{n\times n}$ such that $\mathbf B = \mathbf P^\intercal \mathbf A \mathbf P$.	1
We have that: :$\mathbf A^\intercal = \begin {bmatrix} a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} & a_{r 2} & \cdots & a_{r n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{s 1} & a_{s 2} & \cdots & a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end {bmatrix}$ where $\mathbf A^\intercal$ denotes the [[Definition:Transpose of Matrix|transpose]] of $\mathbf A$. Similarly, we have that: :$\mathbf B^\intercal = \begin{bmatrix} a_{1 1} & a_{1 2} & \ldots & a_{1 n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{r 1} + \lambda a_{s 1} & a_{r 2} + \lambda a_{s 2} & \cdots & a_{r n} + \lambda a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{s 1} & a_{s 2} & \cdots & a_{s n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n 1} & a_{n 2} & \cdots & a_{n n} \\ \end {bmatrix}$ From [[Multiple of Row Added to Row of Determinant]]: :$\map \det {\mathbf B^\intercal} = \map \det {\mathbf A^\intercal}$ From from [[Determinant of Transpose]]: :$\map \det {\mathbf B^\intercal} = \map \det {\mathbf B}$ :$\map \det {\mathbf A^\intercal} = \map \det {\mathbf A}$ and the result follows. {{qed}}	1
Let $\struct {A_R, \oplus}$ be an [[Definition:Associative Algebra|associative algebra]] over the [[Definition:Ring (Abstract Algebra)|ring]] $A_R$. Then: :$\struct {A_R, \oplus}$ has a unique [[Definition:Multiplicative Inverse|multiplicative inverse]] for every [[Definition:Zero Vector|non-zero]] $a \in A_R$ {{iff}}: :$\struct {A_R, \oplus}$ is a [[Definition:Unitary Division Algebra|unitary division algebra]].	1
Let $\struct {R, +_R, \times_R}$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\struct{G, +_G, \circ}$ be a [[Definition:Right Module|right module]] over $\struct {R, +_R, \times_R}$. Let $\circ’ : R \times G \to G$ be the [[Definition:Binary Operation|binary operation]] defined by: :$\forall \lambda \in R: \forall x \in G: \lambda \circ’ x = x \circ \lambda $ Then $\struct{G, +_G, \circ’, \circ}$ is a [[Definition:Bimodule|bimodule]] over $\struct {R, +_R, \times_R}$.	1
Let $\struct {R, \norm {\, \cdot \,} }$ be a [[Definition:Normed Division Ring|normed division ring]]. Let $d$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\struct {R, \norm {\, \cdot \,} }$. Let $\mathcal C$ be the [[Definition:Ring of Cauchy Sequences|ring of Cauchy sequences over $R$]]. Let $\mathcal N$ be the [[Definition:Set|set]] of [[Definition:Null Sequence in Normed Division Ring|null sequences]] in $R$. Let $\mathcal C \,\big / \mathcal N$ be the [[Quotient Ring of Cauchy Sequences is Division Ring|quotient ring of Cauchy sequences]] of $\mathcal C$ by the [[Null Sequences form Maximal Left and Right Ideal|maximal ideal]] $\mathcal N$. Let $\norm {\, \cdot \,}: \mathcal C \,\big / \mathcal N \to \R_{\ge 0}$ be the [[Quotient Ring of Cauchy Sequences is Normed Division Ring|norm on the quotient ring $\mathcal C \,\big / \mathcal N$]] defined by: :$\displaystyle \forall \sequence {x_n} + \mathcal N: \norm {\sequence {x_n} + \mathcal N} = \lim_{n \mathop \to \infty} \norm{x_n}$ Let $d'$ be the [[Definition:Metric Induced by Norm on Division Ring|metric induced]] by $\struct {\mathcal C \,\big / \mathcal N, \norm {\, \cdot \,} }$ Let $\phi: R \to \mathcal C \,\big / \mathcal N$ be the [[Definition:Mapping|mapping]] from $R$ to the [[Definition:Quotient Ring|quotient ring]] $\mathcal C \,\big / \mathcal N$ defined by: :$\forall a \in R: \map \phi a = \sequence {a, a, a, \ldots} + \mathcal N$ where $\sequence {a, a, a, \ldots} + \mathcal N$ is the [[Definition:Left Coset|left coset]] in $\mathcal C \, \big / \mathcal N$ that contains the constant [[Definition:Sequence|sequence]] $\sequence {a, a, a, \ldots}$. Then: :$\struct {\mathcal C \,\big / \mathcal N, d'}$ is the [[Definition:Completion (Metric Space)|metric completion]] of $\struct {R,d}$ and: :$\map \phi R$ is a [[Definition:Everywhere Dense|dense subset]] of $\mathcal C \,\big / \mathcal N$	1
:the constant [[Definition:Sequence|sequence]] $\tuple {\lambda, \lambda, \lambda, \dots}$ is [[Definition:Convergent Sequence in Normed Division Ring|convergent]] and $\displaystyle \lim_{n \mathop \to \infty} \lambda = \lambda$	1
Let $\map {\R^3} {x, y, z}$ denote the [[Definition:Real Cartesian Space|real Cartesian space]] of [[Definition:Dimension of Vector Space|$3$ dimensions]].. Let $\tuple {\mathbf i, \mathbf j, \mathbf k}$ be the [[Definition:Standard Ordered Basis on Vector Space|standard ordered basis on $\R^3$]]. Let $\mathbf f := \tuple {\map {f_x} {\mathbf x}, \map {f_y} {\mathbf x}, \map {f_z} {\mathbf x} }: \R^3 \to \R^3$ be a [[Definition:Vector-Valued Function|vector-valued function]] on $\R^3$. Let $\map g {x, y, z}: \R^3 \to \R$ be a [[Definition:Real-Valued Function|real-valued function]] on $\R^3$. Let $\nabla \times \mathbf f$ denote the [[Definition:Curl Operator|curl]] of $f$. Then: :$\nabla \times \paren {g \, \mathbf f} = \map g {\nabla \times \mathbf f} + \paren {\nabla g} \times \mathbf f$	1
:$\forall x \in R: \norm x_1 = \norm x_2^\alpha$	1
Let $\struct {K, +_K, \times_K}$ be a [[Definition:Division Ring|division ring]]. Let $\struct {V, +_V, \circ_V}_K$ be a [[Definition:Vector Space|$K$-vector space]]. Let $\struct {W, +_W, \circ_W}_K$ be a [[Definition:R-Algebraic Structure|$K$-algebraic structure]]. Let $\phi: V \to W$ be a [[Definition:R-Algebraic Structure Homomorphism|homomorphism]], i.e. a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Then the [[Definition:Homomorphic Image|homomorphic image]] of $\phi$ is a [[Definition:Vector Space|$K$-vector space]].	1
{{ProofWanted}} {{Namedfor|Raoul Bott|name2 = John Willard Milnor|name3 = Michel André Kervaire|cat = Bott|cat2 = Milnor|cat3 = Kervaire}}	1
:$\mathcal N$ is a [[Definition:Maximal Left Ideal of Ring|maximal left ideal]].	1
$T$ is [[Definition:Separable Space|separable]].	1
Let $R$ be a [[Definition:Commutative Ring|commutative ring]]. Let $\left({S, +, *}\right)$ be a [[Definition:Commutative Ring|commutative ring]]. Let $f: R \to S$ be a [[Definition:Ring Homomorphism|ring homomorphism]]. Let $\left({S_R, *}\right)$ be the [[Definition:Algebra Defined by Ring Homomorphism|algebra defined by the ring homomorphism]] $f$. Then $\left({S_R, *}\right)$ is an [[Definition:Commutative Algebra|commutative algebra]].	1
We are to show that: :$x \divides y \text{ and } y \divides x \iff \ideal x = \ideal y$ Thus: {{begin-eqn}} {{eqn | o = | r = x \divides y \text{ and } y \divides x | c = {{Defof|Associate|subdef = Integral Domain|index = 1|Associate in Integral Domain}} }} {{eqn | o = \leadstoandfrom | r = \ideal y \subseteq \ideal x \text{ and } \ideal x \subseteq \ideal y | c = [[Element in Integral Domain is Divisor iff Principal Ideal is Superset]] }} {{eqn | o = \leadstoandfrom | r = \ideal x = \ideal y | c = {{Defof|Set Equality|index = 2}} }} {{end-eqn}} {{qed}}	1
From [[Matrix Product as Linear Transformation]], $\mathbf {Ax} = \mathbf 0$ defines a [[Definition:Linear Transformation on Vector Space|linear transformation]] from $\R^m$ to $\R^n$. The result then follows from [[Linear Transformation Maps Zero Vector to Zero Vector]].	1
{{WLOG}}, we will only prove $OB$ [[Definition:Angle Bisector|bisects]] $\angle AOC$. Let the [[Definition:Vector (Euclidean Space)|position vector]] of $A$, $B$ and $C$ with respect to $O$ be $\mathbf a$, $\mathbf b$ and $\mathbf c$ respectively. By definition of [[Definition:Rhombus|rhombus]], we have: {{begin-eqn}} {{eqn | n = a | l = \mathbf a + \mathbf c | r = \mathbf b | c = [[Parallelogram Law]] }} {{eqn | n = b | l = \norm {\mathbf a} | r = \norm {\mathbf c} | c = }} {{end-eqn}} From the above we have: {{begin-eqn}} {{eqn | l = \cos \angle \mathbf a, \mathbf b | r = \frac {\mathbf a \cdot \mathbf b} {\norm {\mathbf a} \norm {\mathbf b} } | c = {{Defof|Dot Product|index = 2}} }} {{eqn | r = \frac {\mathbf a \cdot \paren {\mathbf a + \mathbf c} } {\norm {\mathbf a} \norm {\mathbf b} } | c = from $(a)$ above: $\mathbf b = \mathbf a + \mathbf c$ }} {{eqn | r = \frac {\mathbf a \cdot \mathbf a + \mathbf a \cdot \mathbf c} {\norm {\mathbf a} \norm {\mathbf b} } | c = [[Dot Product Distributes over Addition]] }} {{eqn | r = \frac { {\norm {\mathbf a} }^2 + \mathbf a \cdot \mathbf c} {\norm {\mathbf a} \norm {\mathbf b} } | c = [[Dot Product of Vector with Itself]] }} {{eqn | r = \frac { {\norm {\mathbf c} }^2 + \mathbf a \cdot \mathbf c} {\norm {\mathbf c} \norm {\mathbf b} } | c = from $(b)$ above: $\norm {\mathbf a} = \norm {\mathbf c}$ }} {{eqn | r = \frac {\mathbf c \cdot \mathbf c + \mathbf a \cdot \mathbf c} {\norm {\mathbf c} \norm {\mathbf b} } | c = [[Dot Product of Vector with Itself]] }} {{eqn | r = \frac {\mathbf c \cdot \left({\mathbf a + \mathbf c}\right)} {\norm {\mathbf c} \norm {\mathbf b} } | c = [[Dot Product Distributes over Addition]] }} {{eqn | r = \frac {\mathbf c \cdot \mathbf b} {\norm {\mathbf c} \norm {\mathbf b} } | c = from $(a)$ above: $\mathbf b = \mathbf a + \mathbf c$ }} {{eqn | r = \cos \angle \mathbf c, \mathbf b | c = {{Defof|Dot Product|index = 2}} }} {{end-eqn}} By definition of [[Definition:Dot Product/Definition 2|dot product]], the angle between the vectors is between $0$ and $\pi$. From [[Shape of Cosine Function]], [[Definition:Cosine|cosine]] is [[Definition:Injection|injective]] on this interval. Hence: :$\angle \mathbf a, \mathbf b = \angle \mathbf c, \mathbf b$ The result follows. {{qed}}	1
Let: :$z_1 = x_1 + i y_1$ :$z_2 = x_2 + i y_2$ :$z_3 = x_3 + i y_3$ Then: {{begin-eqn}} {{eqn | l = z_1 \times \paren {z_2 + z_3} | r = \paren {x_1 + i y_1} \times \paren {\paren {x_2 + i y_2} + \paren {x_3 + i y_3} } | c = }} {{eqn | r = \paren {x_1 + i y_1} \times \paren {\paren {x_2 + x_3} + i \paren {y_2 + y_3} } | c = {{Defof|Complex Addition}} }} {{eqn | r = x_1 \left({y_2 + y_3}\right) - y_1 \left({x_2 + x_3}\right) | c = {{Defof|Vector Cross Product|subdef = Complex|index = 1|Complex Cross Product}} }} {{eqn | r = x_1 y_2 + x_1 y_3 - y_1 x_2 - y_1 x_3 | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = x_1 y_2 - y_1 x_2 + x_1 y_3 - y_1 x_3 | c = [[Real Addition is Commutative]] }} {{eqn | r = z_1 \times z_2 + z_1 \times z_3 | c = {{Defof|Vector Cross Product|subdef = Complex|index = 1|Complex Cross Product}} }} {{end-eqn}} {{qed}}	1
Let $\mathbf A \left({t}\right)$ be a continuous matrix function with period $T$. Let $\Phi \left({t}\right)$ be a [[Definition:Fundamental Matrix|fundamental matrix]] of the [[Definition:Floquet System|Floquet system]] $\mathbf x' = \mathbf A \left({t}\right) \mathbf x$. Then $\Phi \left({t + T}\right)$ is also a [[Definition:Fundamental Matrix|fundamental matrix]]. Moreover, there exists: : A nonsingular, continuously differentiable matrix function $\mathbf P \left({t}\right)$ with period $T$ : A constant (possibly complex) matrix $\mathbf B$ such that: ::$\Phi \left({t}\right) = \mathbf P \left({t}\right) e^{\mathbf Bt}$	1
Let $\R$ be the set of [[Definition:Real Number|real numbers]]. Then the [[Definition:Real Vector Space|real vector space $\R^n$]] is a [[Definition:Vector Space|vector space]].	1
Let $C_n$ be the [[Definition:Square Matrix|square]] [[Definition:Cauchy Matrix|Cauchy matrix]] of [[Definition:Order of Square Matrix|order $n$]]: :$C_n = \begin{bmatrix} \dfrac 1 {x_1 + y_1} & \dfrac 1 {x_1 + y_2} & \cdots & \dfrac 1 {x_1 + y_n} \\ \dfrac 1 {x_2 + y_1} & \dfrac 1 {x_2 + y_2} & \cdots & \dfrac 1 {x_2 + y_n} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac 1 {x_n + y_1} & \dfrac 1 {x_n + y_2} & \cdots & \dfrac 1 {x_n + y_n} \\ \end{bmatrix}$ Then its [[Definition:Inverse Matrix|inverse]] $C_n^{-1} = \sqbrk b_n$ can be specified as: :$\begin{bmatrix} b_{ij} \end{bmatrix} = \begin{bmatrix} \dfrac {\displaystyle \prod_{k \mathop = 1}^n \paren {x_j + y_k} \paren {x_k + y_i} } {\displaystyle \paren {x_j + y_i} \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne j} } \paren {x_j - x_k} } \paren {\prod_{\substack {1 \mathop \le k \mathop \le n \\ k \mathop \ne i} } \paren {y_i - y_k} } } \end{bmatrix}$	1
Let $R$ be a [[Definition:Division Ring|division ring]]. Let $V$ be an [[Definition:Vector Space|$R$-vector space]]. Let $F \subseteq V$ be a [[Definition:Finite Set|finite]] [[Definition:Generator of Module|generator]] of $V$ over $R$. Let $L \subseteq V$ be [[Definition:Linearly Independent Set|linearly independent]] over $R$. Then: :$\size L \le \size F$	1
Let $\struct {R, \norm{\,\cdot\,} }$ be a [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean normed division ring]]. Let $\tau$ be the [[Definition:Topology Induced by Division Ring Norm|topology induced]] by the [[Definition:Norm on Division Ring|norm]] $\norm{\,\cdot\,}$. Then the [[Definition:Topological Space|topological space]] $\struct {R, \tau}$ is [[Definition:Totally Disconnected Space|totally disconnected]].	1
=== Lemma === Let$\mathbf J_n$ be the $n \times n$ [[Definition:Square Ones Matrix|square ones matrix]]. Let $\mathbf A$ be an $n \times n$ matrix. Let $\mathbf A_{ij}$ denote the [[Definition:Cofactor|cofactor]] of element $a_{ij}$ in $\map \det {\mathbf A}$, $1 \le i, j \le n$. Then: {{begin-eqn}} {{eqn | l = \map \det {\mathbf A - \mathbf J_n} | r = \map \det {\mathbf A} - \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n \mathbf A_{i j} | c = [[Cofactor Sum Identity]] }} {{end-eqn}} {{qed|lemma}} Let $\mathbf A = \mathbf B^{-1}$. Then: {{begin-eqn}} {{eqn | l = \mathbf A \mathbf B | r = \mathbf I | c = {{Defof|Invertible Matrix}} }} {{eqn | l = \mathbf B | r = \dfrac {\adj {\mathbf A} } {\map \det {\mathbf A} } | c = [[Matrix Product with Adjugate Matrix]] with $\mathbf B = \mathbf A^{-1}$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n b_{ij} | r = \dfrac 1 {\map \det {\mathbf A} } \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n \mathbf A_{ij} }} {{eqn | r = \map \det {\mathbf B} \sum_{i \mathop = 1}^n \sum_{j \mathop = 1}^n \mathbf A_{ij} | c = [[Determinant of Matrix Product]] and $A B = I$ }} {{eqn | r = \map \det {\mathbf B} \paren {\map \det {\mathbf A} - \map \det {\mathbf A - \mathbf J_n} } | c = Apply the '''Lemma''' }} {{eqn | r = \map \det {\mathbf B} \map \det {\mathbf A} - \map \det {\mathbf B} \map \det {\mathbf B^{-1} - \mathbf J_n} | c = Definition $A = B^{-1}$ }} {{eqn | r = 1 - \map \det {\mathbf B} \map \det {\mathbf B^{-1} - \mathbf J_n} | c = [[Determinant of Matrix Product]] and $\mathbf B \mathbf A = \mathbf I$ }} {{end-eqn}} {{qed}} [[Category:Matrix Theory]] 4e6rv083tspz41uzh9ugbnfccilb15a	1
Let $\struct {R, +, \times}$ be a [[Definition:Ring (Abstract Algebra)|ring]]. Then $\struct {R, +, \times}$ is a [[Definition:Right Module|right module]] over $\struct {R, +, \times}$.	1
Let $\mathbf T_n$ be a [[Definition:Lower Triangular Matrix|lower triangular matrix]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf T_n}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf T_n$. Then $\map \det {\mathbf T_n}$ is equal to the product of all the [[Definition:Diagonal Element|diagonal elements]] of $\mathbf T_n$. That is: :$\displaystyle \map \det {\mathbf T_n} = \prod_{k \mathop = 1}^n a_{k k}$	1
By hypothesis there is a [[Definition:Basis of Vector Space|basis]] $B$ of $E$ with $n$ elements. Then $H \cup B$ is a [[Definition:Generator of Module|generator]] for $E$. So by [[Vector Space has Basis Between Linearly Independent Set and Finite Spanning Set]] there exists a [[Definition:Basis of Vector Space|basis]] $C$ of $E$ such that $H \subseteq C \subseteq H \cup B$. {{Qed}}	1
By [[Three Points in Ultrametric Space have Two Equal Distances/Corollary 2|Corollary 2]] then: :$\norm {x + y} = \norm {x - y} = \norm {y - x} = \max \set {\norm x, \norm y} = \norm y$ {{qed}}	1
{{WIP}}	1
Let $S$ be a [[Definition:Subring|subring]] of the [[Definition:Ring (Abstract Algebra)|ring]] $\struct {R, +, \circ}$. Let $\circ_S$ be the [[Definition:Restriction of Operation|restriction]] of $\circ$ to $S \times R$. Then $\struct {R, +, \circ_S}_S$ is an [[Definition:Module|$S$-module]]. If $\struct {R, +, \circ}$ has a [[Definition:Unity of Ring|unity]], $1_R$, and $1_R \in S$, then $\struct {R, +, \circ_S}_S$ is a [[Definition:Unitary Module|unitary $S$-module]].	1
Let $K$ be a [[Definition:Division Ring|division ring]]. Let $\struct {S, +, \circ}_K$ be a [[Definition:R-Algebraic Structure|$K$-algebraic structure]] with one operation. Let $T$ be a [[Definition:Closed for Scalar Product|closed subset]] of $S$. Let $\struct {T, +_T, \circ_T}_K$ be a [[Definition:Vector Space|$K$-vector space]] where: :$+_T$ is the [[Definition:Restriction of Operation|restriction]] of $+$ to $T \times T$ and :$\circ_T$ is the restriction of $\circ$ to $K \times T$. Then $\struct {T, +_T, \circ_T}_K$ is a '''(vector) subspace''' of $\struct {S, +, \circ}_K$.	1
For every [[Definition:Integer|integer]] $n$ such that $n > 1$, $n$ can be expressed as the [[Definition:Integer Multiplication|product]] of one or more [[Definition:Prime Number|primes]], uniquely up to the order in which they appear.	1
By definition of [[Definition:Inverse Matrix|inverse matrix]]: :$\mathbf A^{-1} \mathbf A = \mathbf I$ where $\mathbf I$ is the [[Definition:Unit Matrix|unit matrix]]. Thus the [[Definition:Inverse Matrix|inverse]] of $\mathbf A^{-1}$ is $\mathbf A$. Hence the result. {{qed}} [[Category:Inverse Matrices]] c92664gz3mew2c1geggrxuvcjmj5c4x	1
Let $\psi: S_1 \to S_2$ be defined by: :$\forall x \in S_1: \map \psi x = \displaystyle \lim_{n \mathop \to \infty} \map {\psi'} {x_n}$ where $x = \displaystyle \lim_{n \mathop \to \infty} x_n$ for some [[Definition:Sequence|sequence]] $\sequence {x_n} \subseteq R_1$ Then $\psi$ is a [[Definition:Well-Defined Mapping|well-defined mapping]].	1
{{begin-eqn}} {{eqn | l = \cmod {z + w}^2 | r = \paren {z + w} \paren {\overline z + \overline w} | c = [[Product of Complex Number with Conjugate]] }} {{eqn | r = z \overline z + w \overline w + w \overline z + z \overline w | c = }} {{eqn | r = \cmod z^2 + \cmod w^2 + w \overline z + z \overline w | c = [[Product of Complex Number with Conjugate]] }} {{eqn | r = \cmod z^2 + \cmod w^2 + w \overline z + \overline {\paren {\overline z} } \overline w | c = [[Complex Conjugation is Involution]] }} {{eqn | r = \cmod z^2 + \cmod w^2 + \overline z w + \overline {\paren {\overline z w} } | c = [[Product of Complex Conjugates]] and [[Complex Multiplication is Commutative]] }} {{eqn | r = \cmod z^2 + \cmod w^2 + 2 \map \Re {z \overline w} | c = [[Sum of Complex Number with Conjugate]] }} {{eqn | o = \le | r = \cmod z^2 + \cmod w^2 + 2 \cmod {z \overline w} | c = [[Modulus Larger than Real Part]] }} {{eqn | r = \cmod z^2 + \cmod w^2 + 2 \cmod z \cmod {\overline w} | c = [[Complex Modulus of Product of Complex Numbers]] }} {{eqn | r = \cmod z^2 + \cmod w^2 + 2 \cmod z \cmod w | c = [[Complex Modulus equals Complex Modulus of Conjugate]] }} {{eqn | r = \paren {\cmod z + \cmod w}^2 | c = [[Square of Sum]] }} {{eqn | ll= \leadsto | l = \cmod {z + w} | o = \le | r = \cmod z + \cmod w | c = }} {{end-eqn}} {{qed}}	1
By definition of the [[Definition:Unit Matrix|unit matrix]]: :$I_{a b} = \delta_{a b}$ where: :$I_{a b}$ denotes the [[Definition:Element of Matrix|element]] of $\mathbf I$ whose [[Definition:Index of Matrix Element|indices]] are $\tuple {a, b}$. By definition, $\mathbf E$ is the [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $m$]] formed by applying $e$ to the [[Definition:Unit Matrix|unit matrix]] $\mathbf I$. That is, all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $i$ of $\mathbf I$ are to have the corresponding [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $j$ added to them after the latter have been [[Definition:Ring Product|multiplied]] by $\lambda$. By definition of [[Definition:Unit Matrix|unit matrix]]: :all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $i$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{i i}$, which is $1$. :all [[Definition:Element of Matrix|elements]] of [[Definition:Row of Matrix|row]] $j$ are $0$ except for [[Definition:Element of Matrix|element]] $I_{j j}$, which is $1$. Thus in $\mathbf E$: :where $a \ne i$, $E_{a b} = \delta_{a b}$ :where $a = i$: ::$E_{a b} = \delta_{a b}$ where $b \ne j$ ::$E_{a b} = \delta_{a b} + \lambda \cdot 1$ where $b = j$ That is: :$E_{a b} = \delta_{a b}$ for all [[Definition:Element of Matrix|elements]] of $\mathbf E$ except where $a = i$ and $b = j$, at which [[Definition:Element of Matrix|element]]: :$E_{a b} = \delta_{a b} + \lambda$ That is: :$E_{a b} = \delta_{a b} + \lambda \cdot \delta_{a i} \cdot \delta_{j b}$ Hence the result. {{qed}}	1
Let $\struct {K, +, \circ}$ be a [[Definition:Division Ring|division ring]]. Let $n \in \N_{>0}$. Let $\struct {K^n, +, \times}_K$ be the '''[[Definition:Vector Space on Cartesian Product|$K$-vector space $K^n$]]'''. Then $\struct {K^n, +, \times}_K$ is a [[Definition:Vector Space|$K$-vector space]].	1
Let $z_1$ and $z_2$ be [[Definition:Complex Number as Vector|complex numbers in vector form]] such that $z_1 \ne 0$ and $z_2 \ne 0$. Then $z_1$ and $z_2$ are [[Definition:Parallel Vectors|parallel]] {{iff}}: :$z_1 \times z_2 = 0$ where $z_1 \times z_2$ denotes the [[Definition:Complex Cross Product|complex cross product]] of $z_1$ with $z_2$.	1
Let $\mathbf A = \sqbrk a_n$ be a [[Definition:Square Matrix|square matrix]] of [[Definition:Order of Square Matrix|order $n$]]. {{TFAE|def = Determinant of Matrix|view = the determinant of $\mathbf A$}}	1
Checking the three defining properties of an [[Definition:Equivalence Relation|equivalence relation]] in turn: === Reflexivity === For any [[Definition:Hilbert Space|Hilbert space]] $H$, we have the [[Definition:Identity Mapping|identity map]] $\operatorname{id}: H \to H$. [[Identity Mapping is Linear]] yields $\operatorname{id}$ to be a [[Definition:Linear Mapping|linear map]]. Also, [[Identity Mapping is Bijection]] ensures that $\operatorname{id}$ is a [[Definition:Surjection|surjection]]. Lastly, for any $g, h \in H$, we have: :$\left\langle{g, h}\right\rangle = \left\langle{\operatorname{id} \left({g}\right), \operatorname{id} \left({h}\right)}\right\rangle$ Therefore, the three defining properties for a [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] are satisfied by $\operatorname{id}$. Hence, any [[Definition:Hilbert Space|Hilbert space]] $H$ is isomorphic to itself. It follows that [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] is [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetry === Let $H, K$ be [[Definition:Isomorphism (Hilbert Spaces)|isomorphic]] [[Definition:Hilbert Space|Hilbert spaces]], and let $U: H \to K$ be a [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]]. From [[Hilbert Space Isomorphism is Bijection]] and the definition of [[Definition:Bijection/Definition 3|bijection]], it follows that $U$ has a two-sided inverse, $U^{-1}: K \to H$. [[Inverse of Linear Map is Linear]] yields $U^{-1}$ to be a [[Definition:Linear Mapping|linear map]] as well. [[Bijection iff Inverse is Bijection]] yields that $U^{-1}$ is also a [[Definition:Surjection|surjection]]. For $k, l \in K$, the computation $\left\langle{k, l}\right\rangle_K = \left\langle{U U^{-1}k, U U^{-1}l}\right\rangle_K = \left\langle{U^{-1}k, U^{-1}l}\right\rangle_H$ shows that $U^{-1}$ preserves the inner product (where in the last step, the fact that $U$ is an isomorphism is used). Hence, $U^{-1}$ is an isomorphism, so $K$ and $H$ are also isomorphic. It follows that [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] is [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitivity === Let $H, K, L$ be [[Definition:Hilbert Space|Hilbert spaces]], and let $U: H \to K, V: K \to L$ be [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphisms]]. Consider the [[Definition:Composition of Mappings|composition]] $VU : H \to L$. From [[Composition of Linear Maps is Linear]], $VU$ is a [[Definition:Linear Mapping|linear map]]. [[Composite of Surjections is Surjection]] yields $VU$ to be a [[Definition:Surjection|surjection]]. Lastly, for $g, h \in H$, we have: :$\left\langle{g, h}\right\rangle_H = \left\langle{Ug, Uh}\right\rangle_K = \left\langle{VUg, VUh}\right\rangle_L$ Hence $VU$ is a [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]], and $H$ and $L$ are isomorphic. It follows that [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] is [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} Having verified these three conditions, it follows that [[Definition:Isomorphism (Hilbert Spaces)|Hilbert space isomorphism]] is an [[Definition:Equivalence Relation|equivalence relation]], {{qed}}	1
{{ProofWanted|tedious}}	1
Because $R$ has [[Definition:Characteristic of Ring|characteristic]] $p > 0$, the [[Definition:Set|set]]: :$\set {n \cdot 1_k: n \in \Z}$ has [[Definition:Cardinality|cardinality]] $p - 1$. Therefore: :$\sup \set {\norm {n \cdot 1_R}: n \in \Z} = \max \set {\norm {1 \cdot 1_R}, \norm {2 \cdot 1_R}, \cdots, \norm {\paren {p - 1} \cdot 1_R} } < +\infty$ By [[Characterisation of Non-Archimedean Division Ring Norms/Corollary 2|Corollary 2]]: :$\norm{\,\cdot\,}$ is [[Definition:Non-Archimedean Division Ring Norm|non-Archimedean]] and $C = 1$. {{qed}} [[Category:Characterisation of Non-Archimedean Division Ring Norms]] qpz1930i9qr4qtkaa39un5u70jdvup9	1
Let $U$ be a subset of $\R^3$ which is [[Definition:Compact Subset of Real Euclidean Space|compact]] and has a [[Definition:Piecewise Smooth|piecewise smooth]] [[Definition:Boundary (Geometry)|boundary]] $\partial U$. Let $\mathbf F: \R^3 \to \R^3$ be a [[Definition:Smooth Mapping|smooth]] [[Definition:Vector Function|vector function]] defined on a [[Definition:Neighborhood (Metric Space)|neighborhood]] of $U$. Then: :$\displaystyle \iiint \limits_U \paren {\nabla \cdot \mathbf F} \rd V = \iint \limits_{\partial U} \mathbf F \cdot \mathbf n \rd S$ where $\mathbf n$ is the [[Definition:Normal to Surface|normal]] to $\partial U$.	1
Let the first [[Definition:Column of Matrix|column]] of $\mathbf A$ containing a non-[[Definition:Field Zero|zero]] [[Definition:Element of Matrix|element]] be [[Definition:Column of Matrix|column]] $j$. Let such a non-[[Definition:Field Zero|zero]] [[Definition:Element of Matrix|element]] be in [[Definition:Row of Matrix|row]] $i$. Take [[Definition:Element of Matrix|element]] $a_{i j} \ne 0$ and perform the [[Definition:Elementary Row Operation|elementary row operations]]: :$(1): \quad r_i \to \dfrac {r_i} {a_{i j}}$ :$(2): \quad r_1 \leftrightarrow r_i$ This gives a [[Definition:Matrix|matrix]] with $1$ in the $\tuple {1, j}$ position: :$\begin {bmatrix} 0 & \cdots & 0 & 1 & b_{1, j + 1} & \cdots & b_{1 n} \\ 0 & \cdots & 0 & b_{2 j} & b_{2, j + 1} & \cdots & b_{2 n} \\ \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & b_{m j} & b_{m, j + 1} & \cdots & b_{m n} \\ \end {bmatrix}$ Now the [[Definition:Elementary Row Operation|elementary row operations]] $r_k \to r_k - b_{k j} r_1, k \in \set {2, 3, \ldots, m}$ gives the [[Definition:Matrix|matrix]]: :$\begin{bmatrix} 0 & \cdots & 0 & 1 & c_{1, j + 1} & \cdots & c_{1 n} \\ 0 & \cdots & 0 & 0 & c_{2, j + 1} & \cdots & c_{2 n} \\ \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & 0 & c_{m, j + 1} & \cdots & c_{m n} \\ \end{bmatrix}$ If some [[Definition:Zero Row or Column|zero rows]] have appeared, do some further [[Definition:Elementary Row Operation|elementary row operations]], that is row interchanges, to put them at the bottom. We now repeat the process with the remaining however-many-there-are [[Definition:Row of Matrix|rows]]: :$\begin{bmatrix} \cdots & 0 & 1 & d_{1, j + 1} & \cdots & d_{1, k - 1} & d_{1 k} & d_{1, k + 1} & \cdots & d_{1 n} \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 1 & d_{2, k + 1} & \cdots & d_{2 n} \\ \cdots & 0 & 0 & 0 & \cdots & 0 & d_{3 k} & d_{3, k + 1} & \cdots & d_{3 n} \\ \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \cdots & 0 & 0 & 0 & \cdots & 0 & d_{n k} & d_{m, k + 1} & \cdots & d_{m n} \\ \end{bmatrix}$ Then we can get the [[Definition:Reduced Echelon Matrix|reduced echelon form]] by: :$r_i \to r_i - d_{i k} r_2, i \in \set {1, 3, 4, \ldots, m}$ as follows: :$\begin{bmatrix} \cdots & 0 & 1 & {e_{1, j + 1 } } & \cdots & {e_{1, k - 1} } & 0 & {e_{1, k + 1} } & \cdots & {e_{1 n} } \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 1 & {e_{2, k + 1} } & \cdots & {e_{2 n} } \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 0 & {e_{3, k + 1} } & \cdots & {e_{3 n} } \\ \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \cdots & 0 & 0 & 0 & \cdots & 0 & 0 & {e_{m, k + 1} } & \cdots & {e_{m n} } \\ \end{bmatrix}$ Thus we progress, until the entire [[Definition:Matrix|matrix]] is in [[Definition:Reduced Echelon Matrix|reduced echelon]] form. {{Qed}}	1
By [[Properties of Norm on Division Ring/Norm of Negative of Unity|Norm of Negative of Unity]] then: :$\norm{-1_R} = 1$. Then: {{begin-eqn}} {{eqn | l = \norm{-x} | r = \norm{-1_R \circ x} | c = [[Product with Ring Negative]] }} {{eqn | r = \norm{-1_R} \norm{x} | c = [[Definition:Norm Axioms|Norm axiom (N2) (Multiplicativity)]] }} {{eqn | r = \norm{x} | c = [[Properties of Norm on Division Ring/Norm of Negative of Unity|Norm of Negative of Unity]] }} {{end-eqn}} as desired. {{qed}}	1
Let $\mathbf A = \begin{bmatrix} a_{11} & a_{12} & \ldots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \\ \end{bmatrix}$. Then $\mathbf A^\intercal = \begin{bmatrix} a_{11} & a_{21} & \ldots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \\ \end{bmatrix}$. Let $b_{r s} = a_{s r}$ for $1 \le r, s \le n$. We need to show that $\map \det {\sqbrk a_n} = \map \det {\sqbrk b_n}$. By the definition of [[Definition:Determinant of Matrix|determinant]] and [[Permutation of Determinant Indices]], we have: {{begin-eqn}} {{eqn | l = \map \det {\sqbrk b_n} | r = \sum_\lambda \map {\sgn} \lambda b_{1 \map \lambda 1} b_{2 \map \lambda 2} \cdots b_{n \map \lambda n} | c = }} {{eqn | r = \sum_\lambda \map {\sgn} \lambda a_{\map \lambda 1 1} a_{\map \lambda 2 2} \cdots a_{\map \lambda n n} | c = }} {{eqn | r = \map \det {\sqbrk a_n} | c = }} {{end-eqn}} {{qed}}	1
Let $T = \struct {S, \set {\O, S} }$ be an [[Definition:Indiscrete Space|indiscrete topological space]] such that $S$ has more than one element. Then $T$ is [[Definition:Separable Space|separable]].	1
Let $\mathbf A = \sqbrk a_n$ and $\mathbf B = \sqbrk b_n$ be a [[Definition:Square Matrix|square matrices]] of [[Definition:Order of Square Matrix|order $n$]]. Let $\map \det {\mathbf A}$ be the [[Definition:Determinant of Matrix|determinant]] of $\mathbf A$. Let $\mathbf A \mathbf B$ be the [[Definition:Matrix Product (Conventional)|(conventional) matrix product]] of $\mathbf A$ and $\mathbf B$. Then: :$\map \det {\mathbf A \mathbf B} = \map \det {\mathbf A} \map \det {\mathbf B}$ That is, the [[Definition:Determinant of Matrix|determinant]] of the [[Definition:Matrix Product (Conventional)|product]] is equal to the [[Definition:Multiplication|product]] of the [[Definition:Determinant of Matrix|determinants]].	1
Let $\sequence {x_n}$ be [[Definition:Convergent Sequence in Normed Division Ring|convergent]] to the [[Definition:Limit of Sequence (Normed Division Ring)|limit]] $l$ in $\struct {R, \norm {\,\cdot\,} }$. By [[Modulus of Limit/Normed Division Ring|modulus of limit in normed division ring]], $\sequence {\norm {x_n} }$ is a [[Definition:Convergent Real Sequence|convergent sequence in $\R$]]. By [[Convergent Real Sequence is Bounded]], $\sequence {\norm {x_n} }$ is [[Definition:Bounded Real Sequence|bounded]]. That is: :$\exists M \in \R_{> 0}: \forall n, \norm {x_n} = \size {\norm {x_n} } \le M$ Thus, by definition, $\sequence {x_n}$ is [[Definition:Bounded Sequence in Normed Division Ring|bounded]]. {{qed}}	1
Let $E$ be a [[Definition:Vector Space|vector space]] of $n$ [[Definition:Dimension of Vector Space|dimensions]]. Let $H$ be a [[Definition:Linearly Independent Set|linearly independent subset]] of $E$. {{:Linearly Independent Set is Contained in some Basis}}	1
{{begin-eqn}} {{eqn | l = \mathbf b \times \mathbf a | r = \begin {bmatrix} b_i \\ b_j \\ b_k \end {bmatrix} \times \begin {bmatrix} a_i \\ a_j \\ a_k \end {bmatrix} }} {{eqn | r = \begin {bmatrix} b_j a_k - a_j b_k \\ b_k a_i - b_i a_k \\ b_i a_j - a_i b_j \end {bmatrix} }} {{eqn | l = \mathbf a \times \mathbf b | r = \begin {bmatrix} a_i \\ a_j \\ a_k \end {bmatrix} \times \begin {bmatrix} b_i \\ b_j \\ b_k \end {bmatrix} }} {{eqn | r = \begin {bmatrix} a_j b_k - a_k b_j \\ a_k b_i - a_i b_k \\ a_i b_j - a_j b_i \end {bmatrix} }} {{eqn | r = \begin {bmatrix} -\paren {a_k b_j - a_j b_k} \\ -\paren {a_i b_k - a_k b_i} \\ -\paren {a_j b_i - a_i b_j} \end {bmatrix} }} {{eqn | r = -1 \begin {bmatrix} b_j a_k - a_j b_k \\ b_k a_i - b_i a_k \\ b_i a_j - a_i b_j \end {bmatrix} }} {{eqn | r = -\paren {\mathbf b \times \mathbf a} }} {{end-eqn}} {{qed}}	1
Let $f, g, h \in \map \CC I$ such that: :$f, g, h : I \to \R$ Let $\lambda, \mu \in \R$. Let $\map 0 x$ be a [[Definition:Real-Valued Function|real-valued function]] such that: :$\map 0 x : I \to 0$. Let us use [[Definition:Real Number|real number]] [[Definition:Real Addition|addition]] and [[Definition:Real Multiplication|multiplication]]. $\forall x \in I$ define [[Definition:Pointwise Addition of Real-Valued Functions|pointwise addition]] as: :$\map {\paren {f + g}} x := \map f x +_\R \map g x$. Define [[Definition:Pointwise Scalar Multiplication of Real-Valued Functions|pointwise scalar multiplication]] as: :$\map {\paren {\lambda \cdot f}} x := \lambda \times_\R \map f x$ Let $\map {\paren {-f} } x := -\map f x$. === Closure Axiom === By [[Sum Rule for Continuous Functions]], $f + g \in \map \CC I$ {{qed|lemma}} === Commutativity Axiom === By [[Pointwise Addition on Real-Valued Functions is Commutative]], $f + g = g + f$ {{qed|lemma}} === Associativity Axiom === By [[Pointwise Addition is Associative]], $\paren {f + g} + h = f + \paren {g + h}$. {{qed|lemma}} === Identity Axiom === {{begin-eqn}} {{eqn | l = \map {\paren {0 + f} } x | r = \map 0 x +_\R \map f x | c = {{Defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | r = 0 +_\R \map f x | c = Definition of $\map 0 x$ }} {{eqn | r = \map f x }} {{end-eqn}} {{qed|lemma}} === Inverse Axiom === {{begin-eqn}} {{eqn | l = \map {\paren {f + \paren {-f} } } x | r = \map f x +_\R \map {\paren {-f} } x | c = {{Defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | r = \map f x +_\R \paren {-1} \times_\R \map f x | c = Definition of $\map {\paren {-f} } x$ }} {{eqn | r = 0 }} {{end-eqn}} {{qed|lemma}} === Distributivity over Scalar Addition === {{begin-eqn}} {{eqn | l = \map {\paren { \paren {\lambda +_\R \mu} f} } x | r = \paren {\lambda +_\R \mu} \times_\R \map f x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \lambda \times_\R \map f x +_\R \mu \times_\R \map f x | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \map {\paren {\lambda \cdot f} } x +_\R \map {\paren {\mu\cdot f} } x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \map {\paren {\lambda \cdot f + \mu \cdot f} } x | c = {{Defof|Pointwise Addition of Real-Valued Functions}} }} {{end-eqn}} {{qed|lemma}} === Distributivity over Vector Addition === {{begin-eqn}} {{eqn | l = \lambda \times_\R \map {\paren {f + g} } x | r = \lambda \times_\R \paren {\map f x +_\R \map g x} | c = {{Defof|Pointwise Addition of Real-Valued Functions}} }} {{eqn | r = \lambda \times_R \map f x +_\R \lambda \times_\R \map g x | c = [[Real Multiplication Distributes over Addition]] }} {{eqn | r = \map {\paren{\lambda \cdot f} } x +_\R \map {\paren{\lambda \cdot g} } x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \map {\paren {\lambda \cdot f + \mu \cdot f} } x | c = {{Defof|Pointwise Addition of Real-Valued Functions}} }} {{end-eqn}} {{qed|lemma}} === Associativity with Scalar Multiplication === {{begin-eqn}} {{eqn | l = \map {\paren {\paren {\lambda \times_\R \mu} \cdot f} } x | r = \paren {\lambda \times_\R \mu} \times_\R \map f x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \lambda \times_\R \paren {\mu \times_\R \map f x} | c = [[Real Multiplication is Associative]] }} {{eqn | r = \lambda \times_\R \map {\paren {\mu \cdot f} } x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \map {\paren {\lambda \cdot \paren {\mu \cdot f} } } x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{end-eqn}} {{qed|lemma}} === Identity for Scalar Multiplication === {{begin-eqn}} {{eqn | l = \map {\paren {1 \cdot f} } x | r = 1 \times_\R \map f x | c = {{Defof|Pointwise Scalar Multiplication of Real-Valued Functions}} }} {{eqn | r = \map f x }} {{end-eqn}} {{qed}}	1
{{begin-eqn}} {{eqn | l = \cmod {z_1 + z_2} | o = \ge | r = \cmod {z_1} - \cmod {z_2} | c = [[Triangle Inequality/Complex Numbers/Corollary 1|Triangle Inequality for Complex Numbers: Corollary 1]] }} {{eqn | l = \cmod {z_1 + z_2} | o = \ge | r = \cmod {z_2} - \cmod {z_1} | c = [[Triangle Inequality/Complex Numbers/Corollary 1|Triangle Inequality for Complex Numbers: Corollary 1]] }} {{eqn | r = -\paren {\cmod {z_1} - \cmod {z_2} } | c = }} {{eqn | ll= \leadsto | l = \cmod {z_1 + z_2} | o = \ge | r = \cmod {\cmod {z_1} - \cmod {z_2} } | c = [[Negative of Absolute Value/Corollary 2|Negative of Absolute Value: Corollary 2]] }} {{end-eqn}} {{qed}}	1
=== [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal/Forward Implication|Necessary Condition]] === {{:Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal/Forward Implication}} === [[Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal/Reverse Implication|Sufficient Condition]] === {{:Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal/Reverse Implication}} [[Category:Principal Ideals]] [[Category:Principal Ideal Domains]] [[Category:Factorization]] [[Category:Principal Ideal of Principal Ideal Domain is of Irreducible Element iff Maximal]] [[Category:Maximal Ideals of Rings]] sqqhg4vygfe3be9qkn9ckj88k6xwfri	1
:$\mathcal N \ne \O$	1
Because $x, y \in R \setminus 0_R$: :$\norm x_1 , \norm y_1, \norm x_2 , \norm y_2 > 0$. Because $\norm{x}_1 , \norm {y}_1 \ne 1$, by [[Equivalence of Definitions of Equivalent Division Ring Norms/Open Unit Ball Equivalent implies Norm is Power of Other Norm/Lemma 2/Lemma 2.2|Lemma 2]]: :$\norm x_2 , \norm y_2 \ne 1$. Hence: :$\log \norm x_1 , \log \norm y_1, \log \norm x_2, \log \norm y_2 \ne 0$ and $\alpha, \beta$ are well-defined. Let $r = \dfrac n m \in \Q$ be any [[Definition:Rational Number|rational number]] where $n, m \in \Z$ are [[Definition:Integer|integers]] and $m \ne 0$. Then: {{begin-eqn}} {{eqn | l = \norm y_1^n < \norm x_1^m | o = \leadstoandfrom | r = \dfrac {\norm y_1^n} {\norm x_1^m} < 1 }} {{eqn | o = \leadstoandfrom | r = \norm y_1^n \norm {x^{-1} }_1^m | c = [[Norm of Inverse in Division Ring]] }} {{eqn | o = \leadstoandfrom | r = \norm {y^n}_1 \norm {\paren {x^{-1} }^m}_1 < 1 | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = \leadstoandfrom | r = \norm {y^n \paren {x^{-1} }^m}_1 < 1 | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = \leadstoandfrom | r = \norm {y^n \paren {x^{-1} }^m}_2 < 1 | c = By assumption }} {{eqn | o = \leadstoandfrom | r = \norm {y^n}_2 \norm {\paren {x^{-1} }^m}_2 < 1 | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = \leadstoandfrom | r = \norm y_2^n \norm {x^{-1} }_2^m | c = [[Definition:Norm on Division Ring|Norm Axiom $(\text N 2)$: Multiplicativity]] }} {{eqn | o = \leadstoandfrom | r = \dfrac {\norm y_2^n} {\norm x_2^m} < 1 | c = [[Norm of Inverse in Division Ring]] }} {{eqn | r = \norm y_2^n < \norm x_2^m | o = \leadstoandfrom }} {{end-eqn}} By [[Logarithm is Strictly Increasing]]: :$\log \norm y_1^n < \log \norm x_1^m \iff \log \norm y_2^n < \log \norm x_2^m$ By [[Sum of Logarithms]]: :$n \log \norm y_1 < m \log \norm x_1 \iff n \log \norm y_2 < m \log \norm x_2$ Because $m, \log \norm x_1, \log \norm x_2 \ne 0$: :$r = \dfrac n m < \dfrac {\log \norm x_1} {\log \norm y_1} \iff r = \dfrac n m < \dfrac {\log \norm x_2} {\log \norm y_2}$ By [[Between two Real Numbers exists Rational Number]]: :$\dfrac {\log \norm x_1} {\log \norm y_1} = \dfrac {\log \norm x_2} {\log \norm y_2}$ Because $\log \norm x_2, \log \norm y_2 \ne 0$: :$\dfrac {\log \norm x_1} {\log \norm x_2} = \dfrac {\log \norm y_1} {\log \norm y_2}$ That is: :$\alpha = \beta$ {{qed}} [[Category:Equivalence of Definitions of Equivalent Division Ring Norms]] 0w9fh9fzzjpsmeh7kgmw6d8fc585lm2	1
By [[P-Product Metric Induces Product Topology|$p$-Product Metric Induces Product Topology]] and [[Continuous Mapping is Continuous on Induced Topological Spaces]], it suffices to consider the case $p = \infty$. Let $\tuple {x_0, y_0} \in R \times R$. Let $\epsilon > 0$ be given. Let $\tuple {x, y} \in R \times R$ such that: :$\map {d_\infty} {\tuple {x, y},\tuple{x_0, y_0} } < \dfrac \epsilon 2$ By the definition of the [[Definition:P-Product Metric|product metric $d_\infty$]] then: :$\max \set {\map d {x, x_0}, \map d {y, y_0} } < \dfrac \epsilon 2$ or equivalently: :$\map d {x, x_0} < \dfrac \epsilon 2$ :$\map d {y, y_0} < \dfrac \epsilon 2$ Then: {{begin-eqn}} {{eqn | l = \map d {x_0 + y_0, x + y} | r = \norm {\paren {x_0 + y_0} - \paren {x+y} } | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \norm {\paren {x_0 - x} + \paren {y_0 - y} } | c = }} {{eqn | r = \norm {x_0 - x } + \norm {y_0 - y} | o = \le | c = [[Definition:Norm Axioms|Norm Axiom (N3) (Triangle Inequality)]] }} {{eqn | r = \map d {x_0, x} + \map d {y_0, y} | o = \le | c = {{Defof|Metric Induced by Norm on Division Ring}} }} {{eqn | r = \dfrac \epsilon 2 + \dfrac \epsilon 2 | o = < | c = }} {{eqn | r = \epsilon }} {{end-eqn}} Since $\tuple {x_0, y_0}$ and $\epsilon$ were arbitrary, by the definition of [[Definition:Continuous Mapping (Metric Space)|continuity]] then the [[Definition:Mapping|mapping]]: :$+ : \struct {R \times R, d_\infty} \to \struct {R, d}$ is [[Definition:Continuous Mapping (Metric Space)|continuous]]. {{qed}}	1
The [[Cauchy-Binet Formula]] gives: :$\displaystyle \det \left({\mathbf A \mathbf B}\right) = \sum_{1 \mathop \le j_1 \mathop < j_2 \mathop < \cdots \mathop < j_m \le n} \det \left({\mathbf A_{j_1 j_2 \ldots j_m}}\right) \det \left({\mathbf B_{j_1 j_2 \ldots j_m}}\right)$ where: :$\mathbf A$ is an [[Definition:Matrix|$m \times n$ matrix]] :$\mathbf B$ is an [[Definition:Matrix|$n \times m$ matrix]]. :For $1 \le j_1, j_2, \ldots, j_m \le n$: ::$\mathbf A_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Column of Matrix|columns]] $j_1, j_2, \ldots, j_m$ of $\mathbf A$. ::$\mathbf B_{j_1 j_2 \ldots j_m}$ denotes the [[Definition:Matrix|$m \times m$ matrix]] consisting of [[Definition:Row of Matrix|rows]] $j_1, j_2, \ldots, j_m$ of $\mathbf B$. When $m = n$, the only set $j_1, j_2, \ldots, j_m$ that fulfils $1 \le j_1 < j_2 < \cdots < j_m \le n$ is $\left\{ {1, 2, \ldots, n}\right\}$. Hence the result. {{qed}}	1
Let $V$ be a [[Definition:Vector Space|vector space]] over a [[Definition:Division Ring|division ring]] $K$. Let $U \subseteq V$ be a non-[[Definition:Empty Set|empty]] [[Definition:Subset|subset]] of $V$ such that: :$\forall u, v \in U: \forall \lambda \in K: u + \lambda v \in U$ Then $U$ is a [[Definition:Vector Subspace|subspace]] of $V$.	1
Let $I := \closedint a b$ be a [[Definition:Closed Real Interval|closed real interval]]. We have that: :[[Space of Continuously Differentiable on Closed Interval Real-Valued Functions with Pointwise Addition and Pointwise Scalar Multiplication form Vector Space]] :[[C^k Norm is Norm|$\map {C^1} I$ norm on the space of continuously differentiable on closed interval real-valued functions is a norm]] By definition, $\struct {\map {\CC^1} I, \norm {\, \cdot \,}_{1, \infty} }$ is a [[Definition:Normed Vector Space|normed vector space]]. {{qed}}	1
It is to be shown that $\sim$ is [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. === Reflexive === Let $\mathcal F \in \mathcal A$ be a [[Definition:Atlas|$C^k$-atlas]]. That $\mathcal F$ is [[Definition:Compatible Atlases|compatible]] with itself is precisely condition $(2)$ of the definition of [[Definition:Atlas|$C^k$-atlas]]. {{qed|lemma}} === Symmetric === Let $\mathcal F$ and $\mathcal G$ be [[Definition:Atlas|$C^k$-atlases]] and suppose that $\mathcal F \sim \mathcal G$. Let $\left({U, \phi}\right) \in \mathcal F$ and let $\left({V, \psi}\right) \in \mathcal G$ be [[Definition:Chart|charts]]. Then by hypothesis: :$\phi \circ \psi^{-1}: \psi \left({U \cap V}\right) \to \phi \left({U \cap V}\right)$ is a [[Definition:Differentiability Class|$C^k$ mapping]]. Because $\phi$ and $\psi$ are [[Definition:Homeomorphism|homeomorphisms]], we have that: :$\psi \circ \phi^{-1}: \phi \left({U \cap V}\right) \to \psi \left({U \cap V}\right)$ is also a [[Definition:Homeomorphism|homeomorphism]], and in particular [[Definition:Continuous Mapping (Topology)|continuous]]. By the [[Inverse Function Theorem]], $\psi \circ \phi^{-1}$ is also a [[Definition:Differentiability Class|$C^k$ mapping]]. Since the [[Definition:Chart|charts]] were arbitrary, we conclude that $\mathcal G \sim \mathcal F$. {{qed|lemma}} === Transitive === Let $\mathcal F$, $\mathcal G$ and $\mathcal H$ be [[Definition:Atlas|$C^k$-atlases]], and suppose that $\mathcal F \sim \mathcal G$ and $\mathcal G \sim \mathcal H$. {{finish|That's messy; probably easiest to use partition of unity compatible with $\mathcal G$, or via common refinement}} {{explain|Clarification is needed as to why this result should be categorised in [[:Category:Manifolds]].}} [[Category:Manifolds]] 3mb3byl8pdxtdl55kkak3fht2cf2sff	1
Let $\mathbf V, \mathbf V'$ be [[Definition:Vector Space|vector spaces]], with respective [[Definition:Zero Vector|zeroes]] $\mathbf 0, \mathbf 0'$. Let $T: \mathbf V \to \mathbf V'$ be a [[Definition:Linear Transformation on Vector Space|linear transformation]]. Then: :$T$ is [[Definition:Injection|injective]] {{iff}} $\map \ker T = \set {\mathbf 0}$ where: :$\mathbf 0$ is the [[Definition:Zero Vector|zero]] of the [[Definition:Domain of Mapping|domain]] of $T$ :$\map \ker T$ is the [[Definition:Kernel of Linear Transformation|kernel of $T$]].	1
Let $H_n$ be the [[Definition:Hilbert Matrix|Hilbert matrix]] of [[Definition:Order of Square Matrix|order $n$]]: :$\begin{bmatrix} a_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac 1 {i + j - 1} \end{bmatrix}$ Then its [[Definition:Inverse Matrix|inverse]] $H_n^{-1} = \sqbrk b_n$ can be specified as: :$\begin{bmatrix} b_{i j} \end{bmatrix} = \begin{bmatrix} \dfrac {\paren {-1}^{i + j} \paren {i + n - 1}! \paren {j + n - 1}!} {\paren {\paren {i - 1}!}^2 \paren {\paren {j - 1}!}^2 \paren {n - j}! \paren {n - i}! \paren {i + j - 1} } \end{bmatrix}$	1
:$\displaystyle \lim_{n \mathop \to \infty} x_n^k = a$ in $\struct {\Q, \norm {\,\cdot\,}_p}$	1

Let $n \in \Z_{>0}$ be a [[Definition:Strictly Positive Integer|strictly positive integer]]. Let $\map \sigma n$ denote the [[Definition:Sigma Function|$\sigma$ function]] on $n$. Then: :$\map \sigma n = \displaystyle \sum_{1 \mathop \le n - GP_k \mathop < n} -\paren {-1}^{\ceiling {k / 2} } \map \sigma {n - GP_k} + n \sqbrk {\exists k \in \Z: GP_k = n}$	0
All the [[Definition:Sylow p-Subgroup|Sylow $p$-subgroups]] of a [[Definition:Finite Group|finite group]] are [[Definition:Conjugate of Group Subset|conjugate]].	0
By [[Finite Suprema Set and Lower Closure is Smallest Ideal]]: :$X \subseteq {\operatorname{finsups}\left({X}\right)}^\preceq$ By definition of [[Definition:Non-Empty Set|non-empty set]]: :${\operatorname{finsups}\left({X}\right)}^\preceq$ is a [[Definition:Non-Empty Set|non-empty set]]. We will prove that :$\operatorname{finsups}\left({X}\right)$ is [[Definition:Directed Subset|directed]]. Let $x, y \in \operatorname{fininfs}\left({X}\right)$ By definition of [[Definition:Finite Suprema Set|finite suprema set]]: :$\exists A \in \mathit{Fin}\left({X}\right): x = \sup A \land A$ admits a [[Definition:Supremum of Set|supremum]] and :$\exists B \in \mathit{Fin}\left({X}\right): y = \sup B \land B$ admits an [[Definition:Supremum of Set|supremum]] where $\mathit{Fin}\left({X}\right)$ denotes the [[Definition:Set of Sets|set]] of all [[Definition:Finite Set|finite]] [[Definition:Subset|subsets]] of $X$. Define $C = A \cup B$. By [[Union of Subsets is Subset]]: :$C \subseteq X$ By [[Union of Finite Sets is Finite]]: :$C$ is [[Definition:Finite Set|finite]]. Then :$C \in \mathit{Fin}\left({X}\right)$ By [[Existence of Non-Empty Finite Suprema in Join Semilattice]]: :$C \ne \varnothing \implies C$ admits a [[Definition:Supremum of Set|supremum]]. By [[Union is Empty iff Sets are Empty]]: :$C = \varnothing \implies A = \varnothing$ So :$C$ admits a [[Definition:Supremum of Set|supremum]]. By definition of [[Definition:Finite Suprema Set|finite suprema set]]: :$\sup C \in \operatorname{finsups}\left({X}\right)$ By [[Set is Subset of Union]]: :$A \subseteq C$ and $B \subseteq C$ Thus by [[Supremum of Subset]]: :$x \preceq \sup C$ and $y \preceq \sup C$ Hence $\operatorname{finsups}\left({X}\right)$ is [[Definition:Directed Subset|directed]]. {{qed|lemma}} By [[Directed iff Lower Closure Directed]]: :${\operatorname{finsups}\left({X}\right)}^\preceq$ is [[Definition:Directed Subset|directed]]. By [[Lower Closure is Lower Set]]: :${\operatorname{finsups}\left({X}\right)}^\preceq$ is [[Definition:Lower Set|lower]]. Hence ${\operatorname{finsups}\left({X}\right)}^\preceq$ is [[Definition:Ideal (Order Theory)|ideal]] in $P$. {{qed}}	0
Let $a \in \N$ be an [[Definition:Even Integer|even]] [[Definition:Perfect Number|perfect number]]. We can extract the highest power of $2$ out of $a$ that we can, and write $a$ in the form: :$a = m 2^{n-1}$ where $n \ge 2$ and $m$ is [[Definition:Odd Integer|odd]]. Since $a$ is [[Definition:Perfect Number|perfect]] and therefore $\map \sigma a = 2 a$: {{begin-eqn}} {{eqn| l = m 2^n | r = 2 a | c = }} {{eqn| r = \map \sigma a | c = }} {{eqn| r = \map \sigma {m 2^{n - 1} } | c = }} {{eqn| r = \map \sigma m \map \sigma {2^{n - 1} } | c = [[Sigma Function is Multiplicative]] }} {{eqn| r = \map \sigma m {2^n - 1} | c = [[Sigma Function of Power of Prime]] }} {{end-eqn}} So: :$\map \sigma m = \dfrac {m 2^n} {2^n - 1}$ But $\map \sigma m$ is an [[Definition:Integer|integer]] and so $2^n - 1$ [[Definition:Divisor of Integer|divides]] $m 2^n$. From [[Consecutive Integers are Coprime]], $2^n$ and $2^n - 1$ are [[Definition:Coprime Integers|coprime]]. So from [[Euclid's Lemma]] $2^n - 1$ [[Definition:Divisor of Integer|divides]] $m$. Thus $\dfrac m {2^n - 1}$ [[Definition:Divisor of Integer|divides]] $m$. Since $2^n - 1 \ge 3$ it follows that: :$\dfrac m {2^n - 1} < m$ Now we can express $\map \sigma m$ as: :$\map \sigma m = \dfrac {m 2^n} {2^n - 1} = m + \dfrac m {2^n - 1}$ This means that the sum of all the [[Definition:Divisor of Integer|divisors]] of $m$ is equal to $m$ itself plus one other divisor of $m$. Hence $m$ must have exactly two [[Definition:Divisor of Integer|divisors]], so it must be [[Definition:Prime Number|prime]] by definition. This means that the other [[Definition:Divisor of Integer|divisor]] of $m$, apart from $m$ itself, must be $1$. That is: :$\dfrac m {2^n - 1} = 1$ Hence the result. {{qed}}	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]]. Let $\exp z$ be the [[Definition:Complex Exponential Function|exponential of $z$]]. Then:	0
From [[Reduction Formula for Primitive of Power of a x + b by Power of p x + q/Decrement of Power|Reduction Formula for Primitive of Power of $a x + b$ by Power of $p x + q$: Decrement of Power]]: :$\displaystyle \int \paren {a x + b}^m \paren {p x + q}^n \rd x = \frac {\paren {a x + b}^{m + 1} \paren {p x + q}^n} {\paren {m + n + 1} a} - \frac {n \paren {b p - a q} } {\paren {m + n + 1} a} \int \paren {a x + b}^m \paren {p x + q}^{n - 1} \rd x$ Setting $a := 1, b := 0, p x + q := a x + b$: {{begin-eqn}} {{eqn | l = \int x^m \paren {a x + b}^n \rd x | r = \frac {\paren {1 x + 0}^{m + 1} \paren {a x + b}^n} {\paren {m + n + 1} 1} - \frac {n \paren {0 a - 1 b} } {\paren {m + n + 1} 1} \int \paren {1 x + 0}^m \paren {a x + b}^{n - 1} \rd x | c = }} {{eqn | r = \frac {x^{m + 1} \paren {a x + b}^n} {m + n + 1} - \frac {n b} {\paren {m + n + 1} } \int x^m \paren {a x + b}^{n - 1} \rd x | c = }} {{end-eqn}} {{qed}}	0
Let $\map f t = \sin \sqrt t$. Then: {{begin-eqn}} {{eqn | l = \map {f'} t | r = \dfrac {\cos \sqrt t} {2 \sqrt t} | c = }} {{eqn | l = \map f 0 | r = 0 | c = }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \laptrans {\map {f'} t} | r = \dfrac 1 2 \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | c = }} {{eqn | r = s \, \map F s - \map f 0 | c = [[Laplace Transform of Derivative]] }} {{eqn | r = \dfrac {\sqrt \pi} {2 s^{1/2} } \map \exp {-\dfrac 1 {4 s} } | c = [[Laplace Transform of Sine of Root]] }} {{eqn | ll= \leadsto | l = \laptrans {\dfrac {\cos \sqrt t} {\sqrt t} } | r = \sqrt {\dfrac \pi s} \, \map \exp {-\dfrac 1 {4 s} } | c = }} {{end-eqn}} {{qed}}	0
Let $\struct {S, \preceq}$ be a [[Definition:Woset|woset]]. Let $T \subseteq S$. Let $f: S \to T$ be an [[Definition:Order Isomorphism|order isomorphism]]. Then $\forall x \in S: x \preceq \map f x$.	0
By the definition of an [[Definition:Irreducible Space|irreducible]] subset, $Y\subset X$ is irreducible if and only if any nonempty [[Definition:Open Set|open]] subset in $Y$ intersects. Since the open subsets in $\bar{Y}$ is the same as the open subsets in $Y$, any two of them still trivially intersects in $\bar{Y}$, showing that $\bar{Y}$ is also irreducible. More generally, we can also show that if $\bar{Y}$ is irreducible for a subset $Y\subset X$, then $Y$ is also irreducible in $X$. We prove it by contradiction. Assume $Y$ is not irreducible, then there exist two closed proper subsets $Y_1$, $Y_2$ such that $Y=Y_1\cup Y_2$, then $\bar{Y}=\bar{Y_1}\cup \bar{Y_2}$, which contradicts with the assumption.	0
:$\map \sin {x + \dfrac {3 \pi} 2} = -\cos x$	0
=== $(1)$ implies $(2)$ === Let $\gcd \set{a, b}$ be the [[Definition:Greatest Common Divisor/Integers/Definition 1|greatest common divisor of integers $a$ and $b$ by definition 1]]. Then by definition: :$\gcd \set{a, b}$ is the [[Definition:Greatest Element|largest]] $d \in \Z_{>0}$ such that $d \divides a$ and $d \divides b$. From [[Common Divisor Divides GCD]]: :$c \divides a \land c \divides b \implies c \divides d$ Thus $\gcd \set{a, b}$ is the [[Definition:Greatest Common Divisor/Integers/Definition 2|greatest common divisor of integers $a$ and $b$ by definition 2]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Let $\gcd \set{a, b}$ be the [[Definition:Greatest Common Divisor/Integers/Definition 2|greatest common divisor of integers $a$ and $b$ by definition 2]]. Then by definition $\gcd \set{a, b}$ is the [[Definition:Strictly Positive Integer|(strictly) positive integer]] $d \in \Z_{>0}$ such that: : $(1): \quad d \divides a \land d \divides b$ : $(2): \quad c \divides a \land c \divides b \implies c \divides d$ From $d \divides a \land d \divides b$, we see that $d$ is a [[Definition:Common Divisor of Integers|common divisor]] of $a$ and $b$. From $c \divides a \land c \divides b$, we see that $c$ is also a [[Definition:Common Divisor of Integers|common divisor]] of $a$ and $b$. Also, we have that $c \divides d$. From [[Absolute Value of Integer is not less than Divisors]], we see that (in the [[Definition:Domain of Relation|domain]] of $\Z_{>0}$): :$c \divides d \implies c \le d$ Thus, whatever $c$ may be, it is no larger than $d$. Therefore, $d$ must be the greatest of all the [[Definition:Common Divisor of Integers|common divisors]] of $a$ and $b$. Thus $\gcd \set{a, b}$ is the [[Definition:Greatest Common Divisor/Integers/Definition 1|greatest common divisor of integers $a$ and $b$ by definition 1]]. {{qed}}	0
Let $z \in \C$ be a [[Definition:Complex Number|complex number]] expressed in [[Definition:Polar Form of Complex Number|complex form]]: :$z = r \paren {\cos x + i \sin x}$ Then: :$\forall p \in \Q: \paren {r \paren {\cos x + i \sin x} }^p = r^p \paren {\map \cos {p x} + i \, \map \sin {p x} }$	0
The [[Definition:Harmonic Series|harmonic series]]: :$\displaystyle \sum_{n \mathop = 1}^\infty \frac 1 n$ [[Definition:Divergent Series|diverges]].	0
'''Permutation theory''' is a branch of [[Definition:Abstract Algebra|abstract algebra]] which studies the properties of [[Definition:Permutation (Ordered Selection)|permutations]].	0
Let $a_1, a_2, \dotsc, a_n \in \Z$ be [[Definition:Integer|integers]]. Let their [[Definition:Prime Decomposition|prime decompositions]] be given by: :$\displaystyle a_i = \prod_{\substack {p_{i j} \mathop \divides a_i \\ \text {$p_{i j}$ is prime} } } {p_{i j} }^{e_{i j} }$ Then there exists a [[Definition:Set|set]] $T$ of [[Definition:Prime Number|prime numbers]]: :$T = \set {t_1, t_2, \dotsc, t_v}$ such that: :$t_1 < t_2 < \dotsb < t_v$ :$\displaystyle a_i = \prod_{j \mathop = 1}^v {t_j}^{g_{i j} }$	0
{{begin-eqn}} {{eqn | l = \tan 345^\circ | r = \tan \left({360^\circ - 15^\circ}\right) | c = }} {{eqn | r = -\tan 15^\circ | c = [[Tangent of Conjugate Angle]] }} {{eqn | r = -\left({2 - \sqrt 3}\right) | c = [[Tangent of 15 Degrees]] }} {{end-eqn}} {{qed}}	0
Let $G_n = \sequence {a_n}_{0 \mathop \le i \mathop \le n}$ be a [[Definition:Geometric Sequence of Integers|geometric sequence of integers]]. Let $a_0 = 1$. Let $p$ be a [[Definition:Prime Number|prime number]] such that: :$p \divides a_n$ where $\divides$ denotes [[Definition:Divisor of Integer|divisibility]]. Then $p \divides a_1$. {{:Euclid:Proposition/IX/12}}	0
By definition, $\struct {K, +, \times}$ is a [[Definition:Subset|subset]] of $F$ which is a [[Definition:Field (Abstract Algebra)|field]]. By definition of [[Definition:Field (Abstract Algebra)|field]], $\struct {K^*, \times}$ and $\struct {F^*, \times}$ are [[Definition:Group|groups]] such that $K \subseteq F$. So $\struct {K^*, \times}$ is a [[Definition:Subgroup|subgroup]] of $\struct {F^*, \times}$. By [[Identity of Subgroup]], the [[Definition:Identity Element|identity]] of $\struct {F^*, \times}$, which is $1$, is also the identity of $\struct {K^*, \times}$. {{qed}}	0
Let us verify the axioms $(C1)$ up to $(C3)$ for a [[Definition:Metacategory|metacategory]]. Let $f^{\text{op}}: C^{\text{op}} \to D^{\text{op}}$ and $g^{\text{op}}: D^{\text{op}} \to E^{\text{op}}$ be [[Definition:Morphism (Category Theory)|morphisms]] in $\mathbf C^{\text{op}}$. Then $f: D \to C$ and $g: E \to D$ are [[Definition:Morphism (Category Theory)|morphisms]] in $\mathbf C$, and so is $f \circ g: E \to C$. Therefore, also $g^{\text{op}} \circ f^{\text{op}}: C^{\text{op}} \to E^{\text{op}}$ is a [[Definition:Morphism (Category Theory)|morphism]] in $\mathbf C^{\text{op}}$, and $(C1)$ is shown to hold. For $(C2)$, observe that for $f^{\text{op}}: C^{\text{op}} \to D^{\text{op}}$, we have: {{begin-eqn}} {{eqn|l = f^{\text{op} } \circ \operatorname{id}_{C^{\text{op} } } |r = f^{\text{op} } \circ \operatorname{id}_C^{\text{op} } |c = Definition of $\operatorname{id}_{C^{\text{op} } }$ }} {{eqn|r = \left({\operatorname{id}_C \circ f}\right)^{\text{op} } |c = Definition of [[Definition:Composition of Morphisms|composition]] }} {{eqn|r = f^{\text{op} } |c = Since $\mathbf C$ is a [[Definition:Metacategory|metacategory]] }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn|l = \operatorname{id}_{D^{\text{op} } } \circ f^{\text{op} } |r = \operatorname{id}_D^{\text{op} } \circ f^{\text{op} } |c = Definition of $\operatorname{id}_{D^{\text{op} } }$ }} {{eqn|r = \left({f \circ \operatorname{id}_D}\right)^{\text{op} } |c = Definition of [[Definition:Composition of Morphisms|composition]] }} {{eqn|r = f^{\text{op} } |c = Since $\mathbf C$ is a [[Definition:Metacategory|metacategory]] }} {{end-eqn}} Hence $(C2)$ is shown to hold. To show $(C3)$, reason as follows: {{begin-eqn}} {{eqn|l = \left({f^{\text{op} } \circ g^{\text{op} } }\right) \circ h^{\text{op} } |r = \left({g \circ f}\right)^{\text{op} } \circ h^{\text{op} } |c = Definition of [[Definition:Composition of Morphisms|composition]] }} {{eqn|r = \left({h \circ \left({g \circ f}\right)}\right)^{\text{op} } |c = Definition of [[Definition:Composition of Morphisms|composition]] }} {{eqn|r = \left({\left({h \circ g}\right) \circ f}\right)^{\text{op} } |c = Since $\mathbf C$ is a [[Definition:Metacategory|metacategory]] }} {{eqn|r = f^{\text{op} } \circ \left({h \circ g}\right)^{\text{op} } }} {{eqn|r = f^{\text{op} } \circ \left({g^{\text{op} } \circ h^{\text{op} } }\right) }} {{end-eqn}} Hence $\mathbf C^{\text{op}}$ is a [[Definition:Metacategory|metacategory]]. {{qed}} [[Category:Dual Categories]] 9i5b7ic5zh4k5h7ylvkmimpf1r80359	0
From [[Euler Phi Function of Integer]]: :$\displaystyle \map \phi n = n \prod_{p \mathop \divides n} \paren {1 - \frac 1 p}$ As $n$ is [[Definition:Square-Free Integer|square-free]]: :$\displaystyle n = \prod_{p \mathop \divides n} p$ Hence: :$\displaystyle \map \phi n = \prod_{p \mathop \divides n} p \paren {1 - \frac 1 p}$ and so: :$\displaystyle \map \phi n = \prod_{p \mathop \divides n} \paren {p - 1}$ When $p = 2$ we have that: :$p - 1 = 1$ and so: :$\displaystyle \prod_{p \mathop \divides n} \paren {p - 1} = \prod_{\substack {p \mathop \divides n \\ p \mathop > 2} } \paren {p - 1}$ Hence the result. {{qed}}	0
For $z$ to be a [[Definition:Root of Polynomial|root]] of $F$, $z$ must be [[Definition:Algebraic Number over Field|algebraic over $F$]]. Let us write: :$\map P x = \map m x \, \map q x + \map r x$ where $\map q x$ and $\map r x$ are [[Definition:Polynomial|polynomials]] in $F$. Then either $\map r x = 0$ or $\map \deg {\map r x} < \map \deg {\map m x}$. Then: :$\map P z = \map m z \, \map q z + \map r z$ But as $z$ is a [[Definition:Root of Polynomial|root]] of both $\map P x$ and $\map m x$, we have that: :$\map P z = \map m z = 0$ and so: :$\map r z = 0$ So if $\map r x \ne 0$ we have that $\map r x$ is a [[Definition:Polynomial|polynomial]] of smaller [[Definition:Degree of Polynomial|degree]] than $\map m x$. This [[Definition:Contradiction|contradicts]] the [[Definition:Minimal Polynomial|minimality]] of $\map m x$. Thus $\map r x = 0$ and so $\map P x$ is a [[Definition:Multiple of Ring Element|multiple]] of $\map m x$. {{qed}}	0
Let $L = \left({S, \wedge, \preceq}\right)$ be a [[Definition:Bounded Above Set|bounded above]] [[Definition:Meet Semilattice|meet semilattice]]. Let $F = \left({\mathit{Filt}\left({L}\right), \subseteq}\right)$ be an [[Definition:Inclusion Ordered Set|inclusion ordered set]], where $\mathit{Filt}\left({L}\right)$ denotes [[Definition:Set of Sets|set]] of all [[Definition:Filter|filters]] on $L$. Then $F$ is a [[Definition:Complete Lattice|complete lattice]].	0
Let $\left({G, \circ}\right)$ be a non-[[Definition:Abelian Group|abelian group]] whose [[Definition:Identity Element|identity]] is $e$. For a group $\left({G, \circ}\right)$ to be non-[[Definition:Abelian Group|abelian]], we require: : $\exists x, y \in G: x \circ y \ne y \circ x$ Suppose $x \circ y \in \left\{ {x, y, e}\right\}$. : $x \circ y = e \implies y \circ x = e$ and $\left({G, \circ}\right)$ is [[Definition:Abelian Group|abelian]]. {{WLOG}}, suppose $x \circ y = x$. {{begin-eqn}} {{eqn | l = x \circ y = x | o = \implies | r = y = e | c = }} {{eqn | o = \implies | r = y \circ x = x | c = }} {{eqn | o = \implies | r = x \circ y = y \circ x | c = }} {{end-eqn}} and again, $\left({G, \circ}\right)$ is [[Definition:Abelian Group|abelian]]. Similarly for $x \circ y = y$. Again, the same applies if $y \circ x \in \left\{ {x, y, e}\right\}$. So, if $x \circ y \ne y \circ x$, then; : $x \circ y$ and $y \circ x$ must be different [[Definition:Element|elements]] : $x \circ y$ and $y \circ x$ must both different from $e, x$ and $y$. Thus, in a non-[[Definition:Abelian Group|abelian group]], there needs to be at least $5$ [[Definition:Element|elements]]: :$e, x, y, x \circ y, y \circ x$ {{qed}}	0
Let $\map \tau n$ denote the [[Definition:Tau Function|$\tau$ (tau) function]] of $n$: :$\map \tau n$ is the number of [[Definition:Divisor of Integer|divisors]] of $n$. Then: {{begin-eqn}} {{eqn | l = \map \tau {720 \, 720} | r = 240 | c = {{TauLink|720,720|720 \, 720}} }} {{eqn | l = \map \tau {831 \, 600} | r = 240 | c = {{TauLink|831,600|831 \, 600}} }} {{eqn | l = \map \tau {942 \, 480} | r = 240 | c = {{TauLink|942,480|942 \, 480}} }} {{eqn | l = \map \tau {982 \, 800} | r = 240 | c = {{TauLink|982,800|982 \, 800}} }} {{eqn | l = \map \tau {997 \, 920} | r = 240 | c = {{TauLink|997,920|997 \, 920}} }} {{end-eqn}} {{ProofWanted|Prove that these are indeed the smallest}}	0
:$\displaystyle x^4 = \frac {\pi^4} 5 + \sum_{n \mathop = 1}^\infty \frac {8 n^2 \pi^2 - 48} {n^4} \cos n \pi \cos n x$	0
:$\displaystyle \sum_{1 \mathop \le i \mathop < j \mathop \le n} \left({u_j - u_k}\right) \left({v_j - v_k}\right) = n \sum_{j \mathop = 1}^n u_j v_j - \sum_{j \mathop = 1}^n u_j \sum_{j \mathop = 1}^n v_j$	0
=== Sufficient condition === Let $\struct {F, +, \cdot}$ be a [[Definition:Field (Abstract Algebra)|field]] of order $q$. By [[Characteristic of Galois Field is Prime]], the [[Definition:Characteristic of Ring|characteristic]] of $F$ is a [[Definition:Prime Number|prime number]] $p$. By [[Field of Prime Characteristic has Unique Prime Subfield]] the [[Definition:Prime Subfield|prime subfield]] of $F$ is $\F_p := \Z / p \Z$. By [[Vector Space on Field Extension is Vector Space]], $F$ is an $\F_p$-[[Definition:Vector Space|vector space]]. Since $F$ is [[Definition:Finite Set|finite]], $F$ has a [[Definition:Finite Set|finite]] [[Definition:Basis (Linear Algebra)|basis]] over $\F_p$. By [[Same Dimensional Vector Spaces are Isomorphic]], this means that with $k$ equal to the [[Definition:Dimension (Linear Algebra)|dimension]] of $F$ there is an [[Definition:Vector Space Isomorphism|isomorphism of vector spaces]]: :$F \simeq \F_p^k$ Finally by the definition of the [[Definition:Product of Cardinals|product of cardinals]]: :$\card F = \card {\F_p}^k = p^k$ So the order of $F$ is a [[Definition:Prime Power|prime power]]. {{qed|lemma}} === Necessary condition === {{ProofWanted|need splitting fields to complete}} [[Category:Galois Fields]] ngc5i1ix4oowgkypqf7oklmvvqqayo2	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Then: :$\forall n \in \N_{> 0}: \paren {a + b}^{p^n} \equiv a^{p^n} + b^{p^n} \pmod p$	0
{{begin-eqn}} {{eqn | l = \sum_{k \mathop = 0}^n \binom {-2} k | r = \sum_{k \mathop = 0}^n \left({-1}\right) \binom {k - \left({-2}\right) - 1} k | c = [[Negated Upper Index of Binomial Coefficient]] }} {{eqn | r = \sum_{k \mathop = 0}^n \left({-1}\right) \binom {k + 1} k | c = }} {{eqn | r = \sum_{k \mathop = 0}^n \left({-1}\right) \left({k + 1}\right) | c = [[Binomial Coefficient with Self minus One]] }} {{eqn | r = 1 - 2 + 3 - 4 + \cdots \pm \left({n + 1}\right) | c = }} {{eqn | r = \left({1 + 2 + 3 + 4 + \cdots + \left({n + 1}\right)}\right) - 2 \times \left({2 + 4 + 6 + 8 + \cdots + m}\right) | c = where $m = n$ or $m = n + 1$ according to whether $n$ is [[Definition:Odd Integer|odd]] or [[Definition:Even Integer|even]] }} {{end-eqn}} When $n$ is [[Definition:Even Integer|even]], we have: {{begin-eqn}} {{eqn | r = \left({1 + 2 + 3 + 4 + \cdots + \left({n + 1}\right)}\right) - 2 \times \left({2 + 4 + 6 + 8 + \cdots + n}\right) | o = | c = }} {{eqn | r = \left({1 + 2 + 3 + 4 + \cdots + \left({n + 1}\right)}\right) - 4 \left({1 + 2 + 3 + 4 + \cdots + \frac n 2}\right) | c = }} {{eqn | r = \sum_{k \mathop = 2}^{n + 1} k - 4 \sum_{k \mathop = 1}^{\frac n 2} k | c = }} {{eqn | r = \frac {\left({n + 1}\right) \left({n + 2}\right)} 2 - 4 \frac {\frac n 2 \left({\frac n 2 + 1}\right)} 2 | c = [[Closed Form for Triangular Numbers]] }} {{eqn | r = \frac {\left({n + 1}\right) \left({n + 2}\right)} 2 - \frac {n \left({n + 2}\right)} 2 | c = simplifying }} {{eqn | r = \frac {\left({n + 2}\right) \left({n + 1 - n}\right)} 2 | c = simplifying }} {{eqn | r = \frac {n + 2} 2 | c = simplifying }} {{end-eqn}} As $n$ is [[Definition:Even Integer|even]], $n + 1$ is [[Definition:Odd Integer|odd]], and so: :$\dfrac {n + 2} 2 = \left({-1}\right)^n \left\lceil {\dfrac {n + 1} 2}\right\rceil$ {{qed|lemma}} When $n$ is [[Definition:Odd Integer|odd]], we have: {{begin-eqn}} {{eqn | r = \left({1 + 2 + 3 + 4 + \cdots + \left({n + 1}\right)}\right) - 2 \times \left({2 + 4 + 6 + 8 + \cdots + n + 1}\right) | o = | c = }} {{eqn | r = \left({1 + 2 + 3 + 4 + \cdots + \left({n + 1}\right)}\right) - 4 \left({1 + 2 + 3 + 4 + \cdots + \frac {n + 1} 2}\right) | c = }} {{eqn | r = \sum_{k \mathop = 2}^{n + 1} k - 4 \sum_{k \mathop = 1}^{\frac {n + 1} 2} k | c = }} {{eqn | r = \frac {\left({n + 1}\right) \left({n + 2}\right)} 2 - 4 \frac {\frac {n + 1} 2 \left({\frac {n + 1} 2 + 1}\right)} 2 | c = [[Closed Form for Triangular Numbers]] }} {{eqn | r = \frac {\left({n + 1}\right) \left({n + 2}\right)} 2 - \frac {\left({n + 1}\right) \left({n + 3}\right)} 2 | c = simplifying }} {{eqn | r = \frac {\left({n + 1}\right) \left({\left({n + 2}\right) - \left({n + 3}\right)}\right)} 2 | c = simplifying }} {{eqn | r = \left({-1}\right) \frac {n + 1} 2 | c = simplifying }} {{end-eqn}} As $n$ is [[Definition:Odd Integer|odd]], $n + 1$ is [[Definition:Even Integer|even]], and so $\dfrac {n + 1} 2$ is an [[Definition:Integer|integer]]. Thus from [[Real Number is Integer iff equals Ceiling]]: :$\left({-1}\right) \dfrac {n + 1} 2 = \left({-1}\right)^n \left\lceil {\dfrac {n + 1} 2}\right\rceil$ {{qed|lemma}} Thus: :$\displaystyle \sum_{k \mathop = 0}^n \binom {-2} k = \left({-1}\right)^n \left\lceil {\dfrac {n + 1} 2}\right\rceil$ whether $n$ is [[Definition:Odd Integer|odd]] or [[Definition:Even Integer|even]]. Hence the result. {{qed}}	0
In this case, $p$ and $q$ have opposite sign. The proof then follows the same lines as [[Minkowski's Inequality for Sums/Index Greater than 1|the proof for $p > 1$]], except that the [[Reverse Hölder's Inequality for Sums]] is applied instead.	0
Let $e_S$ be an identity of $\left({S, \circ}\right)$. Then by [[Definition:Identity Element|definition]], $e_S$ is both a [[Definition:Left Identity|left identity]] and a [[Definition:Right Identity|right identity]]. By [[More than one Left Identity then no Right Identity]], if there is more than one of either, there cannot be one of the other. So there can be only one of each. By [[Left and Right Identity are the Same]], they are one and the same thing. {{qed}}	0
Let $R$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $G$ be a [[Definition:Group|group]]. Let $R \sqbrk G$ be the corresponding [[Definition:Group Ring|group ring]]. Let $S$ be a [[Definition:Commutative Ring with Unity|commutative ring with unity]]. Let $\phi: R \to S$ be a [[Definition:Ring Homomorphism|ring homomorphism]]. Let $\beta : G \to R^\times$ be a [[Definition:Group Homomorphism|group homomorphism]], where $R^\times$ is the [[Definition:Multiplicative Group of Ring|multiplicative group]] of $R$. Then there exists a unique [[Definition:Ring Homomorphism|ring homomorphism]] from $R \sqbrk G$ to $S$ which extends $\phi$ and $\beta$.	0
Let $f$ be a [[Definition:Real Function|real function]] which is [[Definition:Continuous Real Function|continuous]] on the [[Definition:Open Real Interval|open interval]] $\openint a \to$, $a \in \R$, such that: :$\displaystyle \lim_{x \mathop \to +\infty} \map f x = +\infty$ Let $g$ be a [[Definition:Real Function|real function]] defined on some [[Definition:Open Real Interval|open interval]] $\openint b \to$ such that, for [[Definition:Sufficiently Large|sufficiently large]] $x$: :$\map g x > \map f x$ Then: :$\displaystyle \lim_{x \mathop \to +\infty} \map g x = +\infty$	0
Let $\Si: \R \to \R$ denote the [[Definition:Sine Integral Function|sine integral function]]. Then $\Si$ has a [[Definition:Limit at Infinity|(finite) limit at infinity]]: :$\displaystyle \lim_{x \mathop \to +\infty} \map \Si x = \frac \pi 2$	0
Let $\struct {S, \circ}$ be an [[Definition:Algebraic Structure|algebraic structure]]. Then $a \in S$ is [[Definition:Cancellable Element|cancellable]] {{iff}}: ::the [[Definition:Left Regular Representation|left regular representation]] $\map {\lambda_a} x$ is [[Definition:Injection|injective]] and ::the [[Definition:Right Regular Representation|right regular representation]] $\map {\rho_a} x$ is [[Definition:Injection|injective]].	0
From [[Max yields Supremum of Operands]]: :$\max \set {x_{k_1}, x_{k_2}, \dotsc, x_{k_m}} = \sup \set {x_{k_1}, x_{k_2}, \dotsc, x_{k_m} }$ and :$\max \set {x_1, x_2, \dotsc, x_n} = \sup \set {x_1, x_2, \dotsc, x_n}$ Since $\set {k_1, k_2, \dotsc, k_m} \subseteq \set {1, 2, \dotsc, n}$ then: :$\set {x_{k_1}, x_{k_2}, \dotsc, x_{k_m} } \subseteq \set {x_1, x_2, \dotsc, x_n}$ From [[Supremum of Subset]]: :$\sup \set {x_{k_1}, x_{k_2}, \dotsc, x_{k_m} } \preceq \sup \set {x_1, x_2, \dotsc, x_n}$ {{qed}} [[Category:Max and Min Operations]] dlvc1y82jhfq4bosca2cw83mjb3m8al	0
:$\coth x = \dfrac 1 {\tanh x}$ where $\tanh$ and $\coth$ denote [[Definition:Hyperbolic Tangent|hyperbolic tangent]] and [[Definition:Hyperbolic Cotangent|hyperbolic cotangent]] respectively.	0
Let $X$ be a [[Definition:Continuous Random Variable|continuous random variable]] of the [[Definition:Exponential Distribution|exponential distribution]] with parameter $\beta$ for some $\beta \in \R_{> 0}$. Then the [[Definition:Median of Continuous Random Variable|median]] of $X$ is equal to $\beta \ln 2$.	0
{{begin-eqn}} {{eqn | o = | r = \frac {\cosh \paren {x + y} + \cosh \paren {x - y} } 2 }} {{eqn | r = \frac {\paren {\cosh x \cosh y + \sinh x \sinh y} + \cosh \paren {x - y} } 2 | c = [[Hyperbolic Cosine of Sum]] }} {{eqn | r = \frac {\paren {\cosh x \cosh y + \sinh x \sinh y} + \paren {\cosh x \cosh y - \sinh x \sinh y} } 2 | c = [[Hyperbolic Cosine of Difference]] }} {{eqn | r = \frac {2 \cosh x \cosh y} 2 }} {{eqn | r = \cosh x \cosh y }} {{end-eqn}} {{qed}}	0
Consider the [[Definition:Relative Complement|relative complement]] of $I_a$ in $\R$: :$J = \relcomp \R I = \R \setminus I = \hointr a \to$ Let $\epsilon \in \R_{>0}$. Consider the [[Definition:Open Ball|open $\epsilon$-ball]] $\map {B_\epsilon} a$. Whatever the value of $\epsilon$ is, $a - \epsilon$ is not in $\map {B_\epsilon} a$. So, by definition, $J$ is not an [[Definition:Open Set (Metric Space)|open set]] of $\R$. By [[Relative Complement of Relative Complement]], $\relcomp \R J = I$. By definition, it follows that $I_a$ is not a [[Definition:Closed Set (Metric Space)|closed set]] of $\R$. [[Definition:Mutatis Mutandis|Mutatis mutandis]], the same proof shows that neither is $I_b$ a [[Definition:Closed Set (Metric Space)|closed set]] of $\R$. {{qed}} [[Category:Real Intervals]] [[Category:Real Number Line with Euclidean Metric]] 29glcnf6iq1fmqb0kswaa274dyowxzc	0
Let $\R$ denote the [[Definition:Real Number Line with Euclidean Metric|real number line with the usual (Euclidean) metric]]. Let $I := \openint a b \subseteq \R$ be an [[Definition:Open Real Interval|open real interval]]. Then $I$ is the [[Definition:Open Ball|open $\epsilon$-ball]] $\map {B_\epsilon} \alpha$ of some $\alpha \in \R$.	0
{{Refactor|The analyis has already been performed in [[Birthday Paradox]]. Extract that general case and make it a theorem, and then introduce [[Birthday Paradox]] and this page as examples.}} Let there be $n$ people in the room. Let $\map p n$ be the [[Definition:Probability|probability]] that no two people in the room have the same birthday. For simplicity, let us ignore [[Definition:Leap Year|leap years]] and assume there are $365$ days in the [[Definition:Year|year]]. Let the birthday of person $1$ be established. The [[Definition:Probability|probability]] that person $2$ shares person $1$'s birthday is $\dfrac 1 {365}$. Thus, the [[Definition:Probability|probability]] that person $2$ does not share person $1$'s birthday is $\dfrac {364} {365}$. Similarly, the [[Definition:Probability|probability]] that person $3$ does not share the birthday of either person $1$ or person $2$ is $\dfrac {363} {365}$. And further, the [[Definition:Probability|probability]] that person $n$ does not share the birthday of any of the people indexed $1$ to $n - 1$ is $\dfrac {365 - \paren {n - 1} } {365}$. Hence the total [[Definition:Probability|probability]] that none of the $n$ people share a birthday is given by: :$\map p n = \dfrac {364} {365} \dfrac {363} {365} \dfrac {362} {365} \cdots \dfrac {365 - n + 1} {365}$ {{begin-eqn}} {{eqn | l = \map p n | r = \dfrac {364} {365} \dfrac {363} {365} \dfrac {362} {365} \cdots \dfrac {365 - n + 1} {365} | c = }} {{eqn | r = \dfrac {365!} {365^n} \binom {365} n | c = }} {{end-eqn}} Setting $n = 53$ and evaluating the above gives: :$\map p {53} \approx 0.01887$ or: :$\map p {53} \approx \dfrac 1 {53.01697}$ {{qed}}	0
Let $a, b$ be [[Definition:Real Number|real numbers]] with $a < b$. Let $f : \closedint a b \to \R$ be a [[Definition:Continuous Real Function|continuous function]]. Let $f$ be [[Definition:Differentiable Real Function|differentiable]] on $\openint a b$, with [[Definition:Bounded Real-Valued Function|bounded]] [[Definition:Derivative|derivative]]. Then $f$ is of [[Definition:Bounded Variation|bounded variation]].	0
Let $f$ be a [[Definition:Real Function|real function]] defined on an [[Definition:Real Interval|interval]] $I$. Let $f$ be [[Definition:Strictly Monotone Real Function|strictly monotone]] and [[Definition:Continuous on Interval|continuous]] on $I$. Let $g$ be the [[Definition:Inverse of Mapping|inverse mapping]] to $f$. Let $J := f \left[{I}\right]$ be the [[Definition:Image of Mapping|image]] of $I$ under $f$. Then $g$ is [[Definition:Strictly Monotone Real Function|strictly monotone]] and [[Definition:Continuous on Interval|continuous]] on $J$.	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] which is [[Definition:Second-Countable Space|second-countable]]. Then $T$ is also a [[Definition:Lindelöf Space|Lindelöf space]].	0
{{ProofWanted|Use [[Polynomial is Linear Combination of Monomials]]}} [[Category:Polynomial Theory]] 2x9lzdnsljh8sd5unxy7zgtw235pu2z	0
=== [[Zero Choose Zero|Binomial Coefficient $\dbinom 0 0$]] === {{:Zero Choose Zero}} === [[Zero Choose n|Binomial Coefficient $\dbinom 0 n$]] === {{:Zero Choose n}} === [[One Choose n|Binomial Coefficient $\dbinom 1 n$]] === {{:One Choose n}} === [[N Choose Negative Number is Zero]] === {{:N Choose Negative Number is Zero}} === [[Binomial Coefficient with Zero]] === {{:Binomial Coefficient with Zero}} === [[Binomial Coefficient with One]] === {{:Binomial Coefficient with One}} === [[Binomial Coefficient with Self]] === {{:Binomial Coefficient with Self}} === [[Binomial Coefficient with Self minus One]] === {{:Binomial Coefficient with Self minus One}} === [[Binomial Coefficient with Two]] === {{:Binomial Coefficient with Two}}	0
Let $E_3 \in \Sigma_3$. Then $\map {g^{-1} } {E_3} \in \Sigma_2$, and $\map {f^{-1} } {\map {g^{-1} } {E_3} } \in \Sigma_1$ as $f, g$ are [[Definition:Measurable Mapping|measurable]]. That is, $\map {\paren {g \circ f}^{-1} } {E_3} \in \Sigma_1$ for all $E_3 \in \Sigma_3$. Hence, $g \circ f$ is [[Definition:Measurable Mapping|$\Sigma_1 \, / \, \Sigma_3$-measurable]]. {{qed}}	0
Let $m \in \Z_{> 0}$ be a [[Definition:Strictly Positive Integer|(strictly) positive integer]]. Let $\Z'_m$ be the [[Definition:Reduced Residue System|reduced residue system modulo $m$]]: :$\Z'_m = \set {\eqclass k m \in \Z_m: k \perp m}$ Let $S = \struct {\Z'_m, \times_m}$ be the [[Definition:Algebraic Structure|algebraic structure]] consisting of $\Z'_m$ under the [[Definition:Modulo Multiplication|modulo multiplication]]. Then $S$ is [[Definition:Closed Algebraic Structure|closed]], in the sense that: :$\forall a, b \in \Z'_m: a \times_m b \in \Z'_m$	0
Since $S$ is [[Definition:Bounded Above Set|bounded above]], $\exists M \in \Z: \forall s \in S: s \le M$. Hence we can define the set $S' = \set {-s: s \in S}$. $S'$ is [[Definition:Bounded Below Set|bounded below]] by $-M$. So from [[Set of Integers Bounded Below by Integer has Smallest Element]], $S'$ has a [[Definition:Smallest Element|smallest element]], $-g_S$, say, where $\forall s \in S: -g_S \le -s$. Therefore $g_S \in S$ (by definition of $S'$) and $\forall s \in S: s \le g_S$. So $g_S$ is the [[Definition:Greatest Element|greatest element]] of $S$. {{qed}}	0
{{begin-eqn}} {{eqn | l = z | r = \sinh \operatorname {arsinh} z }} {{eqn | ll= \leadstoandfrom | l = z | r = \frac {e^{\operatorname{arsinh} z} - e^{-\operatorname{arsinh} z} } 2 | c = {{Defof|Inverse Hyperbolic Sine}} }} {{eqn | ll= \leadstoandfrom | l = 2 z e^{\operatorname{arsinh} z} | r = e^{2 \operatorname{arsinh} z} - 1 | c = Multiplication by $2 e^{\operatorname{arsinh} z}$ }} {{eqn | ll= \leadstoandfrom | l = 0 | r = e^{2 \operatorname{arsinh} z} - 2 z e^{\operatorname{arsinh} z} - 1 }} {{eqn | ll= \leadstoandfrom | l = e^{\operatorname{arsinh} z} | r = z + \sqrt {z^2 + 1} | c = [[Quadratic Formula]], $e^z > 0, \sqrt {z^2 + 1} > z$ }} {{eqn | ll= \leadstoandfrom | l = \operatorname{arsinh} z | r = \map \ln {z + \sqrt {z^2 + 1 } } }} {{end-eqn}} {{qed}} [[Category:Inverse Hyperbolic Sine]] mkdlysytrenv0xs7a0zsgl31zghtzvk	0
We have: {{begin-eqn}} {{eqn | l = \cos 5 \theta + i \sin 5 \theta | r = \paren {\cos \theta + i \sin \theta}^5 | c = [[De Moivre's Formula]] }} {{eqn | r = \paren {\cos \theta}^5 + \binom 5 1 \paren {\cos \theta}^4 \paren {i \sin \theta} + \binom 5 2 \paren {\cos \theta}^3 \paren {i \sin \theta}^2 }} {{eqn | o = | ro=+ | r = \binom 5 3 \paren {\cos \theta}^2 \paren {i \sin \theta}^3 + \binom 5 4 \paren {\cos \theta} \paren {i \sin \theta}^4 + \paren {i \sin \theta}^5 | c = [[Binomial Theorem]] }} {{eqn | r = \cos^5 \theta + 5 i \cos^4 \theta \sin \theta - 10 \cos^3 \theta \sin^2 \theta | c = substituting for [[Definition:Binomial Coefficient|binomial coefficients]] }} {{eqn | o = | ro=- | r = 10 i \cos^2 \theta \sin^3 \theta + 5 \cos \theta \sin^4 \theta + i \sin^5 \theta | c = and using $i^2 = -1$ }} {{eqn | n = 1 | r = \cos^5 \theta - 10 \cos^3 \theta \sin^2 \theta + 5 \cos \theta \sin^4 \theta }} {{eqn | o = | ro=+ | r = i \paren {5 \cos^4 \theta \sin \theta - 10 \cos^2 \theta \sin^3 \theta + \sin^5 \theta} | c = rearranging }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \sin 5 \theta | r = 5 \cos^4 \theta \sin \theta - 10 \cos^2 \theta \sin^3 \theta + \sin^5 \theta | c = equating [[Definition:Imaginary Part|imaginary parts]] in $(1)$ }} {{eqn | r = 5 \paren {1 - \sin^2 \theta}^2 \sin \theta - 10 \paren {1 - \sin^2 \theta} \sin^3 \theta + \sin^5 \theta | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = 5 \sin \theta - 20 \sin^3 \theta + 16 \sin^5 \theta | c = multiplying out and gathering terms }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\d x} {-\sqrt {x^2 - a^2} } = \ln \size {x - \sqrt {x^2 - a^2} } + C$	0
Let $T = \struct {S, \tau}$ be a [[Definition:Topological Space|topological space]] where $S$ is a [[Definition:Finite Set|finite set]]. Then $T$ is [[Definition:Sequentially Compact Space|sequentially compact]].	0
Let $f: \C \to \C$ be a [[Definition:Complex Function|complex function]]. Then $f$ is an '''[[Definition:Entire Function|entire function]]''' {{iff}} $f$ can be given by an [[Definition:Everywhere Convergence|everywhere convergent]] [[Definition:Power Series|power series]]: :$\displaystyle \map f z = \sum_{n \mathop = 0}^\infty a_n z^n; \quad \lim_{n \mathop \to \infty} \sqrt [n] {\size {a_n} } = 0$	0
:$\ds \int \sin x \map \ln {\sin x} \rd x = \cos x \paren {1 - \map \ln {\sin x} } + \ln \size {\tan \frac x 2} + C$	0
:$\cos 330 \degrees = \cos \dfrac {11 \pi} 6 = \dfrac {\sqrt 3} 2$	0
:$\sinh x + \sinh y = 2 \sinh \left({\dfrac {x + y} 2}\right) \cosh \left({\dfrac {x - y} 2}\right)$	0
{{begin-eqn}} {{eqn | l = \tan \paren {a + b i} | r = \dfrac {\sin a \cosh b + i \cos a \sinh b} {\cos a \cosh b - i \sin a \sinh b} | c = [[Tangent of Complex Number/Formulation 1|Tangent of Complex Number: Formulation 1]] }} {{eqn | r = \dfrac {\tan a \cosh b + i \sinh b} {\cosh b - i \tan a \sinh b} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\dfrac 1 {\cos a}$ }} {{eqn | r = \frac {\tan a + i \tanh b} {1 - i \tan a \tanh b} | c = multiplying [[Definition:Denominator|denominator]] and [[Definition:Numerator|numerator]] by $\dfrac 1 {\cosh b}$ }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \frac {x^2 \rd x} {a x^2 + b x + c} | r = \int \frac 1 a \paren {1 - \frac {b x + c} {a x^2 + b x + c} } \rd x | c = by division }} {{eqn | r = \frac 1 a \int \rd x - \frac b a \int \frac {x \rd x} {a x^2 + b x + c} - \frac c a \int \frac {\d x} {a x^2 + b x + c} | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac x a - \frac b a \int \frac {x \rd x} {a x^2 + b x + c} - \frac c a \int \frac {\d x} {a x^2 + b x + c} | c = [[Primitive of Constant]] }} {{eqn | r = \frac x a - \frac b a \paren {\frac 1 {2 a} \ln \size {a x^2 + b x + c} - \frac b {2 a} \int \frac {\d x} {a x^2 + b x + c} } - \frac c a \int \frac {\d x} {a x^2 + b x + c} | c = [[Primitive of x over a x squared plus b x plus c|Primitive of $\dfrac x {a x^2 + b x + c}$]] }} {{eqn | r = \frac x a - \frac b {2 a^2} \ln \size {a x^2 + b x + c} - \frac {b^2} {2 a^2} \int \frac {\d x} {a x^2 + b x + c} - \frac {2 a c} {2 a^2} \int \frac {\d x} {a x^2 + b x + c} | c = multiplying out }} {{eqn | r = \frac x a - \frac b {2 a^2} \ln \size {a x^2 + b x + c} - \frac {b^2 - 2 a c} {2 a^2} \int \frac {\d x} {a x^2 + b x + c} | c = simplification }} {{end-eqn}} {{qed}}	0
:$\lambda f$ is [[Definition:Continuous Mapping (Metric Spaces)|continuous]] on $S$.	0
Let $f: \R \to \R$ be a [[Definition:Real Periodic Function|real periodic function]] with [[Definition:Period of Function|period]] $P$. Let $L$ be a [[Definition:Periodic Element|periodic element]] of $f$. Then $P \divides L$.	0
It is to be demonstrated that $\struct {\Z, -}$ does not satisfy the [[Definition:Semigroup Axioms|semigroup axioms]]. We then have [[Subtraction on Numbers is Not Associative]]. So, for example: :$3 - \paren {2 - 1} = 2 \ne \paren {3 - 2} - 1 = 0$ Thus it has been demonstrated that $\struct {\Z, -}$ does not satisfy {{SemigroupAxiom|1}}. Hence the result. {{qed}}	0
Let $V \subset S$ be an [[Definition:Open Set (Topology)|open set]]. By assumption, we have that, for all $i \in I$, $U_i = \left({f \restriction_{C_i} }\right)^{-1} \left({V}\right)$ is also open. From the definition of a [[Definition:Restriction of Mapping|restriction]], we have that $U_i = C_i \cap f^{-1} \left({V}\right)$. Therefore, we can compute: {{begin-eqn}} {{eqn|l = \left({f \restriction_C}\right)^{-1} \left({V}\right) |r = C \cap f^{-1} \left({V}\right) |c = Definition of [[Definition:Restriction of Mapping|restriction]] }} {{eqn|r = \left({ \bigcup_{i \mathop \in I} C_i }\right) \cap f^{-1} \left({V}\right) |c = Definition of $C$ }} {{eqn|r = \bigcup_{i \mathop \in I} \left({ C_i \cap f^{-1} \left({V}\right) }\right) |c = [[Intersection Distributes over Union]] }} {{eqn|r = \bigcup_{i \mathop \in I} U_i |c = Definition of $U_i$ }} {{end-eqn}} That is, $U = \left({f \restriction_C}\right)^{-1} \left({V}\right)$ is a [[Definition:Set Union|union]] of open sets. Therefore, $U$ is itself [[Definition:Open Set (Topology)|open]] by definition of a [[Definition:Topology|topology]]. It follows that $f \restriction_C$ is also continuous by the definition of [[Definition:Continuous Mapping (Topology)|continuity]]. {{qed}}	0
By definition of [[Definition:Modulo Operation|modulo operation]]: :$x \bmod y := x - y \floor {\dfrac x y}$ for $y \ne 0$. We have: {{begin-eqn}} {{eqn | l = \dfrac {0 \cdotp 11} {-0 \cdotp 1} | r = \dfrac {1 \cdotp 1} {-1} | c = }} {{eqn | r = -1 \cdotp 1 | c = }} {{end-eqn}} and so: :$\floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } = -2$ Thus: {{begin-eqn}} {{eqn | l = 0 \cdotp 11 \bmod -0 \cdotp 1 | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \floor {\dfrac {0 \cdotp 11} {-0 \cdotp 1} } | c = }} {{eqn | r = 0 \cdotp 11 - \paren {-0 \cdotp 1} \times \paren {-2} | c = }} {{eqn | r = 0 \cdotp 11 - 0 \cdotp 2 | c = }} {{eqn | r = -0 \cdotp 09 | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \int_0^\infty e^{-a x^2} \rd x | r = \int_0^\infty e^{-\paren {\sqrt a x}^2} \rd x }} {{eqn | r = \frac 1 {\sqrt a} \int_0^\infty e^{-t^2} \rd t | c = [[Integration by Substitution|substituting]] $t = \sqrt a x$ }} {{eqn | r = \frac 1 2 \sqrt {\frac \pi a} | c = [[Integral to Infinity of Exponential of -t^2|Integral to Infinity of $e^{-t^2}$]] }} {{end-eqn}} {{qed}}	0
:$\map \coth {a + b i} = \dfrac {\coth a - \coth a \cot^2 b} {\coth^2 a + \cot^2 b} + \dfrac {\coth b + \coth^2 a \cot b} {\coth^2 a + \cot^2 b} i$	0
{{begin-eqn}} {{eqn | n = 1 | l = e^{i x} | r = \cos x + i \sin x | c = [[Euler's Formula/Real Domain|Euler's Formula]] }} {{eqn | n = 2 | l = e^{-i x} | r = \cos x - i \sin x | c = [[Euler's Formula/Real Domain/Corollary|Euler's Formula: Corollary]] }} {{eqn | ll= \leadsto | l = e^{i x} + e^{-i x} | r = \paren {\cos x + i \sin x} + \paren {\cos x - i \sin x} | c = $(1) + (2)$ }} {{eqn | r = 2 \cos x | c = simplifying }} {{eqn | ll= \leadsto | l = \frac {e^{i x} + e^{-i x} } 2 | r = \cos x | c = }} {{end-eqn}} {{qed}}	0
Let $n \in \N$ be a [[Definition:Natural Number|natural number]]. Let $p_n$ denote the $n$th [[Definition:Prime Number|prime number]]. Consider the [[Definition:Product Notation (Algebra)|product]]: :$\displaystyle \prod_{k \mathop = 1}^n \frac 1 {1 - 1 / p_k}$ By [[Sum of Geometric Sequence]], we have: {{begin-eqn}} {{eqn | l = \frac 1 {1 - \frac 1 2} | r = 1 + \frac 1 2 + \frac 1 {2^2} + \cdots | c = }} {{eqn | l = \frac 1 {1 - \frac 1 3} | r = 1 + \frac 1 3 + \frac 1 {3^2} + \cdots | c = }} {{eqn | l = \frac 1 {1 - \frac 1 5} | r = 1 + \frac 1 5 + \frac 1 {5^2} + \cdots | c = }} {{eqn | o = \cdots | c = }} {{eqn | l = \frac 1 {1 - \frac 1 {p_n} } | r = 1 + \frac 1 {p_n} + \frac 1 {p_n^2} + \cdots | c = }} {{end-eqn}} Consider what happens when all these [[Definition:Series|series]] are multiplied together. A new [[Definition:Series|series]] will be generated whose terms consist of all possible products of one term selected from each of the [[Definition:Series|series]] on the {{RHS}}. This new [[Definition:Series|series]] will [[Definition:Convergent Series|converge]] in any order to the product of the terms on the {{LHS}}. By the [[Fundamental Theorem of Arithmetic]], every [[Definition:Integer|integer]] greater than $1$ is uniquely expressible as a product of powers of different [[Definition:Prime Number|primes]]. Hence the product of these [[Definition:Series|series]] is the [[Definition:Series|series]] of [[Definition:Reciprocal|reciprocals]] of all [[Definition:Strictly Positive Integer|(strictly) positive integers]] whose [[Definition:Prime Factor|prime factors]] are no greater than $p_n$. In particular, all [[Definition:Strictly Positive Integer|(strictly) positive integers]] up to $p_n$ have this property. So: :$\displaystyle \prod_{k \mathop = 1}^n \frac 1 {1 - 1 / p_k}$ {{begin-eqn}} {{eqn | l = \prod_{k \mathop = 1}^n \frac 1 {1 - 1 / p_k} | o = \ge | r = \sum_{k \mathop = 1}^{p_n} \frac 1 k | c = }} {{eqn | o = > | r = \int_1^{p_n + 1} \dfrac {\mathrm d x} x | c = }} {{eqn | r = \map \ln {p_n + 1} | c = }} {{eqn | r = \ln p_n | c = }} {{end-eqn}} It follows by taking [[Definition:Reciprocal|reciprocals]] that: :$\displaystyle \prod_{k \mathop = 1}^n \paren {1 - \frac 1 {p_k} } < \frac 1 {\ln p_n}$ Taking [[Definition:Logarithm|logarithms]] of each side: :$(1): \quad \displaystyle \sum_{k \mathop = 1}^n \map \ln {1 - \frac 1 {p_k} } < - \ln \ln p_n$ Next, note that the [[Definition:Straight Line|line]] $y = 2 x$ in the [[Definition:Cartesian Plane|cartesian plane]] lies below the curve $y = \map \ln {1 + x}$ on the [[Definition:Half-Open Real Interval|interval]] $\closedint {-\frac 1 2} 0$. Also note that all [[Definition:Prime Number|primes]] are greater than or equal to $2$. Thus it follows that: :$-\dfrac 2 {p_k} < \map \ln {1 - \dfrac 1 {p_k} }$ Applying this to $(1)$ yields: :$\displaystyle -2 \sum_{k \mathop = 1}^n \dfrac 1 {p_k} < -\ln \ln p_n$ and so: :$\displaystyle \sum_{k \mathop = 1}^n \dfrac 1 {p_k} > \dfrac 1 2 \ln \ln p_n$ But: :$\displaystyle \lim_{n \mathop \to \infty} \ln \ln p_n \to \infty$ and so the [[Definition:Series|series]]: :$\displaystyle \sum_{p \mathop \in \Bbb P} \frac 1 p$ is [[Definition:Divergent Series|divergent]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \tan x \rd x | r = \ln \size {\sec x} | c = [[Primitive of Tangent Function/Secant Form|Primitive of $\tan x$: Secant Form]] }} {{eqn | ll= \leadsto | l = \int \tan a x \rd x | r = \frac 1 a \paren {\ln \size {\sec a x} } + C | c = [[Primitive of Function of Constant Multiple]] }} {{eqn | r = \frac {\ln \size {\sec a x} } a + C | c = simplifying }} {{end-eqn}} {{qed}}	0
For $n \ge 0$: {{begin-eqn}} {{eqn | l = \int_\alpha^{\alpha + n L} \map f x \d x | r = \int_\alpha^0 \map f x \d x + \sum_{k \mathop = 0}^{n - 1} \int_{k L}^{\paren {k + 1} L} \map f x \d x + \int_{n L}^{\alpha + n L} \map f x \d x | c = [[Sum of Integrals on Adjacent Intervals for Integrable Functions/Corollary]] }} {{eqn | r = \int_\alpha^0 \map f x \d x + \sum_{k \mathop = 0}^{n - 1} \int_{k L}^{\paren {k + 1} L} \map f {x - k L} \d x + \int_{n L}^{\alpha + n L} \map f {x - n L} \d x | c = [[General Periodicity Property]] }} {{eqn | r = \int_\alpha^0 \map f x \d x + \sum_{k \mathop = 0}^{n - 1} \int_0^L \map f x \d x + \int_0^\alpha \map f x \d x | c = [[Integration by Substitution]] }} {{eqn | r = n \int_0^L \map f x \d x | c = [[Reversal of Limits of Definite Integral]] }} {{end-eqn}} For $n < 0$: {{begin-eqn}} {{eqn | l = \int_\alpha^{\alpha + n L} \map f x \d x | r = -\int_{\alpha + n L}^\alpha \map f x \d x | c = [[Reversal of Limits of Definite Integral]] }} {{eqn | r = -\int_{\alpha + n L}^{\alpha + n L + \paren {-n L} } \map f x \d x }} {{eqn | r = -\paren {-n \int_0^L \map f x \d x} | c = by the above; $-n > 0$ }} {{eqn | r = n \int_0^L \map f x \d x }} {{end-eqn}} Hence the result. {{qed}} [[Category:Definite Integrals]] [[Category:Periodic Functions]] bewj8ikxf8sp0alog7mr5srk702adv3	0
By definition of [[Definition:Homeomorphism/Topological Spaces/Definition 4|homeomorphism]], $\phi$ is a [[Definition:Closed Mapping|closed]] [[Definition:Everywhere Continuous Mapping (Topology)|continuous]] [[Definition:Bijection|bijection]]. The result follows from [[T1 Space is Preserved under Closed Bijection|$T_1$ (Fréchet) Space is Preserved under Closed Bijection]]. {{qed}}	0
{{begin-eqn}} {{eqn | l = \laptrans {t^2 \cos a t} | r = -\map {\dfrac {\d^2} {\d s^2} } {\laptrans {\cos a t} } | c = [[Higher Order Derivatives of Laplace Transform]] }} {{eqn | r = -\map {\dfrac {\d^2} {\d s^2} } {\dfrac a {s^2 + a^2} } | c = [[Laplace Transform of Sine]] }} {{eqn | r = \dfrac {2 s^3 - 6 a^2 s} {\paren {s^2 + a^2}^3} | c = [[Quotient Rule for Derivatives]] }} {{end-eqn}} {{qed}}	0
:$\map {\dfrac \d {\d x} } {\cos x} = -\sin x$	0
=== Necessary Condition === By assumption the [[Definition:Boundary Condition|boundary conditions]] are [[Definition:Self-Adjoint Boundary Conditions|self-adjoint]]. Then there exists $\map g {x, \mathbf y}$ such that: :$\map {p_i} {x, \mathbf y, \map {\boldsymbol \psi} {\mathbf y} } = \dfrac {\partial \map g {x \mathbf y} } {\partial y_i}$ Because $\mathbf p \in C^1$: :$g \in C^2$ {{handwaving|Why?}} [[Definition:Derivative|Differentiate]] both sides {{WRT|Differentiation}} $y_k$: :$\dfrac {\partial \map {p_i} {x, \mathbf y, \map {\boldsymbol \psi} {\mathbf y} } } {\partial y_k} = \dfrac {\partial^2 \map g {x, \mathbf y} } {\partial y_k \partial y_i}$ By the [[Schwarz-Clairaut Theorem]], [[Definition:Partial Derivative|partial derivatives]] [[Definition:Commuting Elements|commute]], hence indices can be mutually replaced: :$\dfrac {\partial \map {p_i} {x, \mathbf y, \map {\boldsymbol \psi} {\mathbf y} } } {\partial y_k} = \dfrac {\partial \map {p_k} {x, \mathbf y, \map {\boldsymbol \psi} {\mathbf y} } } {\partial y_i}$ Fixing $x = a$ provides the result. {{qed|lemma}} === Sufficient condition === By assumption: :$\valueat {\dfrac {\partial p_i} {\partial y_j} } {x \mathop = a} = \valueat {\dfrac {\partial p_j} {\partial y_i} } {x \mathop = a}$ Then: :$\exists \map g {x, \mathbf y} \in C^2: \valueat {\dfrac {\partial p_i} {\partial y_j} } {x \mathop = a} = \valueat {\dfrac {\partial p_j} {\partial y_i} } {x \mathop = a} = \valueat {\dfrac {\partial^2 g} {\partial y_i \partial y_j} } {x \mathop = a}$ In other words: :$\bigvalueat {p_i} {x \mathop = a} = \valueat {\dfrac {\partial g} {\partial y_i} } {x \mathop = a}$ {{handwaving|why?}} Hence, the [[Definition:Boundary Condition|boundary conditions]] are [[Definition:Self-Adjoint Boundary Conditions|self-adjoint]]. {{qed}}	0
The [[Definition:Second Order ODE|second order ODE]]: :$(1): \quad x^2 y'' + x y' - y = 0$ has the [[Definition:General Solution|general solution]]: :$y = C_1 x + \dfrac {C_2} x$	0
:$\displaystyle \int \frac {\d x} {x \paren {a^2 - x^2}^2} = \frac 1 {2 a^2 \paren {a^2 - x^2} } + \frac 1 {2 a^4} \map \ln {\frac {x^2} {a^2 - x^2} } + C$ for $x^2 < a^2$.	0
{{begin-eqn}} {{eqn | l = \int x^3 \paren {\sqrt {a^2 - x^2} }^3 \rd x | r = \int x \paren {x^2} \paren {\sqrt {a^2 - x^2} }^3 \rd x | c = }} {{eqn | r = \int x \paren {x^2 - a^2 + a^2} \paren {\sqrt {a^2 - x^2} }^3 \rd x | c = [[Primitive of Power]] }} {{eqn | r = -\int x \paren {a^2 - x^2} \paren {\sqrt {a^2 - x^2} }^3 \rd x + a^2 \int x \paren {\sqrt {a^2 - x^2} }^3 \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = -\int x \paren {\sqrt {a^2 - x^2} }^5 \rd x + a^2 \int x \paren {\sqrt {a^2 - x^2} }^3 \rd x | c = simplifying }} {{end-eqn}} Let: {{begin-eqn}} {{eqn | l = z | r = a^2 - x^2 }} {{eqn | ll= \leadsto | l = \frac {\d z} {\d x} | r = -2 x | c = [[Power Rule for Derivatives]] }} {{eqn | ll= \leadsto | o = | r = -\int x \paren {\sqrt {a^2 - x^2} }^5 \rd x + a^2 \int x \paren {\sqrt {a^2 - x^2} }^3 \rd x | c = }} {{eqn | r = -\int \frac {\sqrt z z^{5/2} } {-2 \sqrt z} \rd z + a^2 \int \frac {\sqrt z z^{3/2} } {-2 \sqrt z} \rd z | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 2 \int z^{5/2} \rd z - \frac {a^2} 2 \int z^{3/2} \rd z | c = simplifying }} {{eqn | r = \frac 1 2 \frac {z^{7/2} } {7/2} - \frac {a^2} 2 \frac {z^{5/2} } {5/2} + C | c = [[Primitive of Power]] }} {{eqn | r = \frac {z^{7/2} } 7 - a^2 \frac {z^{5/2} } 5 + C | c = simplifying }} {{eqn | r = \frac {\paren {\sqrt {a^2 - x^2} }^7} 7 - \frac {a^2 \paren {\sqrt {a^2 - x^2} }^5} 5 + C | c = substituting for $z$ }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {\left({1 - \sin a x}\right)^2} = \frac 1 {2 a} \tan \left({\frac \pi 4 + \frac {a x} 2}\right) + \frac 1 {6 a} \tan^3 \left({\frac \pi 4 + \frac {a x} 2}\right) + C$	0
Let: {{begin-eqn}} {{eqn | l = u | r = \sqrt{a x + b} | c = }} {{eqn | ll= \leadsto | l = x | r = \frac {u^2 - b} a | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \int \frac {p x + q} {\sqrt {a x + b} } \rd x | r = \int \paren {p \paren {\frac {u^2 - b} a} + q} \frac {2 u} {a u} \rd x | c = [[Primitive of Function of Root of a x + b|Primitive of Function of Root of $a x + b$]] }} {{eqn | r = \frac 2 {a^2} \int \paren {p u^2 - b p + a q} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 2 {a^2} \paren {\frac {p u^3} 3 - b p z + a q z} + C | c = [[Primitive of Power]] }} {{eqn-intertext|Note that at this point we can assume $u > 0$, otherwise $\sqrt {a x + b}$ would not have been defined.}} {{eqn | r = \frac 2 {a^2} \paren {\frac {p \paren {\sqrt {a x + b} }^3} 3 - b p \sqrt {a x + b} + a q \sqrt {a x + b} } + C | c = substituting for $u$ }} {{eqn | r = \frac {2 \paren {a p x - 2 b p + 3 a q} } {3 a^2} \sqrt {a x + b} + C | c = simplification }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\d x} {x^m \sqrt {a x + b} } = -\frac {\sqrt {a x + b} } {\paren {m - 1} b x^{m - 1} } - \frac {\paren {2 m - 3} a} {\paren {2 m - 2} b} \int \frac {\d x} {x^{m - 1} \sqrt {a x + b} }$	0
$(1)$ is in the form: :$\dfrac {\d y} {\d x} + \map P x y = \map Q x$ where $\map P x = -\dfrac 1 x$. Thus: {{begin-eqn}} {{eqn | l = \int \map P x \rd x | r = \int -\frac 1 x \rd x | c = }} {{eqn | r = -\ln x | c = }} {{eqn | ll= \leadsto | l = e^{\int P \rd x} | r = e^{-\ln x} | c = }} {{eqn | r = \frac 1 x | c = }} {{end-eqn}} Thus from [[Solution by Integrating Factor]], $(1)$ can be rewritten as: :$\map {\dfrac {\d} {\d x} } {\dfrac y x} = \dfrac 1 x k x = k$ and the [[Definition:General Solution|general solution]] is: :$\dfrac y x = k x + C$ or: :$y = k x^2 + C x$ {{qed}} [[Category:Examples of Linear First Order ODEs]] 2c5s81vvm5t4d1tgvkb8zr7atz6asrc	0
{{begin-eqn}} {{eqn | l = \laptrans {\map {I_\R} t} | r = \laptrans t | c = {{Defof|Identity Mapping}} }} {{eqn | r = \int_0^{\to +\infty} t e^{-st} \rd t | c = {{Defof|Laplace Transform}} }} {{end-eqn}} From [[Integration by Parts]]: :$\displaystyle \int f g' \rd t = f g - \int f'g \rd t$ Here: {{begin-eqn}} {{eqn | l = f | r = t | c = }} {{eqn | ll= \leadsto | l = f' | r = 1 | c = [[Derivative of Identity Function]] }} {{eqn | l = g' | r = e^{-st} }} {{eqn | ll= \leadsto | l = g | r = -\frac 1 s e^{-s t} | c = [[Primitive of Exponential Function]] }} {{end-eqn}} So: {{begin-eqn}} {{eqn | l = \int t e^{-s t} \rd t | r = -\frac t s e^{-s t} - \frac 1 s \int e^{-s t} \rd t }} {{eqn | r = -\frac t s e^{-s t} - \frac 1 {s^2} e^{-s t} | c = [[Primitive of Exponential Function]] }} {{end-eqn}} Evaluating at $t = 0$ and $t \to +\infty$: {{begin-eqn}} {{eqn | l = \laptrans t | r = \intlimits {-\frac t s e^{-s t} - \frac 1 {s^2} e^{-s t} } {t \mathop = 0} {t \mathop \to +\infty} }} {{eqn | r = -\frac 1 s \lim_{t \mathop \to +\infty} \frac t { e^{s t} } - \paren {0 - \frac 1 {s^2} } | c = [[Exponential of Zero and One]], [[Exponent Combination Laws/Negative Power|Exponent Combination Laws: Negative Power]] }} {{eqn | r = 0 + \frac 1 {s^2} | c = [[Limit at Infinity of Polynomial over Complex Exponential]] }} {{eqn | r = \frac 1 {s^2} }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\frac \d {\d x} } {\sech^{-1} u} | r = \map {\frac \d {\d u} } {\sech^{-1} u} \frac {\d u} {\d x} | c = [[Chain Rule for Derivatives]] }} {{eqn | r = \dfrac {-1} {u \sqrt {1 - u^2} } \frac {\d u} {\d x} | c = [[Derivative of Inverse Hyperbolic Secant]] }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \frac {\mathrm d x} {\cos^n a x} = \frac {\sin a x} {a \left({n - 1}\right) \cos^{n - 1} a x} + \frac {n - 2} {n - 1} \int \frac {\mathrm d x} {\cos^{n - 2} a x}$	0
{{begin-eqn}} {{eqn | l = z | r = p + q e^{a x} | c = }} {{eqn | ll= \leadsto | l = \frac {\d z} {\d x} | r = a q e^{a x} | c = [[Derivative of Exponential of a x|Derivative of $e^{a x}$]] }} {{eqn | r = a \paren {z - p} | c = in terms of $z$ }} {{eqn | ll= \leadsto | l = \int \frac {\d x} {p + q e^{a x} } | r = \int \frac {\d z} {a \paren {z - p} z} | c = [[Integration by Substitution]] }} {{eqn | r = \frac 1 a \int \frac {\d z} {z \paren {z - p} } | c = [[Primitive of Constant Multiple of Function]] }} {{eqn | r = \frac 1 a \paren {\frac 1 {-p} \ln \size {\frac z {z - p} } } + C | c = [[Primitive of Reciprocal of x by a x + b|Primitive of $\dfrac 1 {x \paren {a x + b} }$]] }} {{eqn | r = \frac {-1} {a p} \ln \size {\frac {p + q e^{a x} } {q e^{a x} } } + C | c = substituting for $z$ }} {{eqn | r = \frac {-1} {a p} \paren {\ln \size {p + q e^{a x} } - \ln \size {q e^{a x} } } + C | c = [[Difference of Logarithms]] }} {{eqn | r = \frac {-1} {a p} \paren {\ln \size {p + q e^{a x} } - \paren {\ln \size {e^{a x} } + \ln \size q} } + C | c = [[Sum of Logarithms]] }} {{eqn | r = \frac {-1} {a p} \ln \size {p + q e^{a x} } + \frac 1 {a p} \map \ln {e^{a x} } + \frac 1 {a p} \ln \size q + C | c = $e^{a x}$ always [[Definition:Positive Real Number|positive]] }} {{eqn | r = \frac {-1} {a p} \ln \size {p + q e^{a x} } + \frac 1 {a p} \map \ln {e^{a x} } + C | c = $\dfrac 1 {a p} \ln \size q$ subsumed into [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{eqn | r = \frac {-1} {a p} \ln \size {p + q e^{a x} } + \frac 1 {a p} a x + C | c = [[Exponential of Natural Logarithm]] }} {{eqn | r = \frac x p - \frac 1 {a p} \ln \size {p + q e^{a x} } + C | c = simplification }} {{end-eqn}} {{qed}}	0
The [[Definition:First Order Ordinary Differential Equation|first order ordinary differential equation]]: :$F = \map M {x, y} + \map N {x, y} \dfrac {\d y} {\d x} = 0$ is an [[Definition:Exact Differential Equation|exact differential equation]] {{iff}}: :$\dfrac {\partial M} {\partial y} = \dfrac {\partial N} {\partial x}$ The [[Definition:General Solution|general solution]] of such an [[Definition:Exact Differential Equation|equation]] is: :$\map f {x, y} = C$ where: :$\dfrac {\partial f} {\partial x} = M$ :$\dfrac {\partial f} {\partial y} = N$	0
:$\displaystyle \int \frac {\mathrm d x} {\sin a x \cos a x} = \frac 1 a \ln \left\vert{\tan a x}\right\vert + C$	0
:$\displaystyle \int \frac {x^m \rd x} {x^3 + a^3} = \frac {x^{m - 2} } {m - 2} - a^3 \int \frac {x^{m - 3} \rd x} {x^3 + a^3}$	0
{{begin-eqn}} {{eqn | l = \int \sinh a x \cosh a x \rd x | r = \int \cosh a x \sinh a x \rd x | c = }} {{eqn | r = \frac {\cosh^2 a x} {2 a} + C | c = [[Primitive of Power of Hyperbolic Cosine of a x by Hyperbolic Sine of a x|Primitive of $\cosh^n a x \sinh a x$]] using $n = 1$ }} {{eqn | r = \frac {1 + \sinh^2 a x} {2 a} + C | c = [[Difference of Squares of Hyperbolic Cosine and Sine]] }} {{eqn | r = \frac 1 {2 a} + \frac {\sinh^2 a x} {2 a} + C | c = simplifying }} {{eqn | r = \frac {\sinh^2 a x} {2 a} + C | c = subsuming $\dfrac 1 {2 a}$ into [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
The [[Definition:Set|set]] of [[Definition:Rational Number|rational numbers]] $\Q$ does not form a [[Definition:Discrete Space|discrete space]].	0
:$\displaystyle \int \frac {\d x} {x \paren {x^2 - a^2}^2} = \frac {-1} {2 a^2 \left({x^2 - a^2}\right)} + \frac 1 {2 a^4} \ln \left({\frac {x^2} {x^2 - a^2} }\right) + C$ for $x^2 > a^2$.	0
[[Definition:By Hypothesis|By hypothesis]], $\map {f'} {x_0}$ exists. We have: {{begin-eqn}} {{eqn | l = \map f x - \map f {x_0} | r = \frac {\map f x - \map f {x_0} } {x - x_0} \cdot \paren {x - x_0} | c = }} {{eqn | o = \to | r = \map {f'} {x_0} \cdot 0 | c = as $x \to x_0$ }} {{end-eqn}} Thus: :$\map f x \to \map f {x_0}$ as $x \to x_0$ or in other words: :$\displaystyle \lim_{x \mathop \to x_0} \map f x = \map f {x_0}$ The result follows by definition of [[Definition:Continuous Real Function at Point|continuous]]. {{qed}}	0
=== [[Sine of x minus Cosine of x/Sine Form|Sine Form]] === {{:Sine of x minus Cosine of x/Sine Form}} === [[Sine of x minus Cosine of x/Cosine Form|Cosine Form]] === {{:Sine of x minus Cosine of x/Cosine Form}}	0
{{begin-eqn}} {{eqn | l = a | o = \perp | r = b }} {{eqn | ll= \leadsto | l = \gcd \set {a, b} | r = 1 | c = {{Defof|Coprime Integers}} }} {{eqn | ll= \leadsto | l = \exists m, n \in \Z: m a + n b | r = 1 | c = [[Bézout's Lemma]] }} {{end-eqn}} Then we have: {{begin-eqn}} {{eqn | l = \exists m, n \in \Z: m a + n b | r = 1 | c = }} {{eqn | ll= \leadsto | l = \gcd \set {a, b} | o = \divides | r = 1 | c = [[Set of Integer Combinations equals Set of Multiples of GCD]] }} {{eqn | ll= \leadsto | l = \gcd \set {a, b} | r = 1 | c = }} {{eqn | ll= \leadsto | l = a | o = \perp | r = b | c = {{Defof|Coprime Integers}} }} {{end-eqn}} {{qed}}	0
Let $f_1: \Z_{>0} \to \Z_{>0}$ be the [[Definition:Constant Mapping|constant function]]: :$\forall n \in \Z_{>0}: \map {f_1} n = 1$ Thus we have: :$\displaystyle \map \tau n = \sum_{d \mathop \divides n} 1 = \sum_{d \mathop \divides n} \map {f_1} d$ But from [[Unity Function is Completely Multiplicative]], $f_1$ is [[Definition:Multiplicative Arithmetic Function|multiplicative]]. The result follows from [[Sum Over Divisors of Multiplicative Function]]. {{qed}} [[Category:Multiplicative Functions]] [[Category:Tau Function]] lo34yjiju2pxjv7407tw8mj35vs52kr	0
The operation of [[Definition:Real Multiplication|multiplication]] on the [[Definition:Set|set]] of [[Definition:Real Number|real numbers]] $\R$ is [[Definition:Distributive Operation|distributive]] over the operation of [[Definition:Real Addition|addition]]: :$\forall x, y, z \in \R:$ ::$x \times \paren {y + z} = x \times y + x \times z$ ::$\paren {y + z} \times x = y \times x + z \times x$	0
Let: {{begin-eqn}} {{eqn | l = \int \frac {x^2 \rd x} {a^2 - x^2} | r = \int \frac {x^2 - a^2 + a^2} {a^2 - x^2} \rd x | c = }} {{eqn | r = \int \frac {- \left({a^2 - x^2}\right)} {a^2 - x^2} \rd x + \int \frac {a^2} {a^2 - x^2} \rd x | c = [[Linear Combination of Integrals]] }} {{eqn | r = - \int \rd x + a^2 \int \frac {\rd x} {a^2 - x^2} | c = [[Linear Combination of Integrals]] }} {{eqn | r = -x + a^2 \int \frac {\rd x} {a^2 - x^2} + C | c = [[Primitive of Constant]] }} {{eqn | r = -x + a^2 \left({\frac 1 a \tanh^{-1} \frac x a}\right) + C | c = [[Primitive of Reciprocal of a squared minus x squared/Inverse Hyperbolic Tangent Form|Primitive of Reciprocal of $a^2 - x^2$: $\tanh^{-1}$ form]] }} {{eqn | r = -x + a \tanh^{-1} \frac x a + C | c = simplifying }} {{end-eqn}} {{qed}} [[Category:Primitive of x squared over a squared minus x squared]] pg0zh6tf9dnyu8rdcx9ic2og1wdag3n	0
From [[Bessel Function of the First Kind of Order Zero]]: {{begin-eqn}} {{eqn | l = \map {J_0} t | r = \sum_{k \mathop = 0}^\infty \dfrac {\paren {-1}^k} {\paren {k!}^2} \paren {\dfrac t 2}^{2 k} | c = }} {{eqn | r = 1 - \dfrac {t^2} {2^2} + \dfrac {t^4} {2^2 \times 4^2} - \dfrac {t^6} {2^2 \times 4^2 \times 6^2} + \dotsb | c = }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = \laptrans {\map {J_0} t} | r = \laptrans {\sum_{k \mathop = 0}^\infty \dfrac {\paren {-1}^k} {\paren {k!}^2} \paren {\dfrac t 2}^{2 k} } | c = }} {{eqn | r = \sum_{k \mathop = 0}^\infty \dfrac {\paren {-1}^k} {2^{2 k} \paren {k!}^2} \laptrans {t^{2 k} } | c = [[Linear Combination of Laplace Transforms]] }} {{eqn | r = \sum_{k \mathop = 0}^\infty \dfrac {\paren {-1}^k} {2^{2 k} \paren {k!}^2} \dfrac {\paren {2 k}!} {s^{2 k + 1} } | c = [[Laplace Transform of Positive Integer Power]] }} {{eqn | r = \sum_{k \mathop = 0}^\infty \paren {-1}^k \paren {\dfrac 1 2}^{2 k} \dbinom {2 k} k \dfrac 1 {s^{2 k + 1} } | c = {{Defof|Binomial Coefficient}}: $\dbinom {2 k} k = \dfrac {\paren {2 k}!} {\paren {k!}^2}$ }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \dfrac 1 {\sqrt {s^2 + 1} } | r = \dfrac 1 {\sqrt {s^2} \sqrt {1 + \paren {1 / s}^2} } | c = }} {{eqn | r = \dfrac 1 s \paren {1 + \paren {\dfrac 1 s}^2}^{-1/2} | c = {{Defof|Rational Power}} }} {{eqn | r = \dfrac 1 s \sum_{k \mathop = 0}^\infty \dbinom {-1/2} k \paren {\dfrac 1 s}^{2 k} | c = [[General Binomial Theorem]] }} {{eqn | r = \dfrac 1 s \sum_{k \mathop = 0}^\infty \dfrac {\paren {-1}^k} {4^k} \dbinom {2 k} k \paren {\dfrac 1 s}^{2 k} | c = [[Binomial Coefficient of Minus Half]] }} {{eqn | r = \sum_{k \mathop = 0}^\infty \paren {-1}^k \dfrac 1 {2^{2 k} } \dbinom {2 k} k \paren {\dfrac 1 {s^{2 k + 1} } } | c = rearranging, and bringing $\dfrac 1 s$ inside the [[Definition:Summation|summation]] }} {{eqn | r = \sum_{k \mathop = 0}^\infty \paren {-1}^k \paren {\dfrac 1 2}^{2 k} \dbinom {2 k} k \dfrac 1 {s^{2 k + 1} } | c = further rearrangement }} {{end-eqn}} The two expressions match, and the result follows. {{qed}}	0
=== Necessary Condition === Let $f$ be [[Definition:Continuous at Point of Metric Space|continuous]] at $a \in A_1$. Let $\displaystyle \lim_{n \mathop \to \infty} x_n = a$. Let $V$ be a [[Definition:Neighborhood (Metric Space)|neighborhood]] of $f \left({a}\right)$. Then by [[Metric Space Continuity by Inverse of Mapping between Neighborhoods]] $f^{-1} \left[{V}\right]$ is a [[Definition:Neighborhood (Metric Space)|neighborhood]] of $a$. By [[Limit of Sequence in Metric Space in Neighborhood]]: :$\exists N \in \N: n > N \implies x_n \in f^{-1} \left[{V}\right]$ Thus for each [[Definition:Neighborhood (Metric Space)|neighborhood]] $V$ of $f \left({a}\right)$ :$\exists N \in \N: n > N \implies f \left({x_n}\right) \in V$ By [[Limit of Sequence in Metric Space in Neighborhood]]: : $\displaystyle \lim_{n \mathop \to \infty} f \left({x_n}\right) = f \left({a}\right)$ {{qed|lemma}} === Sufficient Condition === Aiming for a [[Proof by Contradiction|contradiction]], suppose $f$ is not [[Definition:Continuous at Point of Metric Space|continuous]] at $a \in A_1$. Then there is a [[Definition:Neighborhood (Metric Space)|neighborhood]] $V$ of $f \left({a}\right)$ such that for each [[Definition:Neighborhood (Metric Space)|neighborhood]] $U$ of $a$: :$f \left[{U}\right] \nsubseteq V$ In particular, for each [[Definition:Open Ball|open $\epsilon$-ball]] $B_\epsilon \left({a}\right)$ such that $\forall n \in \N_{>0}: \epsilon = \dfrac 1 n$: :$f \left({B_\epsilon \left({a}\right)}\right) \nsubseteq V$ Thus for each $n \in \N_{>0}$ there exists a point $x_n$ such that: :$x_n \subseteq B_\epsilon \left({a}\right)$ and: :$f \left({x_n}\right) \notin V$ Now: :$d_1 \left({a, x_n}\right) < \dfrac 1 n$ and therefore: :$\displaystyle \lim_{n \mathop \to \infty} x_n = a$ whereas $\displaystyle \lim_{n \mathop \to \infty} f \left({x_n}\right) = f \left({a}\right)$ is impossible, as: :$\forall n \in \N_{>0}: f \left({x_n}\right) \notin V$ {{qed}}	0
The [[Definition:Local Minimum|local minimum]] of the [[Definition:Gamma Function|Gamma function]] on the [[Definition:Positive Real Number|positive real numbers]] occurs at the point: :$\left({1 \cdotp 46163 21449 68362 34126 26595, 0 \cdotp 88560 31944 10888 70027 88159}\right)$ {{OEIS|A030169|order = $x$-coordinate}} {{OEIS|A030171|order = $y$-coordinate}}	0
Let $S$ be the [[Definition:Set|set]] of all [[Definition:Divisor of Integer|divisors]] of $n$. Then from [[Absolute Value of Integer is not less than Divisors]]: :$\forall m \in S: -n \le m \le n$ Thus $S$ is [[Definition:Finite Set|finite]]. {{qed}} [[Category:Number Theory]] [[Category:Divisors]] dhwii31ikzevtdctqmctmo6ixevhs9s	0
{{Proofread}} Let $C$ be any arbitrary closed curve which defines a region $R$ where the function $\map f z$ is [[Definition:Analytic Function|analytic]]. Let $z_0$ be any point in the region $R$ such that: :$\dfrac {\map f z} {z - z_0}$ is [[Definition:Analytic Function|analytic]] everywhere except at $z_0$. We draw a circle $C_1$ with center at $z_0$ and radius $r$ such that $r \to 0$. This makes $C$ and $C_1$ a multiply connected region. {{explain|A diagram at this point would be useful.}} According to Cauchy's Integral Theorem for a multiply connected region: {{begin-eqn}} {{eqn | l = I | o = := | r = \oint_C \frac {\map f z} {z - z_0} \rd z | c = }} {{eqn | r = \oint_{C_1} \frac {\map f z} {z - z_0} \rd z | c = }} {{eqn | r = \oint_{C_1} \frac {\map f {z_0} + \paren {\map f z - \map f {z_0} } } {z - z_0} \rd z | c = }} {{eqn | r = \map f {z_0} \oint_{C_1} \frac {\rd z} {z - z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z | c = }} {{end-eqn}} Let: {{begin-eqn}} {{eqn | l = z - z_0 | r = r e^{i \theta} | c = }} {{eqn | ll= \leadsto | l = \d z | r = i r e^{i \theta} \rd \theta | c = }} {{eqn | ll= \leadsto | l = \oint_{C_1} \frac {\rd z} {z - z_0} | r = \int_0^{2 \pi} \frac {i r e^{i \theta} } {r e^{i \theta} } \rd \theta | c = }} {{eqn | r = i \int_0^{2 \pi} \rd \theta | c = }} {{eqn | r = 2 \pi i | c = }} {{end-eqn}} Now: :$\displaystyle I = 2 \pi i \map f {z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z$ According to Epsilon-Delta definition of limit, for every $\left|{z - z_0}\right| < \delta$ there exists a $\epsilon \in \R_{>0}$ such that: :$\cmod {\map f z - \map f {z_0} } < \epsilon$ Hence: {{begin-eqn}} {{eqn | l = \cmod {\oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z} | o = \le | r = \oint_{C_1} \frac {\cmod {\map f z - \map f {z_0} } } {z - z_0} \cmod {\d z} | c = }} {{eqn | o = \le | r = \frac {\epsilon} {\delta} \oint_{C_1} \cmod {\d z} | c = }} {{eqn | r = 2 \pi \epsilon | c = }} {{end-eqn}} {{explain|The precise meaning of $\cmod {\d z}$}} As $\epsilon \to 0$: :$\displaystyle \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z = 0$ So: {{begin-eqn}} {{eqn | l = I | r = \map f {z_0} \oint_{C_1} \frac {\rd z} {z - z_0} + \oint_{C_1} \frac {\map f z - \map f {z_0} } {z - z_0} \rd z | c = }} {{eqn | r = 2 \pi i \map f {z_0} + 0 | c = }} {{eqn | ll= \leadsto | l = \oint_C \frac {\map f z} {z - z_0} \rd z | r = 2 \pi i \, \map f {z_0} | c = }} {{end-eqn}} {{qed}} {{Namedfor|Augustin Louis Cauchy|cat = Cauchy}}	0
Define a [[Definition:Mapping|mapping]] $f: \powerset {\N_{>0} } \to \R$ thus: :$\map f S = 0.d_1 d_2 \ldots$, interpreted as a [[Definition:Ternary Notation|ternary expansion]] where $\sequence {d_n}$ is the characteristic function of $S$. That is: :$\displaystyle \map f S = \sum_{i \mathop \in S} 3^{-i}$ By the [[Real Numbers are Uncountable/Proof 2 using Ternary Notation/Lemma|lemma]], $f$ is an [[Definition:Injection|injection]]. {{AimForCont}} that $\R$ is [[Definition:Countable Set|countable]]. Then there is an [[Definition:Injection|injection]] $g: \R \to \N$. By [[Composite of Injections is Injection]], $g \circ f: \powerset \N \to \N$ is an [[Definition:Injection|injection]]. But this [[Definition:Contradiction|contradicts]] [[No Injection from Power Set to Set]]. Hence, by [[Proof by Contradiction]], $\R$ is not [[Definition:Countable Set|countable]]. {{qed}}	0
Let $z$ be expressed in [[Definition:Polar Form of Complex Number|polar form]]: :$z := r \left({\cos \theta + i \sin \theta}\right)$ Then: {{begin-eqn}} {{eqn | l = \overline z | r = r \left({\cos \theta - i \sin \theta}\right) | c = [[Polar Form of Complex Conjugate]] }} {{eqn | r = r \left({\cos \left({-\theta}\right) + i \sin \left({-\theta}\right)}\right) | c = [[Cosine Function is Even]], [[Sine Function is Odd]] }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = \dfrac 1 z | r = \dfrac 1 r \left({\cos \theta - i \sin \theta}\right) | c = [[Polar Form of Reciprocal of Complex Number]] }} {{eqn | r = \dfrac 1 r \left({\cos \left({-\theta}\right) + i \sin \left({-\theta}\right)}\right) | c = [[Cosine Function is Even]], [[Sine Function is Odd]] }} {{end-eqn}} The result follows by definition of the [[Definition:Argument of Complex Number|argument]] of a [[Definition:Complex Number|complex number]] {{qed}}	0
Let $u' = \eqclass {b + u, b} {}$. Let $v' = \eqclass {c + v, c} {}$. Then: {{begin-eqn}} {{eqn | l = u' | o = > | r = v' | c = }} {{eqn | ll= \leadstoandfrom | l = \eqclass {b + u, b} {} | o = > | r = \eqclass {c + v, c} {} | c = }} {{eqn | ll= \leadstoandfrom | l = b + u + c | o = > | r = c + v + b | c = }} {{eqn | ll= \leadstoandfrom | l = u | o = > | r = v | c = }} {{end-eqn}} {{qed}}	0
Let $x$ be an element of one of the [[Definition:Standard Number Field|standard number fields]]: $\Q, \R, \C$ such that $x \ne 1$. Let $n \in \N_{>0}$. Then: :$\displaystyle \sum_{j \mathop = 0}^{n - 1} x^j = \frac {x^n - 1} {x - 1}$	0
Let $x \in \R$ be a [[Definition:Real Number|real number]]. Then there exists some [[Definition:Decreasing Sequence|decreasing]] [[Definition:Rational Sequence|rational sequence]] that [[Definition:Convergent Real Sequence|converges]] to $x$.	0
The proof proceeds by [[Principle of Mathematical Induction|induction]] on $r$. For all $r \in \Z_{>0}$, let $P \left({r}\right)$ be the [[Definition:Proposition|proposition]]: :$\displaystyle \sum_k \binom r k \binom {s + k} n \left({-1}\right)^{r - k} = \binom s {n - r}$ === Basis for the Induction === $P \left({0}\right)$ is the case: {{begin-eqn}} {{eqn | l = \sum_k \binom 0 k \binom {s + k} n \left({-1}\right)^{0 - k} | r = \delta_{0 k} \binom {s + k} n \left({-1}\right)^{0 - k} | c = [[Zero Choose n]] }} {{eqn | r = \binom s n | c = All terms vanish but for $k = 0$ }} {{eqn | r = \binom s {n - 0} | c = }} {{end-eqn}} Thus $P \left({0}\right)$ is seen to hold. This is the [[Principle of Mathematical Induction#Basis for the Induction|basis for the induction]]. === Induction Hypothesis === Now it needs to be shown that, if $P \left({m}\right)$ is true, where $m \ge 0$, then it logically follows that $P \left({m + 1}\right)$ is true. This is the [[Principle of Mathematical Induction#Induction Hypothesis|induction hypothesis]]: :$\displaystyle \sum_k \binom m k \binom {s + k} n \left({-1}\right)^{m - k} = \binom s {n - m}$ from which it is to be shown that: :$\displaystyle \sum_k \binom {m + 1} k \binom {s + k} n \left({-1}\right)^{m + 1 - k} = \binom s {n - \left({m + 1}\right)}$ === Induction Step === This is the [[Principle of Mathematical Induction#Induction Step|induction step]]: {{begin-eqn}} {{eqn | o = | r = \sum_k \binom {m + 1} k \binom {s + k} n \left({-1}\right)^{m + 1 - k} | c = }} {{eqn | r = \sum_k \left({\binom m k + \binom m {k - 1} }\right) \binom {s + k} n \left({-1}\right)^{m + 1 - k} | c = [[Pascal's Rule]] }} {{eqn | r = \sum_k \binom m k \binom {s + k} n \left({-1}\right)^{m + 1 - k} + \sum_k \binom m {k - 1} \binom {s + k} n \left({-1}\right)^{m + 1 - k} | c = }} {{eqn | r = -\sum_k \binom m k \binom {s + k} n \left({-1}\right)^{m - k} + \sum_k \binom m {k - 1} \binom {s + k} n \left({-1}\right)^{m + 1 - k} | c = }} {{eqn | r = -\binom s {n - m} + \sum_k \binom m {k - 1} \binom {s + k} n \left({-1}\right)^{m + 1 - k} | c = [[Sum over k of r Choose k by s+k Choose n by -1^r-k/Proof 1#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = -\binom s {n - m} + \sum_k \binom m {k - 1} \left({\binom {s + k - 1} n + \binom {s + k - 1} {n - 1} }\right) \left({-1}\right)^{m + 1 - k} | c = [[Pascal's Rule]] }} {{eqn | r = -\binom s {n - m} + \sum_k \binom m {k - 1} \binom {s + k - 1} n \left({-1}\right)^{m - \left({k - 1}\right)} | c = }} {{eqn | o = | r = + \sum_k \binom m {k - 1} \binom {s + k - 1} {n - 1} \left({-1}\right)^{m - \left({k - 1}\right)} | c = }} {{eqn | r = -\binom s {n - m} + \sum_k \binom m k \binom {s + k} n \left({-1}\right)^{m - k} + \sum_k \binom m k \binom {s + k} {n - 1} \left({-1}\right)^{m - k} | c = [[Translation of Index Variable of Summation]] }} {{eqn | r = -\binom s {n - m} + \binom s {n - m} + \binom s {n - 1 - m} | c = [[Sum over k of r Choose k by s+k Choose n by -1^r-k/Proof 1#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \binom s {n - \left({m + 1}\right)} | c = simplifying }} {{end-eqn}} So $P \left({m}\right) \implies P \left({m + 1}\right)$ and the result follows by the [[Principle of Mathematical Induction]]. Therefore: :$\displaystyle \sum_k \binom r k \binom {s + k} n \left({-1}\right)^{r - k} = \binom s {n - r}$ for all $s \in \R, r \in \Z_{\ge 0}, n \in \Z$. {{qed}}	0
Apply [[Definition:Principle of Mathematical Induction|mathematical induction]] on $n$. Let $\map P n$ be the statement that {{begin-eqn}} {{eqn | l = \map G z | o = \equiv | r = \displaystyle \sum_{m \mathop = 0}^{n+1} \map {e_m} { \set {x_1,\ldots,x_n} } z^{m} }} {{eqn | r = \displaystyle \prod_{k \mathop = 1}^n \paren {1+x_kz} }} {{end-eqn}} [[Definition:Principle of Mathematical Induction/Basis for the Induction|Basis for the induction]]: Set $U = \set {x_1}$ for $n=1$. Expand the formal series: {{begin-eqn}} {{eqn | l = \map G z | r = \map {e_0} U + \map {e_1} {U} z + \displaystyle\sum_{m \mathop = 2}^\infty \map {e_m} {U} z^m }} {{eqn | ll = \leadsto | r = \map {e_0} U + \map {e_1} {U} z | c = The summation has all zero terms. }} {{eqn | ll = \leadsto | r = 1 + x_1 z }} {{end-eqn}} Then $\map P 1$ holds. [[Definition:Principle of Mathematical Induction/Induction Step|Induction step]]: Assume $\map P n$ holds. Let's prove $\map P {n+1}$ holds. The induction step uses a recursion relation: {{begin-eqn}} {{eqn | n = 1 | l = e_m \paren { \set {x_1,\ldots,x_n,x_{n+1} } } | r = x_{n+1} e_{m-1} \paren { \set {x_1,\ldots,x_n} } + e_m \paren { \set {x_1,\ldots,x_n} } | c = [[Elementary Symmetric Function/Examples/Recursion]] }} {{end-eqn}} Let $\map G z$ be defined by statement $\map P n$. Let $\map {G^*} z$ be defined by statement $\map P {n+1}$. Then: {{begin-eqn}} {{eqn | l = \map {G^*} z | r = \displaystyle \prod_{k \mathop = 1}^{n+1} \paren {1+x_kz} }} {{eqn | ll = \leadsto | r = \map G z \paren { 1 + x_{n+1} z } }} {{eqn | ll = \leadsto | r = \map G z + x_{n+1} z \map G z }} {{eqn | ll = \leadsto | r = \sum_{m \mathop = 0}^n \map {e_m} {U} z^m + \sum_{m \mathop = 1}^{n+1} x_{n+1} \map {e_{m-1} } {U} z^{m} | c = use hypothesis $\map P n$ }} {{eqn | ll = \leadsto | r = \map {e_0} {U} + \sum_{m \mathop = 1}^{n+1} \paren { \map {e_m} {U} + x_{n+1} \map {e_{m-1} } {U} } z^{m} | c = because $\map {e_{n+1} } {U} = 0$ }} {{eqn | ll = \leadsto | r = \map {e_0} {U} + \sum_{m \mathop = 1}^{n+1} \map {e_m} {\set {x_1,\ldots,x_n,x_{n+1} } } z^{m} | c = by recursion relation (1) }} {{eqn | ll = \leadsto | r = \sum_{m \mathop = 0}^{n+1} \map {e_m} {\set {x_1,\ldots,x_n,x_{n+1} } } z^{m} | c = because $\map {e_0} {X} = 1$ for all sets $X$ }} {{end-eqn}} Then $\map P {n+1}$ holds, completing the induction. {{qed}}	0
Clearly $\map \len 0 = 0$. For $n > 0$, we have: :$\displaystyle \map \len n = \sum_{y \mathop = 1}^n \map {\operatorname {div} } {n, \map p y}$ where: :$\map {\operatorname {div} } {n, m}$ is defined as: ::$\map {\operatorname {div} } {n, y} = \begin{cases} 1 & : y \divides n \\ 0 & : y \nmid n \end{cases}$ :$\map p y$ is the $y$th [[Definition:Prime Number|prime number]]. Let $g: \N^2 \to \N$ be the [[Definition:Function|function]] defined by: :$\displaystyle \map g {n, z} = \begin{cases} 0 & : z = 0 \\ \displaystyle \sum_{y \mathop = 1}^z \map {\operatorname {div} } {n, \map p y} & : z > 0 \end{cases}$ We have that: :[[Divisor Relation is Primitive Recursive|$\operatorname{div}$ is primitive recursive]] :[[Prime Enumeration Function is Primitive Recursive|$p: \N \to \N$ is primitive recursive]] :[[Bounded Summation is Primitive Recursive]]. So it follows that $g$ is also [[Definition:Primitive Recursive Function|primitive recursive]]. Finally, as $\map \len n = \map g {n, n}$ it follows that $\len$ is [[Definition:Primitive Recursive Function|primitive recursive]]. {{qed}} [[Category:Primitive Recursive Functions]] b2iqin2w62d35didrb6ccj0hijnve70	0
Let $S_n$ denote the [[Definition:Symmetric Group on n Letters|symmetric group on $n$ letters]]. Every [[Definition:Element|element]] of $S_n$ can be expressed as a product of [[Definition:Transposition|transpositions]].	0
Let $x, y \in \N_m$. By the [[Division Theorem]]: {{begin-eqn}} {{eqn | ll= \exists q, r \in \Z: | l = x + y | r = m q + r | c = for $0 \le r < m$ }} {{eqn | ll= \exists p, s \in \Z: | l = x y | r = m p + s | c = for $0 \le s < m$ }} {{end-eqn}} Then $x +_m y = r$ and $x \times_m y = s$, so: {{begin-eqn}} {{eqn | l = q_m \left({x +_m y}\right) | r = q_m \left({r}\right) | c = }} {{eqn | r = q_m \left({m q}\right) + q_m \left({r}\right) | c = }} {{eqn | r = q_m \left({m q + r}\right) | c = }} {{eqn | r = q_m \left({x + y}\right) | c = }} {{eqn | r = q_m \left({x}\right) + q_m \left({y}\right) | c = }} {{end-eqn}} and similarly $q_m \left({x \times_m y}\right) = q_m \left({x y}\right) = q_m \left({x}\right) q_m \left({y}\right)$. So the restriction of $q_m$ to $\N_m$ is a homomorphism from $\left({\N_m, +_m, \times_m}\right)$ into $\left({\Z / \left({m}\right), +_{\left({m}\right)}, \times_{\left({m}\right)}}\right)$. Let $a \in \Z$. Then $\exists q, r \in \Z: a = q m + r: 0 \le r < m$, so $q_m \left({a}\right) = q_m \left({r}\right) \in q_m \left({\N_m}\right)$. Therefore $\Z / \left({m}\right) = q_m \left({\Z}\right) = q_m \left({\N_m}\right)$. Therefore the restriction of $q_m$ to $\N_m$ is [[Definition:Surjection|surjective]]. If $0 < r < m$, then $r \notin \left({m}\right)$ and thus $q_m \left({r}\right) \ne 0$. Thus the [[Definition:Kernel of Ring Homomorphism|kernel]] of the restriction of $q_m$ to $\N_m$ contains only zero. Therefore by the [[Quotient Theorem for Group Epimorphisms]], the restriction of $q_m$ to $\N_m$ is an [[Definition:Isomorphism (Abstract Algebra)|isomorphism]] from $\N_m$ to $\Z / \left({m}\right)$. {{wtd|Reference is made throughout to $\left({\N_m, +_m, \times_m}\right)$. It needs to be shown that this is the same (at least up to isomorphism) as the [[Definition:Ring of Integers Modulo m|ring of integers modulo $m$]] $\left({\Z, +_m, \times_m}\right)$.)}}	0
We have that: :$145 = 5 \times 29$ Both $5$ and $29$ can be expressed as the [[Definition:Integer Addition|sum]] of two [[Definition:Distinct|distinct]] [[Definition:Square Number|square numbers]]: {{begin-eqn}} {{eqn | l = 5 | r = 1^2 + 2^2 }} {{eqn | l = 29 | r = 2^2 + 5^2 }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | r = \paren {1^2 + 2^2} \paren {2^2 + 5^2} | c = }} {{eqn | r = \paren {1 \times 2 + 2 \times 5}^2 + \paren {1 \times 5 - 2 \times 2}^2 | c = [[Brahmagupta-Fibonacci Identity]] }} {{eqn | r = \paren {2 + 10}^2 + \paren {5 - 4}^2 | c = }} {{eqn | r = 12^2 + 1^2 | c = }} {{eqn | r = 144 + 1 | c = }} {{eqn | r = 145 | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | r = \paren {1^2 + 2^2} \paren {2^2 + 5^2} | c = }} {{eqn | r = \paren {1 \times 2 - 2 \times 5}^2 + \paren {1 \times 5 + 2 \times 2}^2 | c = [[Brahmagupta-Fibonacci Identity/Corollary|Brahmagupta-Fibonacci Identity/Corollary]] }} {{eqn | r = \paren {2 - 10}^2 + \paren {5 + 4}^2 | c = }} {{eqn | r = \paren {8 - 2}^2 + \paren {5 + 4}^2 | c = }} {{eqn | r = 8^2 + 9^2 | c = }} {{eqn | r = 64 + 81 | c = }} {{eqn | r = 145 | c = }} {{end-eqn}} {{qed}}	0
=== Integral Index === Let $r \in \Z$. Then: {{begin-eqn}} {{eqn | l = \binom r m \binom m k | r = \frac {r^{\underline m} } {m!} \frac {m^{\underline k} } {k!} | c = }} {{eqn | r = \frac {r! m!} {m! \left({r - m}\right)! k! \left({m - k}\right)!} | c = }} {{eqn | r = \frac {r! \left({r - k}\right)!} {k! \left({r - k}\right)! \left({m - k}\right)! \left({r - m}\right)!} | c = }} {{eqn | r = \binom r k \binom {r - k} {m - k} | c = }} {{end-eqn}} {{qed|lemma}} === Real Index === Both sides of the above equation are [[Definition:Polynomial|polynomials]] in $r$. Since these polynomials agree for all $r \in \Z$, they must agree for all $r \in \R$. {{qed}}	0
:$\forall x \in \R: \left \lceil {x - 1} \right \rceil \le x < \left \lceil {x} \right \rceil$	0
:$\ceiling {-1 \cdotp 1} = -1$	0
Let $m, n \in \N$ be [[Definition:Natural Numbers|natural numbers]]. Let us define the [[Definition:Function|function]] $\operatorname{rem}: \N^2 \to \N$: :$\map \rem {n, m} = \begin{cases} \text{the remainder when } n \text{ is divided by } m & : m \ne 0 \\ 0 & : m = 0 \end{cases}$ where the $\text{remainder}$ is as defined in the [[Division Theorem]]: :If $n = m q + r$, where $0 \le r < m$, then $r$ is the [[Definition:Remainder|remainder]]. Then $\rem$ is [[Definition:Primitive Recursive Function|primitive recursive]].	0
A [[Definition:Linear Code|linear code]] $C$ can be obtained from $G$ by: :taking the [[Definition:Set|set]] $U$ of all [[Definition:Finite Sequence|sequences]] of [[Definition:Length of Sequence|length]] $k$ over $\Z_p$ and expressing them as $1 \times k$ [[Definition:Matrix|matrices]] :forming all possible [[Definition:Matrix Product (Conventional)|matrix products]] $u G$ for all $u \in U$.	0
Let $\map G z$ be the [[Definition:Generating Function|generating function]] for the [[Definition:Sequence|sequence]] $\sequence {a_n}$. Then: {{begin-eqn}} {{eqn | l = \frac \d {\d z} \map G z | r = \sum_{k \mathop \ge 0} \left({k + 1}\right) a_{k + 1} z^k | c = }} {{eqn | r = a_1 + 2 a_2 z + 3 a_3 z^3 + \cdots | c = }} {{end-eqn}}	0
Let $n, k \in \Z$ such that $n \ge k \ge 0$. Then: :$\dbinom n k \ge \left({\dfrac {\left({n - k}\right) e} k}\right)^k \dfrac 1 {e k}$ where $\dbinom n k$ denotes a [[Definition:Binomial Coefficient|binomial coefficient]].	0
For ease of analysis, let us assume that: * Each of $P$ and $Q$ have already had the appropriate [[Clear Registers Program]] $Z \left({2, \rho \left({P}\right)}\right)$ and $Z \left({2, \rho \left({Q}\right)}\right)$ appended to them; * Each of $P$ and $Q$ have already had the appropriate amendments made to their [[Definition:Unlimited Register Machine#Termination|exit jumps]] so as to lead to the first line of the appropriate [[Clear Registers Program]]. (Note that if the latter is a [[Definition:Unlimited Register Machine#Null Program|null URM program]], this will be the line immediately following the end of the program). Thus we can discuss $P$ and $Q$ without reference to these technical details. When [[Composition of One-Variable URM Computable Functions#Concatenation of two URM Programs|concatenating programs]], the only amendments made to the programs themselves are to the contents of the <tt>Jump</tt>. So it is clear that the number, type and order of the [[Definition:Unlimited Register Machine#Basic Instructions|basic instructions]] of both programs are the same. All we have to consider are the amendments to the <tt>Jump</tt>s. Suppose the [[Definition:Unlimited Register Machine#Length of Program|lengths]] of $P$ and $Q$ are as follows: * $\lambda \left({P}\right) = t$; * $\lambda \left({Q}\right) = s$. First we look at the <tt>Jump</tt>s of $R$. When we form $\left({Q * R}\right)$, every <tt>Jump</tt> of $R$ of the form $J \left({m, n, q}\right)$ is replaced by $J \left({m, n, q + s}\right)$. Then when forming $P * \left({Q * R}\right)$ the <tt>Jump</tt> $J \left({m, n, q + s}\right)$ is replaced by $J \left({m, n, q + s + t}\right)$. On the other hand, when we form $\left({P * Q}\right) * R$, every <tt>Jump</tt> of $R$ of the form $J \left({m, n, q}\right)$ is replaced directly by $J \left({m, n, q + \left({s + t}\right)}\right)$. Thus the <tt>Jump</tt>s of $R$ have been replaced by the same instructions in each of $\left({P * Q}\right) * R$ and $P * \left({Q * R}\right)$. Now we look at the <tt>Jump</tt>s of $Q$. When forming $\left({Q * R}\right)$, we have already ensured that every <tt>Jump</tt> of $Q$ of the form $J \left({m, n, q}\right)$ where $q > s$ has been replaced by $J \left({m, n, s + 1}\right)$. So when forming $P * \left({Q * R}\right)$ the <tt>Jump</tt> $J \left({m, n, s + 1}\right)$ is replaced by $J \left({m, n, s + t + 1}\right)$. On the other hand, when forming $\left({P * Q}\right)$, every <tt>Jump</tt> of $Q$ of the form $J \left({m, n, q}\right)$ is replaced by $J \left({m, n, q + t}\right)$. We have that $\left({P * Q}\right)$ has $s + t$ instructions. When $q > s$, we take account of the fact that $q + t > s + t$. So in $\left({P * Q}\right) * R$, <tt>Jump</tt>s of the form $J \left({m, n, s + 1}\right)$ are replaced by $J \left({m, n, s + t + 1}\right)$. This agrees with the corresponding <tt>Jump</tt>s in $P * \left({Q * R}\right)$. Similarly, we can show that all other <tt>Jump</tt>s of $Q$, and all <tt>Jump</tt>s of $P$, are amended in exactly the same way in each of $\left({P * Q}\right) * R$ and $P * \left({Q * R}\right)$. Hence the result. {{qed}} [[Category:URM Programs]] 1pn5stq6phjehkuuq9jwon8qf6t8qeh	0
Let $n, k \in \Z_{\ge 0}$ such that $k > n$. {{:Stirling Number of the Second Kind of Number with Greater}}	0
Let $n \in \Z_{\ge 0}$ be a [[Definition:Positive Integer|positive integer]]. Then: :$\displaystyle \sum_k \paren {-1}^k {n \brack k} = \delta_{n 0} - \delta_{n 1}$ where: :$\displaystyle {n \brack k}$ denotes an [[Definition:Unsigned Stirling Numbers of the First Kind|unsigned Stirling number of the first kind]] :$\delta_{n 0}$ denotes the [[Definition:Kronecker Delta|Kronecker delta]].	0
{{begin-eqn}} {{eqn | l = \sum_k \left({-1}\right)^{n - k} \left[{n \atop k}\right] x^k | r = x^{\underline n} | c = {{Defof|Unsigned Stirling Numbers of the First Kind}} }} {{eqn | ll= \leadsto | l = \sum_k \left({-1}\right)^{n - k} \left[{n \atop k}\right] \left({-x}\right)^k | r = \left({-x}\right)^{\underline n} | c = putting $-x$ for $x$ }} {{eqn | ll= \leadsto | l = \left({-1}\right)^n \sum_k \left[{n \atop k}\right] x^k | r = \left({-x}\right)^{\underline n} | c = }} {{eqn | ll= \leadsto | l = \sum_k \left[{n \atop k}\right] x^k | r = \left({-1}\right)^n \left({-x}\right)^{\underline n} | c = }} {{eqn | r = x^{\overline n} | c = [[Rising Factorial in terms of Falling Factorial of Negative]] }} {{end-eqn}} {{qed}}	0
The case where $n = 1$ can be taken separately. From [[Binomial Coefficient with Zero]]: :$\dbinom 1 0 = 1$ demonstrating that the result holds for $n = 1$. Let $n \in \N: n > 1$. From the [[Definition:Binomial Coefficient|definition of binomial coefficients]]: :$\dbinom n {n - 1} = \dfrac {n!} {\left({n - 1}\right)! \left({n - \left({n - 1}\right)}\right)!} = \dfrac {n!} {\left({n - 1}\right)! \ 1!}$ the result following directly from the definition of the [[Definition:Factorial|factorial]]. {{qed}}	0
For any [[Definition:Real Number|real number]] $x: -1 \le x \le 1$: :$\arcsin x = \dfrac 1 i \map \ln {i x + \sqrt {1 - x^2} }$ where $\arcsin x$ is the [[Definition:Arcsine|arcsine]] and $i^2 = -1$.	0
:[[File:Euclid-III-5.png|250px]] Let $ABC$ and $BDCG$ be [[Definition:Circle|circles]] which cut one another at $B$ and $C$. Suppose they had the same [[Definition:Center of Circle|center]] $E$. Join $EC$ and let $EG$ be drawn at random through $F$. As $E$ is the [[Definition:Center of Circle|center]] of $ABC$, by {{EuclidDefLink|I|15|Circle}}, we have that $EC = EF$. Similarly, as $E$ is also the [[Definition:Center of Circle|center]] of $BDCG$, we have that $EC = EG$. But they are clearly unequal by the method of construction. So from this [[Proof by Contradiction|contradiction]], the two [[Definition:Circle|circles]] can not have the same [[Definition:Center of Circle|center]]. {{qed}} {{Euclid Note|5|III}}	0
Let $OABC$ be a [[Definition:Rhombus|rhombus]]. Then: :$(1): \quad OB$ [[Definition:Angle Bisector|bisects]] $\angle AOC$ and $\angle ABC$ :$(2): \quad AC$ [[Definition:Angle Bisector|bisects]] $\angle OAB$ and $\angle OCB$ [[File:RhombusBisectAngles.png|400px]]	0
:[[File:Euclid-XI-23b.png|400px]] Let the [[Definition:Straight Line|straight lines]] $AB$ and $LO$ be set out. Le $AB > LO$. Let the [[Definition:Semicircle|semicircle]] $ACB$ be described on $AB$. :[[File:Euclid-XI-23-Lemma.png|400px]] Using {{EuclidPropLink|book = IV|prop = 1|title = Fitting Chord Into Circle}}: :Let $AC = LO$ be fitted into the [[Definition:Semicircle|semicircle]] $ACB$. Let $BC$ be joined. From {{EuclidPropLink|book = III|prop = 31|title = Relative Sizes of Angles in Segments}}: :$\angle ACB$ is a [[Definition:Right Angle|right angle]]. Therefore from {{EuclidPropLink|book = I|prop = 47|title = Pythagoras's Theorem}}: :$AB^2 = AC^2 + CB^2$ Hence: :$AB^2 - AC^2 = CB^2$ But $AC = LO$. Therefore: :$AB^2 - LO^2 = CB^2$ So if we then cut of $OR = BC$: :$AB^2 - LO^2 = OR^2$ {{qed}} {{Euclid Note|23|XI}}	0
'''Depth''' is [[Definition:Linear Measure|linear measure]] in a [[Definition:Dimension (Geometry)|dimension]] [[Definition:Perpendicular|perpendicular]] to both [[Definition:Length (Linear Measure)|length]] and [[Definition:Breadth (Linear Measure)|breadth]]. The choice of '''depth''' is often arbitrary, although in [[Definition:Dimension (Geometry)|two-dimensional]] diagrams of [[Definition:Dimension (Geometry)|three-dimensional]] [[Definition:Geometric Figure|figures]], '''depth''' is usually imagined as being the dimension [[Definition:Perpendicular|perpendicular]] to the [[Definition:Plane|plane]] the [[Definition:Geometric Figure|figure]] is drawn in.	0
Checking in turn each of the criteria for [[Definition:Equivalence Relation|equivalence]]: === Reflexivity === Let $\triangle A$ be a [[Definition:Triangle (Geometry)|triangle]]. By definition, by [[Triangle Side-Side-Side Equality]], $\triangle A$ is trivially [[Definition:Congruence (Geometry)|congruent]] to itself. Thus $\cong$ is seen to be [[Definition:Reflexive Relation|reflexive]]. {{qed|lemma}} === Symmetry === Let $\triangle A \cong \triangle B$. Then: :all the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Side of Polygon|sides]] of $\triangle B$ :all the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle B$. It follows directly that: :all the [[Definition:Side of Polygon|sides]] of $\triangle B$ are equal to the [[Definition:Side of Polygon|sides]] of $\triangle A$ :all the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle B$ are equal to the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle A$. That is: : $\triangle B \cong \triangle A$ Thus $\cong$ is seen to be [[Definition:Symmetric Relation|symmetric]]. {{qed|lemma}} === Transitivity === Let: :$\triangle A \cong \triangle B$ :$\triangle B \cong \triangle C$ Then: :all the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Side of Polygon|sides]] of $\triangle B$ :all the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle B$. and: :all the [[Definition:Side of Polygon|sides]] of $\triangle B$ are equal to the [[Definition:Side of Polygon|sides]] of $\triangle C$ :all the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle B$ are equal to the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle C$. From [[Equality is Equivalence Relation]], it follows that: :all the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Side of Polygon|sides]] of $\triangle C$ :all the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle A$ are equal to the [[Definition:Angle|angles]] contained by the [[Definition:Side of Polygon|sides]] of $\triangle C$. That is: : $\triangle A \cong \triangle C$ Thus $\cong$ is seen to be [[Definition:Transitive Relation|transitive]]. {{qed|lemma}} $\cong$ has been shown to be [[Definition:Reflexive Relation|reflexive]], [[Definition:Symmetric Relation|symmetric]] and [[Definition:Transitive Relation|transitive]]. Hence by definition it is an [[Definition:Equivalence Relation|equivalence relation]]. {{qed}}	0
:$\sin \alpha \cos \beta = \dfrac {\map \sin {\alpha + \beta} + \map \sin {\alpha - \beta} } 2$	0
A '''perfect square dissection''' is a [[Definition:Dissection|dissection]] of an [[Definition:Integer Square|integer square]] into a number of smaller [[Definition:Integer Square|integer squares]] all of different sizes.	0
Let $\triangle ABC$ be a [[Definition:Triangle (Geometry)|triangle]]. Let $D$ be a point in the interior of $\triangle ABC$. Then there exists a [[Definition:Point|point]] $E$ such that $E$ lies on both $AD$ and $BC$.	0
=== [[Double Angle Formulas/Sine|Double Angle Formula for Sine]] === {{:Double Angle Formulas/Sine}} === [[Double Angle Formulas/Cosine|Double Angle Formula for Cosine]] === {{:Double Angle Formulas/Cosine}} === [[Double Angle Formulas/Tangent|Double Angle Formula for Tangent]] === {{:Double Angle Formulas/Tangent}}	0
:[[File:EllipseEquidistanceMajorAxis.png|400px]] By the [[Definition:Equidistance Property of Ellipse|equidistance property]] of $K$: :$d_1 + d_2 = d$ applies to all [[Definition:Point|points]] $P$ on $K$. Thus it also applies to the two [[Definition:Vertex of Ellipse|vertices]] $V_1$ and $V_2$: :$V_1 F_1 + V_1 F_2 = d$ :$V_2 F_1 + V_2 F_2 = d$ Adding: :$V_1 F_1 + V_2 F_1 + V_1 F_2 + V_2 F_2 = 2 d$ But: :$V_1 F_1 + V_2 F_1 = V_1 V_2$ :$V_1 F_2 + V_2 F_2 = V_1 V_2$ and so: :$2 V_1 V_2 = 2 d$ By definition, the [[Definition:Major Axis of Ellipse|major axis]] is $V_1 V_2$. Hence the result. {{qed}}	0
:$\displaystyle \int x^2 \cosh^{-1} \frac x a \ \mathrm d x = \begin{cases} \displaystyle \frac {x^3} 3 \cosh^{-1} \frac x a - \frac {\left({x^2 + 2 a^2}\right) \sqrt {x^2 - a^2} } 9 + C & : \cosh^{-1} \frac x a > 0 \\ \displaystyle \frac {x^3} 3 \cosh^{-1} \frac x a - \frac {\left({x^2 + 2 a^2}\right) \sqrt {x^2 - a^2} } 9 + C & : \cosh^{-1} \frac x a < 0 \end{cases}$	0
The area of a $\triangle ABC$ is given by the formula: :$(ABC) = \rho_a \left({s - a}\right) = \rho_b \left({s - b}\right) = \rho_c \left({s - c}\right) = \rho s = \sqrt {\rho_a \rho_b \rho_c \rho}$ where: :$s$ is the [[Definition:Semiperimeter|semiperimeter]] :$I$ is the [[Definition:Incenter of Triangle|incenter]] :$\rho$ is the [[Definition:Inradius of Triangle|inradius]] :$I_a, I_b, I_c$ are the [[Definition:Excenter of Triangle|excenters]] :$\rho_a, \rho_b, \rho_c$ are the [[Definition:Exradius of Triangle|exradii]] from $I_a, I_b, I_c$, respectively.	0
Let $\size x < a$. Then: {{begin-eqn}} {{eqn | l = \int \frac {\d x} {x^2 - a^2} | r = -\frac 1 a \tanh^{-1} {\frac x a} + C | c = [[Primitive of Reciprocal of a squared minus x squared/Inverse Hyperbolic Tangent Form|Primitive of $\dfrac 1 {x^2 - a^2}$: $\tanh^{-1}$ form]] }} {{eqn | r = -\frac 1 a \paren {\dfrac 1 2 \map \ln {\dfrac {a + x} {a - x} } } + C | c = [[Inverse Hyperbolic Tangent of x over a in Logarithm Form|$\tanh^{-1} \dfrac x a$ in Logarithm Form]] }} {{eqn | r = -\dfrac 1 {2 a} \map \ln {\dfrac {a + x} {a - x} } + C | c = simplifying }} {{eqn | r = \dfrac 1 {2 a} \map \ln {\dfrac {a - x} {a + x} } + C | c = [[Logarithm of Reciprocal]] }} {{end-eqn}}	0
We use the technique of [[Definition:Formation of Ordinary Differential Equation by Elimination|formation of ordinary differential equation by elimination]]. [[Definition:Differentiation|Differentiating]] $(1)$ {{WRT|Differentiation}} $x$ gives: :$2 x + 2 y \dfrac {\d y} {\d x} = 2 c$ from which: :$\dfrac {\d y} {\d x} = \dfrac {y^2 - x^2} {2 x y}$ Thus from [[Orthogonal Trajectories of One-Parameter Family of Curves]], the [[Definition:Orthogonal Trajectories|family of orthogonal trajectories]] is given by: :$\dfrac {\d y} {\d x} = \dfrac {2 x y} {x^2 - y^2}$ Let: :$\map M {x, y} = 2 x y$ :$\map N {x, y} = x^2 - y^2$ Put $t x, t y$ for $x, y$: {{begin-eqn}} {{eqn | l = \map M {t x, t y} | r = 2 t x t y | c = }} {{eqn | r = t^2 \paren {2 x y} | c = }} {{eqn | r = t^2 \, \map M {x, y} | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = \map N {t x, t y} | r = \paren {t x}^2 - \paren {t y}^2 | c = }} {{eqn | r = t^2 \map N {x^2 - y^2} | c = }} {{eqn | r = t \map N {x, y} | c = }} {{end-eqn}} Thus both $M$ and $N$ are [[Definition:Homogeneous Real Function|homogeneous functions]] of [[Definition:Degree of Homogeneous Real Function|degree]] $2$. Thus, by definition, $(1)$ is a [[Definition:Homogeneous Differential Equation|homogeneous differential equation]]. By [[Solution to Homogeneous Differential Equation]], its solution is: :$\displaystyle \ln x = \int \frac {\d z} {\map f {1, z} - z} + C$ where: :$\map f {x, y} = \dfrac {2 x y} {x^2 - y^2}$ Thus: {{begin-eqn}} {{eqn | l = \ln x | r = \int \frac {\d z} {\dfrac {2 z} {1 - z^2} - z} + C_1 | c = }} {{eqn | r = \int \frac {1 - z^2} {z \paren {1 + z^2} } \rd z + C_1 | c = }} {{eqn | r = \int \frac {\d z} {z \paren {1 + z^2} } \rd z - \int \frac z {\paren {1 + z^2} } \rd z + C_1 | c = }} {{eqn | r = \frac 1 2 \map \ln {\frac {z^2} {z^2 + 1} } - \frac 1 2 \map \ln {z^2 + 1} + C_1 | c = }} {{eqn | r = \frac 1 2 \map \ln {\frac {z^2} {\paren {z^2 + 1}^2} } + C_1 | c = }} {{eqn | ll= \leadsto | l = C_2 x^2 | r = \frac {z^2} {\paren {z^2 + 1}^2} | c = }} {{eqn | ll= \leadsto | l = C_3 x | r = \frac {y / x} {\paren {y / x}^2 + 1} | c = }} {{eqn | ll= \leadsto | l = x^2 + y^2 | r = 2 C y | c = }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = I_n | r = \int \cos^n x \rd x }} {{eqn | r = \dfrac {\cos^{n - 1} x \sin x} n + \dfrac {n - 1} n I_{n-2} | c = [[Reduction Formula for Integral of Power of Cosine]] }} {{eqn | l = I_0 | r = \int \left({\cos x}\right)^0 \rd x | c = }} {{eqn | r = \int \rd x | c = }} {{eqn | r = x + C | c = [[Primitive of Constant]] }} {{eqn | ll= \leadsto | l = I_2 | r = \frac {\cos x \sin x} 2 + \frac x 2 + \frac C 2 | c = setting $n = 2$ }} {{eqn | r = \frac {\sin 2 x} 4 + \frac x 2 + C' | c = [[Double Angle Formula for Sine]] }} {{end-eqn}} {{qed}}	0
:[[File:EllipseFocus MajorMinorAxes.png|500px]] Let the [[Definition:Focus of Ellipse|foci]] of $K$ be $F_1$ and $F_2$. Let the [[Definition:Vertex of Ellipse|vertices]] of $K$ be $V_1$ and $V_2$. Let the [[Definition:Covertex of Ellipse|covertices]] of $K$ be $C_1$ and $C_2$. Let $P = \left({x, y}\right)$ be an arbitrary [[Definition:Point|point]] on the [[Definition:Locus|locus]] of $K$. From the [[Definition:Equidistance Property of Ellipse|equidistance property]] of $K$ we have that: :$F_1 P + F_2 P = d$ where $d$ is a [[Definition:Constant|constant]] for this particular [[Definition:Ellipse|ellipse]]. This is true for all [[Definition:Point|points]] on $K$. In particular, it holds true for $V_2$, for example. Thus: {{begin-eqn}} {{eqn | l = d | r = F_1 V_2 + F_2 V_2 | c = }} {{eqn | r = \paren {a + c} + \paren {a - c} | c = }} {{eqn | r = 2 a | c = }} {{end-eqn}} It also holds true for $C_2$: :$F_1 C_2 + F_2 C_2 = d$ Then: {{begin-eqn}} {{eqn | l = F_1 C_2^2 | r = O F_1^2 + O C_2^2 | c = [[Pythagoras's Theorem]] }} {{eqn | r = c^2 + b^2 | c = }} {{end-eqn}} and: {{begin-eqn}} {{eqn | l = F_1 C_2^2 | r = O F_1^2 + O C_2^2 | c = [[Pythagoras's Theorem]] }} {{eqn | r = c^2 + b^2 | c = }} {{end-eqn}} Thus: {{begin-eqn}} {{eqn | l = F_1 C_2 + F_2 C_2 | r = 2 \sqrt {b^2 + c^2} | c = }} {{eqn | r = 2 a | c = as $2 a = d$ }} {{eqn | ll= \leadsto | l = a | r = \sqrt {b^2 + c^2} | c = }} {{eqn | ll= \leadsto | r = a^2 | l = b^2 + c^2 | c = }} {{end-eqn}} {{qed}}	0
The equation of the [[Definition:Witch of Agnesi|Witch of Agnesi]] is given in [[Definition:Cartesian Coordinate System|cartesian coordinates]] as: :$y = \dfrac {8 a^3} {x^2 + 4 a^2}$	0
:$\forall x \in \R_{\ne 0}: \dfrac x x = 1$	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]]. Let $a, b \in G$ such that $a$ and $b$ [[Definition:Commuting Elements|commute]]. Then the following results hold:	0
Let $\left({G, \circ}\right)$ be a [[Definition:Group|group]]. Let $\iota: G \to G$ be the [[Definition:Inversion Mapping|inversion mapping]] on $G$. Then $\iota$ is indeed a [[Definition:Mapping|mapping]].	0
The [[Definition:Integer|set of integers]] $\Z$ is not [[Definition:Well-Ordered Set|well-ordered]] under the [[Definition:Usual Ordering|usual ordering]] $\le$.	0
Let $\N$ be the [[Definition:Natural Numbers|natural numbers]]. Then for all $m, n \in \N$: :$m \times n = 0 \iff m = 0 \lor n = 0$ That is, $\N$ has no [[Definition:Proper Zero Divisor|proper zero divisors]].	0
Let $P$ be the [[Definition:Strict Positivity Property|(strict) positivity property]] on $D$. Let $<$ be the [[Definition:Strict Total Ordering|(strict) total ordering]] defined on $D$ as: :$a < b \iff a \le b \land a \ne b$ Let $N$ be the [[Definition:Strict Negativity Property|strict negativity property]] on $D$. We consider all possibilities in turn. $(1): \quad a = 0_D$ or $b = 0_D$ In this case, both the {{LHS}} $\size a \times \size b$ and the {{RHS}} are equal to [[Definition:Ring Zero|zero]]. So: :$\size a \times \size b = \size {a \times b}$ $(2): \quad \map P a, \map P b$ First: {{begin-eqn}} {{eqn | l = \map P a, \map P b | o = \leadsto | r = \size a = a, \size b = b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{eqn | o = \leadsto | r = \size a \times \size b = a \times b | c = }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \map P a, \map P b | o = \leadsto | r = \map P {a \times b} | c = [[Definition:Strict Positivity Property|Strict Positivity Property: $(P \, 2)$]] }} {{eqn | o = \leadsto | r = \size {a \times b} = a \times b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{end-eqn}} So: :$\size a \times \size b = \size {a \times b}$ $(3): \quad \map P a, \map N b$ First: {{begin-eqn}} {{eqn | l = \map P a, \map N b | o = \leadsto | r = \size a = a, \size b = -b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{eqn | o = \leadsto | r = \size a \times \size b = -a \times b | c = [[Product with Ring Negative]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \map P a, \map N b | o = \leadsto | r = \map N {a \times b} | c = [[Properties of Strict Negativity|Properties of Strict Negativity: $(5)$]] }} {{eqn | o = \leadsto | r = \map P {-a \times b} | c = {{Defof|Strict Negativity Property}} }} {{eqn | o = \leadsto | r = \size {a \times b} = -a \times b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{end-eqn}} So: :$\size a \times \size b = \size {a \times b}$ Similarly $\map N a, \map P b$. $(4): \quad \map N a, \map N b$ First: {{begin-eqn}} {{eqn | l = \map N a, \map N b | o = \leadsto | r = \size a = -a, \size b = -b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{eqn | o = \leadsto | r = \size a \times \size b = a \times b | c = [[Product of Ring Negatives]] }} {{end-eqn}} Then: {{begin-eqn}} {{eqn | l = \map N a, \map N b | o = \leadsto | r = \map P {a \times b} | c = [[Properties of Strict Negativity|Properties of Strict Negativity: $(4)$]] }} {{eqn | o = \leadsto | r = \map P {a \times b} | c = {{Defof|Strict Negativity Property}} }} {{eqn | o = \leadsto | r = \size {a \times b} = a \times b | c = {{Defof|Absolute Value on Ordered Integral Domain}} }} {{end-eqn}} So: :$\size a \times \size b = \size {a \times b}$ In all cases the result holds. {{qed}}	0
Define $P$ by: {{begin-eqn}} {{eqn | l = \map P {A, K} | o = := | r = \prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \frac {1 - \map f p^{K + 1} } {1 - \map f p} | c = where $\mathbb P$ denotes the [[Definition:Set|set]] of [[Definition:Prime Number|prime numbers]] }} {{eqn | r = \prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \paren {\sum_{k \mathop = 0}^K \map f p^k} | c = [[Sum of Geometric Sequence]] }} {{eqn | r = \sum_{v \mathop \in \prod \limits_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \set {0 \,.\,.\, K} } \paren {\prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \map f p^{v_p} } | c = [[Product of Summations is Summation Over Cartesian Product of Products]] }} {{eqn | r = \sum_{v \mathop \in \prod \limits_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \set {0 \,.\,.\, K} } \map f {\prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } p^{v_p} } | c = as $f$ is [[Definition:Completely Multiplicative Function|completely multiplicative]] }} {{end-eqn}} Change the summing variable using: {{begin-eqn}} {{eqn | l = \sum_{v \mathop \in V} \map g {\map h v} | r = \sum_{w \mathop \in \set {\map h v: v \mathop \in V} } \map g w | c = where $h$ is a one to one mapping }} {{end-eqn}} The [[Fundamental Theorem of Arithmetic]] guarantees a unique factorization for each positive natural number. Therefore this function is one to one: :$\displaystyle \map h v = \prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } p^{v_p}$ Then: {{begin-eqn}} {{eqn | l = \map P {A, K} | r = \sum_{n \mathop \in \map Q {A, K} } \map f n | c = change of summing variable }} {{end-eqn}} where $\map Q {A, K}$ is defined as: :$\displaystyle \map Q {A, K} := \set {\prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } p^{-v_p} : v \in \prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \set {0 \,.\,.\, K} }$ Consider: {{begin-eqn}} {{eqn | l = W | r = \lim_{\substack {A \mathop \to \infty \\ K \mathop \to \infty} } \map Q {A, K} }} {{eqn | r = \set {\prod_{p \mathop \in \mathbb P} p^{-v_p}: v \in \prod_{p \mathop \in \mathbb P} \set {0 \,.\,.\, \infty} } | c = }} {{end-eqn}} The construction defines it as the set of all possible products of positive powers of primes. From the definition of a prime number, every positive natural number may be expressed as a prime or a product of powers of primes: :$k \in \N^+ \implies k \in W$ and also every element of W is a positive natural number: :$k \in W \implies k \in \N^+$ So $W = \N^+$. Then taking limits on $\map P {A, K}$: {{begin-eqn}} {{eqn | l = \lim_{\substack {A \mathop \to \infty \\ K \mathop \to \infty} } \map P {A, K} | r = \lim_{\substack {A \mathop \to \infty \\ K \mathop \to \infty} } \prod_{\substack {p \mathop \in \mathbb P \\ p \mathop \le A} } \frac {1 - \map f p^{K + 1} } {1 - \map f p} | c = taking limits of both sides of the definition of $\map P {A, K}$ }} {{eqn | r = \prod_{p \mathop \in \mathbb P} \frac 1 {1 - \map f p} | c = $\map f p^{K + 1} \to 0$, because $\displaystyle \sum_{n \mathop = 1}^{\infty} \map f n$ is [[Definition:Convergence|convergent]] }} {{eqn | r = \lim_{\substack {A \mathop \to \infty \\ K \mathop \to \infty} } \sum_{n \mathop \in \map Q {A, K} } \map f n | c = from the expression for $\map P {A, K}$ }} {{eqn | r = \sum_{n \mathop \in \N^+} \map f n | c = substituting for $\N^+$: order of summation is not defined }} {{eqn | r = \sum_{n \mathop = 1}^\infty \map f n | c = [[Definition:Absolutely Convergent Series|absolutely convergent]], so the order does not alter the limit }} {{end-eqn}} {{qed}}	0
We have that $35 = 5 \times 7$. Then we have that $5$ and $7$ are [[Definition:Prime Number|primes]] such that $5 < 7$ and $5$ does not [[Definition:Divisor of Integer|divide]] $7 - 1$. Thus [[Cyclic Groups of Order p q|Cyclic Groups of Order $p q$]] can be applied. {{Qed}}	0
The forward implication is immediate from $\preceq$ being [[Definition:Relation Compatible with Operation|compatible]] with $\circ$: :$\forall m, n, p \in S: m \preceq n \implies m \circ p \preceq n \circ p$ Conversely, suppose that $m \circ p \preceq n \circ p$. Suppose that $n \prec m$. Then as $\preceq$ is [[Definition:Relation Compatible with Operation|compatible]] with $\circ$: :$n \circ p \preceq m \circ p$ Since $\preceq$ is an [[Definition:Ordering|ordering]], this implies: :$n \circ p = m \circ p$ By [[Definition:Naturally Ordered Semigroup Axioms|axiom $(\text {NO} 2)$]], it follows that: :$n = m$ contradicting our assumption that $n \prec m$. Hence, since $\preceq$ is a [[Definition:Total Ordering|total ordering]]: :$m \preceq n$ as desired. {{qed}}	0
Let $c a \equiv c b \pmod n$. Then we have that $c a - c b = k n$ for some $k \in \Z$ by definition of [[Definition:Congruence (Number Theory)|congruence]]. Now $d = \gcd \set {c, n}$, so from [[Integers Divided by GCD are Coprime]] we have: :$\exists r, s \in Z: r \perp s: c = d r, n = d s$ So we substitute for $c$ and $n$ in $c a - c b = k n$: :$d r a - d r b = k d s$ which leads us to: :$r \paren {a - b} = k s$ So $s \divides \paren {a - b}$ and as $r \perp s$, from [[Euclid's Lemma]] $s \divides \paren {a - b}$. So $a \equiv b \pmod s$ where $s = \dfrac n d$. Now suppose $a \equiv b \pmod {n / d}$ where $d = \gcd \set {c, n}$. Then: :$\exists k \in \Z: a - b = k \dfrac n d$ Hence: :$c a - c b = \dfrac {k c} d n$ As $d = \gcd \set {c, n}$ we have $d \divides c$ and so $\dfrac c d \in \Z$. So: :$c a \equiv c b \pmod n$ {{qed}} [[Category:Modulo Multiplication]] [[Category:Modulo Arithmetic]] [[Category:Cancellability of Congruences]] pky8rvdw5ydnippe4xss52lr97tegog	0
=== Necessary Condition === Follows directly from [[Topological Group is Hausdorff iff Identity is Closed]]. {{explain|How?}} {{qed|lemma}} === Sufficient Condition === Let $G$ be a [[Definition:Hausdorff Space|Hausdorff]] [[Definition:Topological Group|topological group]]. Let $H \leq G$ be a [[Definition:Closed Set (Topology)|closed]] [[Definition:Discrete Subgroup|discrete subgroup]]. Let $e$ be the [[Definition:Identity Element|identity]] of $G$. By [[Set in Discrete Topology is Clopen]], $\left\{{e}\right\}$ is [[Definition:Closed Set (Topology)|closed]] in $H$. By [[Closed Set in Topological Subspace/Corollary|Closed Set in Topological Subspace: Corollary]], $\left\{{e}\right\}$ is [[Definition:Closed Set (Topology)|closed]] in $G$. By [[Topological Group is Hausdorff iff Identity is Closed]], $G$ is [[Definition:Hausdorff Space|Hausdorff]]. {{qed}}	0
Let $n \in \N_{>0}$ be a non-zero [[Definition:Natural Number|natural number]]. Let $\N^*_n$ denote the [[Definition:Initial Segment of One-Based Natural Numbers|Initial segment]] $\set {1, 2, \ldots, n}$ of the non-zero [[Definition:Natural Number|natural numbers]]. Then every [[Definition:Non-Empty Set|non-empty]] [[Definition:Subset|subset]] of $\N^*_n$ has a [[Definition:Greatest Element|greatest element]].	0
Let $x, y \in \Z$ such that $x \le y$. Let $x$ and $y$ be defined as from the [[Definition:Integer/Formal Definition|formal definition of integers]]: :$x = \eqclass {x_1, x_2} {}$ and $y = \eqclass {y_1, y_2} {}$ where $x_1, x_2, y_1, y_2 \in \N$. === $(1)$ implies $(2)$ === Let $\le$ be an [[Definition:Ordering on Integers/Definition 1|ordering on integers by definition $1$]]. Then by definition: :$y - x$ is [[Definition:Non-Negative Integer|non-negative]] That is: :$\map \PP {y - x}$ where $\PP$ is the [[Definition:Positivity Property|positivity property]]. Thus: :$\eqclass {y_1, y_2} {} - \eqclass {x_1, x_2} {}$ is [[Definition:Non-Negative Integer|non-negative]] By definition of [[Definition:Integer Subtraction|integer subtraction]]: :$\eqclass {y_1, y_2} {} + \eqclass {x_2, x_1} {}$ is [[Definition:Non-Negative Integer|non-negative]] and by the [[Definition:Integer/Formal Definition|formal definition of integers]]: :$\eqclass {y_1 + x_2, y_2 + x_1} {}$ is [[Definition:Non-Negative Integer|non-negative]] We have that $y_1 + x_2$ and $y_2 + x_1$ are [[Definition:Natural Number|natural numbers]]. Thus by definition of [[Definition:Ordering on Natural Numbers|natural number ordering]]: :$y_1 + x_2 \ge y_2 + x_1$ Thus $\le$ is an [[Definition:Ordering on Integers/Definition 2|ordering on integers by definition $2$]]. {{qed|lemma}} === $(2)$ implies $(1)$ === Let $\le$ be an [[Definition:Ordering on Integers/Definition 2|ordering on integers by definition $2$]]. Then by definition: :$x_1 + y_2 \le x_2 + y_1$ That is: :$x_2 + y_1 \ge x_1 + y_2$ Hence by the [[Definition:Integer/Formal Definition|formal definition of integers]]: :$\eqclass {y_1 + x_2, y_2 + x_1} {}$ is [[Definition:Non-Negative Integer|non-negative]] By definition of [[Definition:Integer Addition|integer addition]]: :$\eqclass {y_1, y_2} {} + \eqclass {x_2, x_1} {}$ is [[Definition:Non-Negative Integer|non-negative]] By definition of [[Definition:Integer Subtraction|integer subtraction]]: :$\eqclass {y_1, y_2} {} - \eqclass {x_1, x_2} {}$ is [[Definition:Non-Negative Integer|non-negative]] That is: :$\map \PP {y - x}$ where $\PP$ is the [[Definition:Positivity Property|positivity property]]. Thus $\le$ is an [[Definition:Ordering on Integers/Definition 1|ordering on integers by definition $1$]]. {{qed}} [[Category:Orderings on Integers]] 02kdwdfrxraur2rcdib88vt3lwow89j	0
From the [[Definition:Field Axioms|field axioms]]: {{begin-axiom}} {{axiom | n = \text M 0 | lc= [[Definition:Closed Algebraic Structure|Closure]] under [[Definition:Field Product|product]] | q = \forall x, y \in F | m = x \circ y \in F }} {{axiom | n = \text M 1 | lc= [[Definition:Associative|Associativity]] of [[Definition:Field Product|product]] | q = \forall x, y, z \in F | m = \paren {x \circ y} \circ z = x \circ \paren {y \circ z} }} {{axiom | n = \text M 2 | lc= [[Definition:Commutative Operation|Commutativity]] of [[Definition:Field Product|product]] | q = \forall x, y \in F | m = x \circ y = y \circ x }} {{axiom | n = \text M 3 | lc= [[Definition:Identity Element|Identity element]] for [[Definition:Field Product|product]] | q = \exists 1_F \in F, 1_F \ne 0_F: \forall x \in F | m = x \circ 1_F = x = 1_F \circ x | rc= $1_F$ is called the [[Definition:Unity of Field|unity]] }} {{axiom | n = \text M 4 | lc= [[Definition:Inverse Element|Inverse elements]] for [[Definition:Field Product|product]] | q = \forall x \in F^*: \exists x^{-1} \in F^* | m = x \circ x^{-1} = 1_F = x^{-1} \circ x }} {{end-axiom}} Hence the result. {{qed}}	0
Let $x \equiv^l y \pmod H$ denote that $x$ is [[Definition:Left Congruence Modulo Subgroup|left congruent modulo $H$]] to $y$. Then the following statements are [[Definition:Logical Equivalence|equivalent]]: {{begin-eqn}} {{eqn | n = 1 | l = x | o = \equiv^l | r = y \pmod H }} {{eqn | n = 2 | l = x^{-1} y | o = \in | r = H }} {{eqn | n = 3 | l = \exists h \in H: x^{-1} y | r = h }} {{eqn | n = 4 | l = \exists h \in H: y | r = x h }} {{end-eqn}}	0
Let $x \in a H \cap b K$. Then: {{begin-eqn}} {{eqn | l = x | o = \in | r = a H }} {{eqn | ll= \leadsto | l = x H | r = a H | c = [[Left Cosets are Equal iff Element in Other Left Coset]] }} {{end-eqn}} and similarly: {{begin-eqn}} {{eqn | l = x | o = \in | r = b K }} {{eqn | ll= \leadsto | l = x K | r = b K | c = [[Left Cosets are Equal iff Element in Other Left Coset]] }} {{end-eqn}} Hence: {{begin-eqn}} {{eqn | l = a H \cap b K | r = x H \cap x K }} {{eqn | r = x \paren {H \cap K} | c = [[Product of Subset with Intersection/Corollary|Corollary to Product of Subset with Intersection]] }} {{end-eqn}} Hence the result by definition of [[Definition:Left Coset|left coset]]. {{qed}}	0
Consider the numbers of $\set {1, 2, \ldots, n}$ which are not [[Definition:Integer Multiple|multiples]] of $p$. There are $\floor {\dfrac n p}$ complete sets of $p - 1$ such consecutive elements of $\set {1, 2, \ldots, n}$. Each one of these has a product which is [[Definition:Congruence Modulo Integer|congruent]] to $-1 \pmod p$ by [[Wilson's Theorem]]. There are also $a_0$ left over which are [[Definition:Congruence Modulo Integer|congruent]] to $a_0! \pmod p$. Thus: :the contributions of the [[Definition:Divisor of Integer|divisors]] which are not [[Definition:Integer Multiple|multiples]] of $p$ is $\paren {-1}^{\floor {n / p} } a_0!$ :the contributions of the [[Definition:Divisor of Integer|divisors]] which are [[Definition:Integer Multiple|multiples]] of $p$ is the same as the contribution in $\floor {\dfrac n p}!$ Thus the argument can be repeated on $\floor {\dfrac n p}!$ until the formula is complete. {{qed}}	0
Let $\struct {G, \circ}$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity element]] is $e$. Then: :$\forall a, b \in G: a \circ b^{-1} = e \iff a = b$	0
{{BeginTableau|p \land \neg q \vdash \neg \paren {p \implies q} }} {{Premise|1|p \land \neg q}} {{Assumption|2|p \implies q| Assume the opposite of what is to be proved ...}} {{Simplification|3|1|p|1|1}} {{Simplification|4|1|\neg q|1|2}} {{ModusPonens|5|1, 2|q|2|3}} {{NonContradiction|6|1, 2|5|4|... and demonstrate a contradiction}} {{Contradiction|7|1|\neg \paren {p \implies q}|2|6}} {{EndTableau}} {{qed}} [[Category:Conjunction with Negative Equivalent to Negation of Implication]] 3llyeaaoyezyiibkb14p0clorrmu0zz	0
{{BeginTableau|\left({p \lor q}\right) \land \left({r \lor s}\right) \vdash \left({p \lor r}\right) \lor \left({q \land s}\right)}} {{Premise|1|\left({p \lor q}\right) \land \left({r \lor s}\right)}} {{SequentIntro|2|1|\left({p \land \left({r \lor s}\right)}\right) \lor \left({q \land \left({r \lor s}\right)}\right)|1|[[Rule of Distribution/Conjunction Distributes over Disjunction/Right Distributive/Formulation 1|Conjunction Distributes over Disjunction]]}} {{TheoremIntro|3|p \land \left({r \lor s}\right) \implies p|[[Rule of Simplification/Sequent Form/Formulation 2|Simplification]]}} {{TheoremIntro|4|q \land \left({r \lor s}\right) \implies \left({q \land r}\right) \lor \left({q \land s}\right)|[[Rule of Distribution/Conjunction Distributes over Disjunction/Left Distributive/Formulation 2/Forward Implication|Conjunction Distributes over Disjunction]]}} {{TheoremIntro|5|q \land r \implies r|[[Rule of Simplification/Sequent Form/Formulation 2|Simplification]]}} {{TheoremIntro|6|q \land s \implies q \land s|[[Law of Identity/Formulation 2]]}} {{SequentIntro|7||\left({q \land r}\right) \lor \left({q \land s}\right) \implies r \lor \left({q \land s}\right)|5,6|[[Constructive Dilemma/Formulation 1|Constructive Dilemma]]}} {{SequentIntro|8||q \land \left({r \lor s}\right) \implies r \lor \left({q \land s}\right)|4,7|[[Hypothetical Syllogism/Formulation 1|Hypothetical Syllogism]]}} {{SequentIntro|9||\left({p \lor q}\right) \land \left({r \lor s}\right) \implies p \lor \left({r \lor \left({q \land s}\right)}\right)|3,8|[[Constructive Dilemma/Formulation 1|Constructive Dilemma]]}} {{ModusPonens|10|1|p \lor \left({r \lor \left({q \land s}\right)}\right)|9|1}} {{SequentIntro|11|1|\left({p \lor r}\right) \lor \left({q \land s}\right)|10|[[Rule of Association/Disjunction/Formulation 1|Rule of Association]]}} {{EndTableau}} {{qed}} [[Category:Conjunction]] [[Category:Disjunction]] 75311jdpussf09y7sbnft5t38dm2bjf	0
We apply the [[Method of Truth Tables]] to the proposition. As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. $\begin{array}{|cccc||cccc|} \hline \neg & p & \implies & q & \neg & q & \implies & p \\ \hline T & F & T & F & T & F & T & F \\ T & F & T & T & F & T & T & F \\ F & T & F & F & T & F & F & T \\ F & T & T & T & F & T & T & T \\ \hline \end{array}$ {{qed}}	0
Let $T$ be a [[Definition:Theory (Logic)|complete $\mathcal{L}$-theory]]. Let $\mathfrak{C}$ be a [[Definition:Monster Model|monster model]] for $T$. Let $A\subseteq B$ be [[Definition:Subset|subsets]] of the universe of $\mathfrak{C}$. Let $\pi(\bar x)$ be an [[Definition:Type|$n$-type]] over $B$. $\pi$ [[Definition:Fork|forks]] over $A$ if and only if a finite subset of $\pi$ forks over $A$.	0
Apply the [[Method of Truth Tables]]: :$\begin{array}{|ccc||ccc|} \hline p & \downarrow & q & q & \downarrow & p \\ \hline F & T & F & F & T & F \\ F & F & T & T & F & F \\ T & F & F & F & F & T \\ T & F & T & T & F & T \\ \hline \end{array}$ As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. {{qed}}	0
{{BeginTableau|\vdash \left({\left({p \implies q}\right) \land \left({r \implies s}\right)}\right) \implies \left({\left({p \land r}\right) \implies \left({q \land s}\right)}\right)}} {{Assumption|1|\left({\left({p \implies q}\right) \land \left({r \implies s}\right)}\right)}} {{SequentIntro|2|1|\left({p \land r}\right) \implies \left({q \land s}\right)|1|[[Praeclarum Theorema/Formulation 1|Praeclarum Theorema: Formulation 1]]}} {{Implication|3|1|\left({\left({p \implies q}\right) \land \left({r \implies s}\right)}\right) \implies \left({\left({p \land r}\right) \implies \left({q \land s}\right)}\right)|1|2}} {{EndTableau}} {{qed}}	0
: $\vdash \left({q \implies \neg p}\right) \implies \left({p \implies \neg q}\right)$	0
{{BeginTableau|\left({\left({p \lor q}\right) \land \left({p \implies r}\right) \land \left({q \implies r}\right)}\right) \implies r}} {{TheoremIntro|1|\left({\left({p \lor r}\right) \land \left({p \implies q}\right) \land \left({r \implies s}\right)}\right) \implies \left({q \lor s}\right)|[[Constructive Dilemma/Formulation 2|Constructive Dilemma: Formulation 2]]}} {{Substitution|2||\left({\left({p \lor q}\right) \land \left({p \implies r}\right) \land \left({q \implies r}\right)}\right) \implies \left({r \lor r}\right)|1|r|q|q|r|s|r}} {{Assumption|3|\left({p \lor q}\right) \land \left({p \implies r}\right) \land \left({q \implies r}\right)}} {{ModusPonens|4|3|r \lor r|2|3}} {{Idempotence|5|3|r|4|Disjunction}} {{Implication|6||\left({\left({p \lor q}\right) \land \left({p \implies r}\right) \land \left({q \implies r}\right)}\right) \implies r|3|5}} {{EndTableau}} {{qed}}	0
[[Definition:Hilbert Proof System/Instance 2|Instance 2]] of the [[Definition:Hilbert Proof System|Hilbert proof systems]] is a [[Definition:Complete Proof System|complete proof system]] for [[Definition:Boolean Interpretation|boolean interpretations]]. That is, for every [[Definition:WFF of Propositional Logic|WFF]] $\mathbf A$: :$\models_{\mathrm{BI}} \mathbf A$ implies $\vdash_{\mathscr H_2} \mathbf A$	0
Let $\mathbf 2$ denote [[Definition:Two (Boolean Algebra)|two]]. Then $\mathbf 2$ is a [[Definition:Boolean Algebra|Boolean algebra]].	0
Let $\mathcal L$ be the [[Definition:Language of Predicate Logic|language of predicate logic]] with a [[Definition:Countable Set|countable]] [[Definition:Signature for Predicate Logic|signature]]. Let $T$ be an $\mathcal L$-[[Definition:Theory (Logic)|theory]]. Let $\left\{{p_i: i \in \N}\right\}$ be a [[Definition:Countable Set|countable set]] of [[Definition:Isolated Type|non-isolated]] [[Definition:Type|$n$-types of $T$]]. There is a [[Definition:Countable Set|countable]] $\mathcal L$-[[Definition:First Order Structure|structure]] $\mathcal M$ such that [[Definition:Model (Logic)|$\mathcal M \models T$]] and $\mathcal M$ [[Definition:Omission|omits]] each $p_i$.	0
:$\neg p \implies q \vdash p \lor q$	0
:$p \dashv \vdash \neg \neg p$	0
We apply the [[Method of Truth Tables]]: :$\begin{array}{|ccc||ccc|} \hline p & \uparrow & q & q & \uparrow & p \\ \hline F & T & F & F & T & F \\ F & T & T & T & T & F \\ T & T & F & F & T & T \\ T & F & T & T & F & T \\ \hline \end{array}$ As can be seen by inspection, the [[Definition:Truth Value|truth values]] under the [[Definition:Main Connective (Propositional Logic)|main connectives]] match for all [[Definition:Boolean Interpretation|boolean interpretations]]. {{qed}}	0
Let $F_n$ denote the $n$th [[Definition:Fibonacci Number|Fibonacci number]]. Let $m, r$ be [[Definition:Non-Negative Integer|non-negative integers]]. Then: :$F_{m n + r} \equiv \paren {\begin{cases} F_r & : m \bmod 4 = 0 \\ \paren {-1}^{r + 1} F_{n - r} & : m \bmod 4 = 1 \\ \paren {-1}^n F_r & : m \bmod 4 = 2 \\ \paren {-1}^{r + 1 + n} F_{n - r} & : m \bmod 4 = 3 \end{cases} } \pmod {F_n}$	0
From [[Functionally Complete Logical Connectives/Negation and Disjunction|Functionally Complete Logical Connectives: Negation and Disjunction]], any boolean expression can be expressed in terms of $\lor$ and $\neg$. From [[NOR with Equal Arguments]]: :$\neg p \dashv \vdash p \downarrow p$ From [[Disjunction in terms of NOR]]: :$p \lor q \dashv \vdash \paren {p \downarrow q} \downarrow \paren {p \downarrow q}$ demonstrating that $p \lor q$ can be represented solely in terms of $\downarrow$. That is, $\set {\downarrow}$ is [[Definition:Functionally Complete|functionally complete]]. {{qed}}	0
: $\vdash \left({p \iff q}\right) \iff \left({\neg p \iff \neg q}\right)$	0
{{BeginTableau|\top \vdash \neg \bot}} {{Premise|1|\top}} {{Assumption|2|\bot|If a contradiction were assumed ...}} {{Explosion|3|2|\neg \top|2}} {{NonContradiction|4|1, 2|1|3}} {{Contradiction|5|1|\neg \bot|2|4}} {{EndTableau|lemma}} {{BeginTableau|\neg \bot \vdash \top}} {{Premise|1|\neg \bot}} {{Assumption|2|\neg \top|To assume a non-truth ...}} {{SequentIntro|3|2|\bot|2|from above result}} {{Reductio|4|1|\top|2|3}} {{EndTableau|qed}}	0
Let $l = \displaystyle \lim_{x \mathop \to a^+} \frac{f' \left({x}\right)}{g' \left({x}\right)}$. Let $\epsilon > 0$. By the definition of [[Definition:Limit of Real Function|limit]], we ought to find a $\delta > 0$ such that: :$\forall x: \left\vert{x - a}\right\vert < \delta \implies \left\vert{\dfrac {f \left({x}\right)} {g \left({x}\right)} - l}\right\vert < \epsilon$ Fix $\delta$ such that: :$\forall x: \left\vert{x - a}\right\vert < \delta \implies \left\vert{\dfrac {f' \left({x}\right)} {g' \left({x}\right)} - l}\right\vert < \epsilon$ which is possible by the definition of [[Definition:Limit of Real Function|limit]]. Let $x$ be such that $\left\vert{x - a}\right\vert < \delta$. By the [[Cauchy Mean Value Theorem]] with $b = x$: : $\exists \xi \in \left({a \,.\,.\, x}\right): \dfrac {f' \left({\xi}\right)} {g' \left({\xi}\right)} = \dfrac {f \left({x}\right) - f \left({a}\right)} {g \left({x}\right) - g \left({a}\right)}$ Since $f \left({a}\right) = g \left({a}\right) = 0$, we have: : $\exists \xi \in \left({a \,.\,.\, x}\right): \dfrac {f' \left({\xi}\right)} {g' \left({\xi}\right)} = \dfrac {f \left({x}\right)} {g \left({x}\right)}$ Now, as $a < \xi < x$, it follows that $\left\vert{\xi - a}\right\vert < \delta$ as well. Therefore: :$\left\vert{\dfrac {f \left({x}\right)} {g \left({x}\right)} - l }\right\vert = \left\vert{ \dfrac {f' \left({\xi}\right)} {g' \left({\xi}\right)} - l}\right\vert < \epsilon$ which leads us to the desired conclusion that: :$\displaystyle \lim_{x \mathop \to a^+} \frac {f \left({x}\right)} {g \left({x}\right)} = \lim_{x \mathop \to a^+} \frac {f' \left({x}\right)} {g' \left({x}\right)}$ {{qed}}	0
This page gathers together the [[Definition:Primitive (Calculus)|primitives]] of some expressions involving $\left({\sqrt{a^2 - x^2} }\right)^3$.	0
:$\ds \int \frac {\d x} {\sqrt {x^2 + a^2} } = \map \ln {x + \sqrt {x^2 + a^2} } + C$	0
{{begin-eqn}} {{eqn | l = \ln x | r = 2 \paren {\sum_{n \mathop = 0}^\infty \frac 1 {2 n + 1} \paren {\frac {x - 1} {x + 1} }^{2 n + 1} } }} {{eqn | r = 2 \paren {\frac {x - 1} {x + 1} + \frac 1 3 \paren {\frac {x - 1} {x + 1} }^3 + \frac 1 5 \paren {\frac {x - 1} {x + 1} }^5 + \cdots} }} {{end-eqn}} valid for all $x \in \R$ such that $-1 < x < 1$.	0
Let $r$ be the [[Definition:Common Ratio|common ratio]] of $\sequence {a_k}$. Let $p, q$ be consecutive [[Definition:Term of Geometric Sequence|terms]] of $r$. [[Definition:By Hypothesis|By hypothesis]] $p, q \in \Q$. Then, by definition of [[Definition:Geometric Sequence|geometric sequence]]: :$q = r p$ It follows that: :$r = \dfrac q p$ From [[Rational Numbers form Field]], $\Q$ is [[Definition:Closed Algebraic Structure|closed]] under [[Definition:Division|division]]. Thus $r \in \Q$ and hence the result. {{qed}} [[Category:Geometric Sequences]] qoxjqks99ap78b23fvywcsbpiwnffa0	0
Let $\N$ be defined as the [[Definition:Von Neumann Construction of Natural Numbers|von Neumann construction]] $\omega$. By definition of the [[Definition:Ordering on Von Neumann Construction of Natural Numbers|ordering on von Neumann construction]]: :$m \le n \iff m \subseteq n$ From [[Von Neumann Construction of Natural Numbers is Minimally Inductive]], $\omega$ is [[Definition:Minimally Inductive Class under General Mapping|minimally inductive class]] under the [[Definition:Successor Mapping|successor mapping]]. The result is then a direct application of [[Characteristics of Minimally Inductive Class under Progressing Mapping/Image of Proper Subset is Subset|Characteristics of Minimally Inductive Class under Progressing Mapping: Image of Proper Subset is Subset]]: :$m \subset n \implies m^+ \subseteq n$ {{qed}}	0
{{begin-eqn}} {{eqn | o = | r = \int \frac {\mathrm d x} {\left({a x + b}\right)^2 \left({p x + q}\right)} | c = }} {{eqn | r = \int \left({\frac 1 {b p - a q} \left({\frac {-a p} {\left({b p - a q}\right) \left({a x + b}\right)} + \frac {-a} {\left({a x + b}\right)^2} + \frac {p^2} {\left({b p - a q}\right) \left({p x + q}\right)} }\right)}\right) \ \mathrm d x | c = [[Primitive of Reciprocal of a x + b squared by p x + q/Partial Fraction Expansion|Partial Fraction Expansion]] }} {{eqn | r = \frac 1 {b p - a q} \left({\frac {-a p} {b p - a q} \int \frac {\mathrm d x} {a x + b} - a \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + \frac {p^2} {b p - a q} \int \frac {\mathrm d x} {p x + q} }\right) | c = [[Linear Combination of Integrals]] }} {{eqn | r = \frac 1 {b p - a q} \left({\frac {-a p} {b p - a q} \frac 1 a \ln \left\vert{a x + b}\right\vert - a \int \frac {\mathrm d x} {\left({a x + b}\right)^2} + \frac {p^2} {b p - a q} \frac 1 p \ln \left\vert{p x + q}\right\vert }\right) + C | c = [[Primitive of Reciprocal of a x + b|Primitive of Reciprocal of $a x + b$]] }} {{eqn | r = \frac 1 {b p - a q} \left({\frac {-p} {b p - a q} \ln \left\vert{a x + b}\right\vert - a \frac {-1} {a \left({a x + b}\right)} + \frac p {b p - a q} \ln \left\vert{p x + q}\right\vert }\right) + C | c = [[Primitive of Reciprocal of a x + b squared|Primitive of Reciprocal of $\left({a x + b}\right)^2$]] }} {{eqn | r = \frac 1 {b p - a q} \left({\frac 1 {a x + b} + \frac p {b p - a q} \ln \left\vert{\frac {p x + q} {a x + b} }\right\vert}\right) + C | c = [[Difference of Logarithms]] }} {{end-eqn}} {{qed}}	0
:$\tan 240 \degrees = \tan \dfrac {4 \pi} 3 = \sqrt 3$	0
Let $\phi_i: \mathbb H_\mathbf i \to \C$ be defined as: :$\forall \mathbf x = \mathbf a \mathbf 1 + b \mathbf i \in \mathbb H_\mathbf i: \phi_i \left({\mathbf x}\right) = a + b i$ where in this context $i$ is the [[Definition:Imaginary Unit|imaginary unit]]. Similarly we can define $\phi_j$ and $\phi_k$: :$\forall \mathbf x = \mathbf a \mathbf 1 + c \mathbf j \in \mathbb H_\mathbf j: \phi_j \left({\mathbf x}\right) = a + c i$ :$\forall \mathbf x = \mathbf a \mathbf 1 + d \mathbf k \in \mathbb H_\mathbf k: \phi_k \left({\mathbf x}\right) = a + d i$ === Proof of Bijectivity === First note that each of $\phi_i, \phi_j, \phi_k$ are [[Definition:Bijection|bijections]], as follows: * [[Definition:Injection|Injections]]: Let $\mathbf x_1 = \mathbf a_1 \mathbf 1 + b_1 \mathbf i, \mathbf x_2 = \mathbf a_2 \mathbf 1 + b_2 \mathbf i$. Then: :$\phi_i \left({x_1}\right) = \phi_i \left({x_2}\right) \implies a_1 + b_1 i = a_2 + b_2 i \implies a_1 = a_2, b_1 = b_2 \implies \mathbf x_1 = \mathbf x_2$ and similarly with $\phi_j$ and $\phi_k$. * [[Definition:Surjection|Surjections]]: Let $x = a + b i \in \C$. Then: : $\exists \mathbf x = \mathbf a \mathbf 1 + b \mathbf i \in \mathbb H_\mathbf i: \phi_i \left({\mathbf x}\right) = x$ and similarly with $\phi_j$ and $\phi_k$. Thus it is established that $\phi_i, \phi_j, \phi_k$ are [[Definition:Bijection|bijections]]. {{qed|lemma}} === Proof of Morphism Property === Let $\mathbf x_1 = \mathbf a_1 \mathbf 1 + b_1 \mathbf i, \mathbf x_2 = \mathbf a_2 \mathbf 1 + b_2 \mathbf i$. First we show that $+$ has the [[Definition:Morphism Property|morphism property]] under $\phi_i$: {{begin-eqn}} {{eqn | l=\phi_i \left({x_1}\right) + \phi_i \left({x_2}\right) | r=\phi_i \left({a_1 \mathbf 1 + b_1 \mathbf i}\right) + \phi_i \left({a_2 \mathbf 1 + b_2 \mathbf i}\right) | c= }} {{eqn | r=\left({a_1 + b_1 i}\right) + \left({a_2 + b_2 i}\right) | c= }} {{eqn | r=\left({a_1 + a_2}\right) + \left({b_1 + b_2}\right) i | c= }} {{eqn | r=\phi_i \left({x_1 + x_2}\right) | c= }} {{end-eqn}} and similarly for $\phi_j$ and $\phi_k$. Next we show that $\times$ has the [[Definition:Morphism Property|morphism property]] under $\phi_i$: {{begin-eqn}} {{eqn | l=\phi_i \left({x_1 \times x_2}\right) | r=\phi_i \left({a_1 a_2 - b_1 b_2 \mathbf i}\right) + \phi_i \left({a_1 b_2 + b_1 a_2 \mathbf i}\right) | c=by [[Definition:Quaternion Multiplication|quaternion multiplication]] - most terms are zero }} {{eqn | r=\left({a_1 a_2 - b_1 b_2}\right) + \left({a_1 b_2 + b_1 a_2}\right) i | c= }} {{eqn | r=\left({a_1 + b_1 i}\right) \times \left({a_2 + b_2 i}\right) | c=by [[Definition:Complex Multiplication|complex multiplication]] }} {{eqn | r=\phi_i \left({x_1}\right) \times \phi_i \left({x_2}\right) | c= }} {{end-eqn}} Similarly for $\phi_j$ and $\phi_k$. So we have shown that $\phi_i, \phi_j, \phi_k$ are [[Definition:Ring Homomorphism|ring homomorphisms]]. {{qed|lemma}} So all of these mappings are [[Definition:Bijection|bijective]] [[Definition:Ring Homomorphism|homomorphisms]], that is, [[Definition:Ring Isomorphism|isomorphisms]]. {{qed}}	0
Let $\struct {T, \circ}$ be a [[Definition:Commutative Algebraic Structure|commutative algebraic structure]]. Let $f, g \in T^S$. Then: {{begin-eqn}} {{eqn | l = \map {\paren {f \oplus g} } x | r = \map f x \circ \map g x | c = {{Defof|Pointwise Operation}} }} {{eqn | r = \map g x \circ \map f x | c = $\circ$ is [[Definition:Commutative Operation|commutative]] }} {{eqn | r = \map {\paren {g \oplus f} } x | c = {{Defof|Pointwise Operation}} }} {{end-eqn}} {{qed}}	0
Let $U \subset \C$ be some [[Definition:Open Set (Complex Analysis)|open set]] and let $f$ be an [[Definition:Analytic Function|analytic function]] defined on $U$. Then either $f$ is a [[Definition:Constant Mapping|constant function]], or the set $\set {z \in U: \map f z = 0}$ is totally disconnected.	0
Let $T_A = \left({A, \tau_A}\right)$ and $T_B = \left({B, \tau_B}\right)$ be [[Definition:Topological Space|topological spaces]]. Let $b \in B$ be any point in $B$. Let $f_b: A \to B$ be the [[Definition:Constant Mapping|constant mapping]] defined by: :$\forall x \in A: f_b \left({x}\right) = b$ Then $f_b$ is [[Definition:Continuous Mapping (Topology)|continuous]].	0
{{begin-eqn}} {{eqn | l = \sin^4 x | r = \paren {\sin^2 x}^2 }} {{eqn | r = \paren {\frac {1 - \cos 2 x} 2}^2 | c = [[Square of Sine]] }} {{eqn | r = \frac {1 - 2 \cos 2 x + \cos^2 2 x} 4 | c = multiplying out }} {{eqn | r = \frac {1 - 2 \cos 2 x + \frac {1 + \cos 4 x} 2} 4 | c = [[Square of Cosine]] }} {{eqn | r = \frac {2 - 4 \cos 2 x + 1 + \cos 4 x} 8 | c = multiplying top and bottom by $2$ }} {{eqn | r = \frac {3 - 4 \cos 2 x + \cos 4 x} 8 | c = rearrangement }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = y | r = \arctan x | c = }} {{eqn | ll= \leadsto | l = x | r = \tan y | c = {{Defof|Real Arctangent}} }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d y} | r = \sec^2 y | c = [[Derivative of Tangent Function]] }} {{eqn | r = 1 + \tan^2 y | c = [[Difference of Squares of Secant and Tangent]] }} {{eqn | r = 1 + x^2 | c = Definition of $x$ }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \frac 1 {1 + x^2} | c = [[Derivative of Inverse Function]] }} {{end-eqn}} {{qed}}	0
:$\map \coth {a + b i} = \dfrac {\coth a - \coth a \cot^2 b} {\coth^2 a + \cot^2 b} + \dfrac {\coth b + \coth^2 a \cot b} {\coth^2 a + \cot^2 b} i$	0
A direct implementation of [[Cosine of Multiple of Pi]]: :$\forall n \in \Z: \cos n \pi = \paren {-1}^n$ In this case, $n = 2$ and so: :$\cos 2 \pi = \paren {-1}^2 = 1$ {{qed}}	0
The [[Definition:Algebraic Structure|algebraic structure]] $\struct {\R, \times}$ consisting of the [[Definition:Set|set]] of [[Definition:Real Number|real numbers]] $\R$ under [[Definition:Real Multiplication|multiplication]] $\times$ is not a [[Definition:Group|group]].	0
:$\map \arctan {\dfrac 1 x} = \arccot x$	0
The proof will proceed by the [[Principle of Complete Finite Induction]] on $\Z_{>0}$. Let $S$ be the [[Definition:Set|set]] defined as: :$\displaystyle S := \set {n \in \Z_{>0}: a^n - b^n = \paren {a - b} \sum_{j \mathop = 0}^{n - 1} a^{n - j - 1} b^j}$ That is, $S$ is to be the [[Definition:Set|set]] of all $n$ such that: :$\displaystyle a^n - b^n = \paren {a - b} \sum_{j \mathop = 0}^{n - 1} a^{n - j - 1} b^j$ === Basis for the Induction === We have that: :$\displaystyle a^1 - b^1 = \paren {a - b} \sum_{j \mathop = 0}^0 a^{1 - 0 - 1} b^j$ So $1 \in S$. This is the [[Definition:Basis for the Induction|basis for the induction]]. === Induction Hypothesis === It is to be shown that if $r \in S$ for all $r$ such that $1 \le r \le k$, then it follows that $k + 1 \in S$. This is the [[Definition:Induction Hypothesis|induction hypothesis]]: :$\forall r \in \Z_{>0}: 1 \le r \le k: \displaystyle a^r - b^r = \paren {a - b} \sum_{j \mathop = 0}^{r - 1} a^{r - j - 1} b^j$ It is to be demonstrated that it follows that: :$\displaystyle a^{k + 1} - b^{k + 1} = \paren {a - b} \sum_{j \mathop = 0}^k a^{k - j} b^j$ === Induction Step === This is the [[Definition:Induction Step|induction step]]: {{begin-eqn}} {{eqn | l = a^{k + 1} - b^{k + 1} | r = \paren {a + b} \paren {a^k - b^k} - a b \paren {a^{k - 1} - b^{k - 1} } | c = }} {{eqn | r = \paren {a + b} \paren {a - b} \sum_{j \mathop = 0}^{k - 1} a^{k - j - 1} b^j - a b \paren {a - b} \sum_{j \mathop = 0}^{k - 2} a^{k - j - 2} b^j | c = [[Difference of Two Powers/Proof 4#Induction Hypothesis|Induction Hypothesis]] }} {{eqn | r = \paren {a - b} \paren {\paren {a + b} \sum_{j \mathop = 0}^{k - 1} a^{k - j - 1} b^j - a b \sum_{j \mathop = 0}^{k - 2} a^{k - j - 2} b^j} | c = }} {{eqn | r = \paren {a - b} \paren {\sum_{j \mathop = 0}^{k - 1} a^{k - j} b^j + \sum_{j \mathop = 0}^{k - 1} a^{k - j - 1} b^{j + 1} - \sum_{j \mathop = 0}^{k - 2} a^{k - j - 1} b^{j + 1} } | c = }} {{eqn | r = \paren {a - b} \paren {\sum_{j \mathop = 0}^{k - 1} a^{k - j} b^j + a^0 b^k} }} {{eqn | r = \paren {a - b} \sum_{j \mathop = 0}^k a^{k - j} b^j }} {{end-eqn}} So $\forall r \in S: 0 \le r \le k: r \in S \implies k + 1 \in S$ and the result follows by the [[Principle of Complete Finite Induction]]: :$\forall n \in \Z_{>0}: \displaystyle a^n - b^n = \paren {a - b} \sum_{j \mathop = 0}^{n - 1} a^{n - j - 1} b^j$ {{qed}}	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Then $p$ is [[Definition:Deficient Number|deficient]].	0
Let [[Definition:Real Number|real numbers]] be selected at [[Definition:Random Selection|random]] following a [[Definition:Continuous Uniform Distribution|continuous uniform distribution]] from the [[Definition:Closed Real Interval|interval]] $\closedint 0 1$ until their total sum is greater than $1$. The [[Definition:Expectation|expectation]] of the [[Definition:Integer|number]] of selections is [[Definition:Euler's Number|Euler's number $e$]].	0
Every non-zero [[Definition:Integer|integer]] is [[Definition:Cancellable Element|cancellable]] for [[Definition:Integer Multiplication|multiplication]]. That is: :$\forall x, y, z \in \Z, x \ne 0: x y = x z \iff y = z$	0
:$\displaystyle \int \frac 1 {1 + x^4} \rd x = \frac 1 {2 \sqrt 2} \paren {\map \arctan {\frac 1 {\sqrt 2} \paren {x - \frac 1 x} } + \frac 1 2 \ln \size {\frac {x^2 + \sqrt 2 x + 1} {x^2 - \sqrt 2 x + 1} } } + C$	0
{{begin-eqn}} {{eqn | l = 199 + 0 \times 210 | r = 199 | c = which is the $46$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 1 \times 210 | r = 409 | c = which is the $80$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 2 \times 210 | r = 619 | c = which is the $114$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 3 \times 210 | r = 829 | c = which is the $145$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 4 \times 210 | r = 1039 | c = which is the $175$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 5 \times 210 | r = 1249 | c = which is the $204$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 6 \times 210 | r = 1459 | c = which is the $232$nd [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 7 \times 210 | r = 1669 | c = which is the $263$rd [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 8 \times 210 | r = 1879 | c = which is the $289$th [[Definition:Prime Number|prime]] }} {{eqn | l = 199 + 9 \times 210 | r = 2089 | c = which is the $316$th [[Definition:Prime Number|prime]] }} {{end-eqn}} {{OEIS|A033168}} But note that $199 + 10 \times 210 = 2299 = 11^2 \times 19$ and so is not [[Definition:Prime Number|prime]]. {{ProofWanted|It remains to be shown that there are no smaller such APs}}	0
We define $f \left({x}\right)$ as a '''prime-representing function''' {{iff}}: :$\forall x \in \N: f \left({x}\right) \in \Bbb P$ where: :$\N$ denotes the [[Definition:Set|set]] of all [[Definition:Natural Number|natural numbers]] :$\Bbb P$ denotes the [[Definition:Set|set]] of all [[Definition:Prime Number|prime numbers]]. Let $p_n$ be the $n$th [[Definition:Prime Number|prime number]]. From [[Difference between Consecutive Primes]]: :$p_{n+1} - p_n < K p_n^{5/8}$ where $K$ is an unknown but fixed [[Definition:Positive Integer|positive integer]]. === Lemma 1 === :$\forall N > K^8 \in \Z: \exists p \in \Bbb P: N^3 < p < \left({N + 1}\right)^3 - 1$ ==== Proof ==== Let $p_n$ be the greatest [[Definition:Prime Number|prime]] less than $N^3$. {{begin-eqn}} {{eqn | l = N^3 | o = < | r = p_{n+1} | c = because $p_n$ is the '''greatest''' [[Definition:Prime Number|prime]] less than $N^3$ }} {{eqn | o = < | r = p_n + K {p_n}^{\frac 5 8} | c = [[Difference between Consecutive Primes]] }} {{eqn | o = < | r = N^3 + K N^{\frac {15} 8} | c = because $p_n < N^3$ }} {{eqn | o = < | r = N^3 + N^2 | c = because $N > K^8$ }} {{eqn | o = < | r = N^3 + 3N^2 + 3N }} {{eqn | r = \left({N + 1}\right)^3 - 1 }} {{end-eqn}} Therefore: :$N^3 < p_{n+1} < \left({N + 1}\right)^3 - 1$ {{qed|lemma}} Let $P_0 > K^8$ be a [[Definition:Prime Number|prime number]]. By [[Mills' Theorem#Lemma 1|Lemma 1]], there exists an [[Definition:Infinite Sequence|infinite sequence]] of [[Definition:Prime Number|primes]]: :$P_0, P_1, P_2, \ldots$ such that: :$\forall n \in \N_{>0}: {P_n}^3 < P_{n+1} < \left({P_n + 1}\right)^3 - 1$ Then we define two [[Definition:Mapping|functions]] $u, v: \N \to \Bbb P$: :$\forall n \in \N: u \left({n}\right) = {P_n}^{3^{-n} }$ :$\forall n \in \N: v \left({n}\right) = \left({P_n + 1}\right)^{3^{-n} }$ It is trivial that $v \left({n}\right) > u \left({n}\right)$. === Lemma 2 === :$\forall n \in \N_{>0}: u \left({n + 1}\right) > u \left({n}\right)$ ==== Proof ==== {{begin-eqn}} {{eqn | l = u \left({n + 1}\right) | r = {P_{n+1} }^{3^{-\left({n+1}\right)} } }} {{eqn | o = > | r = \left({P_n^3}\right)^{3^{-n-1} } | c = because $P_{n+1} > {P_n}^3$ }} {{eqn | r = {P_n}^{3 \times 3^{-n-1} } }} {{eqn | r = {P_n}^{3^{-n} } }} {{eqn | r = u \left({n}\right) }} {{end-eqn}} {{qed|lemma}} === Lemma 3 === :$\forall n \in \N_{>0}: v \left({n + 1}\right) < v \left({n}\right)$ ==== Proof ==== {{begin-eqn}} {{eqn | l = v \left({n + 1}\right) | r = \left({P_{n+1} + 1}\right)^{3^{-\left({n+1}\right)} } }} {{eqn | o = < | r = \left({\left({\left({P_n + 1}\right)^3 - 1}\right) + 1}\right)^{3^{-n-1} } | c = because $P_{n+1} < \left({P_n + 1}\right)^3 - 1$ }} {{eqn | r = \left({\left({P_n + 1}\right)^3}\right)^{3^{-n-1} } }} {{eqn | r = \left({P_n + 1}\right)^ {3^{-n} } }} {{eqn | r = v \left({n}\right) }} {{end-eqn}} {{qed|lemma}} It follows trivially that $u \left({n}\right)$ is [[Definition:Bounded Real-Valued Function|bounded]] and [[Definition:Strictly Monotone Real Function|strictly monotone]]. Therefore, there exists a number $A$ which is defined as: :$A := \lim_{n \mathop \to \infty} u \left({n}\right)$ From [[Mills' Theorem#Lemma 2|Lemma 2]] and [[Mills' Theorem#Lemma 3|Lemma 3]], we have: :$u \left({n}\right) < A < v \left({n}\right)$ {{begin-eqn}} {{eqn | l = u \left({n}\right) | o = < | r = A | rr= < | rrr=\left({n}\right) }} {{eqn | ll= \leadsto | l = {P_n}^{3^{-n} } | o = < | r = A | rr= < | rrr=\left({P_n + 1}\right)^{3^{-n} } }} {{eqn | ll= \leadsto | l = P_n | o = < | r = A^{3^n} | rr= < | rrr=P_n + 1 }} {{end-eqn}} The result follows. {{qed}} {{Namedfor|William H. Mills|cat = Mills}}	0
There are $3$ ways $\dfrac 2 5$ can be expressed as a [[Definition:Pandigital Fraction|pandigital fraction]]: :$\dfrac 2 5 = \dfrac {6894} {17235}$ :$\dfrac 2 5 = \dfrac {8694} {21735}$ :$\dfrac 2 5 = \dfrac {9486} {23715}$	0
Let $b = 0$. Then: :$\displaystyle \int \frac {\mathrm d x} {a x^2 + b x + c} = \begin{cases} \dfrac 1 {\sqrt {a c} } \arctan \left({x \sqrt {\dfrac a c} }\right) + C & : a c > 0 \\ \dfrac 1 {2 \sqrt {-a c} } \ln \left\vert{\dfrac {a x - \sqrt {-a c} } {a x + \sqrt {-a c} } }\right\vert + C & : a c < 0 \\ \dfrac {-1} {a x} + C & : c = 0 \end{cases}$	0
[[File:Primitive-of-1-over-x-squared-minus-a-squared.png|thumb|right|600px|$\color {blue} {\dfrac 1 {x^2 - a^2} } \qquad \color {green} {-\dfrac 1 a \tanh^{-1} \dfrac x a} \qquad \color {red} {-\dfrac 1 a \coth^{-1} \dfrac x a}$]] Let $a \in \R_{>0}$ be a [[Definition:Strictly Positive Real Number|strictly positive real]] [[Definition:Constant|constant]].	0
Since $x^4 = \paren {-x}^4$, $x^4$ is an [[Definition:Even Function|even function]]. By [[Fourier Series for Even Function over Symmetric Range]], the [[Definition:Fourier Series|Fourier series]] of $\map f x$ can be expressed as: :$x^4 \sim \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty a_n \cos n x$ where for all $n \in \Z_{> 0}$: {{begin-eqn}} {{eqn | l = a_n | r = \dfrac 2 \pi \displaystyle \int_0^\pi x^4 \cos n x \ \d x }} {{eqn | r = \dfrac 2 \pi \paren {\intlimits {\frac {\sin n x} n x^4 + \frac {4 \cos n x} {n^2} x^3 - \frac {12 \sin n x} {n^3} x^2 - \frac {24 \cos n x} {n^4} x + \frac {24 \sin n x} {n^5} } {x \mathop = 0} {x \mathop = \pi} } | c = [[Primitive of x fourth by Cosine of a x|Primitive of $x^4 \cos a x$]] }} {{eqn | r = \dfrac 2 \pi \paren {\frac {4 \pi^3 \cos n \pi} {n^2} - \frac {24 \pi \cos n \pi} {n^4} } | c = }} {{eqn | r = \frac {8 n^2 \pi^2 - 48} {n^4} \cos n \pi | c = }} {{end-eqn}} {{begin-eqn}} {{eqn | l = a_0 | r = \dfrac 2 \pi \int_0^\pi x^4 \ \d x }} {{eqn | r = \dfrac 2 \pi \intlimits {\frac {x^5} 5} {x \mathop = 0} {x \mathop = \pi} | c = [[Primitive of Power]] }} {{eqn | r = \frac {2 \pi^4} 5 }} {{end-eqn}} This gives: :$x^4 \sim \dfrac {\pi^4} 5 + \displaystyle \sum_{n \mathop = 1}^\infty \frac {8 n^2 \pi^2 - 48} {n^4} \cos n \pi \cos n x$ {{qed}} [[Category:Examples of Fourier Series]] 4corhmbfraqiwoqpxj7c5d4d2uajbe6	0
=== Proof for Complex Analysis === Suppose $L' \ne L$ is another limit of $f \left({z}\right)$ at $z_0$. Let us take $\epsilon = \dfrac {\left\vert{L - L'}\right\vert} 2$. Then we can find $\delta_1 > 0, \delta_2 > 0$ such that: * $z \in S, 0 < \left\vert{z - z_0}\right\vert < \delta_1 \implies \left\vert{f \left({z}\right) - L}\right\vert < \epsilon$ * $z \in S, 0 < \left\vert{z - z_0}\right\vert < \delta_2 \implies \left\vert{f \left({z}\right) - L'}\right\vert < \epsilon$ Because $z_0$ is a limit point: : $\exists z^* \in S: 0 < \left\vert{z - z_0}\right\vert < \min \left\{{\delta_1, \delta_2}\right\}$ Then: {{begin-eqn}} {{eqn | l=\left\vert{L - L'}\right\vert | r=\left\vert{L - f \left({z^*}\right) + f \left({z^*}\right) - L'}\right\vert | c= }} {{eqn | o=\le | r=\left\vert{L - f \left({z^*}\right)}\right\vert + \left\vert{f \left({z^*}\right) - L'}\right\vert | c=[[Triangle Inequality]] }} {{eqn | o=< | r=\epsilon + \epsilon | c= }} {{eqn | r=2 \epsilon | c= }} {{end-eqn}} This contradicts the choice we made of $\epsilon$. {{qed}} {{wtd|Expand the context to include the more general result for metric spaces etc.}} [[Category:Complex Analysis]] 711tzlnlrulnvtl4yuph2knofq1ko50	0
Let $M_1$ and $M_2$ be [[Definition:Lipschitz Equivalent Metric Spaces|Lipschitz equivalent]]. Then, by definition, $\exists h, k \in \R_{>0}$ such that: : $\forall x, y \in A_1: h d_1 \left({x, y}\right) \le d_2 \left({f \left({x}\right), f \left({y}\right)}\right) \le k d_1 \left({x, y}\right)$ From the definition of [[Definition:Open Ball|open $\epsilon$-ball]]: {{begin-eqn}} {{eqn | l = y | o = \in | r = B_{h \epsilon} \left({f \left({x}\right); d_2}\right) | c = }} {{eqn | ll= \implies | l = d_2 \left({f \left({x}\right), f \left({y}\right)}\right) | o = < | r = h \epsilon | c = }} {{eqn | ll= \implies | l = d_1 \left({x, y}\right) | o = \le | r = \frac {d_2 \left({f \left({x}\right), f \left({y}\right)}\right)} h < \epsilon | c = }} {{eqn | ll= \implies | l = y | o = \in | r = B_\epsilon \left({x; d_1}\right) | c = }} {{end-eqn}} ... and: {{begin-eqn}} {{eqn | l = y | o = \in | r = B_{\epsilon / k} \left({x; d_1}\right) | c = }} {{eqn | ll= \implies | l = d_1 \left({x, y}\right) | o = < | r = \frac \epsilon k | c = }} {{eqn | ll= \implies | l = d_2 \left({f \left({x}\right), f \left({y}\right)}\right) | o = \le | r = k {d_1 \left({x, y}\right)} < \epsilon | c = }} {{eqn | ll= \implies | l = y | o = \in | r = B_\epsilon \left({f \left({x}\right); d_2}\right) | c = }} {{end-eqn}} Thus: : $B_{h \epsilon} \left({f \left({x}\right); d_2}\right) \subseteq B_\epsilon \left({x; d_1}\right)$ : $B_{\epsilon / k} \left({x; d_1}\right) \subseteq B_\epsilon \left({f \left({x}\right); d_2}\right)$ {{qed|lemma}} Now, suppose $U$ is [[Definition:Open Set (Metric Space)|$d_2$-open]]. Let $x \in U$. Then: : $\exists \epsilon \in \R_{>0}: B_\epsilon \left({f \left({x}\right); d_2}\right) \subseteq U$. Hence: : $B_{\epsilon / k} \left({x; d_1}\right) \subseteq U$ Thus $U$ is [[Definition:Open Set (Metric Space)|$d_1$-open]]. Similarly, suppose $U$ is [[Definition:Open Set (Metric Space)|$d_1$-open]]. Let $x \in U$. Then: : $\exists \epsilon \in \R_{>0}: B_\epsilon \left({x; d_1}\right) \subseteq U$ Hence: : $B_{h \epsilon} \left({f \left({x}\right); d_2}\right) \subseteq U$ Thus $U$ is [[Definition:Open Set (Metric Space)|$d_2$-open]]. The result follows by definition of [[Definition:Homeomorphic Metric Spaces|homeomorphic metric spaces]]. {{qed}}	0
From [[Power Series Expansion for Cotangent Function]]: :$(1): \quad \cot x = \displaystyle \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} B_{2 n} \, x^{2 n - 1} } {\left({2 n}\right)!}$ Then: {{begin-eqn}} {{eqn | l = \tan x | r = \cot x - 2 \cot 2 x | c = [[Cotangent Minus Tangent]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} B_{2 n} \, x^{2 n - 1} } {\left({2 n}\right)!} - 2 \sum_{n \mathop = 0}^\infty \frac {\left({-1}\right)^n 2^{2 n} B_{2 n} \, \left({2 x}\right)^{2 n - 1} } {\left({2 n}\right)!} | c = by $(1)$ }} {{eqn | r = \sum_{n \mathop = 0}^\infty \frac {\left({- 1}\right)^n 2^{2 n} \left({1 - 2^{2 n} }\right) B_{2 n} \, x^{2 n - 1} } {\left({2 n}\right)!} | c = }} {{eqn | r = \sum_{n \mathop = 1}^\infty \frac {\left({- 1}\right)^{n - 1} 2^{2 n} \left({2^{2 n} - 1}\right) B_{2 n} \, x^{2 n - 1} } {\left({2 n}\right)!} | c = }} {{end-eqn}} {{qed|lemma}}	0
From [[Beatty's Theorem]], the [[Definition:Beatty Sequence|Beatty sequences]] $\mathcal B_r$ and $\mathcal B_s$ are [[Definition:Complementary Beatty Sequence|complementary]] {{iff}}: :$\dfrac 1 r + \dfrac 1 s = 1$ It remains to be demonstrated that this holds for $r = \phi$ and $s = \phi^2$. Thus: {{begin-eqn}} {{eqn | l = \dfrac 1 \phi + \dfrac 1 {\phi^2} | r = \dfrac {\phi + 1} {\phi^2} | c = }} {{eqn | r = \dfrac {\phi^2} {\phi^2} | c = [[Square of Golden Mean equals One plus Golden Mean]] }} {{eqn | r = 1 | c = }} {{end-eqn}} Hence the result. {{qed}} [[Category:Beatty Sequences]] jxlkrqozmwpp9tlf1o4ypx3ti503jl8	0
For $0 < \theta < 2 \pi$: :$\displaystyle \sum_{n \mathop \in \Z} \dfrac {\sin \paren {n + \alpha} \theta} {n + \alpha} = \pi$	0
$105$ is the smallest [[Definition:Positive Integer|positive integer]] $n$ such that $1$ can be expressed as the [[Definition:Rational Addition|sum]] of [[Definition:Reciprocal|reciprocals]] of [[Definition:Distinct|distinct]] [[Definition:Odd Integer|odd integers]] such that none are less than $\dfrac 1 n$: :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {33} + \dfrac 1 {35} + \dfrac 1 {45} + \dfrac 1 {55} + \dfrac 1 {77} + \dfrac 1 {105}$ {{OEIS|A238795}} There are $5$ ways of expressing $1$ as the [[Definition:Rational Addition|sum]] of [[Definition:Reciprocal|reciprocals]] of only $9$ [[Definition:Distinct|distinct]] [[Definition:Odd Integer|odd integers]], but then the least term is less than $\dfrac 1 {105}$. The $9$-term solutions are as follows: :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {15} + \dfrac 1 {35} + \dfrac 1 {45} + \dfrac 1 {231}$ {{OEIS|A201644}} :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {15} + \dfrac 1 {21} + \dfrac 1 {231} + \dfrac 1 {315}$ {{OEIS|A201648}} :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {15} + \dfrac 1 {33} + \dfrac 1 {45} + \dfrac 1 {385}$ {{OEIS|A201649}} :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {15} + \dfrac 1 {21} + \dfrac 1 {165} + \dfrac 1 {693}$ {{OEIS|A201647}} :$1 = \dfrac 1 3 + \dfrac 1 5 + \dfrac 1 7 + \dfrac 1 9 + \dfrac 1 {11} + \dfrac 1 {15} + \dfrac 1 {21} + \dfrac 1 {135} + \dfrac 1 {10 \, 395}$ {{OEIS|A201646}}	0
:$\displaystyle \sum_k \binom {n + k} {2 k} \binom {2 k} k \frac {\paren {-1}^k} {k + 1} = \sqbrk {n = 0}$	0
[[Definition:Order Isomorphism|Order isomorphism]] between [[Definition:Ordered Set|ordered sets]] is an [[Definition:Equivalence Relation|equivalence relation]]. So any given family of [[Definition:Ordered Set|ordered sets]] can be [[Definition:Partition (Set Theory)|partitioned]] into [[Definition:Disjoint Sets|disjoint]] [[Definition:Equivalence Class|classes]] of isomorphic [[Definition:Set|sets]].	0
For $n \in \N_{>0}$, let $\Lambda \left({n}\right)$ be the [[Definition:Von Mangoldt Function|von Mangoldt function]]. Then: :$\displaystyle \lim_{N \to \infty} \frac 1 N \sum_{n \mathop = 1}^N \Lambda \left({n}\right) = 1$ is [[Definition:Logical Equivalence|logically equivalent]] to the [[Prime Number Theorem]].	0
$\sigma_\FF$ is a [[Definition:Sigma-Algebra|$\sigma$-algebra]] containing $\EE$. $\map \sigma \EE$ is a subset of ''all'' $\sigma$-algebras containing $\FF$, by definition of a generated $\sigma$-algebra. Therefore it contains $\map \sigma \EE$. {{qed}}	0
Let $\struct {D, +, \circ}$ be an [[Definition:Integral Domain|integral domain]] whose [[Definition:Ring Zero|zero]] is $0_D$. Let $D^*$ denote $D \setminus \set {0_D}$, that is, $D$ without its [[Definition:Ring Zero|zero]]. Let $a \in D^*$. Then: :$\forall x, y \in D: a \circ x = a \circ y \implies x = y$ That is, all elements of $D^*$ are [[Definition:Cancellable Element|cancellable]] for the [[Definition:Ring Product|ring product]].	0
By definition, $f^{-1} \subseteq T \times S$ is a [[Definition:Relation|relation]] on $T \times S$. Thus [[Image of Subset under Relation equals Union of Images of Elements]] can be applied directly. {{qed}} [[Category:Preimages under Mappings]] [[Category:Subsets]] [[Category:Set Union]] a3ik6tclkfa5owlyn526l44dveapcw9	0
Let $T = \left({S, \tau}\right)$ be a [[Definition:Topological Space|topological space]]. Then :$T$ is a [[Definition:T0 Space|$T_0$ space]] {{iff}} ::for every [[Definition:Element|points]] $x, y \in S$ if $x \ne y$ then :::there exists a [[Definition:Closed Set (Topology)|closed]] [[Definition:Subset|subset]] $F$ of $S$ such that $x \in F$ and $y \notin F$ ::or :::there exists a [[Definition:Closed Set (Topology)|closed]] [[Definition:Subset|subset]] $F$ of $S$ such that $x \notin F$ and $y \in F$	0
Let $G$ be a [[Definition:Group|group]] whose [[Definition:Identity Element|identity]] is $e$. Let: :$H$ be a [[Definition:Subgroup|subgroup]] of $G$ :$N$ be a [[Definition:Normal Subgroup|normal subgroup]] of $G$. Then: :$N \lhd \gen {N, H} = N H = H N \le G$ where: :$\le$ denotes [[Definition:Subgroup|subgroup]] :$\lhd$ denotes [[Definition:Normal Subgroup|normal subgroup]] :$\gen {N, H}$ denotes a [[Definition:Generator of Subgroup|subgroup generator]] :$N H$ denotes [[Definition:Subset Product|subset product]].	0
Let $G = \struct {V, E}$ be a [[Definition:Graph (Graph Theory)|graph]]. Let $\to$ denote the [[Definition:Relation|relation]] '''is [[Definition:Connected Vertices|connected]] to''' on the set $V$. Then $\to$ is an [[Definition:Equivalence Relation|equivalence relation]].	0
The [[Definition:Dihedral Group|dihedral group]] $D_n$ is of [[Definition:Order of Structure|order]] $2 n$.	0
The [[Definition:Image (Set Theory)/Relation/Element/Singleton|value of a relation]] is always a [[Definition:Small Class|small class]].	0
Consider $R, S, T \subseteq \mathbb U$, where $\mathbb U$ is considered as the [[Definition:Universe (Set Theory)|universe]]. {{begin-eqn}} {{eqn | l = \left({R \cap S}\right) \setminus T | r = \left({R \cap S}\right) \cap \complement \left({T}\right) | c = [[Set Difference as Intersection with Complement]] }} {{eqn | r = \left({R \cap S}\right) \cap \left({\complement \left({T}\right) \cap \complement \left({T}\right)}\right) | c = [[Intersection is Idempotent]] }} {{eqn | r = \left({R \cap \complement \left({T}\right)}\right) \cap \left({S \cap \complement \left({T}\right)}\right) | c = [[Intersection is Associative]] and [[Intersection is Commutative]] }} {{eqn | r = \left({R \setminus T}\right) \cap \left({S \setminus T}\right) | c = [[Set Difference as Intersection with Complement]] }} {{end-eqn}} {{qed}}	0
For all $x \in \R$: :$\displaystyle e^{\cos \left({x}\right)} = e \left({e^{cos \left({x}\right)- 1 } }\right) = e \left({\sum_{n \mathop = 0}^\infty \frac{ \left({-1}\right)^m P_2 \left({2 m}\right)} {2 m!} x^{2 m} }\right)$ where $P_2 \left({2 m}\right)$ is the partition of the set of size 2m into even blocks.	0
Let $p$ be a [[Definition:Prime Number|prime number]]. Let $G$ be a [[Definition:Group|group]] such that: :$\order G = k p^n$ where: :$\order G$ denotes the [[Definition:Order of Structure|order]] of $G$ :$p$ is not a [[Definition:Divisor of Integer|divisor]] of $k$. Then $G$ has at least one [[Definition:Sylow p-Subgroup|Sylow $p$-subgroup]].	0
First we establish some details. Let $H \subseteq X_1$. Let $\mathbb K_1$ be defined as: :$\mathbb K_1 := \set {K \subseteq X_1: H \subseteq K, K \text { closed} }$ That is, let $\mathbb K_1$ be the set of all [[Definition:Closed Set (Topology)|closed sets]] of $T_1$ which contain $H$. Similarly, let $\mathbb K_2$ be defined as: :$\mathbb K_2 := \set {K \subseteq X_2: f \sqbrk H \subseteq K, K \text{ closed} }$ That is, let $\mathbb K_2$ be the set of all [[Definition:Closed Set (Topology)|closed sets]] of $T_2$ which contain $f \sqbrk H$. From the definition of [[Definition:Closure (Topology)|closure]], we have that: :$\displaystyle H^- = \bigcap \mathbb K_1$ That is, the [[Definition:Closure (Topology)|closure]] of $H$ is the [[Definition:Set Intersection|intersection]] of all the [[Definition:Closed Set (Topology)|closed sets]] of $T_1$ which contain $H$. Similarly: :$\displaystyle \paren {f \sqbrk H}^- = \bigcap \mathbb K_2$ That is, the [[Definition:Closure (Topology)|closure]] of $f \sqbrk H$ is the [[Definition:Set Intersection|intersection]] of all the [[Definition:Closed Set (Topology)|closed sets]] of $T_2$ which contain $f \sqbrk H$. We have: {{begin-eqn}} {{eqn | l = f \sqbrk {H^-} | r = f \sqbrk {\bigcap \mathbb K_1} | c = }} {{eqn | o = \subseteq | r = \bigcap_{K \mathop \in \mathbb K_1} f \sqbrk K | c = [[Image of Intersection under Mapping]] }} {{end-eqn}} From [[Image of Subset under Relation is Subset of Image/Corollary 2|Image of Subset under Relation is Subset of Image: Corollary 2]]: :$H \subseteq K \implies f \sqbrk H \subseteq f \sqbrk K$ === Necessary Condition === Suppose $f$ is [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. From the above we have that : :$\displaystyle \paren {f \sqbrk H}^- := \bigcap \mathbb K_2$ As $f$ is continuous, then: :$\forall K \in \mathbb K_2: f^{-1} \sqbrk K$ is [[Definition:Closed Set (Topology)|closed]] in $T_1$ But as $f \sqbrk H \subseteq K$, it follows from [[Image of Subset under Relation is Subset of Image/Corollary 3|Image of Subset under Relation is Subset of Image: Corollary 3]] that: :$H \subseteq f^{-1} \sqbrk K$ So: :$\mathbb K_3 := \set {f^{-1} \sqbrk K: K \text { closed in } T_2, H \subseteq f^{-1} \sqbrk K}$ consists entirely of [[Definition:Closed Set (Topology)|closed sets]] in $T_1$ which are [[Definition:Superset|supersets]] of $H$. That is, $\mathbb K_3 \subseteq \mathbb K_1$. So: :$\displaystyle \bigcap \mathbb K_1 \subseteq \bigcap \mathbb K_3$ and so: :$\displaystyle f \sqbrk {\bigcap \mathbb K_1} \subseteq f \sqbrk {\bigcap \mathbb K_3}$ But from [[Image of Intersection under Mapping]]: :$\displaystyle f \sqbrk {\bigcap \mathbb K_3} \subseteq \bigcap_{K \mathop \in \mathbb K_3} f \sqbrk K$ But: :$\displaystyle \bigcap_{K \mathop \in \mathbb K_3} f \sqbrk K = \bigcap \mathbb K_2$ and so: :$\displaystyle f \sqbrk {\bigcap \mathbb K_1} \subseteq \bigcap \mathbb K_2$ which means that: :$f \sqbrk {H^-} \subseteq \paren {f \sqbrk H}^-$ as we wanted to show. {{qed|lemma}} === Sufficient Condition === Suppose $f$ is not [[Definition:Everywhere Continuous Mapping (Topology)|continuous]]. From [[Continuity Defined from Closed Sets]], $\exists B \subseteq X_2$ which is [[Definition:Closed Set (Topology)|closed]] in $T_2$ such that $f^{-1} \sqbrk B$ is not [[Definition:Closed Set (Topology)|closed]] in $T_1$. By [[Image of Preimage under Mapping]], we have that: :$f \sqbrk {f^{-1} \sqbrk B} \subseteq B$ So from [[Topological Closure of Subset is Subset of Topological Closure]]: :$\paren {f \sqbrk {f^{-1} \sqbrk B} }^- \subseteq B^-$ From [[Closed Set Equals its Closure]] we have that $B^- = B$. Transitively, we get: :$\paren {f \sqbrk {f^{-1} \sqbrk B} }^- \subseteq B$ Because $f^{-1} \sqbrk B$ is not [[Definition:Closed Set (Topology)|closed]] in $T_1$, we have that: :$f^{-1} \sqbrk B \subsetneq \paren {f^{-1} \sqbrk B}^-$ This means there exists an element $x \in \paren {f^{-1} \sqbrk B}^-$ such that $x \notin f^{-1} \sqbrk B$. Therefore $\map f x \notin B$, but $\map f x \in f \sqbrk {\sqbrk {f^{-1} \sqbrk B}^-}$. From above, we had: :$\paren {f \sqbrk {f^{-1} \sqbrk B} }^- \subseteq B$ so there exists a set $A \subseteq X_1$, namely $A = f^{-1} \sqbrk B$, such that: :$f \sqbrk {A^-} \nsubseteq \paren {f \sqbrk A}^-$ {{qed}}	0
Let $X$ and $Y$ be [[Definition:Discrete Random Variable|discrete random variables]] on the [[Definition:Probability Space|probability space]] $\left({\Omega, \Sigma, \Pr}\right)$.	0
This is an instance of [[Jung's Theorem]], setting $n = 2$. {{qed}} {{Namedfor|Heinrich Wilhelm Ewald Jung}}	0
Let $s_i$ be the $i$th element of set $S$. Begin by defining set $A_m$, which is all of the [[Definition:Permutation|permutations]] of $S$ which [[Definition:Fixed Element of Permutation|fixes]] $S_m$. Then the number of [[Definition:Permutation|permutations]], $W$, with ''at least'' one [[Definition:Element|element]] [[Definition:Fixed Element of Permutation|fixed]], $m$, is: :$\displaystyle W = \size {\bigcup_{m \mathop = 1}^n A_m}$ Applying the [[Inclusion-Exclusion Principle]]: {{begin-eqn}} {{eqn | l = W | r = \sum_{m_1 \mathop = 1}^n \size {A_{m_1} } | c = }} {{eqn | o = | ro= - | r = \sum_{m_1, m_2 : 1 \mathop \le m_1 \mathop < m_2 \mathop \le n} \size {A_{m_1} \cap A_{m_2} } | c = }} {{eqn | o = | ro= + | r = \sum_{m_1, m_2, m_3 : 1 \mathop \le m_1 \mathop < m_2 \mathop < m_3 \mathop \le n} \size {A_{m_1} \cap A_{m_2} \cap A_{m_3} } | c = }} {{eqn | o = | ro= - | r = \cdots | c = }} {{end-eqn}} Each value $A_{m_1} \cap \cdots \cap A_{m_p}$ represents the set of [[Definition:Permutation|permutations]] which [[Definition:Fixed Element of Permutation|fix]] $p$ values $m_1, \ldots, m_p$. Note that the number of [[Definition:Permutation|permutations]] which [[Definition:Fixed Element of Permutation|fix]] $p$ values only depends on $p$, not on the particular values of $m$. Thus from [[Cardinality of Set of Subsets]] there are $\dbinom n p$ terms in each summation. So: {{begin-eqn}} {{eqn | l = W | r = \binom n 1 \size {A_1} | c = }} {{eqn | o = | ro= - | r = \binom n 2 \size {A_1 \cap A_2} | c = }} {{eqn | o = | ro= + | r = \binom n 3 \size {A_1 \cap A_2 \cap A_3} | c = }} {{eqn | o = | ro= - | r = \cdots | c = }} {{eqn | o = | ro= + | r = \paren {-1}^{p - 1} \binom n p \size {A_1 \cap \cdots \cap A_p} | c = }} {{eqn | o = | r = \cdots | c = }} {{end-eqn}} $\size {A_1 \cap \cdots \cap A_p}$ is the number of [[Definition:Permutation|permutations]] fixing $p$ [[Definition:Element|elements]] in position. This is equal to the number of [[Definition:Permutation|permutations]] which rearrange the remaining $n - p$ [[Definition:Element|elements]], which is $\paren {n - p}!$. Thus we finally get: :$W = \dbinom n 1 \paren {n - 1}! - \dbinom n 2 \paren {n - 2}! + \dbinom n 3 \paren {n - 3}! - \cdots + \paren {-1}^{p - 1} \dbinom n p \paren {n - p}! \cdots$ That is: :$\displaystyle W = \sum_{p \mathop = 1}^n \paren {-1}^{p - 1} \binom n p \paren {n - p}!$ Noting that $\dbinom n p = \dfrac {n!} {p! \paren {n - p}!}$, this reduces to: :$\displaystyle W = \sum_{p \mathop = 1}^n \paren {-1}^{p - 1} \dfrac {n!} {p!}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \map {\laptrans {\cos {a t} } } s | r = \int_0^{\to +\infty} e^{-s t} \cos {a t} \rd t | c = {{Defof|Laplace Transform}} }} {{eqn | r = \lim_{L \mathop \to \infty} \int_0^L e^{-s t} \cos {a t} \rd t | c = {{Defof|Improper Integral}} }} {{eqn | r = \lim_{L \mathop \to \infty} \intlimits {\frac {e^{-s t} \paren {-s \cos a t + a \sin a t} } {\paren {-s}^2 + a^2} } 0 L | c = [[Primitive of Exponential of a x by Cosine of b x|Primitive of $e^{a x} \cos b x$]] }} {{eqn | r = \lim_{L \mathop \to \infty} \paren {\frac {e^{-s L} \paren {-s \cos a L + a \sin a L} } {s^2 + a^2} - \frac {e^{-s \times 0} \paren {-s \, \map \cos {0 \times a} + a \, \map \sin {0 \times a} } } {s^2 + a^2} } | c = }} {{eqn | r = \lim_{L \mathop \to \infty} \paren {\frac {s \, \map \cos {0 \times a} - a \, \map \sin {0 \times a} } {s^2 + a^2} - \frac {e^{-s L} \paren {-s \cos a L + a \sin a L} } {s^2 + a^2} } | c = }} {{eqn | r = \frac {s \, \map \cos {0 \times a} - a \, \map \sin {0 \times a} } {s^2 + a^2} - 0 | c = [[Exponential Tends to Zero and Infinity|Exponential Tends to Zero]] }} {{eqn | r = \frac {s \cos 0 - a \sin 0} {s^2 + a^2} | c = simplifying }} {{eqn | r = \frac s {s^2 + a^2} | c = [[Sine of Zero is Zero]], [[Cosine of Zero is One]] }} {{end-eqn}} {{qed}}	0
$31$ and $8191$ can be expressed as the [[Definition:Integer Addition|sum]] of successive [[Definition:Integer Power|powers]] starting from $1$ in in $2$ different ways.	0
There exists at least one example of a [[Definition:Semiregular Space|semiregular topological space]] which is not a [[Definition:T3 Space|$T_3$ space]].	0
[[Definition:Isomorphism (Graph Theory)|Graph isomorphism]] is an [[Definition:Equivalence Relation|equivalence relation]].	0
From [[Point in Discrete Space is Neighborhood]], every point $x \in S$ is contained in an [[Definition:Open Set (Topology)|open set]] $\set x$ of $T$. Then from [[Interior Equals Closure of Subset of Discrete Space]] we have that $\set x$ equals its [[Definition:Closure (Topology)|closure]] in $T$. From [[Singleton Set in Discrete Space is Compact]], we have that $\set x$ is [[Definition:Compact Topological Subspace|compact]] in $T$. Hence the result by definition of [[Definition:Strongly Locally Compact Space|strongly locally compact]]. {{qed}}	0
Let $D \subset \C$ be [[Definition:Open Set (Complex Analysis)|open]]. Let $\left\langle{f_n}\right\rangle$ be a [[Definition:Sequence|sequence]] of [[Definition:Analytic Function|analytic functions]] $f_n: D \to \C$. Let the [[Definition:Infinite Product|product]] $\displaystyle \prod_{n \mathop = 1}^\infty f_n$ [[Definition:Locally Uniform Convergence of Product|converge locally uniformly]] to $f$. Then: :$\displaystyle f' = \sum_{n \mathop = 1}^\infty f_n'\cdot \prod_{\substack{k \mathop = 1 \\ k\mathop \ne n} }^\infty f_k$ and the [[Definition:Series|series]] [[Definition:Locally Uniform Convergence|converges locally uniformly]] in $D$.	0
Let $T = \struct {\R, \tau}$ be the [[Definition:Compact Complement Topology|compact complement topology]] on $\R$. Then $T$ is an [[Definition:Irreducible Space|irreducible space]].	0
A [[Definition:Finite Graph|finite]] [[Definition:Loop-Multigraph|loop-multigraph]] is [[Definition:Traversable Graph|traversable]] {{iff}} it is [[Definition:Connected Graph|connected]] and no more than two [[Definition:Vertex of Graph|vertices]] are [[Definition:Odd Vertex of Graph|odd]]. Any [[Definition:Eulerian Trail|Eulerian trail]] which is not an [[Definition:Eulerian Circuit|Eulerian circuit]] must start and end at an [[Definition:Odd Vertex of Graph|odd vertex]].	0
Let $\mathscr B \left({\left[{a \,.\,.\, b}\right], \R}\right)$ be the [[Definition:Set|set]] of all [[Definition:Bounded Real-Valued Function|bounded]] [[Definition:Real Function|real functions]] $f: \left[{a \,.\,.\, b}\right] \to \R$. From [[Supremum Metric on Continuous Real Functions is Subspace of Bounded]], $\left({\mathscr C \left[{a \,.\,.\, b}\right], d_{\mathscr C} }\right)$ is a [[Definition:Metric Subspace|(metric) subspace]] of $\left({\mathscr B \left({\left[{a \,.\,.\, b}\right], \R}\right), d}\right)$. The result follows from [[Subspace of Metric Space is Metric Space]]. {{qed}}	0
Let $G$ be the [[Definition:Graph of Mapping|graph]] of the [[Definition:Real Function|function]] $y = \sin \left({\dfrac 1 x}\right)$ for $x > 0$. Let $J$ be the [[Definition:Line Segment|line segment]] joining the points $\left({0, -1}\right)$ and $\left({0, 1}\right)$ in $\R^2$. Then while $G \cup J$ is [[Definition:Connected Set (Topology)|connected]], it is '''not''' [[Definition:Path-Connected Set (Topology)|path-connected]].	0
Let $\left\langle{x_n}\right\rangle_{n \mathop \in \N}$ be a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] in $S$. For each $n$ pick a [[Definition:Cauchy Sequence (Metric Space)|Cauchy sequence]] $\left\langle{y_{n, m}}\right\rangle_{m \mathop \in \N}$ in $A$ converging to $x_n$ like so: :[[File:CompletenessCriterionProof.png|400px|Caption]] Let $N \in \N$ be such that $d \left({x_{n_1}, x_{n_2}}\right) < \epsilon / 3$ for all $n_1, n_2 > N$. Let $M \in \N$ be such that $d \left({y_{n_i, m}, x_{n_i}}\right) < \epsilon / 3$ for all $m > M$ and all $n_1, n_2 > N$. Let $m > M$. Let $n_1, n_2 > N$. We have: {{begin-eqn}} {{eqn | l = d \left({y_{n_1, m}, y_{n_2, m} }\right) | o = \le | r = d \left({y_{n_1, m}, x_{n_1} }\right) + d \left({x_{n_1}, x_{n_2} }\right) + d \left({x_{n_2}, y_{n_2, m} }\right) | c = two applications of the [[Triangle Inequality]] }} {{eqn | o = < | r = \epsilon }} {{end-eqn}} Therefore $\left\langle{y_{m, n}}\right\rangle_{n \in \N}$ is [[Definition:Cauchy Sequence (Metric Space)|Cauchy]] in $A$ for $m > M$. So $\left\langle{y_{m, n}}\right\rangle_{n \mathop \in \N}$ [[Definition:Convergent Sequence (Metric Space)|converges]] to some [[Definition:Limit of Sequence (Metric Space)|limit]] $y_n \in S$. {{finish|Proof to complete}}	0
{{proof wanted}} {{Namedfor|Friedrich Wilhelm Bessel|cat=Bessel}}	0
Let $T = \struct {S, \tau_{a, b} }$ be a [[Definition:Modified Fort Space|modified Fort space]]. Then $T$ is not [[Definition:Zero Dimensional Space|zero dimensional]].	0
By definition of [[Definition:Lipschitz Equivalent Metrics|Lipschitz equivalence]]: : $\forall x, y \in A: h \map {d_2} {x, y} \le \map {d_1} {x, y} \le k \map {d_2} {x, y}$ for some $h, k \in \R_{>0}$. Let $x \in A$. Let $\epsilon \in \R_{>0}$. Let $\map {B_{h \epsilon} } {x; d_1}$ denote the [[Definition:Open Ball|open $h \epsilon$-ball]] with respect to $d_1$ of $x \in A$. Then: {{begin-eqn}} {{eqn | l = y | o = \in | r = \map {B_{h \epsilon} } {x; d_1} | c = }} {{eqn | ll= \leadsto | l = \map {d_1} {x, y} | o = < | r = h \epsilon | c = }} {{eqn | ll= \leadsto | l = \map {d_2} {x, y} | o = \le | r = \frac {\map {d_1} {x, y} } h | c = }} {{eqn | o = < | r = \epsilon | c = }} {{eqn | ll= \leadsto | l = y | o = \in | r = \map {B_\epsilon} {x; d_2} | c = }} {{eqn | ll= \leadsto | l = \map {B_{h \epsilon} } {x; d_1} | o = \subseteq | r = \map {B_\epsilon} {x; d_2} | c = }} {{end-eqn}} Similarly: {{begin-eqn}} {{eqn | l = y | o = \in | r = \map {B_{\epsilon / k} } {x; d_2} | c = }} {{eqn | ll= \leadsto | l = \map {d_2} {x, y} | o = < | r = \frac \epsilon k | c = }} {{eqn | ll= \leadsto | l = \map {d_1} {x, y} | o = \le | r = k \frac {\map {d_2} {x, y} } h | c = }} {{eqn | o = < | r = \epsilon | c = }} {{eqn | ll= \leadsto | l = y | o = \in | r = \map {B_\epsilon} {x; d_1} | c = }} {{eqn | ll= \leadsto | l = \map {B_{\epsilon / k} } {x; d_2} | o = \subseteq | r = \map {B_\epsilon} {x; d_1} | c = }} {{end-eqn}} Now suppose $U \subseteq A$ is [[Definition:Open Set (Metric Space)|$d_1$-open]]. Let $x \in U$. Then: :$\exists \epsilon \in \R_{>0}: \map {B_\epsilon} {x; d_1} \subseteq U$ Thus: :$\map {B_{\epsilon / k} } {x; d_2} \subseteq \map {B_\epsilon} {x; d_1} \subseteq U$ and so $U$ is [[Definition:Open Set (Metric Space)|$d_2$-open]]. [[Definition:Mutatis Mutandis|Mutatis mutandis]], if $U \subseteq A$ is [[Definition:Open Set (Metric Space)|$d_2$-open]], it follows that $U$ is [[Definition:Open Set (Metric Space)|$d_1$-open]]. {{qed}}	0
Let $\mathbb S = \family {\struct {S_\alpha, \tau_\alpha} }_{\alpha \mathop \in I}$ be an [[Definition:Indexed Family|indexed family]] of [[Definition:Non-Empty Set|non-empty]] [[Definition:Topological Space|topological spaces]] for $\alpha$ in some [[Definition:Indexing Set|indexing set]] $I$. Let $\displaystyle T = \struct {S, \tau} = \displaystyle \prod_{\alpha \mathop \in I} \struct {S_\alpha, \tau_\alpha}$ be the [[Definition:Product Space of Topological Spaces|product space]] of $\mathbb S$. Let $T$ be a [[Definition:Completely Hausdorff Space|completely Hausdorff space]]. Then for each $\alpha \in I$, $\struct {S_\alpha, \tau_\alpha}$ is a [[Definition:Completely Hausdorff Space|completely Hausdorff space]].	0
We show that the [[Definition:Set Complement|complement]] $A \setminus B_\epsilon^- \left({x}\right)$ is [[Definition:Open Set (Metric Space)|open]] in $M$. Let $a \in A \setminus \map {B_\epsilon^-} x$. Then by definition of [[Definition:Closed Ball|closed ball]]: :$\map d {x, a} > \epsilon$ Put: :$\delta := \map d {x, a} - \epsilon > 0$ Then: :$\map d {x, a} - \delta = \epsilon$ Let $b \in \map {B_\delta} a$. Then: {{begin-eqn}} {{eqn | l = \map d {x, b} | o = \ge | r = \map d {x, a} - \map d {a, b} | c = [[Reverse Triangle Inequality]] }} {{eqn | o = > | r = \map d {x, a} - \delta }} {{eqn | r = \epsilon }} {{end-eqn}} and so: :$b \notin \map {B_\epsilon^-} x$ Then: :$\map {B_\delta} a \subseteq A \setminus \map {B_\epsilon^-} x$ so $A \setminus \map {B_\epsilon^-} x$ is [[Definition:Open Set (Metric Space)|open]] in $M$. Hence, by definition of [[Definition:Closed Set (Metric Space)|closed set]]: :$\map {B_\epsilon^-} x$ is [[Definition:Closed Set (Metric Space)|closed]] in $M$. {{qed}} [[Category:Metric Spaces]] [[Category:Closed Balls]] i0t1kn6zyg0qs0ijte71kxgpqtx78ez	0
Let $\left \langle {x_n} \right \rangle$ be a [[Definition:Real Sequence|sequence in $\R$]]. Let $l \in A$ such that $\displaystyle \lim_{n \mathop \to \infty} x_n = l$. Then $\left \langle {x_n} \right \rangle$ is [[Definition:Bounded Real Sequence|bounded]].	0
Let $S$ be a [[Definition:Subset|subset]] of the set of [[Definition:Real Number|real numbers]] $\R$. Let $x \in \R$ be a [[Definition:Real Number|real number]]. Let $\map d {x, S}$ be the [[Definition:Distance between Element and Subset of Real Numbers|distance]] between $x$ and $S$. Then:	0
Let the required [[Definition:Length of Sequence|length]] of the [[Definition:Geometric Sequence|geometric sequence]] $P$ be $n$. Let $r$ be the given [[Definition:Common Ratio of Geometric Sequence|common ratio]]. From [[Common Ratio in Integer Geometric Sequence is Rational]], $r$ is a [[Definition:Rational Number|rational number]]. Let $r = \dfrac p q$ be in [[Definition:Canonical Form of Rational Number|canonical form]]. Thus, by definition: :$p \perp q$ Let $a$ be the first [[Definition:Term of Geometric Sequence|term]] of $P$. Then the [[Definition:Sequence|sequence]] $P$ is: :$P = \tuple {a, a \dfrac p q, a \dfrac {p^2} {q^2}, \ldots, a \dfrac {p^n} {q^n} }$ All the elements of $P$ are [[Definition:Natural Number|natural numbers]], so, in particular: :$a \dfrac {p^n} {q^n} \in \N$ From [[Powers of Coprime Numbers are Coprime]]: :$p^n \perp q^n$ and so from [[Euclid's Lemma]]: :$q^n \divides a$ Thus: :$a = k q^n$ for some $k \in \N$, and so: :$P = \tuple {k q^n, k p q^{n - 1}, k p^2 q^{n - 2}, \ldots, k p^{n - 1} q, k p^n}$ From [[Geometric Sequence with Coprime Extremes is in Lowest Terms]]: :$k q^n \perp k p^n$ from which it follows that $k = 1$. It follows that the required [[Definition:Geometric Sequence|geometric sequence]] is: :$P = \tuple {q^n, p q^{n - 1}, p^2 q^{n - 2}, \ldots, p^{n - 1} q, p^n}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = \int \sin a x \cos a x \rd x | r = \int \frac {\sin 2 a x} 2 \rd x | c = [[Double Angle Formula for Sine]] }} {{eqn | r = \frac {-\cos 2 a x} {4 a} + C | c = [[Primitive of Cosine of a x|Primitive of $\cos a x$]] }} {{eqn | r = \frac {-\paren {1 - 2 \sin^2 a x} } {4 a} + C | c = [[Double Angle Formulas/Cosine/Corollary 2|Double Angle Formula for Cosine: Corollary 2]] }} {{eqn | r = \frac {-1} {4 a} + \frac {\sin^2 a x} {2 a} + C | c = separating fraction }} {{eqn | r = \frac {\sin^2 a x} {2 a} + C | c = subsuming $\dfrac {-1} {4 a}$ into [[Definition:Arbitrary Constant (Calculus)|arbitrary constant]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \map \sinh {i z} | r = \frac {e^{i z} - e^{-i z} } 2 | c = {{Defof|Hyperbolic Sine}} }} {{eqn | r = i \frac {e^{i z} - e^{-i z} } {2 i} | c = multiplying [[Definition:Numerator|top]] and [[Definition:Denominator|bottom]] by $i$ }} {{eqn | r = i \sin z | c = [[Sine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = \cot 75 \degrees | r = \frac {\cos 75 \degrees} {\sin 75 \degrees} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac {\frac {\sqrt 6 - \sqrt 2} 4} {\frac {\sqrt 6 + \sqrt 2} 4} | c = [[Cosine of 75 Degrees|Cosine of $75 \degrees$]] and [[Sine of 75 Degrees|Sine of $75 \degrees$]] }} {{eqn | r = \frac {\sqrt 6 - \sqrt 2} {\sqrt 6 + \sqrt 2} | c = simplifying }} {{eqn | r = \frac {\paren {\sqrt 6 - \sqrt 2}^2} {\paren {\sqrt 6 + \sqrt 2} \paren {\sqrt 6 - \sqrt 2} } | c = multiplying top and bottom by $\sqrt 6 - \sqrt 2$ }} {{eqn | r = \frac {6 - 2 \sqrt 6 \sqrt 2 + 2} {6 - 2} | c = multiplying out, and [[Difference of Two Squares]] }} {{eqn | r = \frac {8 - 4 \sqrt 3} 4 | c = simplifying }} {{eqn | r = 2 - \sqrt 3 | c = dividing top and bottom by $4$ }} {{end-eqn}} {{qed}}	0
Assume $y \in \R$, $ -\dfrac \pi 2 \le y \le \dfrac \pi 2 $. {{begin-eqn}} {{eqn | l = y | r = \arctan x }} {{eqn | ll= \leadstoandfrom | l = x | r = \tan y }} {{eqn | ll= \leadstoandfrom | l = x | r = i \frac {1 - e^{2 i y} } {1 + e^{2 i y} } | c = [[Tangent Exponential Formulation]] }} {{eqn | ll= \leadstoandfrom | l = i x | r = \frac {e^{2 i y} - 1} {e^{2 i y} + 1} | c = $ i^2 = -1 $ }} {{eqn | ll= \leadstoandfrom | l = i x \paren {e^{2 i y} + 1} | r = e^{2 i y} - 1 }} {{eqn | ll= \leadstoandfrom | l = i x e^{2 i y} + i x | r = e^{2 i y} - 1 }} {{eqn | ll= \leadstoandfrom | l = e^{2 i y} - i x e^{2 i y} | r = 1 + i x }} {{eqn | ll= \leadstoandfrom | l = e^{2 i y} | r = \frac {1 + i x} {1 - i x} }} {{eqn | ll= \leadstoandfrom | l = e^{-2 i y} | r = \frac {1 - i x} {1 + i x} }} {{eqn | ll= \leadstoandfrom | l = -2 i y | r = \map \ln {\frac {1 - i x} {1 + i x} } }} {{eqn | ll= \leadstoandfrom | l = y | r = \frac 1 2 i \map \ln {\frac {1 - i x} {1 + i x} } }} {{end-eqn}} {{qed}}	0
Let $x \in \R$ be a [[Definition:Real Number|real number]]. Then: : $\arctan x + \operatorname{arccot} x = \dfrac \pi 2$ where $\arctan$ and $\operatorname{arccot}$ denote [[Definition:Arctangent|arctangent]] and [[Definition:Arccotangent|arccotangent]] respectively.	0
We have: {{begin-eqn}} {{eqn | l = 2 \sin \theta | r = \frac {e^{i \theta} - e^{-i \theta} } i | c = [[Sine Exponential Formulation]] }} {{eqn | r = \frac {e^{i \theta} e^{-i \theta} - e^{-i \theta} e^{-i \theta} } {i e^{-i \theta} } | c = }} {{eqn | r = \left({1 - e^{-2 i \theta} }\right) \left({-i e^{i \theta} }\right) | c = }} {{eqn | r = \left({1 - e^{-2 i \theta} }\right) \left({e^{i \theta} e^{-i \pi / 2} }\right) | c = [[Euler's Formula/Examples/e^-i pi by 2]] }} {{eqn | r = \left({1 - e^{-2 i \theta} }\right) \left({e^{i \theta - i \pi / 2} }\right) | c = }} {{eqn | ll= \leadsto | l = \prod_{k \mathop = 0}^{n - 1} \left({2 \sin \pi \left({x + \frac k n}\right)}\right) | r = \prod_{k \mathop = 0}^{n - 1} \left({1 - e^{-2 i \pi \left({x + k / n}\right)} }\right) \left({e^{i \pi \left({x - \left({1 / 2}\right) + \left({k / n}\right)}\right)} }\right) | c = }} {{eqn | r = \left({1 - e^{-2 i \pi n x} }\right) \left({e^{i \pi \left({n x - 1 / 2}\right)} }\right) | c = }} {{eqn | r = 2 \sin \pi n x | c = }} {{end-eqn}} {{finish|Work to be done to establish the identities used here}} [[Category:Sine Function]] q4axqqt74dpubye91mw95qozyaxc7st	0
{{begin-eqn}} {{eqn | l = \map \cos {2 \pi - \theta} | r = \map \cos {2 \pi} \cos \theta + \map \sin {2 \pi} \sin \theta | c = [[Cosine of Difference]] }} {{eqn | r = 1 \times \cos \theta + 0 \times \sin \theta | c = [[Cosine of Full Angle]] and [[Sine of Full Angle]] }} {{eqn | r = \cos \theta }} {{end-eqn}} {{qed}}	0
:$\displaystyle \int \tan x \rd x = -\ln \size {\cos x} + C$ where $\cos x \ne 0$.	0
{{begin-eqn}} {{eqn | l = \sin 2\theta | r = \frac{1}{2i} \left(e^{2i\theta}-e^{-2i\theta}\right) | c = [[Sine Exponential Formulation]] }} {{eqn | r = \frac{1}{2i} \left(e^{i\theta}+e^{-i\theta}\right) \left(e^{i\theta}-e^{-i\theta}\right) | c = [[Difference of Two Squares]] }} {{eqn | r = 2 \left(\frac{e^{i\theta}-e^{-i\theta} }{2i} \cdot \frac{e^{i\theta} + e^{-i\theta} }{2}\right) }} {{eqn | r = 2 \sin \theta \cos \theta | c = [[Sine Exponential Formulation]], [[Cosine Exponential Formulation]] }} {{end-eqn}} {{qed}}	0
Let $\theta \ne \dfrac {m \pi} 2$ for any $m \in \Z$. Then $\size {\cos 2 \theta} < 1$. {{begin-eqn}} {{eqn | l = \sum_{k \mathop \ge 0} \sin \paren {2 k + 1} \theta r^k | r = \dfrac {\paren {1 + r} \sin \theta} {1 - 2 r \cos 2 \theta + r^2} | c = [[Power Series of Sine of Odd Theta]]: $\size r < 1$ }} {{eqn | ll= \leadsto | l = \sum_{k \mathop = 0}^\infty \paren {\cos 2 \theta}^k \sin \paren {2 k + 1} \theta | r = \dfrac {\paren {1 + \cos 2 \theta} \sin \theta} {1 - 2 \cos 2 \theta \cos 2 \theta + \paren {\cos 2 \theta}^2} | c = setting $r = \cos 2 \theta$ }} {{eqn | r = \dfrac {\paren {1 + \cos 2 \theta} \sin \theta} {1 - \paren {\cos 2 \theta}^2} | c = simplifying }} {{eqn | r = \dfrac {\paren {1 + 2 \cos^2 \theta - 1} \sin \theta} {1 - \paren {2 \cos^2 \theta - 1}^2} | c = [[Double Angle Formula for Cosine]] }} {{eqn | r = \dfrac {2 \cos^2 \theta \sin \theta} {1 - \paren {4 \cos^4 \theta - 4 \cos^2 \theta + 1} } | c = simplifying and expanding }} {{eqn | r = \dfrac {2 \cos^2 \theta \sin \theta} {4 \cos^2 \theta \paren {1 - \cos^2 \theta} } | c = simplifying }} {{eqn | r = \dfrac {2 \cos^2 \theta \sin \theta} {4 \cos^2 \theta \sin^2 \theta} | c = [[Sum of Squares of Sine and Cosine]] }} {{eqn | r = \dfrac 1 {2 \sin \theta} | c = simplifying }} {{eqn | r = \dfrac {\csc \theta} 2 | c = {{Defof|Real Cosecant Function}} }} {{end-eqn}} {{qed}}	0
Let $\map f x$ be an [[Definition:Odd Function|odd]] [[Definition:Real Function|real function]] defined on the [[Definition:Real Interval|interval]] $\openint {-\lambda} \lambda$. Let the [[Definition:Fourier Series|Fourier series]] of $\map f x$ be expressed as: :$\map f x \sim \dfrac {a_0} 2 + \displaystyle \sum_{n \mathop = 1}^\infty \paren {a_n \cos \frac {n \pi x} \lambda + b_n \sin \frac {n \pi x} \lambda}$ Then for all $n \in \Z_{\ge 0}$: :$a_n = 0$	0
From [[Cosecant is Reciprocal of Sine]]: : $\csc \theta = \dfrac 1 {\sin \theta}$ From [[Sine of Straight Angle]]: : $\sin \pi = 0$ Thus $\csc \theta$ is undefined at this value. {{qed}}	0
{{begin-eqn}} {{eqn | l = \tan \theta \cot \theta | r = \frac {\sin \theta} {\cos \theta} \cot \theta | c = [[Tangent is Sine divided by Cosine]] }} {{eqn | r = \frac {\sin \theta} {\cos \theta} \frac {\cos \theta} {\sin \theta} | c = [[Cotangent is Cosine divided by Sine]] }} {{eqn | r = \frac {\sin \theta} {\sin \theta} \frac {\cos \theta} {\cos \theta} | c = }} {{eqn | r = 1 | c = }} {{end-eqn}} {{qed}} [[Category:Tangent Function]] [[Category:Cotangent Function]] 46j184iwn1vblxlg7w7srw5q3g4jcc2	0
As [[Sine Function is Absolutely Convergent]] and [[Cosine Function is Absolutely Convergent]], we have: {{begin-eqn}} {{eqn | l = \cos z + i \sin z | r = \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {z^{2 n} } {\paren {2 n}!} + i \sum_{n \mathop = 0}^\infty \paren {-1}^n \dfrac {z^{2 n + 1} } {\paren {2 n + 1}!} | c = {{Defof|Complex Cosine Function}} and {{Defof|Complex Sine Function}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\paren {-1}^n \dfrac {z^{2 n} } {\paren {2 n}!} + i \paren {-1}^n \dfrac {z^{2 n + 1} } {\paren {2 n + 1}!} } | c = [[Sum of Absolutely Convergent Series]] }} {{eqn | r = \sum_{n \mathop = 0}^\infty \paren {\dfrac {\paren {i z}^{2 n} } {\paren {2 n}!} + \dfrac {\paren {i z}^{2 n + 1} } {\paren {2 n + 1}!} } | c = {{Defof|Imaginary Unit}} }} {{eqn | r = \sum_{n \mathop = 0}^\infty \dfrac {\paren {i z}^n} {n!} }} {{eqn | r = e^{i z} | c = {{Defof|Exponential Function/Complex|subdef = Sum of Series|Complex Exponential Function}} }} {{end-eqn}} {{qed}}	0
:$\displaystyle \pi \, \map \sec {\pi z} = 4 \sum_{n \mathop = 0}^\infty \paren {-1}^n \frac {2 n + 1} {\paren {2 n + 1}^2 - 4 z^2}$ where: :$z \in \C$ is not a [[Definition:Half-Integer|half-integer]] :$\sec$ is the [[Definition:Secant Function|secant function]].	0
By definition, the [[Definition:Absolute Value|absolute value]] function is an [[Definition:Even Function|even function]]: :$\size {-x} = x = \size x$ Thus by [[Fourier Series for Even Function over Symmetric Range]], $\size x$ can be expressed as: :$\displaystyle \size x \sim \frac {a_0} 2 + \sum_{n \mathop = 1}^\infty a_n \cos n x$ where for all $n \in \Z_{\ge 0}$: :$a_n = \displaystyle \frac 2 \pi \int_0^\pi \size x \cos n x \rd x$ On the [[Definition:Real Interval|real interval]] $\openint 0 \pi$: :$\size x = x$ and so for all $n \in \Z_{\ge 0}$: :$a_n = \displaystyle \frac 2 \pi \int_0^\pi x \cos n x \rd x$ Thus [[Half-Range Fourier Cosine Series/Identity Function/0 to Pi|Half-Range Fourier Cosine Series for Identity Function over $\openint 0 \pi$]] can be applied directly. So for $x \in \openint {-\pi} \pi$: :$\displaystyle \size x = \frac \pi 2 - \frac 4 \pi \sum_{n \mathop = 1}^\infty \frac {\map \cos {2 n - 1} x} {\paren {2 n - 1}^2}$ {{qed}}	0
{{begin-eqn}} {{eqn | l = y | r = \arccot x | c = }} {{eqn | ll= \leadsto | l = x | r = \cot y | c = {{Defof|Real Arccotangent}} }} {{eqn | ll= \leadsto | l = \frac {\d x} {\d y} | r = -\csc^2 y | c = [[Derivative of Cotangent Function]] }} {{eqn | r = -\paren {1 + \cot^2 y} | c = [[Difference of Squares of Cosecant and Cotangent]] }} {{eqn | r = -\paren {1 + x^2} | c = Definition of $x$ }} {{eqn | ll= \leadsto | l = \frac {\d y} {\d x} | r = \frac {-1} {1 + x^2} | c = [[Derivative of Inverse Function]] }} {{end-eqn}} {{qed}}	0
{{begin-eqn}} {{eqn | l = z | r = \cot a x | c = }} {{eqn | ll= \implies | l = \frac {\mathrm d z} {\mathrm d x} | r = -a \csc^2 a x | c = [[Derivative of Cotangent Function/Corollary|Derivative of Cotangent Function: Corollary]] }} {{eqn | ll= \implies | l = \int \cot^n a x \csc^2 a x \ \mathrm d x | r = \int \frac {-1} a z^n \ \mathrm d z | c = [[Integration by Substitution]] }} {{eqn | r = \frac {-1} a \frac {z^{n + 1} } {n + 1} | c = [[Primitive of Power]] }} {{eqn | r = \frac {-\cot^{n + 1} a x} {\left({n + 1}\right) a} + C | c = substituting for $z$ and simplifying }} {{end-eqn}} {{qed}}	0
